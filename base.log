nohup: ignoring input
Files already downloaded and verified
Epoch [1/300], Step [10/391],                 Loss: 4.19606, Train_Acc:14.14%
Epoch [1/300], Step [20/391],                 Loss: 3.45815, Train_Acc:13.05%
Epoch [1/300], Step [30/391],                 Loss: 3.07507, Train_Acc:14.51%
Epoch [1/300], Step [40/391],                 Loss: 2.84096, Train_Acc:15.68%
Epoch [1/300], Step [50/391],                 Loss: 2.68377, Train_Acc:17.12%
Epoch [1/300], Step [60/391],                 Loss: 2.57393, Train_Acc:17.96%
Epoch [1/300], Step [70/391],                 Loss: 2.49579, Train_Acc:18.59%
Epoch [1/300], Step [80/391],                 Loss: 2.43047, Train_Acc:19.40%
Epoch [1/300], Step [90/391],                 Loss: 2.37751, Train_Acc:20.21%
Epoch [1/300], Step [100/391],                 Loss: 2.33267, Train_Acc:20.85%
Epoch [1/300], Step [110/391],                 Loss: 2.29367, Train_Acc:21.48%
Epoch [1/300], Step [120/391],                 Loss: 2.25800, Train_Acc:22.36%
Epoch [1/300], Step [130/391],                 Loss: 2.23064, Train_Acc:22.82%
Epoch [1/300], Step [140/391],                 Loss: 2.20087, Train_Acc:23.53%
Epoch [1/300], Step [150/391],                 Loss: 2.17193, Train_Acc:24.31%
Epoch [1/300], Step [160/391],                 Loss: 2.14228, Train_Acc:25.13%
Epoch [1/300], Step [170/391],                 Loss: 2.11688, Train_Acc:25.77%
Epoch [1/300], Step [180/391],                 Loss: 2.09480, Train_Acc:26.33%
Epoch [1/300], Step [190/391],                 Loss: 2.07033, Train_Acc:26.88%
Epoch [1/300], Step [200/391],                 Loss: 2.05050, Train_Acc:27.43%
Epoch [1/300], Step [210/391],                 Loss: 2.03139, Train_Acc:27.92%
Epoch [1/300], Step [220/391],                 Loss: 2.01621, Train_Acc:28.30%
Epoch [1/300], Step [230/391],                 Loss: 1.99762, Train_Acc:28.80%
Epoch [1/300], Step [240/391],                 Loss: 1.97952, Train_Acc:29.26%
Epoch [1/300], Step [250/391],                 Loss: 1.96430, Train_Acc:29.69%
Epoch [1/300], Step [260/391],                 Loss: 1.95190, Train_Acc:30.01%
Epoch [1/300], Step [270/391],                 Loss: 1.94002, Train_Acc:30.35%
Epoch [1/300], Step [280/391],                 Loss: 1.92595, Train_Acc:30.79%
Epoch [1/300], Step [290/391],                 Loss: 1.91237, Train_Acc:31.27%
Epoch [1/300], Step [300/391],                 Loss: 1.90176, Train_Acc:31.55%
Epoch [1/300], Step [310/391],                 Loss: 1.88897, Train_Acc:32.00%
Epoch [1/300], Step [320/391],                 Loss: 1.87669, Train_Acc:32.35%
Epoch [1/300], Step [330/391],                 Loss: 1.86432, Train_Acc:32.74%
Epoch [1/300], Step [340/391],                 Loss: 1.85220, Train_Acc:33.17%
Epoch [1/300], Step [350/391],                 Loss: 1.84254, Train_Acc:33.43%
Epoch [1/300], Step [360/391],                 Loss: 1.83018, Train_Acc:33.84%
Epoch [1/300], Step [370/391],                 Loss: 1.81865, Train_Acc:34.18%
Epoch [1/300], Step [380/391],                 Loss: 1.81002, Train_Acc:34.43%
Epoch [1/300], Step [390/391],                 Loss: 1.79960, Train_Acc:34.78%
Accuary on test images:37.72%
Epoch [2/300], Step [10/391],                 Loss: 1.36395, Train_Acc:48.67%
Epoch [2/300], Step [20/391],                 Loss: 1.36256, Train_Acc:49.26%
Epoch [2/300], Step [30/391],                 Loss: 1.35995, Train_Acc:49.92%
Epoch [2/300], Step [40/391],                 Loss: 1.36720, Train_Acc:49.28%
Epoch [2/300], Step [50/391],                 Loss: 1.36346, Train_Acc:49.25%
Epoch [2/300], Step [60/391],                 Loss: 1.36469, Train_Acc:49.56%
Epoch [2/300], Step [70/391],                 Loss: 1.36037, Train_Acc:49.82%
Epoch [2/300], Step [80/391],                 Loss: 1.35833, Train_Acc:50.03%
Epoch [2/300], Step [90/391],                 Loss: 1.35665, Train_Acc:50.03%
Epoch [2/300], Step [100/391],                 Loss: 1.35113, Train_Acc:50.25%
Epoch [2/300], Step [110/391],                 Loss: 1.34799, Train_Acc:50.43%
Epoch [2/300], Step [120/391],                 Loss: 1.34356, Train_Acc:50.59%
Epoch [2/300], Step [130/391],                 Loss: 1.34604, Train_Acc:50.42%
Epoch [2/300], Step [140/391],                 Loss: 1.33854, Train_Acc:50.76%
Epoch [2/300], Step [150/391],                 Loss: 1.33266, Train_Acc:51.09%
Epoch [2/300], Step [160/391],                 Loss: 1.32928, Train_Acc:51.22%
Epoch [2/300], Step [170/391],                 Loss: 1.32264, Train_Acc:51.40%
Epoch [2/300], Step [180/391],                 Loss: 1.31879, Train_Acc:51.61%
Epoch [2/300], Step [190/391],                 Loss: 1.31094, Train_Acc:51.92%
Epoch [2/300], Step [200/391],                 Loss: 1.30388, Train_Acc:52.26%
Epoch [2/300], Step [210/391],                 Loss: 1.29551, Train_Acc:52.57%
Epoch [2/300], Step [220/391],                 Loss: 1.29378, Train_Acc:52.67%
Epoch [2/300], Step [230/391],                 Loss: 1.28752, Train_Acc:52.82%
Epoch [2/300], Step [240/391],                 Loss: 1.28136, Train_Acc:52.99%
Epoch [2/300], Step [250/391],                 Loss: 1.27663, Train_Acc:53.18%
Epoch [2/300], Step [260/391],                 Loss: 1.27385, Train_Acc:53.27%
Epoch [2/300], Step [270/391],                 Loss: 1.27005, Train_Acc:53.37%
Epoch [2/300], Step [280/391],                 Loss: 1.26519, Train_Acc:53.58%
Epoch [2/300], Step [290/391],                 Loss: 1.26038, Train_Acc:53.79%
Epoch [2/300], Step [300/391],                 Loss: 1.25840, Train_Acc:53.82%
Epoch [2/300], Step [310/391],                 Loss: 1.25252, Train_Acc:54.07%
Epoch [2/300], Step [320/391],                 Loss: 1.24911, Train_Acc:54.24%
Epoch [2/300], Step [330/391],                 Loss: 1.24477, Train_Acc:54.42%
Epoch [2/300], Step [340/391],                 Loss: 1.23939, Train_Acc:54.60%
Epoch [2/300], Step [350/391],                 Loss: 1.23617, Train_Acc:54.79%
Epoch [2/300], Step [360/391],                 Loss: 1.23049, Train_Acc:55.03%
Epoch [2/300], Step [370/391],                 Loss: 1.22567, Train_Acc:55.25%
Epoch [2/300], Step [380/391],                 Loss: 1.22298, Train_Acc:55.41%
Epoch [2/300], Step [390/391],                 Loss: 1.21770, Train_Acc:55.60%
Accuary on test images:58.18%
Epoch [3/300], Step [10/391],                 Loss: 0.97245, Train_Acc:64.77%
Epoch [3/300], Step [20/391],                 Loss: 0.99252, Train_Acc:64.77%
Epoch [3/300], Step [30/391],                 Loss: 0.98066, Train_Acc:65.47%
Epoch [3/300], Step [40/391],                 Loss: 0.98588, Train_Acc:65.20%
Epoch [3/300], Step [50/391],                 Loss: 0.98466, Train_Acc:65.09%
Epoch [3/300], Step [60/391],                 Loss: 0.99060, Train_Acc:64.71%
Epoch [3/300], Step [70/391],                 Loss: 0.99328, Train_Acc:64.55%
Epoch [3/300], Step [80/391],                 Loss: 0.99018, Train_Acc:64.60%
Epoch [3/300], Step [90/391],                 Loss: 0.99288, Train_Acc:64.34%
Epoch [3/300], Step [100/391],                 Loss: 0.99294, Train_Acc:64.34%
Epoch [3/300], Step [110/391],                 Loss: 0.99082, Train_Acc:64.47%
Epoch [3/300], Step [120/391],                 Loss: 0.98929, Train_Acc:64.47%
Epoch [3/300], Step [130/391],                 Loss: 0.99016, Train_Acc:64.55%
Epoch [3/300], Step [140/391],                 Loss: 0.98442, Train_Acc:64.73%
Epoch [3/300], Step [150/391],                 Loss: 0.98365, Train_Acc:64.76%
Epoch [3/300], Step [160/391],                 Loss: 0.98525, Train_Acc:64.67%
Epoch [3/300], Step [170/391],                 Loss: 0.98238, Train_Acc:64.81%
Epoch [3/300], Step [180/391],                 Loss: 0.98173, Train_Acc:64.89%
Epoch [3/300], Step [190/391],                 Loss: 0.97810, Train_Acc:65.03%
Epoch [3/300], Step [200/391],                 Loss: 0.97639, Train_Acc:65.06%
Epoch [3/300], Step [210/391],                 Loss: 0.97278, Train_Acc:65.25%
Epoch [3/300], Step [220/391],                 Loss: 0.97294, Train_Acc:65.27%
Epoch [3/300], Step [230/391],                 Loss: 0.97155, Train_Acc:65.33%
Epoch [3/300], Step [240/391],                 Loss: 0.96630, Train_Acc:65.51%
Epoch [3/300], Step [250/391],                 Loss: 0.96462, Train_Acc:65.58%
Epoch [3/300], Step [260/391],                 Loss: 0.96569, Train_Acc:65.56%
Epoch [3/300], Step [270/391],                 Loss: 0.96507, Train_Acc:65.56%
Epoch [3/300], Step [280/391],                 Loss: 0.96321, Train_Acc:65.63%
Epoch [3/300], Step [290/391],                 Loss: 0.95976, Train_Acc:65.75%
Epoch [3/300], Step [300/391],                 Loss: 0.95875, Train_Acc:65.80%
Epoch [3/300], Step [310/391],                 Loss: 0.95407, Train_Acc:66.01%
Epoch [3/300], Step [320/391],                 Loss: 0.95281, Train_Acc:66.11%
Epoch [3/300], Step [330/391],                 Loss: 0.95069, Train_Acc:66.21%
Epoch [3/300], Step [340/391],                 Loss: 0.94727, Train_Acc:66.30%
Epoch [3/300], Step [350/391],                 Loss: 0.94475, Train_Acc:66.38%
Epoch [3/300], Step [360/391],                 Loss: 0.94200, Train_Acc:66.48%
Epoch [3/300], Step [370/391],                 Loss: 0.94057, Train_Acc:66.55%
Epoch [3/300], Step [380/391],                 Loss: 0.93842, Train_Acc:66.65%
Epoch [3/300], Step [390/391],                 Loss: 0.93487, Train_Acc:66.80%
Accuary on test images:63.76%
Epoch [4/300], Step [10/391],                 Loss: 0.84732, Train_Acc:70.08%
Epoch [4/300], Step [20/391],                 Loss: 0.82134, Train_Acc:71.80%
Epoch [4/300], Step [30/391],                 Loss: 0.80790, Train_Acc:71.95%
Epoch [4/300], Step [40/391],                 Loss: 0.80873, Train_Acc:71.88%
Epoch [4/300], Step [50/391],                 Loss: 0.80615, Train_Acc:71.66%
Epoch [4/300], Step [60/391],                 Loss: 0.81006, Train_Acc:71.17%
Epoch [4/300], Step [70/391],                 Loss: 0.81147, Train_Acc:71.05%
Epoch [4/300], Step [80/391],                 Loss: 0.80905, Train_Acc:71.15%
Epoch [4/300], Step [90/391],                 Loss: 0.81176, Train_Acc:71.09%
Epoch [4/300], Step [100/391],                 Loss: 0.81284, Train_Acc:71.11%
Epoch [4/300], Step [110/391],                 Loss: 0.81306, Train_Acc:71.18%
Epoch [4/300], Step [120/391],                 Loss: 0.81113, Train_Acc:71.22%
Epoch [4/300], Step [130/391],                 Loss: 0.80928, Train_Acc:71.20%
Epoch [4/300], Step [140/391],                 Loss: 0.80363, Train_Acc:71.43%
Epoch [4/300], Step [150/391],                 Loss: 0.80106, Train_Acc:71.61%
Epoch [4/300], Step [160/391],                 Loss: 0.80186, Train_Acc:71.70%
Epoch [4/300], Step [170/391],                 Loss: 0.80101, Train_Acc:71.77%
Epoch [4/300], Step [180/391],                 Loss: 0.80036, Train_Acc:71.76%
Epoch [4/300], Step [190/391],                 Loss: 0.79817, Train_Acc:71.84%
Epoch [4/300], Step [200/391],                 Loss: 0.79607, Train_Acc:71.97%
Epoch [4/300], Step [210/391],                 Loss: 0.79102, Train_Acc:72.22%
Epoch [4/300], Step [220/391],                 Loss: 0.78794, Train_Acc:72.32%
Epoch [4/300], Step [230/391],                 Loss: 0.78430, Train_Acc:72.44%
Epoch [4/300], Step [240/391],                 Loss: 0.77843, Train_Acc:72.66%
Epoch [4/300], Step [250/391],                 Loss: 0.77714, Train_Acc:72.72%
Epoch [4/300], Step [260/391],                 Loss: 0.77899, Train_Acc:72.61%
Epoch [4/300], Step [270/391],                 Loss: 0.77958, Train_Acc:72.58%
Epoch [4/300], Step [280/391],                 Loss: 0.77776, Train_Acc:72.65%
Epoch [4/300], Step [290/391],                 Loss: 0.77538, Train_Acc:72.74%
Epoch [4/300], Step [300/391],                 Loss: 0.77492, Train_Acc:72.77%
Epoch [4/300], Step [310/391],                 Loss: 0.77149, Train_Acc:72.93%
Epoch [4/300], Step [320/391],                 Loss: 0.77205, Train_Acc:72.92%
Epoch [4/300], Step [330/391],                 Loss: 0.76968, Train_Acc:72.99%
Epoch [4/300], Step [340/391],                 Loss: 0.76737, Train_Acc:73.07%
Epoch [4/300], Step [350/391],                 Loss: 0.76614, Train_Acc:73.10%
Epoch [4/300], Step [360/391],                 Loss: 0.76302, Train_Acc:73.22%
Epoch [4/300], Step [370/391],                 Loss: 0.76133, Train_Acc:73.29%
Epoch [4/300], Step [380/391],                 Loss: 0.75938, Train_Acc:73.38%
Epoch [4/300], Step [390/391],                 Loss: 0.75670, Train_Acc:73.47%
Accuary on test images:56.64%
Epoch [5/300], Step [10/391],                 Loss: 0.68831, Train_Acc:76.02%
Epoch [5/300], Step [20/391],                 Loss: 0.68580, Train_Acc:76.48%
Epoch [5/300], Step [30/391],                 Loss: 0.67878, Train_Acc:76.69%
Epoch [5/300], Step [40/391],                 Loss: 0.68197, Train_Acc:76.37%
Epoch [5/300], Step [50/391],                 Loss: 0.68460, Train_Acc:76.39%
Epoch [5/300], Step [60/391],                 Loss: 0.68832, Train_Acc:76.13%
Epoch [5/300], Step [70/391],                 Loss: 0.68520, Train_Acc:76.19%
Epoch [5/300], Step [80/391],                 Loss: 0.68684, Train_Acc:76.07%
Epoch [5/300], Step [90/391],                 Loss: 0.68941, Train_Acc:75.95%
Epoch [5/300], Step [100/391],                 Loss: 0.69168, Train_Acc:75.88%
Epoch [5/300], Step [110/391],                 Loss: 0.69164, Train_Acc:75.87%
Epoch [5/300], Step [120/391],                 Loss: 0.69024, Train_Acc:75.91%
Epoch [5/300], Step [130/391],                 Loss: 0.68806, Train_Acc:76.02%
Epoch [5/300], Step [140/391],                 Loss: 0.68480, Train_Acc:76.14%
Epoch [5/300], Step [150/391],                 Loss: 0.68363, Train_Acc:76.17%
Epoch [5/300], Step [160/391],                 Loss: 0.68474, Train_Acc:76.14%
Epoch [5/300], Step [170/391],                 Loss: 0.68515, Train_Acc:76.13%
Epoch [5/300], Step [180/391],                 Loss: 0.68685, Train_Acc:76.10%
Epoch [5/300], Step [190/391],                 Loss: 0.68511, Train_Acc:76.12%
Epoch [5/300], Step [200/391],                 Loss: 0.68355, Train_Acc:76.20%
Epoch [5/300], Step [210/391],                 Loss: 0.68103, Train_Acc:76.32%
Epoch [5/300], Step [220/391],                 Loss: 0.67831, Train_Acc:76.40%
Epoch [5/300], Step [230/391],                 Loss: 0.67687, Train_Acc:76.41%
Epoch [5/300], Step [240/391],                 Loss: 0.67265, Train_Acc:76.59%
Epoch [5/300], Step [250/391],                 Loss: 0.67173, Train_Acc:76.62%
Epoch [5/300], Step [260/391],                 Loss: 0.67243, Train_Acc:76.65%
Epoch [5/300], Step [270/391],                 Loss: 0.67406, Train_Acc:76.56%
Epoch [5/300], Step [280/391],                 Loss: 0.67297, Train_Acc:76.56%
Epoch [5/300], Step [290/391],                 Loss: 0.67225, Train_Acc:76.60%
Epoch [5/300], Step [300/391],                 Loss: 0.67123, Train_Acc:76.65%
Epoch [5/300], Step [310/391],                 Loss: 0.66822, Train_Acc:76.77%
Epoch [5/300], Step [320/391],                 Loss: 0.66851, Train_Acc:76.75%
Epoch [5/300], Step [330/391],                 Loss: 0.66672, Train_Acc:76.83%
Epoch [5/300], Step [340/391],                 Loss: 0.66516, Train_Acc:76.89%
Epoch [5/300], Step [350/391],                 Loss: 0.66350, Train_Acc:76.99%
Epoch [5/300], Step [360/391],                 Loss: 0.66187, Train_Acc:77.06%
Epoch [5/300], Step [370/391],                 Loss: 0.66014, Train_Acc:77.13%
Epoch [5/300], Step [380/391],                 Loss: 0.65854, Train_Acc:77.21%
Epoch [5/300], Step [390/391],                 Loss: 0.65683, Train_Acc:77.27%
Accuary on test images:70.50%
Epoch [6/300], Step [10/391],                 Loss: 0.61945, Train_Acc:77.97%
Epoch [6/300], Step [20/391],                 Loss: 0.61535, Train_Acc:78.44%
Epoch [6/300], Step [30/391],                 Loss: 0.60977, Train_Acc:78.85%
Epoch [6/300], Step [40/391],                 Loss: 0.61648, Train_Acc:78.77%
Epoch [6/300], Step [50/391],                 Loss: 0.61295, Train_Acc:78.97%
Epoch [6/300], Step [60/391],                 Loss: 0.61494, Train_Acc:78.84%
Epoch [6/300], Step [70/391],                 Loss: 0.61617, Train_Acc:78.91%
Epoch [6/300], Step [80/391],                 Loss: 0.61594, Train_Acc:78.79%
Epoch [6/300], Step [90/391],                 Loss: 0.62137, Train_Acc:78.57%
Epoch [6/300], Step [100/391],                 Loss: 0.62109, Train_Acc:78.61%
Epoch [6/300], Step [110/391],                 Loss: 0.62321, Train_Acc:78.51%
Epoch [6/300], Step [120/391],                 Loss: 0.62294, Train_Acc:78.44%
Epoch [6/300], Step [130/391],                 Loss: 0.62411, Train_Acc:78.47%
Epoch [6/300], Step [140/391],                 Loss: 0.61993, Train_Acc:78.67%
Epoch [6/300], Step [150/391],                 Loss: 0.61899, Train_Acc:78.72%
Epoch [6/300], Step [160/391],                 Loss: 0.61963, Train_Acc:78.69%
Epoch [6/300], Step [170/391],                 Loss: 0.62031, Train_Acc:78.62%
Epoch [6/300], Step [180/391],                 Loss: 0.62316, Train_Acc:78.52%
Epoch [6/300], Step [190/391],                 Loss: 0.62162, Train_Acc:78.56%
Epoch [6/300], Step [200/391],                 Loss: 0.62012, Train_Acc:78.64%
Epoch [6/300], Step [210/391],                 Loss: 0.61645, Train_Acc:78.72%
Epoch [6/300], Step [220/391],                 Loss: 0.61546, Train_Acc:78.77%
Epoch [6/300], Step [230/391],                 Loss: 0.61403, Train_Acc:78.79%
Epoch [6/300], Step [240/391],                 Loss: 0.61089, Train_Acc:78.93%
Epoch [6/300], Step [250/391],                 Loss: 0.61015, Train_Acc:78.97%
Epoch [6/300], Step [260/391],                 Loss: 0.61190, Train_Acc:78.95%
Epoch [6/300], Step [270/391],                 Loss: 0.61279, Train_Acc:78.89%
Epoch [6/300], Step [280/391],                 Loss: 0.61305, Train_Acc:78.93%
Epoch [6/300], Step [290/391],                 Loss: 0.61194, Train_Acc:78.98%
Epoch [6/300], Step [300/391],                 Loss: 0.61153, Train_Acc:78.95%
Epoch [6/300], Step [310/391],                 Loss: 0.60933, Train_Acc:79.04%
Epoch [6/300], Step [320/391],                 Loss: 0.60937, Train_Acc:79.02%
Epoch [6/300], Step [330/391],                 Loss: 0.60789, Train_Acc:79.08%
Epoch [6/300], Step [340/391],                 Loss: 0.60719, Train_Acc:79.09%
Epoch [6/300], Step [350/391],                 Loss: 0.60675, Train_Acc:79.12%
Epoch [6/300], Step [360/391],                 Loss: 0.60557, Train_Acc:79.16%
Epoch [6/300], Step [370/391],                 Loss: 0.60574, Train_Acc:79.14%
Epoch [6/300], Step [380/391],                 Loss: 0.60577, Train_Acc:79.16%
Epoch [6/300], Step [390/391],                 Loss: 0.60464, Train_Acc:79.18%
Accuary on test images:70.10%
Epoch [7/300], Step [10/391],                 Loss: 0.57743, Train_Acc:79.30%
Epoch [7/300], Step [20/391],                 Loss: 0.57413, Train_Acc:79.41%
Epoch [7/300], Step [30/391],                 Loss: 0.56789, Train_Acc:80.23%
Epoch [7/300], Step [40/391],                 Loss: 0.57050, Train_Acc:80.16%
Epoch [7/300], Step [50/391],                 Loss: 0.57385, Train_Acc:80.00%
Epoch [7/300], Step [60/391],                 Loss: 0.57850, Train_Acc:80.00%
Epoch [7/300], Step [70/391],                 Loss: 0.57518, Train_Acc:80.12%
Epoch [7/300], Step [80/391],                 Loss: 0.57471, Train_Acc:80.22%
Epoch [7/300], Step [90/391],                 Loss: 0.58024, Train_Acc:79.91%
Epoch [7/300], Step [100/391],                 Loss: 0.58120, Train_Acc:79.84%
Epoch [7/300], Step [110/391],                 Loss: 0.58275, Train_Acc:79.79%
Epoch [7/300], Step [120/391],                 Loss: 0.58091, Train_Acc:79.86%
Epoch [7/300], Step [130/391],                 Loss: 0.58239, Train_Acc:79.90%
Epoch [7/300], Step [140/391],                 Loss: 0.58040, Train_Acc:79.98%
Epoch [7/300], Step [150/391],                 Loss: 0.58000, Train_Acc:80.04%
Epoch [7/300], Step [160/391],                 Loss: 0.57951, Train_Acc:80.05%
Epoch [7/300], Step [170/391],                 Loss: 0.57953, Train_Acc:80.09%
Epoch [7/300], Step [180/391],                 Loss: 0.58109, Train_Acc:80.07%
Epoch [7/300], Step [190/391],                 Loss: 0.58068, Train_Acc:80.08%
Epoch [7/300], Step [200/391],                 Loss: 0.57905, Train_Acc:80.12%
Epoch [7/300], Step [210/391],                 Loss: 0.57729, Train_Acc:80.19%
Epoch [7/300], Step [220/391],                 Loss: 0.57618, Train_Acc:80.20%
Epoch [7/300], Step [230/391],                 Loss: 0.57475, Train_Acc:80.26%
Epoch [7/300], Step [240/391],                 Loss: 0.57329, Train_Acc:80.27%
Epoch [7/300], Step [250/391],                 Loss: 0.57204, Train_Acc:80.31%
Epoch [7/300], Step [260/391],                 Loss: 0.57403, Train_Acc:80.25%
Epoch [7/300], Step [270/391],                 Loss: 0.57529, Train_Acc:80.20%
Epoch [7/300], Step [280/391],                 Loss: 0.57488, Train_Acc:80.19%
Epoch [7/300], Step [290/391],                 Loss: 0.57385, Train_Acc:80.26%
Epoch [7/300], Step [300/391],                 Loss: 0.57331, Train_Acc:80.28%
Epoch [7/300], Step [310/391],                 Loss: 0.57151, Train_Acc:80.34%
Epoch [7/300], Step [320/391],                 Loss: 0.57163, Train_Acc:80.34%
Epoch [7/300], Step [330/391],                 Loss: 0.57144, Train_Acc:80.34%
Epoch [7/300], Step [340/391],                 Loss: 0.57045, Train_Acc:80.37%
Epoch [7/300], Step [350/391],                 Loss: 0.57023, Train_Acc:80.40%
Epoch [7/300], Step [360/391],                 Loss: 0.56975, Train_Acc:80.41%
Epoch [7/300], Step [370/391],                 Loss: 0.56926, Train_Acc:80.42%
Epoch [7/300], Step [380/391],                 Loss: 0.56944, Train_Acc:80.43%
Epoch [7/300], Step [390/391],                 Loss: 0.56864, Train_Acc:80.48%
Accuary on test images:75.94%
Epoch [8/300], Step [10/391],                 Loss: 0.54680, Train_Acc:81.25%
Epoch [8/300], Step [20/391],                 Loss: 0.54464, Train_Acc:81.21%
Epoch [8/300], Step [30/391],                 Loss: 0.53655, Train_Acc:81.48%
Epoch [8/300], Step [40/391],                 Loss: 0.53602, Train_Acc:81.52%
Epoch [8/300], Step [50/391],                 Loss: 0.53275, Train_Acc:81.73%
Epoch [8/300], Step [60/391],                 Loss: 0.53680, Train_Acc:81.43%
Epoch [8/300], Step [70/391],                 Loss: 0.54086, Train_Acc:81.42%
Epoch [8/300], Step [80/391],                 Loss: 0.54044, Train_Acc:81.40%
Epoch [8/300], Step [90/391],                 Loss: 0.54387, Train_Acc:81.26%
Epoch [8/300], Step [100/391],                 Loss: 0.54379, Train_Acc:81.21%
Epoch [8/300], Step [110/391],                 Loss: 0.54812, Train_Acc:81.16%
Epoch [8/300], Step [120/391],                 Loss: 0.54727, Train_Acc:81.20%
Epoch [8/300], Step [130/391],                 Loss: 0.54920, Train_Acc:81.15%
Epoch [8/300], Step [140/391],                 Loss: 0.54447, Train_Acc:81.36%
Epoch [8/300], Step [150/391],                 Loss: 0.54268, Train_Acc:81.46%
Epoch [8/300], Step [160/391],                 Loss: 0.54135, Train_Acc:81.51%
Epoch [8/300], Step [170/391],                 Loss: 0.54119, Train_Acc:81.53%
Epoch [8/300], Step [180/391],                 Loss: 0.54166, Train_Acc:81.48%
Epoch [8/300], Step [190/391],                 Loss: 0.54220, Train_Acc:81.44%
Epoch [8/300], Step [200/391],                 Loss: 0.54146, Train_Acc:81.50%
Epoch [8/300], Step [210/391],                 Loss: 0.54136, Train_Acc:81.51%
Epoch [8/300], Step [220/391],                 Loss: 0.53876, Train_Acc:81.61%
Epoch [8/300], Step [230/391],                 Loss: 0.53780, Train_Acc:81.61%
Epoch [8/300], Step [240/391],                 Loss: 0.53545, Train_Acc:81.70%
Epoch [8/300], Step [250/391],                 Loss: 0.53520, Train_Acc:81.70%
Epoch [8/300], Step [260/391],                 Loss: 0.53716, Train_Acc:81.64%
Epoch [8/300], Step [270/391],                 Loss: 0.53803, Train_Acc:81.57%
Epoch [8/300], Step [280/391],                 Loss: 0.53912, Train_Acc:81.52%
Epoch [8/300], Step [290/391],                 Loss: 0.53889, Train_Acc:81.53%
Epoch [8/300], Step [300/391],                 Loss: 0.53873, Train_Acc:81.52%
Epoch [8/300], Step [310/391],                 Loss: 0.53734, Train_Acc:81.60%
Epoch [8/300], Step [320/391],                 Loss: 0.53693, Train_Acc:81.61%
Epoch [8/300], Step [330/391],                 Loss: 0.53513, Train_Acc:81.70%
Epoch [8/300], Step [340/391],                 Loss: 0.53390, Train_Acc:81.76%
Epoch [8/300], Step [350/391],                 Loss: 0.53246, Train_Acc:81.83%
Epoch [8/300], Step [360/391],                 Loss: 0.53196, Train_Acc:81.83%
Epoch [8/300], Step [370/391],                 Loss: 0.53129, Train_Acc:81.85%
Epoch [8/300], Step [380/391],                 Loss: 0.53122, Train_Acc:81.89%
Epoch [8/300], Step [390/391],                 Loss: 0.53012, Train_Acc:81.92%
Accuary on test images:70.64%
Epoch [9/300], Step [10/391],                 Loss: 0.50079, Train_Acc:83.05%
Epoch [9/300], Step [20/391],                 Loss: 0.49535, Train_Acc:82.50%
Epoch [9/300], Step [30/391],                 Loss: 0.49846, Train_Acc:82.71%
Epoch [9/300], Step [40/391],                 Loss: 0.50096, Train_Acc:82.70%
Epoch [9/300], Step [50/391],                 Loss: 0.50290, Train_Acc:82.62%
Epoch [9/300], Step [60/391],                 Loss: 0.51533, Train_Acc:82.28%
Epoch [9/300], Step [70/391],                 Loss: 0.51664, Train_Acc:82.34%
Epoch [9/300], Step [80/391],                 Loss: 0.51707, Train_Acc:82.35%
Epoch [9/300], Step [90/391],                 Loss: 0.52298, Train_Acc:82.16%
Epoch [9/300], Step [100/391],                 Loss: 0.52530, Train_Acc:81.94%
Epoch [9/300], Step [110/391],                 Loss: 0.52612, Train_Acc:81.87%
Epoch [9/300], Step [120/391],                 Loss: 0.52509, Train_Acc:81.85%
Epoch [9/300], Step [130/391],                 Loss: 0.52620, Train_Acc:81.86%
Epoch [9/300], Step [140/391],                 Loss: 0.52549, Train_Acc:81.96%
Epoch [9/300], Step [150/391],                 Loss: 0.52746, Train_Acc:81.93%
Epoch [9/300], Step [160/391],                 Loss: 0.52715, Train_Acc:81.92%
Epoch [9/300], Step [170/391],                 Loss: 0.52627, Train_Acc:81.97%
Epoch [9/300], Step [180/391],                 Loss: 0.52714, Train_Acc:81.93%
Epoch [9/300], Step [190/391],                 Loss: 0.52638, Train_Acc:81.98%
Epoch [9/300], Step [200/391],                 Loss: 0.52487, Train_Acc:82.05%
Epoch [9/300], Step [210/391],                 Loss: 0.52268, Train_Acc:82.14%
Epoch [9/300], Step [220/391],                 Loss: 0.52185, Train_Acc:82.12%
Epoch [9/300], Step [230/391],                 Loss: 0.52042, Train_Acc:82.15%
Epoch [9/300], Step [240/391],                 Loss: 0.51846, Train_Acc:82.20%
Epoch [9/300], Step [250/391],                 Loss: 0.51726, Train_Acc:82.23%
Epoch [9/300], Step [260/391],                 Loss: 0.51991, Train_Acc:82.13%
Epoch [9/300], Step [270/391],                 Loss: 0.52160, Train_Acc:82.09%
Epoch [9/300], Step [280/391],                 Loss: 0.52221, Train_Acc:82.05%
Epoch [9/300], Step [290/391],                 Loss: 0.52257, Train_Acc:82.03%
Epoch [9/300], Step [300/391],                 Loss: 0.52212, Train_Acc:82.05%
Epoch [9/300], Step [310/391],                 Loss: 0.52219, Train_Acc:82.06%
Epoch [9/300], Step [320/391],                 Loss: 0.52231, Train_Acc:82.05%
Epoch [9/300], Step [330/391],                 Loss: 0.52046, Train_Acc:82.13%
Epoch [9/300], Step [340/391],                 Loss: 0.51877, Train_Acc:82.18%
Epoch [9/300], Step [350/391],                 Loss: 0.51859, Train_Acc:82.20%
Epoch [9/300], Step [360/391],                 Loss: 0.51819, Train_Acc:82.18%
Epoch [9/300], Step [370/391],                 Loss: 0.51815, Train_Acc:82.18%
Epoch [9/300], Step [380/391],                 Loss: 0.51896, Train_Acc:82.16%
Epoch [9/300], Step [390/391],                 Loss: 0.51843, Train_Acc:82.18%
Accuary on test images:68.56%
Epoch [10/300], Step [10/391],                 Loss: 0.49717, Train_Acc:82.34%
Epoch [10/300], Step [20/391],                 Loss: 0.48314, Train_Acc:83.20%
Epoch [10/300], Step [30/391],                 Loss: 0.48840, Train_Acc:83.02%
Epoch [10/300], Step [40/391],                 Loss: 0.49038, Train_Acc:82.93%
Epoch [10/300], Step [50/391],                 Loss: 0.49664, Train_Acc:82.55%
Epoch [10/300], Step [60/391],                 Loss: 0.50642, Train_Acc:82.33%
Epoch [10/300], Step [70/391],                 Loss: 0.51045, Train_Acc:82.13%
Epoch [10/300], Step [80/391],                 Loss: 0.50984, Train_Acc:82.20%
Epoch [10/300], Step [90/391],                 Loss: 0.51716, Train_Acc:81.98%
Epoch [10/300], Step [100/391],                 Loss: 0.51482, Train_Acc:82.05%
Epoch [10/300], Step [110/391],                 Loss: 0.51555, Train_Acc:82.08%
Epoch [10/300], Step [120/391],                 Loss: 0.51347, Train_Acc:82.15%
Epoch [10/300], Step [130/391],                 Loss: 0.51432, Train_Acc:82.17%
Epoch [10/300], Step [140/391],                 Loss: 0.51174, Train_Acc:82.29%
Epoch [10/300], Step [150/391],                 Loss: 0.51013, Train_Acc:82.39%
Epoch [10/300], Step [160/391],                 Loss: 0.50960, Train_Acc:82.38%
Epoch [10/300], Step [170/391],                 Loss: 0.50991, Train_Acc:82.38%
Epoch [10/300], Step [180/391],                 Loss: 0.51052, Train_Acc:82.35%
Epoch [10/300], Step [190/391],                 Loss: 0.50996, Train_Acc:82.39%
Epoch [10/300], Step [200/391],                 Loss: 0.50931, Train_Acc:82.43%
Epoch [10/300], Step [210/391],                 Loss: 0.50916, Train_Acc:82.41%
Epoch [10/300], Step [220/391],                 Loss: 0.50802, Train_Acc:82.43%
Epoch [10/300], Step [230/391],                 Loss: 0.50572, Train_Acc:82.50%
Epoch [10/300], Step [240/391],                 Loss: 0.50294, Train_Acc:82.58%
Epoch [10/300], Step [250/391],                 Loss: 0.50142, Train_Acc:82.63%
Epoch [10/300], Step [260/391],                 Loss: 0.50241, Train_Acc:82.58%
Epoch [10/300], Step [270/391],                 Loss: 0.50336, Train_Acc:82.56%
Epoch [10/300], Step [280/391],                 Loss: 0.50309, Train_Acc:82.60%
Epoch [10/300], Step [290/391],                 Loss: 0.50219, Train_Acc:82.65%
Epoch [10/300], Step [300/391],                 Loss: 0.50238, Train_Acc:82.63%
Epoch [10/300], Step [310/391],                 Loss: 0.50137, Train_Acc:82.70%
Epoch [10/300], Step [320/391],                 Loss: 0.50131, Train_Acc:82.72%
Epoch [10/300], Step [330/391],                 Loss: 0.50002, Train_Acc:82.80%
Epoch [10/300], Step [340/391],                 Loss: 0.49889, Train_Acc:82.83%
Epoch [10/300], Step [350/391],                 Loss: 0.49808, Train_Acc:82.92%
Epoch [10/300], Step [360/391],                 Loss: 0.49726, Train_Acc:82.95%
Epoch [10/300], Step [370/391],                 Loss: 0.49654, Train_Acc:82.96%
Epoch [10/300], Step [380/391],                 Loss: 0.49602, Train_Acc:82.97%
Epoch [10/300], Step [390/391],                 Loss: 0.49528, Train_Acc:82.99%
Accuary on test images:75.24%
Epoch [11/300], Step [10/391],                 Loss: 0.47730, Train_Acc:84.14%
Epoch [11/300], Step [20/391],                 Loss: 0.45589, Train_Acc:84.88%
Epoch [11/300], Step [30/391],                 Loss: 0.45990, Train_Acc:84.58%
Epoch [11/300], Step [40/391],                 Loss: 0.46594, Train_Acc:84.53%
Epoch [11/300], Step [50/391],                 Loss: 0.47160, Train_Acc:84.14%
Epoch [11/300], Step [60/391],                 Loss: 0.47932, Train_Acc:83.70%
Epoch [11/300], Step [70/391],                 Loss: 0.47988, Train_Acc:83.73%
Epoch [11/300], Step [80/391],                 Loss: 0.47827, Train_Acc:83.85%
Epoch [11/300], Step [90/391],                 Loss: 0.48042, Train_Acc:83.82%
Epoch [11/300], Step [100/391],                 Loss: 0.48167, Train_Acc:83.67%
Epoch [11/300], Step [110/391],                 Loss: 0.48423, Train_Acc:83.56%
Epoch [11/300], Step [120/391],                 Loss: 0.48707, Train_Acc:83.46%
Epoch [11/300], Step [130/391],                 Loss: 0.48921, Train_Acc:83.43%
Epoch [11/300], Step [140/391],                 Loss: 0.48692, Train_Acc:83.53%
Epoch [11/300], Step [150/391],                 Loss: 0.48607, Train_Acc:83.52%
Epoch [11/300], Step [160/391],                 Loss: 0.48401, Train_Acc:83.62%
Epoch [11/300], Step [170/391],                 Loss: 0.48523, Train_Acc:83.59%
Epoch [11/300], Step [180/391],                 Loss: 0.48603, Train_Acc:83.49%
Epoch [11/300], Step [190/391],                 Loss: 0.48862, Train_Acc:83.35%
Epoch [11/300], Step [200/391],                 Loss: 0.48848, Train_Acc:83.32%
Epoch [11/300], Step [210/391],                 Loss: 0.48849, Train_Acc:83.33%
Epoch [11/300], Step [220/391],                 Loss: 0.48798, Train_Acc:83.33%
Epoch [11/300], Step [230/391],                 Loss: 0.48682, Train_Acc:83.36%
Epoch [11/300], Step [240/391],                 Loss: 0.48462, Train_Acc:83.42%
Epoch [11/300], Step [250/391],                 Loss: 0.48452, Train_Acc:83.43%
Epoch [11/300], Step [260/391],                 Loss: 0.48683, Train_Acc:83.42%
Epoch [11/300], Step [270/391],                 Loss: 0.48818, Train_Acc:83.41%
Epoch [11/300], Step [280/391],                 Loss: 0.48728, Train_Acc:83.44%
Epoch [11/300], Step [290/391],                 Loss: 0.48636, Train_Acc:83.50%
Epoch [11/300], Step [300/391],                 Loss: 0.48501, Train_Acc:83.53%
Epoch [11/300], Step [310/391],                 Loss: 0.48319, Train_Acc:83.58%
Epoch [11/300], Step [320/391],                 Loss: 0.48409, Train_Acc:83.57%
Epoch [11/300], Step [330/391],                 Loss: 0.48274, Train_Acc:83.63%
Epoch [11/300], Step [340/391],                 Loss: 0.48202, Train_Acc:83.62%
Epoch [11/300], Step [350/391],                 Loss: 0.48142, Train_Acc:83.65%
Epoch [11/300], Step [360/391],                 Loss: 0.48155, Train_Acc:83.61%
Epoch [11/300], Step [370/391],                 Loss: 0.48149, Train_Acc:83.60%
Epoch [11/300], Step [380/391],                 Loss: 0.48122, Train_Acc:83.61%
Epoch [11/300], Step [390/391],                 Loss: 0.48037, Train_Acc:83.66%
Accuary on test images:69.42%
Epoch [12/300], Step [10/391],                 Loss: 0.48040, Train_Acc:84.14%
Epoch [12/300], Step [20/391],                 Loss: 0.45796, Train_Acc:84.34%
Epoch [12/300], Step [30/391],                 Loss: 0.45225, Train_Acc:84.45%
Epoch [12/300], Step [40/391],                 Loss: 0.45190, Train_Acc:84.77%
Epoch [12/300], Step [50/391],                 Loss: 0.45785, Train_Acc:84.53%
Epoch [12/300], Step [60/391],                 Loss: 0.46360, Train_Acc:84.22%
Epoch [12/300], Step [70/391],                 Loss: 0.46279, Train_Acc:84.19%
Epoch [12/300], Step [80/391],                 Loss: 0.46029, Train_Acc:84.35%
Epoch [12/300], Step [90/391],                 Loss: 0.46611, Train_Acc:84.04%
Epoch [12/300], Step [100/391],                 Loss: 0.46712, Train_Acc:83.84%
Epoch [12/300], Step [110/391],                 Loss: 0.46737, Train_Acc:83.84%
Epoch [12/300], Step [120/391],                 Loss: 0.46653, Train_Acc:83.85%
Epoch [12/300], Step [130/391],                 Loss: 0.46719, Train_Acc:83.88%
Epoch [12/300], Step [140/391],                 Loss: 0.46809, Train_Acc:83.86%
Epoch [12/300], Step [150/391],                 Loss: 0.46852, Train_Acc:83.88%
Epoch [12/300], Step [160/391],                 Loss: 0.46707, Train_Acc:83.92%
Epoch [12/300], Step [170/391],                 Loss: 0.46831, Train_Acc:83.86%
Epoch [12/300], Step [180/391],                 Loss: 0.46882, Train_Acc:83.85%
Epoch [12/300], Step [190/391],                 Loss: 0.46878, Train_Acc:83.82%
Epoch [12/300], Step [200/391],                 Loss: 0.46950, Train_Acc:83.82%
Epoch [12/300], Step [210/391],                 Loss: 0.46896, Train_Acc:83.86%
Epoch [12/300], Step [220/391],                 Loss: 0.46819, Train_Acc:83.94%
Epoch [12/300], Step [230/391],                 Loss: 0.46656, Train_Acc:84.00%
Epoch [12/300], Step [240/391],                 Loss: 0.46548, Train_Acc:84.04%
Epoch [12/300], Step [250/391],                 Loss: 0.46475, Train_Acc:84.04%
Epoch [12/300], Step [260/391],                 Loss: 0.46623, Train_Acc:83.98%
Epoch [12/300], Step [270/391],                 Loss: 0.46802, Train_Acc:83.90%
Epoch [12/300], Step [280/391],                 Loss: 0.46913, Train_Acc:83.84%
Epoch [12/300], Step [290/391],                 Loss: 0.46903, Train_Acc:83.89%
Epoch [12/300], Step [300/391],                 Loss: 0.46895, Train_Acc:83.92%
Epoch [12/300], Step [310/391],                 Loss: 0.46793, Train_Acc:83.99%
Epoch [12/300], Step [320/391],                 Loss: 0.46835, Train_Acc:83.96%
Epoch [12/300], Step [330/391],                 Loss: 0.46760, Train_Acc:83.97%
Epoch [12/300], Step [340/391],                 Loss: 0.46658, Train_Acc:84.02%
Epoch [12/300], Step [350/391],                 Loss: 0.46474, Train_Acc:84.08%
Epoch [12/300], Step [360/391],                 Loss: 0.46451, Train_Acc:84.07%
Epoch [12/300], Step [370/391],                 Loss: 0.46393, Train_Acc:84.10%
Epoch [12/300], Step [380/391],                 Loss: 0.46409, Train_Acc:84.11%
Epoch [12/300], Step [390/391],                 Loss: 0.46345, Train_Acc:84.13%
Accuary on test images:66.44%
Epoch [13/300], Step [10/391],                 Loss: 0.48353, Train_Acc:82.97%
Epoch [13/300], Step [20/391],                 Loss: 0.45539, Train_Acc:83.79%
Epoch [13/300], Step [30/391],                 Loss: 0.45614, Train_Acc:83.96%
Epoch [13/300], Step [40/391],                 Loss: 0.45220, Train_Acc:84.30%
Epoch [13/300], Step [50/391],                 Loss: 0.45051, Train_Acc:84.36%
Epoch [13/300], Step [60/391],                 Loss: 0.45343, Train_Acc:84.26%
Epoch [13/300], Step [70/391],                 Loss: 0.45339, Train_Acc:84.33%
Epoch [13/300], Step [80/391],                 Loss: 0.45348, Train_Acc:84.31%
Epoch [13/300], Step [90/391],                 Loss: 0.45863, Train_Acc:84.14%
Epoch [13/300], Step [100/391],                 Loss: 0.45738, Train_Acc:84.08%
Epoch [13/300], Step [110/391],                 Loss: 0.45914, Train_Acc:84.03%
Epoch [13/300], Step [120/391],                 Loss: 0.45926, Train_Acc:83.98%
Epoch [13/300], Step [130/391],                 Loss: 0.46117, Train_Acc:84.00%
Epoch [13/300], Step [140/391],                 Loss: 0.45983, Train_Acc:84.05%
Epoch [13/300], Step [150/391],                 Loss: 0.45942, Train_Acc:84.14%
Epoch [13/300], Step [160/391],                 Loss: 0.45803, Train_Acc:84.19%
Epoch [13/300], Step [170/391],                 Loss: 0.45845, Train_Acc:84.26%
Epoch [13/300], Step [180/391],                 Loss: 0.45819, Train_Acc:84.24%
Epoch [13/300], Step [190/391],                 Loss: 0.45863, Train_Acc:84.18%
Epoch [13/300], Step [200/391],                 Loss: 0.45952, Train_Acc:84.19%
Epoch [13/300], Step [210/391],                 Loss: 0.45975, Train_Acc:84.19%
Epoch [13/300], Step [220/391],                 Loss: 0.46053, Train_Acc:84.19%
Epoch [13/300], Step [230/391],                 Loss: 0.46094, Train_Acc:84.11%
Epoch [13/300], Step [240/391],                 Loss: 0.45951, Train_Acc:84.18%
Epoch [13/300], Step [250/391],                 Loss: 0.45938, Train_Acc:84.17%
Epoch [13/300], Step [260/391],                 Loss: 0.46090, Train_Acc:84.16%
Epoch [13/300], Step [270/391],                 Loss: 0.46242, Train_Acc:84.09%
Epoch [13/300], Step [280/391],                 Loss: 0.46298, Train_Acc:84.05%
Epoch [13/300], Step [290/391],                 Loss: 0.46231, Train_Acc:84.05%
Epoch [13/300], Step [300/391],                 Loss: 0.46286, Train_Acc:84.02%
Epoch [13/300], Step [310/391],                 Loss: 0.46403, Train_Acc:83.99%
Epoch [13/300], Step [320/391],                 Loss: 0.46433, Train_Acc:83.97%
Epoch [13/300], Step [330/391],                 Loss: 0.46332, Train_Acc:84.01%
Epoch [13/300], Step [340/391],                 Loss: 0.46230, Train_Acc:84.05%
Epoch [13/300], Step [350/391],                 Loss: 0.46071, Train_Acc:84.10%
Epoch [13/300], Step [360/391],                 Loss: 0.45991, Train_Acc:84.12%
Epoch [13/300], Step [370/391],                 Loss: 0.45981, Train_Acc:84.11%
Epoch [13/300], Step [380/391],                 Loss: 0.45920, Train_Acc:84.14%
Epoch [13/300], Step [390/391],                 Loss: 0.45841, Train_Acc:84.18%
Accuary on test images:73.36%
Epoch [14/300], Step [10/391],                 Loss: 0.48977, Train_Acc:83.75%
Epoch [14/300], Step [20/391],                 Loss: 0.47622, Train_Acc:83.91%
Epoch [14/300], Step [30/391],                 Loss: 0.46408, Train_Acc:84.38%
Epoch [14/300], Step [40/391],                 Loss: 0.45639, Train_Acc:84.67%
Epoch [14/300], Step [50/391],                 Loss: 0.45288, Train_Acc:84.81%
Epoch [14/300], Step [60/391],                 Loss: 0.45555, Train_Acc:84.52%
Epoch [14/300], Step [70/391],                 Loss: 0.45844, Train_Acc:84.56%
Epoch [14/300], Step [80/391],                 Loss: 0.46009, Train_Acc:84.51%
Epoch [14/300], Step [90/391],                 Loss: 0.46143, Train_Acc:84.50%
Epoch [14/300], Step [100/391],                 Loss: 0.46201, Train_Acc:84.38%
Epoch [14/300], Step [110/391],                 Loss: 0.46214, Train_Acc:84.40%
Epoch [14/300], Step [120/391],                 Loss: 0.45716, Train_Acc:84.56%
Epoch [14/300], Step [130/391],                 Loss: 0.45795, Train_Acc:84.57%
Epoch [14/300], Step [140/391],                 Loss: 0.45721, Train_Acc:84.61%
Epoch [14/300], Step [150/391],                 Loss: 0.45821, Train_Acc:84.58%
Epoch [14/300], Step [160/391],                 Loss: 0.45581, Train_Acc:84.63%
Epoch [14/300], Step [170/391],                 Loss: 0.45686, Train_Acc:84.57%
Epoch [14/300], Step [180/391],                 Loss: 0.45728, Train_Acc:84.52%
Epoch [14/300], Step [190/391],                 Loss: 0.45798, Train_Acc:84.40%
Epoch [14/300], Step [200/391],                 Loss: 0.46069, Train_Acc:84.33%
Epoch [14/300], Step [210/391],                 Loss: 0.46074, Train_Acc:84.35%
Epoch [14/300], Step [220/391],                 Loss: 0.46079, Train_Acc:84.38%
Epoch [14/300], Step [230/391],                 Loss: 0.45876, Train_Acc:84.43%
Epoch [14/300], Step [240/391],                 Loss: 0.45758, Train_Acc:84.51%
Epoch [14/300], Step [250/391],                 Loss: 0.45706, Train_Acc:84.50%
Epoch [14/300], Step [260/391],                 Loss: 0.45829, Train_Acc:84.46%
Epoch [14/300], Step [270/391],                 Loss: 0.45950, Train_Acc:84.41%
Epoch [14/300], Step [280/391],                 Loss: 0.45917, Train_Acc:84.41%
Epoch [14/300], Step [290/391],                 Loss: 0.45855, Train_Acc:84.43%
Epoch [14/300], Step [300/391],                 Loss: 0.45776, Train_Acc:84.44%
Epoch [14/300], Step [310/391],                 Loss: 0.45690, Train_Acc:84.44%
Epoch [14/300], Step [320/391],                 Loss: 0.45673, Train_Acc:84.42%
Epoch [14/300], Step [330/391],                 Loss: 0.45661, Train_Acc:84.42%
Epoch [14/300], Step [340/391],                 Loss: 0.45584, Train_Acc:84.44%
Epoch [14/300], Step [350/391],                 Loss: 0.45557, Train_Acc:84.45%
Epoch [14/300], Step [360/391],                 Loss: 0.45539, Train_Acc:84.45%
Epoch [14/300], Step [370/391],                 Loss: 0.45509, Train_Acc:84.47%
Epoch [14/300], Step [380/391],                 Loss: 0.45372, Train_Acc:84.53%
Epoch [14/300], Step [390/391],                 Loss: 0.45276, Train_Acc:84.58%
Accuary on test images:78.18%
Epoch [15/300], Step [10/391],                 Loss: 0.44592, Train_Acc:83.98%
Epoch [15/300], Step [20/391],                 Loss: 0.44009, Train_Acc:84.92%
Epoch [15/300], Step [30/391],                 Loss: 0.44145, Train_Acc:85.13%
Epoch [15/300], Step [40/391],                 Loss: 0.43446, Train_Acc:85.33%
Epoch [15/300], Step [50/391],                 Loss: 0.42852, Train_Acc:85.20%
Epoch [15/300], Step [60/391],                 Loss: 0.43814, Train_Acc:84.91%
Epoch [15/300], Step [70/391],                 Loss: 0.43899, Train_Acc:84.99%
Epoch [15/300], Step [80/391],                 Loss: 0.44282, Train_Acc:84.87%
Epoch [15/300], Step [90/391],                 Loss: 0.44646, Train_Acc:84.71%
Epoch [15/300], Step [100/391],                 Loss: 0.44402, Train_Acc:84.73%
Epoch [15/300], Step [110/391],                 Loss: 0.44748, Train_Acc:84.68%
Epoch [15/300], Step [120/391],                 Loss: 0.44671, Train_Acc:84.69%
Epoch [15/300], Step [130/391],                 Loss: 0.44655, Train_Acc:84.71%
Epoch [15/300], Step [140/391],                 Loss: 0.44516, Train_Acc:84.73%
Epoch [15/300], Step [150/391],                 Loss: 0.44548, Train_Acc:84.73%
Epoch [15/300], Step [160/391],                 Loss: 0.44193, Train_Acc:84.88%
Epoch [15/300], Step [170/391],                 Loss: 0.44231, Train_Acc:84.84%
Epoch [15/300], Step [180/391],                 Loss: 0.44412, Train_Acc:84.77%
Epoch [15/300], Step [190/391],                 Loss: 0.44410, Train_Acc:84.76%
Epoch [15/300], Step [200/391],                 Loss: 0.44418, Train_Acc:84.75%
Epoch [15/300], Step [210/391],                 Loss: 0.44487, Train_Acc:84.71%
Epoch [15/300], Step [220/391],                 Loss: 0.44461, Train_Acc:84.73%
Epoch [15/300], Step [230/391],                 Loss: 0.44469, Train_Acc:84.73%
Epoch [15/300], Step [240/391],                 Loss: 0.44526, Train_Acc:84.71%
Epoch [15/300], Step [250/391],                 Loss: 0.44579, Train_Acc:84.71%
Epoch [15/300], Step [260/391],                 Loss: 0.44809, Train_Acc:84.65%
Epoch [15/300], Step [270/391],                 Loss: 0.44973, Train_Acc:84.59%
Epoch [15/300], Step [280/391],                 Loss: 0.45001, Train_Acc:84.56%
Epoch [15/300], Step [290/391],                 Loss: 0.45050, Train_Acc:84.57%
Epoch [15/300], Step [300/391],                 Loss: 0.45005, Train_Acc:84.56%
Epoch [15/300], Step [310/391],                 Loss: 0.44985, Train_Acc:84.58%
Epoch [15/300], Step [320/391],                 Loss: 0.44989, Train_Acc:84.59%
Epoch [15/300], Step [330/391],                 Loss: 0.44886, Train_Acc:84.64%
Epoch [15/300], Step [340/391],                 Loss: 0.44723, Train_Acc:84.69%
Epoch [15/300], Step [350/391],                 Loss: 0.44585, Train_Acc:84.80%
Epoch [15/300], Step [360/391],                 Loss: 0.44515, Train_Acc:84.80%
Epoch [15/300], Step [370/391],                 Loss: 0.44497, Train_Acc:84.80%
Epoch [15/300], Step [380/391],                 Loss: 0.44389, Train_Acc:84.86%
Epoch [15/300], Step [390/391],                 Loss: 0.44262, Train_Acc:84.90%
Accuary on test images:74.90%
Epoch [16/300], Step [10/391],                 Loss: 0.42468, Train_Acc:85.70%
Epoch [16/300], Step [20/391],                 Loss: 0.40793, Train_Acc:86.37%
Epoch [16/300], Step [30/391],                 Loss: 0.40502, Train_Acc:86.59%
Epoch [16/300], Step [40/391],                 Loss: 0.41062, Train_Acc:86.50%
Epoch [16/300], Step [50/391],                 Loss: 0.41005, Train_Acc:86.34%
Epoch [16/300], Step [60/391],                 Loss: 0.42241, Train_Acc:85.79%
Epoch [16/300], Step [70/391],                 Loss: 0.42086, Train_Acc:85.80%
Epoch [16/300], Step [80/391],                 Loss: 0.42831, Train_Acc:85.61%
Epoch [16/300], Step [90/391],                 Loss: 0.43773, Train_Acc:85.19%
Epoch [16/300], Step [100/391],                 Loss: 0.44263, Train_Acc:85.13%
Epoch [16/300], Step [110/391],                 Loss: 0.44217, Train_Acc:85.16%
Epoch [16/300], Step [120/391],                 Loss: 0.44053, Train_Acc:85.10%
Epoch [16/300], Step [130/391],                 Loss: 0.44110, Train_Acc:85.05%
Epoch [16/300], Step [140/391],                 Loss: 0.43905, Train_Acc:85.13%
Epoch [16/300], Step [150/391],                 Loss: 0.43811, Train_Acc:85.11%
Epoch [16/300], Step [160/391],                 Loss: 0.43555, Train_Acc:85.22%
Epoch [16/300], Step [170/391],                 Loss: 0.43833, Train_Acc:85.17%
Epoch [16/300], Step [180/391],                 Loss: 0.43679, Train_Acc:85.20%
Epoch [16/300], Step [190/391],                 Loss: 0.43707, Train_Acc:85.19%
Epoch [16/300], Step [200/391],                 Loss: 0.43821, Train_Acc:85.14%
Epoch [16/300], Step [210/391],                 Loss: 0.43992, Train_Acc:85.08%
Epoch [16/300], Step [220/391],                 Loss: 0.44012, Train_Acc:85.10%
Epoch [16/300], Step [230/391],                 Loss: 0.44027, Train_Acc:85.05%
Epoch [16/300], Step [240/391],                 Loss: 0.43812, Train_Acc:85.12%
Epoch [16/300], Step [250/391],                 Loss: 0.43912, Train_Acc:85.04%
Epoch [16/300], Step [260/391],                 Loss: 0.44142, Train_Acc:84.98%
Epoch [16/300], Step [270/391],                 Loss: 0.44285, Train_Acc:84.91%
Epoch [16/300], Step [280/391],                 Loss: 0.44222, Train_Acc:84.96%
Epoch [16/300], Step [290/391],                 Loss: 0.44159, Train_Acc:84.98%
Epoch [16/300], Step [300/391],                 Loss: 0.44084, Train_Acc:85.01%
Epoch [16/300], Step [310/391],                 Loss: 0.44058, Train_Acc:85.04%
Epoch [16/300], Step [320/391],                 Loss: 0.44258, Train_Acc:84.95%
Epoch [16/300], Step [330/391],                 Loss: 0.44099, Train_Acc:85.00%
Epoch [16/300], Step [340/391],                 Loss: 0.44005, Train_Acc:85.02%
Epoch [16/300], Step [350/391],                 Loss: 0.43897, Train_Acc:85.06%
Epoch [16/300], Step [360/391],                 Loss: 0.43962, Train_Acc:85.05%
Epoch [16/300], Step [370/391],                 Loss: 0.43902, Train_Acc:85.06%
Epoch [16/300], Step [380/391],                 Loss: 0.43774, Train_Acc:85.11%
Epoch [16/300], Step [390/391],                 Loss: 0.43738, Train_Acc:85.15%
Accuary on test images:74.16%
Epoch [17/300], Step [10/391],                 Loss: 0.43059, Train_Acc:85.23%
Epoch [17/300], Step [20/391],                 Loss: 0.43905, Train_Acc:84.34%
Epoch [17/300], Step [30/391],                 Loss: 0.43542, Train_Acc:84.79%
Epoch [17/300], Step [40/391],                 Loss: 0.44478, Train_Acc:84.75%
Epoch [17/300], Step [50/391],                 Loss: 0.44747, Train_Acc:84.30%
Epoch [17/300], Step [60/391],                 Loss: 0.44992, Train_Acc:84.18%
Epoch [17/300], Step [70/391],                 Loss: 0.44481, Train_Acc:84.46%
Epoch [17/300], Step [80/391],                 Loss: 0.44504, Train_Acc:84.38%
Epoch [17/300], Step [90/391],                 Loss: 0.44847, Train_Acc:84.19%
Epoch [17/300], Step [100/391],                 Loss: 0.44734, Train_Acc:84.13%
Epoch [17/300], Step [110/391],                 Loss: 0.44940, Train_Acc:84.14%
Epoch [17/300], Step [120/391],                 Loss: 0.45115, Train_Acc:84.09%
Epoch [17/300], Step [130/391],                 Loss: 0.45362, Train_Acc:84.08%
Epoch [17/300], Step [140/391],                 Loss: 0.45356, Train_Acc:84.07%
Epoch [17/300], Step [150/391],                 Loss: 0.45371, Train_Acc:84.08%
Epoch [17/300], Step [160/391],                 Loss: 0.45381, Train_Acc:84.12%
Epoch [17/300], Step [170/391],                 Loss: 0.45386, Train_Acc:84.14%
Epoch [17/300], Step [180/391],                 Loss: 0.45289, Train_Acc:84.14%
Epoch [17/300], Step [190/391],                 Loss: 0.45186, Train_Acc:84.17%
Epoch [17/300], Step [200/391],                 Loss: 0.45162, Train_Acc:84.21%
Epoch [17/300], Step [210/391],                 Loss: 0.45087, Train_Acc:84.24%
Epoch [17/300], Step [220/391],                 Loss: 0.44969, Train_Acc:84.24%
Epoch [17/300], Step [230/391],                 Loss: 0.44735, Train_Acc:84.32%
Epoch [17/300], Step [240/391],                 Loss: 0.44618, Train_Acc:84.39%
Epoch [17/300], Step [250/391],                 Loss: 0.44440, Train_Acc:84.47%
Epoch [17/300], Step [260/391],                 Loss: 0.44628, Train_Acc:84.43%
Epoch [17/300], Step [270/391],                 Loss: 0.44678, Train_Acc:84.43%
Epoch [17/300], Step [280/391],                 Loss: 0.44448, Train_Acc:84.51%
Epoch [17/300], Step [290/391],                 Loss: 0.44389, Train_Acc:84.53%
Epoch [17/300], Step [300/391],                 Loss: 0.44381, Train_Acc:84.53%
Epoch [17/300], Step [310/391],                 Loss: 0.44368, Train_Acc:84.54%
Epoch [17/300], Step [320/391],                 Loss: 0.44438, Train_Acc:84.54%
Epoch [17/300], Step [330/391],                 Loss: 0.44242, Train_Acc:84.58%
Epoch [17/300], Step [340/391],                 Loss: 0.44090, Train_Acc:84.64%
Epoch [17/300], Step [350/391],                 Loss: 0.43907, Train_Acc:84.72%
Epoch [17/300], Step [360/391],                 Loss: 0.43767, Train_Acc:84.77%
Epoch [17/300], Step [370/391],                 Loss: 0.43762, Train_Acc:84.76%
Epoch [17/300], Step [380/391],                 Loss: 0.43692, Train_Acc:84.79%
Epoch [17/300], Step [390/391],                 Loss: 0.43659, Train_Acc:84.81%
Accuary on test images:70.80%
Epoch [18/300], Step [10/391],                 Loss: 0.47668, Train_Acc:83.12%
Epoch [18/300], Step [20/391],                 Loss: 0.44702, Train_Acc:84.22%
Epoch [18/300], Step [30/391],                 Loss: 0.44273, Train_Acc:84.40%
Epoch [18/300], Step [40/391],                 Loss: 0.43947, Train_Acc:84.79%
Epoch [18/300], Step [50/391],                 Loss: 0.43305, Train_Acc:85.16%
Epoch [18/300], Step [60/391],                 Loss: 0.43825, Train_Acc:84.88%
Epoch [18/300], Step [70/391],                 Loss: 0.43782, Train_Acc:84.94%
Epoch [18/300], Step [80/391],                 Loss: 0.43548, Train_Acc:85.06%
Epoch [18/300], Step [90/391],                 Loss: 0.43734, Train_Acc:85.02%
Epoch [18/300], Step [100/391],                 Loss: 0.43885, Train_Acc:84.91%
Epoch [18/300], Step [110/391],                 Loss: 0.44022, Train_Acc:84.87%
Epoch [18/300], Step [120/391],                 Loss: 0.43875, Train_Acc:84.92%
Epoch [18/300], Step [130/391],                 Loss: 0.43834, Train_Acc:85.01%
Epoch [18/300], Step [140/391],                 Loss: 0.43533, Train_Acc:85.08%
Epoch [18/300], Step [150/391],                 Loss: 0.43525, Train_Acc:85.08%
Epoch [18/300], Step [160/391],                 Loss: 0.43209, Train_Acc:85.21%
Epoch [18/300], Step [170/391],                 Loss: 0.43203, Train_Acc:85.26%
Epoch [18/300], Step [180/391],                 Loss: 0.43136, Train_Acc:85.29%
Epoch [18/300], Step [190/391],                 Loss: 0.43121, Train_Acc:85.29%
Epoch [18/300], Step [200/391],                 Loss: 0.43222, Train_Acc:85.24%
Epoch [18/300], Step [210/391],                 Loss: 0.43181, Train_Acc:85.25%
Epoch [18/300], Step [220/391],                 Loss: 0.43104, Train_Acc:85.23%
Epoch [18/300], Step [230/391],                 Loss: 0.42944, Train_Acc:85.25%
Epoch [18/300], Step [240/391],                 Loss: 0.42838, Train_Acc:85.26%
Epoch [18/300], Step [250/391],                 Loss: 0.42876, Train_Acc:85.25%
Epoch [18/300], Step [260/391],                 Loss: 0.43084, Train_Acc:85.23%
Epoch [18/300], Step [270/391],                 Loss: 0.43270, Train_Acc:85.18%
Epoch [18/300], Step [280/391],                 Loss: 0.43101, Train_Acc:85.21%
Epoch [18/300], Step [290/391],                 Loss: 0.42991, Train_Acc:85.26%
Epoch [18/300], Step [300/391],                 Loss: 0.42888, Train_Acc:85.27%
Epoch [18/300], Step [310/391],                 Loss: 0.42733, Train_Acc:85.33%
Epoch [18/300], Step [320/391],                 Loss: 0.42772, Train_Acc:85.32%
Epoch [18/300], Step [330/391],                 Loss: 0.42660, Train_Acc:85.34%
Epoch [18/300], Step [340/391],                 Loss: 0.42594, Train_Acc:85.34%
Epoch [18/300], Step [350/391],                 Loss: 0.42563, Train_Acc:85.37%
Epoch [18/300], Step [360/391],                 Loss: 0.42582, Train_Acc:85.35%
Epoch [18/300], Step [370/391],                 Loss: 0.42529, Train_Acc:85.36%
Epoch [18/300], Step [380/391],                 Loss: 0.42504, Train_Acc:85.36%
Epoch [18/300], Step [390/391],                 Loss: 0.42419, Train_Acc:85.40%
Accuary on test images:75.00%
Epoch [19/300], Step [10/391],                 Loss: 0.43214, Train_Acc:85.31%
Epoch [19/300], Step [20/391],                 Loss: 0.43418, Train_Acc:84.84%
Epoch [19/300], Step [30/391],                 Loss: 0.42355, Train_Acc:85.13%
Epoch [19/300], Step [40/391],                 Loss: 0.42536, Train_Acc:85.29%
Epoch [19/300], Step [50/391],                 Loss: 0.42218, Train_Acc:85.36%
Epoch [19/300], Step [60/391],                 Loss: 0.43105, Train_Acc:85.10%
Epoch [19/300], Step [70/391],                 Loss: 0.42552, Train_Acc:85.48%
Epoch [19/300], Step [80/391],                 Loss: 0.42471, Train_Acc:85.48%
Epoch [19/300], Step [90/391],                 Loss: 0.42605, Train_Acc:85.44%
Epoch [19/300], Step [100/391],                 Loss: 0.42512, Train_Acc:85.48%
Epoch [19/300], Step [110/391],                 Loss: 0.42688, Train_Acc:85.49%
Epoch [19/300], Step [120/391],                 Loss: 0.42745, Train_Acc:85.45%
Epoch [19/300], Step [130/391],                 Loss: 0.42894, Train_Acc:85.35%
Epoch [19/300], Step [140/391],                 Loss: 0.42628, Train_Acc:85.50%
Epoch [19/300], Step [150/391],                 Loss: 0.42746, Train_Acc:85.48%
Epoch [19/300], Step [160/391],                 Loss: 0.42500, Train_Acc:85.53%
Epoch [19/300], Step [170/391],                 Loss: 0.42571, Train_Acc:85.51%
Epoch [19/300], Step [180/391],                 Loss: 0.42602, Train_Acc:85.51%
Epoch [19/300], Step [190/391],                 Loss: 0.42550, Train_Acc:85.52%
Epoch [19/300], Step [200/391],                 Loss: 0.42366, Train_Acc:85.61%
Epoch [19/300], Step [210/391],                 Loss: 0.42291, Train_Acc:85.61%
Epoch [19/300], Step [220/391],                 Loss: 0.42190, Train_Acc:85.64%
Epoch [19/300], Step [230/391],                 Loss: 0.42205, Train_Acc:85.65%
Epoch [19/300], Step [240/391],                 Loss: 0.42143, Train_Acc:85.62%
Epoch [19/300], Step [250/391],                 Loss: 0.42138, Train_Acc:85.59%
Epoch [19/300], Step [260/391],                 Loss: 0.42329, Train_Acc:85.55%
Epoch [19/300], Step [270/391],                 Loss: 0.42490, Train_Acc:85.47%
Epoch [19/300], Step [280/391],                 Loss: 0.42490, Train_Acc:85.47%
Epoch [19/300], Step [290/391],                 Loss: 0.42508, Train_Acc:85.48%
Epoch [19/300], Step [300/391],                 Loss: 0.42418, Train_Acc:85.49%
Epoch [19/300], Step [310/391],                 Loss: 0.42332, Train_Acc:85.50%
Epoch [19/300], Step [320/391],                 Loss: 0.42402, Train_Acc:85.47%
Epoch [19/300], Step [330/391],                 Loss: 0.42317, Train_Acc:85.51%
Epoch [19/300], Step [340/391],                 Loss: 0.42283, Train_Acc:85.51%
Epoch [19/300], Step [350/391],                 Loss: 0.42192, Train_Acc:85.55%
Epoch [19/300], Step [360/391],                 Loss: 0.42118, Train_Acc:85.54%
Epoch [19/300], Step [370/391],                 Loss: 0.42106, Train_Acc:85.52%
Epoch [19/300], Step [380/391],                 Loss: 0.42085, Train_Acc:85.54%
Epoch [19/300], Step [390/391],                 Loss: 0.42008, Train_Acc:85.59%
Accuary on test images:69.76%
Epoch [20/300], Step [10/391],                 Loss: 0.42128, Train_Acc:84.84%
Epoch [20/300], Step [20/391],                 Loss: 0.40567, Train_Acc:85.98%
Epoch [20/300], Step [30/391],                 Loss: 0.39555, Train_Acc:86.64%
Epoch [20/300], Step [40/391],                 Loss: 0.39172, Train_Acc:86.91%
Epoch [20/300], Step [50/391],                 Loss: 0.39154, Train_Acc:86.89%
Epoch [20/300], Step [60/391],                 Loss: 0.40005, Train_Acc:86.69%
Epoch [20/300], Step [70/391],                 Loss: 0.40530, Train_Acc:86.45%
Epoch [20/300], Step [80/391],                 Loss: 0.40709, Train_Acc:86.31%
Epoch [20/300], Step [90/391],                 Loss: 0.41428, Train_Acc:86.01%
Epoch [20/300], Step [100/391],                 Loss: 0.41371, Train_Acc:85.92%
Epoch [20/300], Step [110/391],                 Loss: 0.41461, Train_Acc:85.89%
Epoch [20/300], Step [120/391],                 Loss: 0.41428, Train_Acc:85.82%
Epoch [20/300], Step [130/391],                 Loss: 0.41512, Train_Acc:85.80%
Epoch [20/300], Step [140/391],                 Loss: 0.41507, Train_Acc:85.77%
Epoch [20/300], Step [150/391],                 Loss: 0.41398, Train_Acc:85.80%
Epoch [20/300], Step [160/391],                 Loss: 0.41499, Train_Acc:85.80%
Epoch [20/300], Step [170/391],                 Loss: 0.41613, Train_Acc:85.80%
Epoch [20/300], Step [180/391],                 Loss: 0.41668, Train_Acc:85.74%
Epoch [20/300], Step [190/391],                 Loss: 0.41789, Train_Acc:85.70%
Epoch [20/300], Step [200/391],                 Loss: 0.41874, Train_Acc:85.66%
Epoch [20/300], Step [210/391],                 Loss: 0.41929, Train_Acc:85.63%
Epoch [20/300], Step [220/391],                 Loss: 0.42006, Train_Acc:85.61%
Epoch [20/300], Step [230/391],                 Loss: 0.42058, Train_Acc:85.58%
Epoch [20/300], Step [240/391],                 Loss: 0.41921, Train_Acc:85.63%
Epoch [20/300], Step [250/391],                 Loss: 0.41984, Train_Acc:85.61%
Epoch [20/300], Step [260/391],                 Loss: 0.42304, Train_Acc:85.53%
Epoch [20/300], Step [270/391],                 Loss: 0.42506, Train_Acc:85.48%
Epoch [20/300], Step [280/391],                 Loss: 0.42586, Train_Acc:85.46%
Epoch [20/300], Step [290/391],                 Loss: 0.42514, Train_Acc:85.51%
Epoch [20/300], Step [300/391],                 Loss: 0.42428, Train_Acc:85.51%
Epoch [20/300], Step [310/391],                 Loss: 0.42323, Train_Acc:85.56%
Epoch [20/300], Step [320/391],                 Loss: 0.42368, Train_Acc:85.53%
Epoch [20/300], Step [330/391],                 Loss: 0.42197, Train_Acc:85.59%
Epoch [20/300], Step [340/391],                 Loss: 0.42167, Train_Acc:85.60%
Epoch [20/300], Step [350/391],                 Loss: 0.42106, Train_Acc:85.62%
Epoch [20/300], Step [360/391],                 Loss: 0.42095, Train_Acc:85.61%
Epoch [20/300], Step [370/391],                 Loss: 0.42156, Train_Acc:85.59%
Epoch [20/300], Step [380/391],                 Loss: 0.42151, Train_Acc:85.58%
Epoch [20/300], Step [390/391],                 Loss: 0.42220, Train_Acc:85.56%
Accuary on test images:71.22%
Epoch [21/300], Step [10/391],                 Loss: 0.41501, Train_Acc:85.94%
Epoch [21/300], Step [20/391],                 Loss: 0.40740, Train_Acc:86.21%
Epoch [21/300], Step [30/391],                 Loss: 0.40050, Train_Acc:86.59%
Epoch [21/300], Step [40/391],                 Loss: 0.40260, Train_Acc:86.64%
Epoch [21/300], Step [50/391],                 Loss: 0.40129, Train_Acc:86.48%
Epoch [21/300], Step [60/391],                 Loss: 0.40242, Train_Acc:86.28%
Epoch [21/300], Step [70/391],                 Loss: 0.40174, Train_Acc:86.28%
Epoch [21/300], Step [80/391],                 Loss: 0.40294, Train_Acc:86.33%
Epoch [21/300], Step [90/391],                 Loss: 0.40742, Train_Acc:86.03%
Epoch [21/300], Step [100/391],                 Loss: 0.40609, Train_Acc:86.12%
Epoch [21/300], Step [110/391],                 Loss: 0.40817, Train_Acc:86.09%
Epoch [21/300], Step [120/391],                 Loss: 0.41024, Train_Acc:85.94%
Epoch [21/300], Step [130/391],                 Loss: 0.41288, Train_Acc:85.92%
Epoch [21/300], Step [140/391],                 Loss: 0.41120, Train_Acc:86.00%
Epoch [21/300], Step [150/391],                 Loss: 0.41110, Train_Acc:86.03%
Epoch [21/300], Step [160/391],                 Loss: 0.41012, Train_Acc:86.11%
Epoch [21/300], Step [170/391],                 Loss: 0.41003, Train_Acc:86.08%
Epoch [21/300], Step [180/391],                 Loss: 0.41154, Train_Acc:85.99%
Epoch [21/300], Step [190/391],                 Loss: 0.41278, Train_Acc:85.98%
Epoch [21/300], Step [200/391],                 Loss: 0.41380, Train_Acc:85.95%
Epoch [21/300], Step [210/391],                 Loss: 0.41414, Train_Acc:85.90%
Epoch [21/300], Step [220/391],                 Loss: 0.41401, Train_Acc:85.90%
Epoch [21/300], Step [230/391],                 Loss: 0.41222, Train_Acc:85.94%
Epoch [21/300], Step [240/391],                 Loss: 0.41082, Train_Acc:86.03%
Epoch [21/300], Step [250/391],                 Loss: 0.41070, Train_Acc:86.02%
Epoch [21/300], Step [260/391],                 Loss: 0.41191, Train_Acc:86.01%
Epoch [21/300], Step [270/391],                 Loss: 0.41457, Train_Acc:85.93%
Epoch [21/300], Step [280/391],                 Loss: 0.41544, Train_Acc:85.90%
Epoch [21/300], Step [290/391],                 Loss: 0.41514, Train_Acc:85.92%
Epoch [21/300], Step [300/391],                 Loss: 0.41537, Train_Acc:85.91%
Epoch [21/300], Step [310/391],                 Loss: 0.41395, Train_Acc:85.96%
Epoch [21/300], Step [320/391],                 Loss: 0.41414, Train_Acc:85.95%
Epoch [21/300], Step [330/391],                 Loss: 0.41361, Train_Acc:85.98%
Epoch [21/300], Step [340/391],                 Loss: 0.41252, Train_Acc:86.04%
Epoch [21/300], Step [350/391],                 Loss: 0.41235, Train_Acc:86.06%
Epoch [21/300], Step [360/391],                 Loss: 0.41166, Train_Acc:86.07%
Epoch [21/300], Step [370/391],                 Loss: 0.41168, Train_Acc:86.05%
Epoch [21/300], Step [380/391],                 Loss: 0.41159, Train_Acc:86.06%
Epoch [21/300], Step [390/391],                 Loss: 0.41100, Train_Acc:86.06%
Accuary on test images:71.12%
Epoch [22/300], Step [10/391],                 Loss: 0.41588, Train_Acc:86.48%
Epoch [22/300], Step [20/391],                 Loss: 0.41095, Train_Acc:86.33%
Epoch [22/300], Step [30/391],                 Loss: 0.40591, Train_Acc:86.25%
Epoch [22/300], Step [40/391],                 Loss: 0.40413, Train_Acc:86.43%
Epoch [22/300], Step [50/391],                 Loss: 0.40267, Train_Acc:86.38%
Epoch [22/300], Step [60/391],                 Loss: 0.40254, Train_Acc:86.35%
Epoch [22/300], Step [70/391],                 Loss: 0.39927, Train_Acc:86.35%
Epoch [22/300], Step [80/391],                 Loss: 0.39750, Train_Acc:86.46%
Epoch [22/300], Step [90/391],                 Loss: 0.40100, Train_Acc:86.41%
Epoch [22/300], Step [100/391],                 Loss: 0.40226, Train_Acc:86.41%
Epoch [22/300], Step [110/391],                 Loss: 0.40530, Train_Acc:86.36%
Epoch [22/300], Step [120/391],                 Loss: 0.40747, Train_Acc:86.32%
Epoch [22/300], Step [130/391],                 Loss: 0.41268, Train_Acc:86.12%
Epoch [22/300], Step [140/391],                 Loss: 0.41291, Train_Acc:86.05%
Epoch [22/300], Step [150/391],                 Loss: 0.41414, Train_Acc:86.03%
Epoch [22/300], Step [160/391],                 Loss: 0.41137, Train_Acc:86.14%
Epoch [22/300], Step [170/391],                 Loss: 0.41178, Train_Acc:86.16%
Epoch [22/300], Step [180/391],                 Loss: 0.41126, Train_Acc:86.19%
Epoch [22/300], Step [190/391],                 Loss: 0.41128, Train_Acc:86.17%
Epoch [22/300], Step [200/391],                 Loss: 0.41223, Train_Acc:86.12%
Epoch [22/300], Step [210/391],                 Loss: 0.41269, Train_Acc:86.10%
Epoch [22/300], Step [220/391],                 Loss: 0.41398, Train_Acc:86.06%
Epoch [22/300], Step [230/391],                 Loss: 0.41490, Train_Acc:86.01%
Epoch [22/300], Step [240/391],                 Loss: 0.41414, Train_Acc:86.04%
Epoch [22/300], Step [250/391],                 Loss: 0.41385, Train_Acc:86.02%
Epoch [22/300], Step [260/391],                 Loss: 0.41548, Train_Acc:85.96%
Epoch [22/300], Step [270/391],                 Loss: 0.41630, Train_Acc:85.91%
Epoch [22/300], Step [280/391],                 Loss: 0.41537, Train_Acc:85.97%
Epoch [22/300], Step [290/391],                 Loss: 0.41563, Train_Acc:85.98%
Epoch [22/300], Step [300/391],                 Loss: 0.41516, Train_Acc:85.98%
Epoch [22/300], Step [310/391],                 Loss: 0.41398, Train_Acc:86.02%
Epoch [22/300], Step [320/391],                 Loss: 0.41306, Train_Acc:86.08%
Epoch [22/300], Step [330/391],                 Loss: 0.41185, Train_Acc:86.12%
Epoch [22/300], Step [340/391],                 Loss: 0.41131, Train_Acc:86.15%
Epoch [22/300], Step [350/391],                 Loss: 0.41134, Train_Acc:86.13%
Epoch [22/300], Step [360/391],                 Loss: 0.41138, Train_Acc:86.10%
Epoch [22/300], Step [370/391],                 Loss: 0.41146, Train_Acc:86.07%
Epoch [22/300], Step [380/391],                 Loss: 0.41040, Train_Acc:86.09%
Epoch [22/300], Step [390/391],                 Loss: 0.40961, Train_Acc:86.12%
Accuary on test images:72.32%
Epoch [23/300], Step [10/391],                 Loss: 0.41308, Train_Acc:85.16%
Epoch [23/300], Step [20/391],                 Loss: 0.41999, Train_Acc:85.35%
Epoch [23/300], Step [30/391],                 Loss: 0.42348, Train_Acc:85.10%
Epoch [23/300], Step [40/391],                 Loss: 0.42367, Train_Acc:85.21%
Epoch [23/300], Step [50/391],                 Loss: 0.42322, Train_Acc:85.20%
Epoch [23/300], Step [60/391],                 Loss: 0.42500, Train_Acc:85.25%
Epoch [23/300], Step [70/391],                 Loss: 0.42368, Train_Acc:85.41%
Epoch [23/300], Step [80/391],                 Loss: 0.42454, Train_Acc:85.38%
Epoch [23/300], Step [90/391],                 Loss: 0.42641, Train_Acc:85.32%
Epoch [23/300], Step [100/391],                 Loss: 0.42544, Train_Acc:85.32%
Epoch [23/300], Step [110/391],                 Loss: 0.42532, Train_Acc:85.44%
Epoch [23/300], Step [120/391],                 Loss: 0.42265, Train_Acc:85.50%
Epoch [23/300], Step [130/391],                 Loss: 0.42522, Train_Acc:85.46%
Epoch [23/300], Step [140/391],                 Loss: 0.42126, Train_Acc:85.61%
Epoch [23/300], Step [150/391],                 Loss: 0.41993, Train_Acc:85.65%
Epoch [23/300], Step [160/391],                 Loss: 0.41738, Train_Acc:85.75%
Epoch [23/300], Step [170/391],                 Loss: 0.41694, Train_Acc:85.78%
Epoch [23/300], Step [180/391],                 Loss: 0.41657, Train_Acc:85.80%
Epoch [23/300], Step [190/391],                 Loss: 0.41689, Train_Acc:85.78%
Epoch [23/300], Step [200/391],                 Loss: 0.41778, Train_Acc:85.77%
Epoch [23/300], Step [210/391],                 Loss: 0.41738, Train_Acc:85.77%
Epoch [23/300], Step [220/391],                 Loss: 0.41717, Train_Acc:85.78%
Epoch [23/300], Step [230/391],                 Loss: 0.41586, Train_Acc:85.82%
Epoch [23/300], Step [240/391],                 Loss: 0.41444, Train_Acc:85.91%
Epoch [23/300], Step [250/391],                 Loss: 0.41591, Train_Acc:85.82%
Epoch [23/300], Step [260/391],                 Loss: 0.41799, Train_Acc:85.75%
Epoch [23/300], Step [270/391],                 Loss: 0.41912, Train_Acc:85.67%
Epoch [23/300], Step [280/391],                 Loss: 0.41835, Train_Acc:85.73%
Epoch [23/300], Step [290/391],                 Loss: 0.41760, Train_Acc:85.77%
Epoch [23/300], Step [300/391],                 Loss: 0.41660, Train_Acc:85.84%
Epoch [23/300], Step [310/391],                 Loss: 0.41510, Train_Acc:85.90%
Epoch [23/300], Step [320/391],                 Loss: 0.41649, Train_Acc:85.87%
Epoch [23/300], Step [330/391],                 Loss: 0.41559, Train_Acc:85.88%
Epoch [23/300], Step [340/391],                 Loss: 0.41552, Train_Acc:85.88%
Epoch [23/300], Step [350/391],                 Loss: 0.41474, Train_Acc:85.91%
Epoch [23/300], Step [360/391],                 Loss: 0.41540, Train_Acc:85.87%
Epoch [23/300], Step [370/391],                 Loss: 0.41506, Train_Acc:85.86%
Epoch [23/300], Step [380/391],                 Loss: 0.41448, Train_Acc:85.89%
Epoch [23/300], Step [390/391],                 Loss: 0.41278, Train_Acc:85.95%
Accuary on test images:74.72%
Epoch [24/300], Step [10/391],                 Loss: 0.42277, Train_Acc:85.39%
Epoch [24/300], Step [20/391],                 Loss: 0.42611, Train_Acc:85.43%
Epoch [24/300], Step [30/391],                 Loss: 0.42173, Train_Acc:85.68%
Epoch [24/300], Step [40/391],                 Loss: 0.42190, Train_Acc:85.78%
Epoch [24/300], Step [50/391],                 Loss: 0.42388, Train_Acc:85.66%
Epoch [24/300], Step [60/391],                 Loss: 0.42737, Train_Acc:85.61%
Epoch [24/300], Step [70/391],                 Loss: 0.42192, Train_Acc:85.79%
Epoch [24/300], Step [80/391],                 Loss: 0.41526, Train_Acc:86.13%
Epoch [24/300], Step [90/391],                 Loss: 0.41615, Train_Acc:86.03%
Epoch [24/300], Step [100/391],                 Loss: 0.41185, Train_Acc:86.20%
Epoch [24/300], Step [110/391],                 Loss: 0.41327, Train_Acc:86.08%
Epoch [24/300], Step [120/391],                 Loss: 0.41297, Train_Acc:86.03%
Epoch [24/300], Step [130/391],                 Loss: 0.41720, Train_Acc:85.96%
Epoch [24/300], Step [140/391],                 Loss: 0.41542, Train_Acc:85.99%
Epoch [24/300], Step [150/391],                 Loss: 0.41641, Train_Acc:85.92%
Epoch [24/300], Step [160/391],                 Loss: 0.41493, Train_Acc:85.95%
Epoch [24/300], Step [170/391],                 Loss: 0.41660, Train_Acc:85.80%
Epoch [24/300], Step [180/391],                 Loss: 0.41772, Train_Acc:85.77%
Epoch [24/300], Step [190/391],                 Loss: 0.41707, Train_Acc:85.83%
Epoch [24/300], Step [200/391],                 Loss: 0.41818, Train_Acc:85.77%
Epoch [24/300], Step [210/391],                 Loss: 0.41728, Train_Acc:85.78%
Epoch [24/300], Step [220/391],                 Loss: 0.41459, Train_Acc:85.89%
Epoch [24/300], Step [230/391],                 Loss: 0.41147, Train_Acc:86.02%
Epoch [24/300], Step [240/391],                 Loss: 0.40887, Train_Acc:86.12%
Epoch [24/300], Step [250/391],                 Loss: 0.40883, Train_Acc:86.08%
Epoch [24/300], Step [260/391],                 Loss: 0.41103, Train_Acc:85.99%
Epoch [24/300], Step [270/391],                 Loss: 0.41193, Train_Acc:85.93%
Epoch [24/300], Step [280/391],                 Loss: 0.41209, Train_Acc:85.94%
Epoch [24/300], Step [290/391],                 Loss: 0.41089, Train_Acc:85.95%
Epoch [24/300], Step [300/391],                 Loss: 0.40934, Train_Acc:85.99%
Epoch [24/300], Step [310/391],                 Loss: 0.40729, Train_Acc:86.06%
Epoch [24/300], Step [320/391],                 Loss: 0.40733, Train_Acc:86.07%
Epoch [24/300], Step [330/391],                 Loss: 0.40657, Train_Acc:86.09%
Epoch [24/300], Step [340/391],                 Loss: 0.40606, Train_Acc:86.12%
Epoch [24/300], Step [350/391],                 Loss: 0.40612, Train_Acc:86.11%
Epoch [24/300], Step [360/391],                 Loss: 0.40689, Train_Acc:86.08%
Epoch [24/300], Step [370/391],                 Loss: 0.40838, Train_Acc:86.01%
Epoch [24/300], Step [380/391],                 Loss: 0.40840, Train_Acc:86.02%
Epoch [24/300], Step [390/391],                 Loss: 0.40808, Train_Acc:86.06%
Accuary on test images:74.82%
Epoch [25/300], Step [10/391],                 Loss: 0.41344, Train_Acc:85.31%
Epoch [25/300], Step [20/391],                 Loss: 0.40087, Train_Acc:86.09%
Epoch [25/300], Step [30/391],                 Loss: 0.39484, Train_Acc:86.25%
Epoch [25/300], Step [40/391],                 Loss: 0.39374, Train_Acc:86.68%
Epoch [25/300], Step [50/391],                 Loss: 0.39637, Train_Acc:86.38%
Epoch [25/300], Step [60/391],                 Loss: 0.40013, Train_Acc:86.20%
Epoch [25/300], Step [70/391],                 Loss: 0.39995, Train_Acc:86.25%
Epoch [25/300], Step [80/391],                 Loss: 0.40408, Train_Acc:86.07%
Epoch [25/300], Step [90/391],                 Loss: 0.40904, Train_Acc:86.04%
Epoch [25/300], Step [100/391],                 Loss: 0.40788, Train_Acc:86.11%
Epoch [25/300], Step [110/391],                 Loss: 0.40841, Train_Acc:86.18%
Epoch [25/300], Step [120/391],                 Loss: 0.40852, Train_Acc:86.17%
Epoch [25/300], Step [130/391],                 Loss: 0.40860, Train_Acc:86.18%
Epoch [25/300], Step [140/391],                 Loss: 0.40657, Train_Acc:86.27%
Epoch [25/300], Step [150/391],                 Loss: 0.40570, Train_Acc:86.31%
Epoch [25/300], Step [160/391],                 Loss: 0.40161, Train_Acc:86.44%
Epoch [25/300], Step [170/391],                 Loss: 0.40066, Train_Acc:86.43%
Epoch [25/300], Step [180/391],                 Loss: 0.39996, Train_Acc:86.41%
Epoch [25/300], Step [190/391],                 Loss: 0.40035, Train_Acc:86.43%
Epoch [25/300], Step [200/391],                 Loss: 0.40135, Train_Acc:86.39%
Epoch [25/300], Step [210/391],                 Loss: 0.40311, Train_Acc:86.30%
Epoch [25/300], Step [220/391],                 Loss: 0.40382, Train_Acc:86.27%
Epoch [25/300], Step [230/391],                 Loss: 0.40253, Train_Acc:86.31%
Epoch [25/300], Step [240/391],                 Loss: 0.40063, Train_Acc:86.33%
Epoch [25/300], Step [250/391],                 Loss: 0.39943, Train_Acc:86.36%
Epoch [25/300], Step [260/391],                 Loss: 0.40132, Train_Acc:86.36%
Epoch [25/300], Step [270/391],                 Loss: 0.40179, Train_Acc:86.38%
Epoch [25/300], Step [280/391],                 Loss: 0.40232, Train_Acc:86.37%
Epoch [25/300], Step [290/391],                 Loss: 0.40160, Train_Acc:86.40%
Epoch [25/300], Step [300/391],                 Loss: 0.40107, Train_Acc:86.37%
Epoch [25/300], Step [310/391],                 Loss: 0.39937, Train_Acc:86.44%
Epoch [25/300], Step [320/391],                 Loss: 0.40064, Train_Acc:86.39%
Epoch [25/300], Step [330/391],                 Loss: 0.40059, Train_Acc:86.38%
Epoch [25/300], Step [340/391],                 Loss: 0.40024, Train_Acc:86.39%
Epoch [25/300], Step [350/391],                 Loss: 0.39982, Train_Acc:86.39%
Epoch [25/300], Step [360/391],                 Loss: 0.40010, Train_Acc:86.37%
Epoch [25/300], Step [370/391],                 Loss: 0.40014, Train_Acc:86.36%
Epoch [25/300], Step [380/391],                 Loss: 0.39965, Train_Acc:86.38%
Epoch [25/300], Step [390/391],                 Loss: 0.39870, Train_Acc:86.40%
Accuary on test images:74.92%
Epoch [26/300], Step [10/391],                 Loss: 0.41650, Train_Acc:86.09%
Epoch [26/300], Step [20/391],                 Loss: 0.39310, Train_Acc:86.64%
Epoch [26/300], Step [30/391],                 Loss: 0.38611, Train_Acc:87.03%
Epoch [26/300], Step [40/391],                 Loss: 0.38124, Train_Acc:87.42%
Epoch [26/300], Step [50/391],                 Loss: 0.38330, Train_Acc:87.34%
Epoch [26/300], Step [60/391],                 Loss: 0.39115, Train_Acc:86.98%
Epoch [26/300], Step [70/391],                 Loss: 0.38763, Train_Acc:87.09%
Epoch [26/300], Step [80/391],                 Loss: 0.38481, Train_Acc:87.19%
Epoch [26/300], Step [90/391],                 Loss: 0.38976, Train_Acc:86.99%
Epoch [26/300], Step [100/391],                 Loss: 0.39051, Train_Acc:86.92%
Epoch [26/300], Step [110/391],                 Loss: 0.39408, Train_Acc:86.74%
Epoch [26/300], Step [120/391],                 Loss: 0.39288, Train_Acc:86.76%
Epoch [26/300], Step [130/391],                 Loss: 0.39516, Train_Acc:86.67%
Epoch [26/300], Step [140/391],                 Loss: 0.39337, Train_Acc:86.75%
Epoch [26/300], Step [150/391],                 Loss: 0.39414, Train_Acc:86.72%
Epoch [26/300], Step [160/391],                 Loss: 0.39444, Train_Acc:86.70%
Epoch [26/300], Step [170/391],                 Loss: 0.39648, Train_Acc:86.59%
Epoch [26/300], Step [180/391],                 Loss: 0.39871, Train_Acc:86.44%
Epoch [26/300], Step [190/391],                 Loss: 0.40118, Train_Acc:86.37%
Epoch [26/300], Step [200/391],                 Loss: 0.40276, Train_Acc:86.34%
Epoch [26/300], Step [210/391],                 Loss: 0.40323, Train_Acc:86.30%
Epoch [26/300], Step [220/391],                 Loss: 0.40255, Train_Acc:86.34%
Epoch [26/300], Step [230/391],                 Loss: 0.40286, Train_Acc:86.29%
Epoch [26/300], Step [240/391],                 Loss: 0.40166, Train_Acc:86.33%
Epoch [26/300], Step [250/391],                 Loss: 0.40162, Train_Acc:86.33%
Epoch [26/300], Step [260/391],                 Loss: 0.40341, Train_Acc:86.28%
Epoch [26/300], Step [270/391],                 Loss: 0.40610, Train_Acc:86.19%
Epoch [26/300], Step [280/391],                 Loss: 0.40533, Train_Acc:86.26%
Epoch [26/300], Step [290/391],                 Loss: 0.40554, Train_Acc:86.26%
Epoch [26/300], Step [300/391],                 Loss: 0.40478, Train_Acc:86.27%
Epoch [26/300], Step [310/391],                 Loss: 0.40372, Train_Acc:86.33%
Epoch [26/300], Step [320/391],                 Loss: 0.40436, Train_Acc:86.33%
Epoch [26/300], Step [330/391],                 Loss: 0.40356, Train_Acc:86.39%
Epoch [26/300], Step [340/391],                 Loss: 0.40378, Train_Acc:86.41%
Epoch [26/300], Step [350/391],                 Loss: 0.40339, Train_Acc:86.43%
Epoch [26/300], Step [360/391],                 Loss: 0.40328, Train_Acc:86.45%
Epoch [26/300], Step [370/391],                 Loss: 0.40392, Train_Acc:86.42%
Epoch [26/300], Step [380/391],                 Loss: 0.40355, Train_Acc:86.42%
Epoch [26/300], Step [390/391],                 Loss: 0.40310, Train_Acc:86.44%
Accuary on test images:76.32%
Epoch [27/300], Step [10/391],                 Loss: 0.40878, Train_Acc:85.86%
Epoch [27/300], Step [20/391],                 Loss: 0.40909, Train_Acc:86.09%
Epoch [27/300], Step [30/391],                 Loss: 0.40199, Train_Acc:86.12%
Epoch [27/300], Step [40/391],                 Loss: 0.40463, Train_Acc:86.17%
Epoch [27/300], Step [50/391],                 Loss: 0.40267, Train_Acc:86.36%
Epoch [27/300], Step [60/391],                 Loss: 0.40893, Train_Acc:86.12%
Epoch [27/300], Step [70/391],                 Loss: 0.40383, Train_Acc:86.34%
Epoch [27/300], Step [80/391],                 Loss: 0.40557, Train_Acc:86.22%
Epoch [27/300], Step [90/391],                 Loss: 0.40852, Train_Acc:86.18%
Epoch [27/300], Step [100/391],                 Loss: 0.40579, Train_Acc:86.23%
Epoch [27/300], Step [110/391],                 Loss: 0.40694, Train_Acc:86.19%
Epoch [27/300], Step [120/391],                 Loss: 0.40951, Train_Acc:86.07%
Epoch [27/300], Step [130/391],                 Loss: 0.41140, Train_Acc:85.98%
Epoch [27/300], Step [140/391],                 Loss: 0.41005, Train_Acc:86.04%
Epoch [27/300], Step [150/391],                 Loss: 0.40853, Train_Acc:86.11%
Epoch [27/300], Step [160/391],                 Loss: 0.40777, Train_Acc:86.06%
Epoch [27/300], Step [170/391],                 Loss: 0.40850, Train_Acc:86.02%
Epoch [27/300], Step [180/391],                 Loss: 0.41056, Train_Acc:85.96%
Epoch [27/300], Step [190/391],                 Loss: 0.41021, Train_Acc:86.02%
Epoch [27/300], Step [200/391],                 Loss: 0.40974, Train_Acc:86.04%
Epoch [27/300], Step [210/391],                 Loss: 0.40922, Train_Acc:86.03%
Epoch [27/300], Step [220/391],                 Loss: 0.40954, Train_Acc:86.06%
Epoch [27/300], Step [230/391],                 Loss: 0.40726, Train_Acc:86.16%
Epoch [27/300], Step [240/391],                 Loss: 0.40616, Train_Acc:86.17%
Epoch [27/300], Step [250/391],                 Loss: 0.40534, Train_Acc:86.18%
Epoch [27/300], Step [260/391],                 Loss: 0.40599, Train_Acc:86.16%
Epoch [27/300], Step [270/391],                 Loss: 0.40704, Train_Acc:86.11%
Epoch [27/300], Step [280/391],                 Loss: 0.40678, Train_Acc:86.11%
Epoch [27/300], Step [290/391],                 Loss: 0.40725, Train_Acc:86.10%
Epoch [27/300], Step [300/391],                 Loss: 0.40650, Train_Acc:86.11%
Epoch [27/300], Step [310/391],                 Loss: 0.40498, Train_Acc:86.15%
Epoch [27/300], Step [320/391],                 Loss: 0.40561, Train_Acc:86.12%
Epoch [27/300], Step [330/391],                 Loss: 0.40427, Train_Acc:86.16%
Epoch [27/300], Step [340/391],                 Loss: 0.40336, Train_Acc:86.19%
Epoch [27/300], Step [350/391],                 Loss: 0.40310, Train_Acc:86.21%
Epoch [27/300], Step [360/391],                 Loss: 0.40261, Train_Acc:86.23%
Epoch [27/300], Step [370/391],                 Loss: 0.40281, Train_Acc:86.21%
Epoch [27/300], Step [380/391],                 Loss: 0.40179, Train_Acc:86.25%
Epoch [27/300], Step [390/391],                 Loss: 0.40081, Train_Acc:86.27%
Accuary on test images:75.38%
Epoch [28/300], Step [10/391],                 Loss: 0.38344, Train_Acc:86.56%
Epoch [28/300], Step [20/391],                 Loss: 0.37864, Train_Acc:87.15%
Epoch [28/300], Step [30/391],                 Loss: 0.38446, Train_Acc:86.74%
Epoch [28/300], Step [40/391],                 Loss: 0.39457, Train_Acc:86.46%
Epoch [28/300], Step [50/391],                 Loss: 0.39249, Train_Acc:86.33%
Epoch [28/300], Step [60/391],                 Loss: 0.39396, Train_Acc:86.35%
Epoch [28/300], Step [70/391],                 Loss: 0.39250, Train_Acc:86.48%
Epoch [28/300], Step [80/391],                 Loss: 0.39428, Train_Acc:86.31%
Epoch [28/300], Step [90/391],                 Loss: 0.39536, Train_Acc:86.28%
Epoch [28/300], Step [100/391],                 Loss: 0.39233, Train_Acc:86.37%
Epoch [28/300], Step [110/391],                 Loss: 0.39389, Train_Acc:86.42%
Epoch [28/300], Step [120/391],                 Loss: 0.39311, Train_Acc:86.36%
Epoch [28/300], Step [130/391],                 Loss: 0.39508, Train_Acc:86.37%
Epoch [28/300], Step [140/391],                 Loss: 0.39158, Train_Acc:86.54%
Epoch [28/300], Step [150/391],                 Loss: 0.39236, Train_Acc:86.53%
Epoch [28/300], Step [160/391],                 Loss: 0.39007, Train_Acc:86.68%
Epoch [28/300], Step [170/391],                 Loss: 0.39159, Train_Acc:86.62%
Epoch [28/300], Step [180/391],                 Loss: 0.39549, Train_Acc:86.46%
Epoch [28/300], Step [190/391],                 Loss: 0.39567, Train_Acc:86.44%
Epoch [28/300], Step [200/391],                 Loss: 0.39760, Train_Acc:86.39%
Epoch [28/300], Step [210/391],                 Loss: 0.39758, Train_Acc:86.42%
Epoch [28/300], Step [220/391],                 Loss: 0.39893, Train_Acc:86.35%
Epoch [28/300], Step [230/391],                 Loss: 0.39802, Train_Acc:86.37%
Epoch [28/300], Step [240/391],                 Loss: 0.39656, Train_Acc:86.45%
Epoch [28/300], Step [250/391],                 Loss: 0.39613, Train_Acc:86.49%
Epoch [28/300], Step [260/391],                 Loss: 0.39798, Train_Acc:86.48%
Epoch [28/300], Step [270/391],                 Loss: 0.39897, Train_Acc:86.47%
Epoch [28/300], Step [280/391],                 Loss: 0.39860, Train_Acc:86.47%
Epoch [28/300], Step [290/391],                 Loss: 0.39912, Train_Acc:86.46%
Epoch [28/300], Step [300/391],                 Loss: 0.39836, Train_Acc:86.48%
Epoch [28/300], Step [310/391],                 Loss: 0.39781, Train_Acc:86.48%
Epoch [28/300], Step [320/391],                 Loss: 0.39864, Train_Acc:86.46%
Epoch [28/300], Step [330/391],                 Loss: 0.39822, Train_Acc:86.48%
Epoch [28/300], Step [340/391],                 Loss: 0.39788, Train_Acc:86.48%
Epoch [28/300], Step [350/391],                 Loss: 0.39724, Train_Acc:86.52%
Epoch [28/300], Step [360/391],                 Loss: 0.39782, Train_Acc:86.44%
Epoch [28/300], Step [370/391],                 Loss: 0.39954, Train_Acc:86.38%
Epoch [28/300], Step [380/391],                 Loss: 0.40036, Train_Acc:86.36%
Epoch [28/300], Step [390/391],                 Loss: 0.40028, Train_Acc:86.35%
Accuary on test images:72.44%
Epoch [29/300], Step [10/391],                 Loss: 0.37320, Train_Acc:88.44%
Epoch [29/300], Step [20/391],                 Loss: 0.37288, Train_Acc:88.20%
Epoch [29/300], Step [30/391],                 Loss: 0.36299, Train_Acc:88.28%
Epoch [29/300], Step [40/391],                 Loss: 0.36234, Train_Acc:88.05%
Epoch [29/300], Step [50/391],                 Loss: 0.36891, Train_Acc:87.59%
Epoch [29/300], Step [60/391],                 Loss: 0.38134, Train_Acc:86.98%
Epoch [29/300], Step [70/391],                 Loss: 0.38118, Train_Acc:87.01%
Epoch [29/300], Step [80/391],                 Loss: 0.38187, Train_Acc:86.89%
Epoch [29/300], Step [90/391],                 Loss: 0.38684, Train_Acc:86.81%
Epoch [29/300], Step [100/391],                 Loss: 0.38836, Train_Acc:86.77%
Epoch [29/300], Step [110/391],                 Loss: 0.39245, Train_Acc:86.60%
Epoch [29/300], Step [120/391],                 Loss: 0.39374, Train_Acc:86.60%
Epoch [29/300], Step [130/391],                 Loss: 0.39557, Train_Acc:86.55%
Epoch [29/300], Step [140/391],                 Loss: 0.39218, Train_Acc:86.67%
Epoch [29/300], Step [150/391],                 Loss: 0.39410, Train_Acc:86.70%
Epoch [29/300], Step [160/391],                 Loss: 0.39538, Train_Acc:86.66%
Epoch [29/300], Step [170/391],                 Loss: 0.39736, Train_Acc:86.56%
Epoch [29/300], Step [180/391],                 Loss: 0.39842, Train_Acc:86.55%
Epoch [29/300], Step [190/391],                 Loss: 0.39993, Train_Acc:86.54%
Epoch [29/300], Step [200/391],                 Loss: 0.40036, Train_Acc:86.54%
Epoch [29/300], Step [210/391],                 Loss: 0.39878, Train_Acc:86.56%
Epoch [29/300], Step [220/391],                 Loss: 0.39938, Train_Acc:86.54%
Epoch [29/300], Step [230/391],                 Loss: 0.39952, Train_Acc:86.54%
Epoch [29/300], Step [240/391],                 Loss: 0.39894, Train_Acc:86.53%
Epoch [29/300], Step [250/391],                 Loss: 0.39975, Train_Acc:86.49%
Epoch [29/300], Step [260/391],                 Loss: 0.40082, Train_Acc:86.43%
Epoch [29/300], Step [270/391],                 Loss: 0.40232, Train_Acc:86.34%
Epoch [29/300], Step [280/391],                 Loss: 0.40222, Train_Acc:86.31%
Epoch [29/300], Step [290/391],                 Loss: 0.40250, Train_Acc:86.34%
Epoch [29/300], Step [300/391],                 Loss: 0.40166, Train_Acc:86.37%
Epoch [29/300], Step [310/391],                 Loss: 0.40103, Train_Acc:86.37%
Epoch [29/300], Step [320/391],                 Loss: 0.40152, Train_Acc:86.37%
Epoch [29/300], Step [330/391],                 Loss: 0.40012, Train_Acc:86.43%
Epoch [29/300], Step [340/391],                 Loss: 0.40018, Train_Acc:86.45%
Epoch [29/300], Step [350/391],                 Loss: 0.40009, Train_Acc:86.46%
Epoch [29/300], Step [360/391],                 Loss: 0.39996, Train_Acc:86.45%
Epoch [29/300], Step [370/391],                 Loss: 0.40063, Train_Acc:86.43%
Epoch [29/300], Step [380/391],                 Loss: 0.40103, Train_Acc:86.42%
Epoch [29/300], Step [390/391],                 Loss: 0.40038, Train_Acc:86.44%
Accuary on test images:75.68%
Epoch [30/300], Step [10/391],                 Loss: 0.37924, Train_Acc:86.41%
Epoch [30/300], Step [20/391],                 Loss: 0.39789, Train_Acc:86.25%
Epoch [30/300], Step [30/391],                 Loss: 0.39229, Train_Acc:86.30%
Epoch [30/300], Step [40/391],                 Loss: 0.38563, Train_Acc:86.68%
Epoch [30/300], Step [50/391],                 Loss: 0.38377, Train_Acc:86.78%
Epoch [30/300], Step [60/391],                 Loss: 0.38734, Train_Acc:86.67%
Epoch [30/300], Step [70/391],                 Loss: 0.38598, Train_Acc:86.71%
Epoch [30/300], Step [80/391],                 Loss: 0.38993, Train_Acc:86.64%
Epoch [30/300], Step [90/391],                 Loss: 0.39677, Train_Acc:86.40%
Epoch [30/300], Step [100/391],                 Loss: 0.39764, Train_Acc:86.37%
Epoch [30/300], Step [110/391],                 Loss: 0.39924, Train_Acc:86.34%
Epoch [30/300], Step [120/391],                 Loss: 0.39720, Train_Acc:86.43%
Epoch [30/300], Step [130/391],                 Loss: 0.39738, Train_Acc:86.37%
Epoch [30/300], Step [140/391],                 Loss: 0.39404, Train_Acc:86.53%
Epoch [30/300], Step [150/391],                 Loss: 0.39609, Train_Acc:86.44%
Epoch [30/300], Step [160/391],                 Loss: 0.39619, Train_Acc:86.44%
Epoch [30/300], Step [170/391],                 Loss: 0.39647, Train_Acc:86.41%
Epoch [30/300], Step [180/391],                 Loss: 0.39661, Train_Acc:86.35%
Epoch [30/300], Step [190/391],                 Loss: 0.39754, Train_Acc:86.32%
Epoch [30/300], Step [200/391],                 Loss: 0.39824, Train_Acc:86.33%
Epoch [30/300], Step [210/391],                 Loss: 0.39802, Train_Acc:86.33%
Epoch [30/300], Step [220/391],                 Loss: 0.39715, Train_Acc:86.39%
Epoch [30/300], Step [230/391],                 Loss: 0.39514, Train_Acc:86.45%
Epoch [30/300], Step [240/391],                 Loss: 0.39235, Train_Acc:86.54%
Epoch [30/300], Step [250/391],                 Loss: 0.39079, Train_Acc:86.58%
Epoch [30/300], Step [260/391],                 Loss: 0.39258, Train_Acc:86.54%
Epoch [30/300], Step [270/391],                 Loss: 0.39300, Train_Acc:86.52%
Epoch [30/300], Step [280/391],                 Loss: 0.39274, Train_Acc:86.54%
Epoch [30/300], Step [290/391],                 Loss: 0.39366, Train_Acc:86.53%
Epoch [30/300], Step [300/391],                 Loss: 0.39386, Train_Acc:86.50%
Epoch [30/300], Step [310/391],                 Loss: 0.39357, Train_Acc:86.49%
Epoch [30/300], Step [320/391],                 Loss: 0.39420, Train_Acc:86.49%
Epoch [30/300], Step [330/391],                 Loss: 0.39295, Train_Acc:86.55%
Epoch [30/300], Step [340/391],                 Loss: 0.39295, Train_Acc:86.54%
Epoch [30/300], Step [350/391],                 Loss: 0.39292, Train_Acc:86.54%
Epoch [30/300], Step [360/391],                 Loss: 0.39361, Train_Acc:86.50%
Epoch [30/300], Step [370/391],                 Loss: 0.39393, Train_Acc:86.49%
Epoch [30/300], Step [380/391],                 Loss: 0.39298, Train_Acc:86.54%
Epoch [30/300], Step [390/391],                 Loss: 0.39272, Train_Acc:86.56%
Accuary on test images:75.66%
Epoch [31/300], Step [10/391],                 Loss: 0.39574, Train_Acc:86.02%
Epoch [31/300], Step [20/391],                 Loss: 0.38894, Train_Acc:86.56%
Epoch [31/300], Step [30/391],                 Loss: 0.37951, Train_Acc:87.08%
Epoch [31/300], Step [40/391],                 Loss: 0.38479, Train_Acc:86.97%
Epoch [31/300], Step [50/391],                 Loss: 0.38390, Train_Acc:86.91%
Epoch [31/300], Step [60/391],                 Loss: 0.39026, Train_Acc:86.71%
Epoch [31/300], Step [70/391],                 Loss: 0.39523, Train_Acc:86.53%
Epoch [31/300], Step [80/391],                 Loss: 0.39618, Train_Acc:86.44%
Epoch [31/300], Step [90/391],                 Loss: 0.40148, Train_Acc:86.17%
Epoch [31/300], Step [100/391],                 Loss: 0.40335, Train_Acc:86.12%
Epoch [31/300], Step [110/391],                 Loss: 0.40134, Train_Acc:86.21%
Epoch [31/300], Step [120/391],                 Loss: 0.40105, Train_Acc:86.28%
Epoch [31/300], Step [130/391],                 Loss: 0.40401, Train_Acc:86.26%
Epoch [31/300], Step [140/391],                 Loss: 0.40223, Train_Acc:86.38%
Epoch [31/300], Step [150/391],                 Loss: 0.40130, Train_Acc:86.43%
Epoch [31/300], Step [160/391],                 Loss: 0.39920, Train_Acc:86.53%
Epoch [31/300], Step [170/391],                 Loss: 0.39837, Train_Acc:86.51%
Epoch [31/300], Step [180/391],                 Loss: 0.39776, Train_Acc:86.42%
Epoch [31/300], Step [190/391],                 Loss: 0.39947, Train_Acc:86.37%
Epoch [31/300], Step [200/391],                 Loss: 0.40029, Train_Acc:86.32%
Epoch [31/300], Step [210/391],                 Loss: 0.40085, Train_Acc:86.32%
Epoch [31/300], Step [220/391],                 Loss: 0.40225, Train_Acc:86.26%
Epoch [31/300], Step [230/391],                 Loss: 0.40046, Train_Acc:86.32%
Epoch [31/300], Step [240/391],                 Loss: 0.39843, Train_Acc:86.40%
Epoch [31/300], Step [250/391],                 Loss: 0.39734, Train_Acc:86.46%
Epoch [31/300], Step [260/391],                 Loss: 0.39791, Train_Acc:86.48%
Epoch [31/300], Step [270/391],                 Loss: 0.39896, Train_Acc:86.42%
Epoch [31/300], Step [280/391],                 Loss: 0.39856, Train_Acc:86.42%
Epoch [31/300], Step [290/391],                 Loss: 0.39984, Train_Acc:86.38%
Epoch [31/300], Step [300/391],                 Loss: 0.40105, Train_Acc:86.34%
Epoch [31/300], Step [310/391],                 Loss: 0.40198, Train_Acc:86.33%
Epoch [31/300], Step [320/391],                 Loss: 0.40356, Train_Acc:86.28%
Epoch [31/300], Step [330/391],                 Loss: 0.40389, Train_Acc:86.26%
Epoch [31/300], Step [340/391],                 Loss: 0.40359, Train_Acc:86.27%
Epoch [31/300], Step [350/391],                 Loss: 0.40326, Train_Acc:86.29%
Epoch [31/300], Step [360/391],                 Loss: 0.40308, Train_Acc:86.28%
Epoch [31/300], Step [370/391],                 Loss: 0.40266, Train_Acc:86.27%
Epoch [31/300], Step [380/391],                 Loss: 0.40081, Train_Acc:86.35%
Epoch [31/300], Step [390/391],                 Loss: 0.39980, Train_Acc:86.39%
Accuary on test images:76.70%
Epoch [32/300], Step [10/391],                 Loss: 0.40624, Train_Acc:86.09%
Epoch [32/300], Step [20/391],                 Loss: 0.37678, Train_Acc:87.19%
Epoch [32/300], Step [30/391],                 Loss: 0.37020, Train_Acc:87.34%
Epoch [32/300], Step [40/391],                 Loss: 0.36864, Train_Acc:87.48%
Epoch [32/300], Step [50/391],                 Loss: 0.36774, Train_Acc:87.41%
Epoch [32/300], Step [60/391],                 Loss: 0.37597, Train_Acc:87.27%
Epoch [32/300], Step [70/391],                 Loss: 0.38359, Train_Acc:87.01%
Epoch [32/300], Step [80/391],                 Loss: 0.38772, Train_Acc:86.77%
Epoch [32/300], Step [90/391],                 Loss: 0.39084, Train_Acc:86.64%
Epoch [32/300], Step [100/391],                 Loss: 0.39025, Train_Acc:86.64%
Epoch [32/300], Step [110/391],                 Loss: 0.38961, Train_Acc:86.68%
Epoch [32/300], Step [120/391],                 Loss: 0.38759, Train_Acc:86.67%
Epoch [32/300], Step [130/391],                 Loss: 0.38947, Train_Acc:86.63%
Epoch [32/300], Step [140/391],                 Loss: 0.38985, Train_Acc:86.55%
Epoch [32/300], Step [150/391],                 Loss: 0.39153, Train_Acc:86.52%
Epoch [32/300], Step [160/391],                 Loss: 0.39231, Train_Acc:86.50%
Epoch [32/300], Step [170/391],                 Loss: 0.39361, Train_Acc:86.48%
Epoch [32/300], Step [180/391],                 Loss: 0.39515, Train_Acc:86.41%
Epoch [32/300], Step [190/391],                 Loss: 0.39423, Train_Acc:86.42%
Epoch [32/300], Step [200/391],                 Loss: 0.39479, Train_Acc:86.34%
Epoch [32/300], Step [210/391],                 Loss: 0.39511, Train_Acc:86.28%
Epoch [32/300], Step [220/391],                 Loss: 0.39521, Train_Acc:86.32%
Epoch [32/300], Step [230/391],                 Loss: 0.39428, Train_Acc:86.33%
Epoch [32/300], Step [240/391],                 Loss: 0.39277, Train_Acc:86.39%
Epoch [32/300], Step [250/391],                 Loss: 0.39229, Train_Acc:86.42%
Epoch [32/300], Step [260/391],                 Loss: 0.39351, Train_Acc:86.42%
Epoch [32/300], Step [270/391],                 Loss: 0.39566, Train_Acc:86.31%
Epoch [32/300], Step [280/391],                 Loss: 0.39467, Train_Acc:86.33%
Epoch [32/300], Step [290/391],                 Loss: 0.39351, Train_Acc:86.38%
Epoch [32/300], Step [300/391],                 Loss: 0.39413, Train_Acc:86.32%
Epoch [32/300], Step [310/391],                 Loss: 0.39485, Train_Acc:86.36%
Epoch [32/300], Step [320/391],                 Loss: 0.39449, Train_Acc:86.38%
Epoch [32/300], Step [330/391],                 Loss: 0.39383, Train_Acc:86.42%
Epoch [32/300], Step [340/391],                 Loss: 0.39286, Train_Acc:86.41%
Epoch [32/300], Step [350/391],                 Loss: 0.39220, Train_Acc:86.42%
Epoch [32/300], Step [360/391],                 Loss: 0.39201, Train_Acc:86.44%
Epoch [32/300], Step [370/391],                 Loss: 0.39175, Train_Acc:86.44%
Epoch [32/300], Step [380/391],                 Loss: 0.39132, Train_Acc:86.43%
Epoch [32/300], Step [390/391],                 Loss: 0.39107, Train_Acc:86.45%
Accuary on test images:73.80%
Epoch [33/300], Step [10/391],                 Loss: 0.39713, Train_Acc:85.31%
Epoch [33/300], Step [20/391],                 Loss: 0.39954, Train_Acc:86.13%
Epoch [33/300], Step [30/391],                 Loss: 0.38454, Train_Acc:86.69%
Epoch [33/300], Step [40/391],                 Loss: 0.38384, Train_Acc:86.82%
Epoch [33/300], Step [50/391],                 Loss: 0.38395, Train_Acc:86.81%
Epoch [33/300], Step [60/391],                 Loss: 0.39044, Train_Acc:86.73%
Epoch [33/300], Step [70/391],                 Loss: 0.38831, Train_Acc:86.69%
Epoch [33/300], Step [80/391],                 Loss: 0.38652, Train_Acc:86.85%
Epoch [33/300], Step [90/391],                 Loss: 0.39415, Train_Acc:86.61%
Epoch [33/300], Step [100/391],                 Loss: 0.39649, Train_Acc:86.39%
Epoch [33/300], Step [110/391],                 Loss: 0.39998, Train_Acc:86.31%
Epoch [33/300], Step [120/391],                 Loss: 0.39862, Train_Acc:86.37%
Epoch [33/300], Step [130/391],                 Loss: 0.40064, Train_Acc:86.32%
Epoch [33/300], Step [140/391],                 Loss: 0.40039, Train_Acc:86.31%
Epoch [33/300], Step [150/391],                 Loss: 0.39942, Train_Acc:86.34%
Epoch [33/300], Step [160/391],                 Loss: 0.39997, Train_Acc:86.34%
Epoch [33/300], Step [170/391],                 Loss: 0.40036, Train_Acc:86.32%
Epoch [33/300], Step [180/391],                 Loss: 0.39988, Train_Acc:86.35%
Epoch [33/300], Step [190/391],                 Loss: 0.39804, Train_Acc:86.43%
Epoch [33/300], Step [200/391],                 Loss: 0.39817, Train_Acc:86.43%
Epoch [33/300], Step [210/391],                 Loss: 0.39840, Train_Acc:86.45%
Epoch [33/300], Step [220/391],                 Loss: 0.39791, Train_Acc:86.48%
Epoch [33/300], Step [230/391],                 Loss: 0.39654, Train_Acc:86.50%
Epoch [33/300], Step [240/391],                 Loss: 0.39585, Train_Acc:86.54%
Epoch [33/300], Step [250/391],                 Loss: 0.39703, Train_Acc:86.52%
Epoch [33/300], Step [260/391],                 Loss: 0.39985, Train_Acc:86.44%
Epoch [33/300], Step [270/391],                 Loss: 0.40053, Train_Acc:86.42%
Epoch [33/300], Step [280/391],                 Loss: 0.39963, Train_Acc:86.47%
Epoch [33/300], Step [290/391],                 Loss: 0.39855, Train_Acc:86.49%
Epoch [33/300], Step [300/391],                 Loss: 0.39663, Train_Acc:86.58%
Epoch [33/300], Step [310/391],                 Loss: 0.39516, Train_Acc:86.58%
Epoch [33/300], Step [320/391],                 Loss: 0.39444, Train_Acc:86.62%
Epoch [33/300], Step [330/391],                 Loss: 0.39300, Train_Acc:86.66%
Epoch [33/300], Step [340/391],                 Loss: 0.39228, Train_Acc:86.67%
Epoch [33/300], Step [350/391],                 Loss: 0.39151, Train_Acc:86.71%
Epoch [33/300], Step [360/391],                 Loss: 0.39124, Train_Acc:86.72%
Epoch [33/300], Step [370/391],                 Loss: 0.39184, Train_Acc:86.68%
Epoch [33/300], Step [380/391],                 Loss: 0.39105, Train_Acc:86.69%
Epoch [33/300], Step [390/391],                 Loss: 0.39064, Train_Acc:86.68%
Accuary on test images:74.06%
Epoch [34/300], Step [10/391],                 Loss: 0.38872, Train_Acc:87.34%
Epoch [34/300], Step [20/391],                 Loss: 0.39576, Train_Acc:87.23%
Epoch [34/300], Step [30/391],                 Loss: 0.38302, Train_Acc:87.03%
Epoch [34/300], Step [40/391],                 Loss: 0.38143, Train_Acc:87.30%
Epoch [34/300], Step [50/391],                 Loss: 0.37873, Train_Acc:87.31%
Epoch [34/300], Step [60/391],                 Loss: 0.38472, Train_Acc:86.95%
Epoch [34/300], Step [70/391],                 Loss: 0.38533, Train_Acc:87.02%
Epoch [34/300], Step [80/391],                 Loss: 0.38585, Train_Acc:86.95%
Epoch [34/300], Step [90/391],                 Loss: 0.38870, Train_Acc:86.93%
Epoch [34/300], Step [100/391],                 Loss: 0.38815, Train_Acc:86.90%
Epoch [34/300], Step [110/391],                 Loss: 0.39026, Train_Acc:86.78%
Epoch [34/300], Step [120/391],                 Loss: 0.39481, Train_Acc:86.63%
Epoch [34/300], Step [130/391],                 Loss: 0.39919, Train_Acc:86.56%
Epoch [34/300], Step [140/391],                 Loss: 0.39928, Train_Acc:86.63%
Epoch [34/300], Step [150/391],                 Loss: 0.39828, Train_Acc:86.68%
Epoch [34/300], Step [160/391],                 Loss: 0.39681, Train_Acc:86.75%
Epoch [34/300], Step [170/391],                 Loss: 0.39571, Train_Acc:86.81%
Epoch [34/300], Step [180/391],                 Loss: 0.39661, Train_Acc:86.76%
Epoch [34/300], Step [190/391],                 Loss: 0.39872, Train_Acc:86.70%
Epoch [34/300], Step [200/391],                 Loss: 0.40111, Train_Acc:86.62%
Epoch [34/300], Step [210/391],                 Loss: 0.40232, Train_Acc:86.58%
Epoch [34/300], Step [220/391],                 Loss: 0.40209, Train_Acc:86.55%
Epoch [34/300], Step [230/391],                 Loss: 0.40104, Train_Acc:86.56%
Epoch [34/300], Step [240/391],                 Loss: 0.39938, Train_Acc:86.62%
Epoch [34/300], Step [250/391],                 Loss: 0.39923, Train_Acc:86.60%
Epoch [34/300], Step [260/391],                 Loss: 0.40019, Train_Acc:86.57%
Epoch [34/300], Step [270/391],                 Loss: 0.40121, Train_Acc:86.50%
Epoch [34/300], Step [280/391],                 Loss: 0.39985, Train_Acc:86.55%
Epoch [34/300], Step [290/391],                 Loss: 0.39969, Train_Acc:86.55%
Epoch [34/300], Step [300/391],                 Loss: 0.39871, Train_Acc:86.58%
Epoch [34/300], Step [310/391],                 Loss: 0.39775, Train_Acc:86.61%
Epoch [34/300], Step [320/391],                 Loss: 0.39797, Train_Acc:86.59%
Epoch [34/300], Step [330/391],                 Loss: 0.39580, Train_Acc:86.65%
Epoch [34/300], Step [340/391],                 Loss: 0.39428, Train_Acc:86.69%
Epoch [34/300], Step [350/391],                 Loss: 0.39348, Train_Acc:86.73%
Epoch [34/300], Step [360/391],                 Loss: 0.39310, Train_Acc:86.74%
Epoch [34/300], Step [370/391],                 Loss: 0.39374, Train_Acc:86.69%
Epoch [34/300], Step [380/391],                 Loss: 0.39342, Train_Acc:86.70%
Epoch [34/300], Step [390/391],                 Loss: 0.39305, Train_Acc:86.72%
Accuary on test images:71.48%
Epoch [35/300], Step [10/391],                 Loss: 0.40554, Train_Acc:86.41%
Epoch [35/300], Step [20/391],                 Loss: 0.39944, Train_Acc:86.72%
Epoch [35/300], Step [30/391],                 Loss: 0.39674, Train_Acc:86.80%
Epoch [35/300], Step [40/391],                 Loss: 0.39511, Train_Acc:86.84%
Epoch [35/300], Step [50/391],                 Loss: 0.38861, Train_Acc:86.84%
Epoch [35/300], Step [60/391],                 Loss: 0.39456, Train_Acc:86.50%
Epoch [35/300], Step [70/391],                 Loss: 0.39176, Train_Acc:86.67%
Epoch [35/300], Step [80/391],                 Loss: 0.38913, Train_Acc:86.73%
Epoch [35/300], Step [90/391],                 Loss: 0.39157, Train_Acc:86.71%
Epoch [35/300], Step [100/391],                 Loss: 0.39040, Train_Acc:86.70%
Epoch [35/300], Step [110/391],                 Loss: 0.39508, Train_Acc:86.62%
Epoch [35/300], Step [120/391],                 Loss: 0.39581, Train_Acc:86.47%
Epoch [35/300], Step [130/391],                 Loss: 0.39567, Train_Acc:86.46%
Epoch [35/300], Step [140/391],                 Loss: 0.39248, Train_Acc:86.52%
Epoch [35/300], Step [150/391],                 Loss: 0.39169, Train_Acc:86.60%
Epoch [35/300], Step [160/391],                 Loss: 0.39075, Train_Acc:86.66%
Epoch [35/300], Step [170/391],                 Loss: 0.39078, Train_Acc:86.62%
Epoch [35/300], Step [180/391],                 Loss: 0.38912, Train_Acc:86.61%
Epoch [35/300], Step [190/391],                 Loss: 0.38876, Train_Acc:86.65%
Epoch [35/300], Step [200/391],                 Loss: 0.38996, Train_Acc:86.63%
Epoch [35/300], Step [210/391],                 Loss: 0.39071, Train_Acc:86.66%
Epoch [35/300], Step [220/391],                 Loss: 0.39069, Train_Acc:86.69%
Epoch [35/300], Step [230/391],                 Loss: 0.39076, Train_Acc:86.68%
Epoch [35/300], Step [240/391],                 Loss: 0.38867, Train_Acc:86.76%
Epoch [35/300], Step [250/391],                 Loss: 0.38849, Train_Acc:86.75%
Epoch [35/300], Step [260/391],                 Loss: 0.38920, Train_Acc:86.72%
Epoch [35/300], Step [270/391],                 Loss: 0.38990, Train_Acc:86.70%
Epoch [35/300], Step [280/391],                 Loss: 0.38935, Train_Acc:86.73%
Epoch [35/300], Step [290/391],                 Loss: 0.38878, Train_Acc:86.74%
Epoch [35/300], Step [300/391],                 Loss: 0.38820, Train_Acc:86.77%
Epoch [35/300], Step [310/391],                 Loss: 0.38804, Train_Acc:86.78%
Epoch [35/300], Step [320/391],                 Loss: 0.38882, Train_Acc:86.79%
Epoch [35/300], Step [330/391],                 Loss: 0.38836, Train_Acc:86.83%
Epoch [35/300], Step [340/391],                 Loss: 0.38720, Train_Acc:86.86%
Epoch [35/300], Step [350/391],                 Loss: 0.38687, Train_Acc:86.87%
Epoch [35/300], Step [360/391],                 Loss: 0.38625, Train_Acc:86.89%
Epoch [35/300], Step [370/391],                 Loss: 0.38655, Train_Acc:86.87%
Epoch [35/300], Step [380/391],                 Loss: 0.38661, Train_Acc:86.87%
Epoch [35/300], Step [390/391],                 Loss: 0.38645, Train_Acc:86.88%
Accuary on test images:74.94%
Epoch [36/300], Step [10/391],                 Loss: 0.40154, Train_Acc:85.78%
Epoch [36/300], Step [20/391],                 Loss: 0.39783, Train_Acc:86.09%
Epoch [36/300], Step [30/391],                 Loss: 0.39591, Train_Acc:86.30%
Epoch [36/300], Step [40/391],                 Loss: 0.38947, Train_Acc:86.76%
Epoch [36/300], Step [50/391],                 Loss: 0.39155, Train_Acc:86.64%
Epoch [36/300], Step [60/391],                 Loss: 0.39563, Train_Acc:86.54%
Epoch [36/300], Step [70/391],                 Loss: 0.39245, Train_Acc:86.65%
Epoch [36/300], Step [80/391],                 Loss: 0.39660, Train_Acc:86.48%
Epoch [36/300], Step [90/391],                 Loss: 0.39977, Train_Acc:86.41%
Epoch [36/300], Step [100/391],                 Loss: 0.39660, Train_Acc:86.52%
Epoch [36/300], Step [110/391],                 Loss: 0.39593, Train_Acc:86.53%
Epoch [36/300], Step [120/391],                 Loss: 0.39527, Train_Acc:86.56%
Epoch [36/300], Step [130/391],                 Loss: 0.39597, Train_Acc:86.56%
Epoch [36/300], Step [140/391],                 Loss: 0.39372, Train_Acc:86.58%
Epoch [36/300], Step [150/391],                 Loss: 0.39371, Train_Acc:86.58%
Epoch [36/300], Step [160/391],                 Loss: 0.39066, Train_Acc:86.71%
Epoch [36/300], Step [170/391],                 Loss: 0.39190, Train_Acc:86.63%
Epoch [36/300], Step [180/391],                 Loss: 0.39436, Train_Acc:86.48%
Epoch [36/300], Step [190/391],                 Loss: 0.39596, Train_Acc:86.39%
Epoch [36/300], Step [200/391],                 Loss: 0.39542, Train_Acc:86.42%
Epoch [36/300], Step [210/391],                 Loss: 0.39380, Train_Acc:86.45%
Epoch [36/300], Step [220/391],                 Loss: 0.39337, Train_Acc:86.47%
Epoch [36/300], Step [230/391],                 Loss: 0.39208, Train_Acc:86.49%
Epoch [36/300], Step [240/391],                 Loss: 0.38976, Train_Acc:86.58%
Epoch [36/300], Step [250/391],                 Loss: 0.38782, Train_Acc:86.63%
Epoch [36/300], Step [260/391],                 Loss: 0.38913, Train_Acc:86.57%
Epoch [36/300], Step [270/391],                 Loss: 0.39016, Train_Acc:86.52%
Epoch [36/300], Step [280/391],                 Loss: 0.38952, Train_Acc:86.56%
Epoch [36/300], Step [290/391],                 Loss: 0.38972, Train_Acc:86.57%
Epoch [36/300], Step [300/391],                 Loss: 0.38872, Train_Acc:86.61%
Epoch [36/300], Step [310/391],                 Loss: 0.38842, Train_Acc:86.61%
Epoch [36/300], Step [320/391],                 Loss: 0.38824, Train_Acc:86.63%
Epoch [36/300], Step [330/391],                 Loss: 0.38775, Train_Acc:86.63%
Epoch [36/300], Step [340/391],                 Loss: 0.38679, Train_Acc:86.69%
Epoch [36/300], Step [350/391],                 Loss: 0.38617, Train_Acc:86.73%
Epoch [36/300], Step [360/391],                 Loss: 0.38651, Train_Acc:86.71%
Epoch [36/300], Step [370/391],                 Loss: 0.38635, Train_Acc:86.71%
Epoch [36/300], Step [380/391],                 Loss: 0.38636, Train_Acc:86.71%
Epoch [36/300], Step [390/391],                 Loss: 0.38569, Train_Acc:86.77%
Accuary on test images:76.74%
Epoch [37/300], Step [10/391],                 Loss: 0.39579, Train_Acc:86.72%
Epoch [37/300], Step [20/391],                 Loss: 0.38553, Train_Acc:87.23%
Epoch [37/300], Step [30/391],                 Loss: 0.38636, Train_Acc:87.06%
Epoch [37/300], Step [40/391],                 Loss: 0.39145, Train_Acc:86.86%
Epoch [37/300], Step [50/391],                 Loss: 0.39338, Train_Acc:86.56%
Epoch [37/300], Step [60/391],                 Loss: 0.39937, Train_Acc:86.30%
Epoch [37/300], Step [70/391],                 Loss: 0.39542, Train_Acc:86.42%
Epoch [37/300], Step [80/391],                 Loss: 0.39766, Train_Acc:86.29%
Epoch [37/300], Step [90/391],                 Loss: 0.40245, Train_Acc:86.08%
Epoch [37/300], Step [100/391],                 Loss: 0.40083, Train_Acc:86.11%
Epoch [37/300], Step [110/391],                 Loss: 0.40133, Train_Acc:86.06%
Epoch [37/300], Step [120/391],                 Loss: 0.40185, Train_Acc:86.03%
Epoch [37/300], Step [130/391],                 Loss: 0.40333, Train_Acc:85.97%
Epoch [37/300], Step [140/391],                 Loss: 0.40176, Train_Acc:86.07%
Epoch [37/300], Step [150/391],                 Loss: 0.40199, Train_Acc:86.06%
Epoch [37/300], Step [160/391],                 Loss: 0.40075, Train_Acc:86.12%
Epoch [37/300], Step [170/391],                 Loss: 0.40132, Train_Acc:86.11%
Epoch [37/300], Step [180/391],                 Loss: 0.39940, Train_Acc:86.24%
Epoch [37/300], Step [190/391],                 Loss: 0.39898, Train_Acc:86.25%
Epoch [37/300], Step [200/391],                 Loss: 0.39933, Train_Acc:86.27%
Epoch [37/300], Step [210/391],                 Loss: 0.39937, Train_Acc:86.34%
Epoch [37/300], Step [220/391],                 Loss: 0.39893, Train_Acc:86.36%
Epoch [37/300], Step [230/391],                 Loss: 0.39659, Train_Acc:86.41%
Epoch [37/300], Step [240/391],                 Loss: 0.39432, Train_Acc:86.47%
Epoch [37/300], Step [250/391],                 Loss: 0.39361, Train_Acc:86.52%
Epoch [37/300], Step [260/391],                 Loss: 0.39620, Train_Acc:86.50%
Epoch [37/300], Step [270/391],                 Loss: 0.39829, Train_Acc:86.39%
Epoch [37/300], Step [280/391],                 Loss: 0.39847, Train_Acc:86.38%
Epoch [37/300], Step [290/391],                 Loss: 0.39882, Train_Acc:86.36%
Epoch [37/300], Step [300/391],                 Loss: 0.39842, Train_Acc:86.39%
Epoch [37/300], Step [310/391],                 Loss: 0.39682, Train_Acc:86.46%
Epoch [37/300], Step [320/391],                 Loss: 0.39629, Train_Acc:86.47%
Epoch [37/300], Step [330/391],                 Loss: 0.39518, Train_Acc:86.52%
Epoch [37/300], Step [340/391],                 Loss: 0.39487, Train_Acc:86.49%
Epoch [37/300], Step [350/391],                 Loss: 0.39390, Train_Acc:86.53%
Epoch [37/300], Step [360/391],                 Loss: 0.39348, Train_Acc:86.51%
Epoch [37/300], Step [370/391],                 Loss: 0.39405, Train_Acc:86.47%
Epoch [37/300], Step [380/391],                 Loss: 0.39434, Train_Acc:86.47%
Epoch [37/300], Step [390/391],                 Loss: 0.39250, Train_Acc:86.53%
Accuary on test images:73.70%
Epoch [38/300], Step [10/391],                 Loss: 0.39517, Train_Acc:85.78%
Epoch [38/300], Step [20/391],                 Loss: 0.38165, Train_Acc:86.64%
Epoch [38/300], Step [30/391],                 Loss: 0.36543, Train_Acc:87.40%
Epoch [38/300], Step [40/391],                 Loss: 0.37012, Train_Acc:87.42%
Epoch [38/300], Step [50/391],                 Loss: 0.37825, Train_Acc:87.11%
Epoch [38/300], Step [60/391],                 Loss: 0.38338, Train_Acc:86.86%
Epoch [38/300], Step [70/391],                 Loss: 0.38289, Train_Acc:87.00%
Epoch [38/300], Step [80/391],                 Loss: 0.38483, Train_Acc:86.86%
Epoch [38/300], Step [90/391],                 Loss: 0.39450, Train_Acc:86.57%
Epoch [38/300], Step [100/391],                 Loss: 0.39386, Train_Acc:86.62%
Epoch [38/300], Step [110/391],                 Loss: 0.39647, Train_Acc:86.53%
Epoch [38/300], Step [120/391],                 Loss: 0.39529, Train_Acc:86.65%
Epoch [38/300], Step [130/391],                 Loss: 0.39561, Train_Acc:86.69%
Epoch [38/300], Step [140/391],                 Loss: 0.39409, Train_Acc:86.75%
Epoch [38/300], Step [150/391],                 Loss: 0.39416, Train_Acc:86.70%
Epoch [38/300], Step [160/391],                 Loss: 0.39616, Train_Acc:86.67%
Epoch [38/300], Step [170/391],                 Loss: 0.39734, Train_Acc:86.65%
Epoch [38/300], Step [180/391],                 Loss: 0.39785, Train_Acc:86.60%
Epoch [38/300], Step [190/391],                 Loss: 0.39734, Train_Acc:86.59%
Epoch [38/300], Step [200/391],                 Loss: 0.39689, Train_Acc:86.62%
Epoch [38/300], Step [210/391],                 Loss: 0.39682, Train_Acc:86.64%
Epoch [38/300], Step [220/391],                 Loss: 0.39740, Train_Acc:86.62%
Epoch [38/300], Step [230/391],                 Loss: 0.39606, Train_Acc:86.64%
Epoch [38/300], Step [240/391],                 Loss: 0.39277, Train_Acc:86.77%
Epoch [38/300], Step [250/391],                 Loss: 0.39027, Train_Acc:86.83%
Epoch [38/300], Step [260/391],                 Loss: 0.39195, Train_Acc:86.79%
Epoch [38/300], Step [270/391],                 Loss: 0.39195, Train_Acc:86.78%
Epoch [38/300], Step [280/391],                 Loss: 0.39162, Train_Acc:86.78%
Epoch [38/300], Step [290/391],                 Loss: 0.39107, Train_Acc:86.80%
Epoch [38/300], Step [300/391],                 Loss: 0.39046, Train_Acc:86.78%
Epoch [38/300], Step [310/391],                 Loss: 0.38956, Train_Acc:86.80%
Epoch [38/300], Step [320/391],                 Loss: 0.39043, Train_Acc:86.80%
Epoch [38/300], Step [330/391],                 Loss: 0.39011, Train_Acc:86.83%
Epoch [38/300], Step [340/391],                 Loss: 0.39019, Train_Acc:86.79%
Epoch [38/300], Step [350/391],                 Loss: 0.38944, Train_Acc:86.82%
Epoch [38/300], Step [360/391],                 Loss: 0.38984, Train_Acc:86.84%
Epoch [38/300], Step [370/391],                 Loss: 0.38976, Train_Acc:86.83%
Epoch [38/300], Step [380/391],                 Loss: 0.38911, Train_Acc:86.85%
Epoch [38/300], Step [390/391],                 Loss: 0.38836, Train_Acc:86.88%
Accuary on test images:68.48%
Epoch [39/300], Step [10/391],                 Loss: 0.41594, Train_Acc:85.23%
Epoch [39/300], Step [20/391],                 Loss: 0.39311, Train_Acc:86.02%
Epoch [39/300], Step [30/391],                 Loss: 0.38148, Train_Acc:86.48%
Epoch [39/300], Step [40/391],                 Loss: 0.38577, Train_Acc:86.58%
Epoch [39/300], Step [50/391],                 Loss: 0.38498, Train_Acc:86.58%
Epoch [39/300], Step [60/391],                 Loss: 0.38916, Train_Acc:86.32%
Epoch [39/300], Step [70/391],                 Loss: 0.39059, Train_Acc:86.40%
Epoch [39/300], Step [80/391],                 Loss: 0.38972, Train_Acc:86.51%
Epoch [39/300], Step [90/391],                 Loss: 0.39374, Train_Acc:86.41%
Epoch [39/300], Step [100/391],                 Loss: 0.39321, Train_Acc:86.45%
Epoch [39/300], Step [110/391],                 Loss: 0.39353, Train_Acc:86.38%
Epoch [39/300], Step [120/391],                 Loss: 0.39189, Train_Acc:86.41%
Epoch [39/300], Step [130/391],                 Loss: 0.39326, Train_Acc:86.38%
Epoch [39/300], Step [140/391],                 Loss: 0.39087, Train_Acc:86.48%
Epoch [39/300], Step [150/391],                 Loss: 0.39070, Train_Acc:86.53%
Epoch [39/300], Step [160/391],                 Loss: 0.39066, Train_Acc:86.50%
Epoch [39/300], Step [170/391],                 Loss: 0.39232, Train_Acc:86.41%
Epoch [39/300], Step [180/391],                 Loss: 0.39275, Train_Acc:86.40%
Epoch [39/300], Step [190/391],                 Loss: 0.39387, Train_Acc:86.31%
Epoch [39/300], Step [200/391],                 Loss: 0.39539, Train_Acc:86.30%
Epoch [39/300], Step [210/391],                 Loss: 0.39639, Train_Acc:86.29%
Epoch [39/300], Step [220/391],                 Loss: 0.39607, Train_Acc:86.32%
Epoch [39/300], Step [230/391],                 Loss: 0.39444, Train_Acc:86.39%
Epoch [39/300], Step [240/391],                 Loss: 0.39174, Train_Acc:86.48%
Epoch [39/300], Step [250/391],                 Loss: 0.39250, Train_Acc:86.43%
Epoch [39/300], Step [260/391],                 Loss: 0.39425, Train_Acc:86.41%
Epoch [39/300], Step [270/391],                 Loss: 0.39467, Train_Acc:86.41%
Epoch [39/300], Step [280/391],                 Loss: 0.39499, Train_Acc:86.40%
Epoch [39/300], Step [290/391],                 Loss: 0.39520, Train_Acc:86.40%
Epoch [39/300], Step [300/391],                 Loss: 0.39494, Train_Acc:86.41%
Epoch [39/300], Step [310/391],                 Loss: 0.39462, Train_Acc:86.44%
Epoch [39/300], Step [320/391],                 Loss: 0.39424, Train_Acc:86.45%
Epoch [39/300], Step [330/391],                 Loss: 0.39302, Train_Acc:86.47%
Epoch [39/300], Step [340/391],                 Loss: 0.39256, Train_Acc:86.50%
Epoch [39/300], Step [350/391],                 Loss: 0.39181, Train_Acc:86.53%
Epoch [39/300], Step [360/391],                 Loss: 0.39231, Train_Acc:86.52%
Epoch [39/300], Step [370/391],                 Loss: 0.39276, Train_Acc:86.51%
Epoch [39/300], Step [380/391],                 Loss: 0.39276, Train_Acc:86.53%
Epoch [39/300], Step [390/391],                 Loss: 0.39178, Train_Acc:86.55%
Accuary on test images:77.14%
Epoch [40/300], Step [10/391],                 Loss: 0.37073, Train_Acc:87.73%
Epoch [40/300], Step [20/391],                 Loss: 0.37809, Train_Acc:87.58%
Epoch [40/300], Step [30/391],                 Loss: 0.36787, Train_Acc:87.63%
Epoch [40/300], Step [40/391],                 Loss: 0.38436, Train_Acc:86.99%
Epoch [40/300], Step [50/391],                 Loss: 0.39513, Train_Acc:86.53%
Epoch [40/300], Step [60/391],                 Loss: 0.40012, Train_Acc:86.38%
Epoch [40/300], Step [70/391],                 Loss: 0.39619, Train_Acc:86.51%
Epoch [40/300], Step [80/391],                 Loss: 0.39509, Train_Acc:86.51%
Epoch [40/300], Step [90/391],                 Loss: 0.39686, Train_Acc:86.51%
Epoch [40/300], Step [100/391],                 Loss: 0.39581, Train_Acc:86.51%
Epoch [40/300], Step [110/391],                 Loss: 0.39710, Train_Acc:86.52%
Epoch [40/300], Step [120/391],                 Loss: 0.39591, Train_Acc:86.50%
Epoch [40/300], Step [130/391],                 Loss: 0.39645, Train_Acc:86.51%
Epoch [40/300], Step [140/391],                 Loss: 0.39529, Train_Acc:86.59%
Epoch [40/300], Step [150/391],                 Loss: 0.39508, Train_Acc:86.57%
Epoch [40/300], Step [160/391],                 Loss: 0.39388, Train_Acc:86.62%
Epoch [40/300], Step [170/391],                 Loss: 0.39332, Train_Acc:86.59%
Epoch [40/300], Step [180/391],                 Loss: 0.39289, Train_Acc:86.53%
Epoch [40/300], Step [190/391],                 Loss: 0.39128, Train_Acc:86.60%
Epoch [40/300], Step [200/391],                 Loss: 0.39120, Train_Acc:86.61%
Epoch [40/300], Step [210/391],                 Loss: 0.39018, Train_Acc:86.66%
Epoch [40/300], Step [220/391],                 Loss: 0.38988, Train_Acc:86.69%
Epoch [40/300], Step [230/391],                 Loss: 0.38761, Train_Acc:86.77%
Epoch [40/300], Step [240/391],                 Loss: 0.38510, Train_Acc:86.84%
Epoch [40/300], Step [250/391],                 Loss: 0.38463, Train_Acc:86.85%
Epoch [40/300], Step [260/391],                 Loss: 0.38583, Train_Acc:86.84%
Epoch [40/300], Step [270/391],                 Loss: 0.38609, Train_Acc:86.84%
Epoch [40/300], Step [280/391],                 Loss: 0.38562, Train_Acc:86.83%
Epoch [40/300], Step [290/391],                 Loss: 0.38560, Train_Acc:86.84%
Epoch [40/300], Step [300/391],                 Loss: 0.38478, Train_Acc:86.87%
Epoch [40/300], Step [310/391],                 Loss: 0.38472, Train_Acc:86.89%
Epoch [40/300], Step [320/391],                 Loss: 0.38457, Train_Acc:86.91%
Epoch [40/300], Step [330/391],                 Loss: 0.38359, Train_Acc:86.95%
Epoch [40/300], Step [340/391],                 Loss: 0.38248, Train_Acc:86.99%
Epoch [40/300], Step [350/391],                 Loss: 0.38249, Train_Acc:87.00%
Epoch [40/300], Step [360/391],                 Loss: 0.38306, Train_Acc:86.95%
Epoch [40/300], Step [370/391],                 Loss: 0.38376, Train_Acc:86.92%
Epoch [40/300], Step [380/391],                 Loss: 0.38371, Train_Acc:86.92%
Epoch [40/300], Step [390/391],                 Loss: 0.38267, Train_Acc:86.96%
Accuary on test images:77.78%
Epoch [41/300], Step [10/391],                 Loss: 0.41071, Train_Acc:86.02%
Epoch [41/300], Step [20/391],                 Loss: 0.39531, Train_Acc:86.95%
Epoch [41/300], Step [30/391],                 Loss: 0.38745, Train_Acc:86.98%
Epoch [41/300], Step [40/391],                 Loss: 0.37915, Train_Acc:87.42%
Epoch [41/300], Step [50/391],                 Loss: 0.37350, Train_Acc:87.39%
Epoch [41/300], Step [60/391],                 Loss: 0.38135, Train_Acc:86.97%
Epoch [41/300], Step [70/391],                 Loss: 0.37888, Train_Acc:87.13%
Epoch [41/300], Step [80/391],                 Loss: 0.37760, Train_Acc:87.20%
Epoch [41/300], Step [90/391],                 Loss: 0.38112, Train_Acc:87.16%
Epoch [41/300], Step [100/391],                 Loss: 0.38073, Train_Acc:87.14%
Epoch [41/300], Step [110/391],                 Loss: 0.38317, Train_Acc:87.02%
Epoch [41/300], Step [120/391],                 Loss: 0.38450, Train_Acc:86.91%
Epoch [41/300], Step [130/391],                 Loss: 0.38455, Train_Acc:86.92%
Epoch [41/300], Step [140/391],                 Loss: 0.38045, Train_Acc:87.14%
Epoch [41/300], Step [150/391],                 Loss: 0.37980, Train_Acc:87.17%
Epoch [41/300], Step [160/391],                 Loss: 0.37782, Train_Acc:87.19%
Epoch [41/300], Step [170/391],                 Loss: 0.37746, Train_Acc:87.18%
Epoch [41/300], Step [180/391],                 Loss: 0.37785, Train_Acc:87.19%
Epoch [41/300], Step [190/391],                 Loss: 0.37808, Train_Acc:87.19%
Epoch [41/300], Step [200/391],                 Loss: 0.38177, Train_Acc:87.05%
Epoch [41/300], Step [210/391],                 Loss: 0.38280, Train_Acc:87.00%
Epoch [41/300], Step [220/391],                 Loss: 0.38405, Train_Acc:87.01%
Epoch [41/300], Step [230/391],                 Loss: 0.38410, Train_Acc:87.01%
Epoch [41/300], Step [240/391],                 Loss: 0.38385, Train_Acc:87.00%
Epoch [41/300], Step [250/391],                 Loss: 0.38391, Train_Acc:87.00%
Epoch [41/300], Step [260/391],                 Loss: 0.38463, Train_Acc:87.00%
Epoch [41/300], Step [270/391],                 Loss: 0.38517, Train_Acc:86.96%
Epoch [41/300], Step [280/391],                 Loss: 0.38471, Train_Acc:86.97%
Epoch [41/300], Step [290/391],                 Loss: 0.38339, Train_Acc:86.99%
Epoch [41/300], Step [300/391],                 Loss: 0.38301, Train_Acc:86.98%
Epoch [41/300], Step [310/391],                 Loss: 0.38283, Train_Acc:86.97%
Epoch [41/300], Step [320/391],                 Loss: 0.38287, Train_Acc:86.98%
Epoch [41/300], Step [330/391],                 Loss: 0.38154, Train_Acc:87.01%
Epoch [41/300], Step [340/391],                 Loss: 0.38088, Train_Acc:87.04%
Epoch [41/300], Step [350/391],                 Loss: 0.38008, Train_Acc:87.09%
Epoch [41/300], Step [360/391],                 Loss: 0.38041, Train_Acc:87.08%
Epoch [41/300], Step [370/391],                 Loss: 0.38075, Train_Acc:87.07%
Epoch [41/300], Step [380/391],                 Loss: 0.38074, Train_Acc:87.06%
Epoch [41/300], Step [390/391],                 Loss: 0.38030, Train_Acc:87.08%
Accuary on test images:73.86%
Epoch [42/300], Step [10/391],                 Loss: 0.39339, Train_Acc:86.33%
Epoch [42/300], Step [20/391],                 Loss: 0.37334, Train_Acc:86.76%
Epoch [42/300], Step [30/391],                 Loss: 0.36655, Train_Acc:87.27%
Epoch [42/300], Step [40/391],                 Loss: 0.37008, Train_Acc:87.13%
Epoch [42/300], Step [50/391],                 Loss: 0.37505, Train_Acc:87.06%
Epoch [42/300], Step [60/391],                 Loss: 0.38021, Train_Acc:86.91%
Epoch [42/300], Step [70/391],                 Loss: 0.38041, Train_Acc:87.04%
Epoch [42/300], Step [80/391],                 Loss: 0.38440, Train_Acc:86.99%
Epoch [42/300], Step [90/391],                 Loss: 0.38802, Train_Acc:86.91%
Epoch [42/300], Step [100/391],                 Loss: 0.38740, Train_Acc:86.94%
Epoch [42/300], Step [110/391],                 Loss: 0.38832, Train_Acc:86.96%
Epoch [42/300], Step [120/391],                 Loss: 0.38604, Train_Acc:86.95%
Epoch [42/300], Step [130/391],                 Loss: 0.38620, Train_Acc:86.88%
Epoch [42/300], Step [140/391],                 Loss: 0.38499, Train_Acc:86.95%
Epoch [42/300], Step [150/391],                 Loss: 0.38334, Train_Acc:87.07%
Epoch [42/300], Step [160/391],                 Loss: 0.38325, Train_Acc:87.07%
Epoch [42/300], Step [170/391],                 Loss: 0.38549, Train_Acc:86.98%
Epoch [42/300], Step [180/391],                 Loss: 0.38499, Train_Acc:86.99%
Epoch [42/300], Step [190/391],                 Loss: 0.38511, Train_Acc:87.00%
Epoch [42/300], Step [200/391],                 Loss: 0.38528, Train_Acc:86.94%
Epoch [42/300], Step [210/391],                 Loss: 0.38693, Train_Acc:86.88%
Epoch [42/300], Step [220/391],                 Loss: 0.38652, Train_Acc:86.88%
Epoch [42/300], Step [230/391],                 Loss: 0.38573, Train_Acc:86.90%
Epoch [42/300], Step [240/391],                 Loss: 0.38388, Train_Acc:86.96%
Epoch [42/300], Step [250/391],                 Loss: 0.38269, Train_Acc:87.00%
Epoch [42/300], Step [260/391],                 Loss: 0.38509, Train_Acc:86.99%
Epoch [42/300], Step [270/391],                 Loss: 0.38618, Train_Acc:86.96%
Epoch [42/300], Step [280/391],                 Loss: 0.38683, Train_Acc:86.95%
Epoch [42/300], Step [290/391],                 Loss: 0.38613, Train_Acc:86.96%
Epoch [42/300], Step [300/391],                 Loss: 0.38572, Train_Acc:86.95%
Epoch [42/300], Step [310/391],                 Loss: 0.38558, Train_Acc:86.95%
Epoch [42/300], Step [320/391],                 Loss: 0.38612, Train_Acc:86.91%
Epoch [42/300], Step [330/391],                 Loss: 0.38538, Train_Acc:86.92%
Epoch [42/300], Step [340/391],                 Loss: 0.38560, Train_Acc:86.92%
Epoch [42/300], Step [350/391],                 Loss: 0.38641, Train_Acc:86.91%
Epoch [42/300], Step [360/391],                 Loss: 0.38672, Train_Acc:86.87%
Epoch [42/300], Step [370/391],                 Loss: 0.38685, Train_Acc:86.82%
Epoch [42/300], Step [380/391],                 Loss: 0.38653, Train_Acc:86.84%
Epoch [42/300], Step [390/391],                 Loss: 0.38554, Train_Acc:86.86%
Accuary on test images:78.24%
Epoch [43/300], Step [10/391],                 Loss: 0.37862, Train_Acc:87.27%
Epoch [43/300], Step [20/391],                 Loss: 0.37699, Train_Acc:87.42%
Epoch [43/300], Step [30/391],                 Loss: 0.37097, Train_Acc:87.37%
Epoch [43/300], Step [40/391],                 Loss: 0.38530, Train_Acc:86.91%
Epoch [43/300], Step [50/391],                 Loss: 0.38786, Train_Acc:86.83%
Epoch [43/300], Step [60/391],                 Loss: 0.39275, Train_Acc:86.65%
Epoch [43/300], Step [70/391],                 Loss: 0.38675, Train_Acc:86.71%
Epoch [43/300], Step [80/391],                 Loss: 0.38424, Train_Acc:86.90%
Epoch [43/300], Step [90/391],                 Loss: 0.38494, Train_Acc:86.83%
Epoch [43/300], Step [100/391],                 Loss: 0.38402, Train_Acc:86.76%
Epoch [43/300], Step [110/391],                 Loss: 0.38770, Train_Acc:86.69%
Epoch [43/300], Step [120/391],                 Loss: 0.38555, Train_Acc:86.76%
Epoch [43/300], Step [130/391],                 Loss: 0.38724, Train_Acc:86.70%
Epoch [43/300], Step [140/391],                 Loss: 0.38542, Train_Acc:86.74%
Epoch [43/300], Step [150/391],                 Loss: 0.38535, Train_Acc:86.71%
Epoch [43/300], Step [160/391],                 Loss: 0.38482, Train_Acc:86.69%
Epoch [43/300], Step [170/391],                 Loss: 0.38508, Train_Acc:86.70%
Epoch [43/300], Step [180/391],                 Loss: 0.38575, Train_Acc:86.75%
Epoch [43/300], Step [190/391],                 Loss: 0.38685, Train_Acc:86.70%
Epoch [43/300], Step [200/391],                 Loss: 0.38728, Train_Acc:86.67%
Epoch [43/300], Step [210/391],                 Loss: 0.38797, Train_Acc:86.62%
Epoch [43/300], Step [220/391],                 Loss: 0.38770, Train_Acc:86.63%
Epoch [43/300], Step [230/391],                 Loss: 0.38678, Train_Acc:86.66%
Epoch [43/300], Step [240/391],                 Loss: 0.38520, Train_Acc:86.71%
Epoch [43/300], Step [250/391],                 Loss: 0.38552, Train_Acc:86.71%
Epoch [43/300], Step [260/391],                 Loss: 0.38744, Train_Acc:86.66%
Epoch [43/300], Step [270/391],                 Loss: 0.38920, Train_Acc:86.63%
Epoch [43/300], Step [280/391],                 Loss: 0.38960, Train_Acc:86.64%
Epoch [43/300], Step [290/391],                 Loss: 0.39005, Train_Acc:86.63%
Epoch [43/300], Step [300/391],                 Loss: 0.39074, Train_Acc:86.60%
Epoch [43/300], Step [310/391],                 Loss: 0.39022, Train_Acc:86.63%
Epoch [43/300], Step [320/391],                 Loss: 0.38973, Train_Acc:86.65%
Epoch [43/300], Step [330/391],                 Loss: 0.38753, Train_Acc:86.73%
Epoch [43/300], Step [340/391],                 Loss: 0.38613, Train_Acc:86.76%
Epoch [43/300], Step [350/391],                 Loss: 0.38426, Train_Acc:86.83%
Epoch [43/300], Step [360/391],                 Loss: 0.38388, Train_Acc:86.84%
Epoch [43/300], Step [370/391],                 Loss: 0.38499, Train_Acc:86.79%
Epoch [43/300], Step [380/391],                 Loss: 0.38554, Train_Acc:86.77%
Epoch [43/300], Step [390/391],                 Loss: 0.38526, Train_Acc:86.80%
Accuary on test images:72.26%
Epoch [44/300], Step [10/391],                 Loss: 0.38901, Train_Acc:86.17%
Epoch [44/300], Step [20/391],                 Loss: 0.36643, Train_Acc:87.19%
Epoch [44/300], Step [30/391],                 Loss: 0.36388, Train_Acc:87.53%
Epoch [44/300], Step [40/391],                 Loss: 0.36499, Train_Acc:87.48%
Epoch [44/300], Step [50/391],                 Loss: 0.36491, Train_Acc:87.53%
Epoch [44/300], Step [60/391],                 Loss: 0.37171, Train_Acc:87.29%
Epoch [44/300], Step [70/391],                 Loss: 0.37126, Train_Acc:87.35%
Epoch [44/300], Step [80/391],                 Loss: 0.37603, Train_Acc:87.29%
Epoch [44/300], Step [90/391],                 Loss: 0.37886, Train_Acc:87.14%
Epoch [44/300], Step [100/391],                 Loss: 0.37772, Train_Acc:87.25%
Epoch [44/300], Step [110/391],                 Loss: 0.38042, Train_Acc:87.21%
Epoch [44/300], Step [120/391],                 Loss: 0.38091, Train_Acc:87.19%
Epoch [44/300], Step [130/391],                 Loss: 0.38307, Train_Acc:87.19%
Epoch [44/300], Step [140/391],                 Loss: 0.38159, Train_Acc:87.26%
Epoch [44/300], Step [150/391],                 Loss: 0.37970, Train_Acc:87.35%
Epoch [44/300], Step [160/391],                 Loss: 0.37881, Train_Acc:87.35%
Epoch [44/300], Step [170/391],                 Loss: 0.38021, Train_Acc:87.29%
Epoch [44/300], Step [180/391],                 Loss: 0.38132, Train_Acc:87.22%
Epoch [44/300], Step [190/391],                 Loss: 0.38119, Train_Acc:87.24%
Epoch [44/300], Step [200/391],                 Loss: 0.38289, Train_Acc:87.19%
Epoch [44/300], Step [210/391],                 Loss: 0.38406, Train_Acc:87.14%
Epoch [44/300], Step [220/391],                 Loss: 0.38472, Train_Acc:87.12%
Epoch [44/300], Step [230/391],                 Loss: 0.38429, Train_Acc:87.10%
Epoch [44/300], Step [240/391],                 Loss: 0.38315, Train_Acc:87.12%
Epoch [44/300], Step [250/391],                 Loss: 0.38139, Train_Acc:87.15%
Epoch [44/300], Step [260/391],                 Loss: 0.38217, Train_Acc:87.12%
Epoch [44/300], Step [270/391],                 Loss: 0.38397, Train_Acc:87.05%
Epoch [44/300], Step [280/391],                 Loss: 0.38404, Train_Acc:87.05%
Epoch [44/300], Step [290/391],                 Loss: 0.38414, Train_Acc:87.02%
Epoch [44/300], Step [300/391],                 Loss: 0.38432, Train_Acc:87.00%
Epoch [44/300], Step [310/391],                 Loss: 0.38306, Train_Acc:87.05%
Epoch [44/300], Step [320/391],                 Loss: 0.38351, Train_Acc:87.04%
Epoch [44/300], Step [330/391],                 Loss: 0.38408, Train_Acc:87.03%
Epoch [44/300], Step [340/391],                 Loss: 0.38320, Train_Acc:87.04%
Epoch [44/300], Step [350/391],                 Loss: 0.38282, Train_Acc:87.06%
Epoch [44/300], Step [360/391],                 Loss: 0.38189, Train_Acc:87.09%
Epoch [44/300], Step [370/391],                 Loss: 0.38238, Train_Acc:87.04%
Epoch [44/300], Step [380/391],                 Loss: 0.38190, Train_Acc:87.06%
Epoch [44/300], Step [390/391],                 Loss: 0.38203, Train_Acc:87.06%
Accuary on test images:76.32%
Epoch [45/300], Step [10/391],                 Loss: 0.41503, Train_Acc:84.84%
Epoch [45/300], Step [20/391],                 Loss: 0.39153, Train_Acc:86.13%
Epoch [45/300], Step [30/391],                 Loss: 0.38234, Train_Acc:86.41%
Epoch [45/300], Step [40/391],                 Loss: 0.37471, Train_Acc:86.89%
Epoch [45/300], Step [50/391],                 Loss: 0.37163, Train_Acc:87.08%
Epoch [45/300], Step [60/391],                 Loss: 0.37179, Train_Acc:87.08%
Epoch [45/300], Step [70/391],                 Loss: 0.37079, Train_Acc:87.20%
Epoch [45/300], Step [80/391],                 Loss: 0.37204, Train_Acc:87.19%
Epoch [45/300], Step [90/391],                 Loss: 0.37815, Train_Acc:87.01%
Epoch [45/300], Step [100/391],                 Loss: 0.38231, Train_Acc:86.85%
Epoch [45/300], Step [110/391],                 Loss: 0.38769, Train_Acc:86.58%
Epoch [45/300], Step [120/391],                 Loss: 0.38825, Train_Acc:86.58%
Epoch [45/300], Step [130/391],                 Loss: 0.38907, Train_Acc:86.56%
Epoch [45/300], Step [140/391],                 Loss: 0.38648, Train_Acc:86.60%
Epoch [45/300], Step [150/391],                 Loss: 0.38535, Train_Acc:86.65%
Epoch [45/300], Step [160/391],                 Loss: 0.38462, Train_Acc:86.64%
Epoch [45/300], Step [170/391],                 Loss: 0.38572, Train_Acc:86.63%
Epoch [45/300], Step [180/391],                 Loss: 0.38689, Train_Acc:86.68%
Epoch [45/300], Step [190/391],                 Loss: 0.38678, Train_Acc:86.65%
Epoch [45/300], Step [200/391],                 Loss: 0.38788, Train_Acc:86.58%
Epoch [45/300], Step [210/391],                 Loss: 0.38752, Train_Acc:86.64%
Epoch [45/300], Step [220/391],                 Loss: 0.38679, Train_Acc:86.70%
Epoch [45/300], Step [230/391],                 Loss: 0.38524, Train_Acc:86.76%
Epoch [45/300], Step [240/391],                 Loss: 0.38362, Train_Acc:86.76%
Epoch [45/300], Step [250/391],                 Loss: 0.38415, Train_Acc:86.78%
Epoch [45/300], Step [260/391],                 Loss: 0.38691, Train_Acc:86.68%
Epoch [45/300], Step [270/391],                 Loss: 0.38728, Train_Acc:86.65%
Epoch [45/300], Step [280/391],                 Loss: 0.38532, Train_Acc:86.71%
Epoch [45/300], Step [290/391],                 Loss: 0.38442, Train_Acc:86.75%
Epoch [45/300], Step [300/391],                 Loss: 0.38453, Train_Acc:86.73%
Epoch [45/300], Step [310/391],                 Loss: 0.38449, Train_Acc:86.71%
Epoch [45/300], Step [320/391],                 Loss: 0.38494, Train_Acc:86.68%
Epoch [45/300], Step [330/391],                 Loss: 0.38546, Train_Acc:86.66%
Epoch [45/300], Step [340/391],                 Loss: 0.38523, Train_Acc:86.67%
Epoch [45/300], Step [350/391],                 Loss: 0.38492, Train_Acc:86.70%
Epoch [45/300], Step [360/391],                 Loss: 0.38412, Train_Acc:86.73%
Epoch [45/300], Step [370/391],                 Loss: 0.38507, Train_Acc:86.68%
Epoch [45/300], Step [380/391],                 Loss: 0.38515, Train_Acc:86.66%
Epoch [45/300], Step [390/391],                 Loss: 0.38429, Train_Acc:86.66%
Accuary on test images:77.68%
Epoch [46/300], Step [10/391],                 Loss: 0.38302, Train_Acc:87.19%
Epoch [46/300], Step [20/391],                 Loss: 0.37061, Train_Acc:87.70%
Epoch [46/300], Step [30/391],                 Loss: 0.35925, Train_Acc:87.97%
Epoch [46/300], Step [40/391],                 Loss: 0.36554, Train_Acc:87.48%
Epoch [46/300], Step [50/391],                 Loss: 0.36835, Train_Acc:87.41%
Epoch [46/300], Step [60/391],                 Loss: 0.37623, Train_Acc:87.12%
Epoch [46/300], Step [70/391],                 Loss: 0.37977, Train_Acc:86.98%
Epoch [46/300], Step [80/391],                 Loss: 0.38181, Train_Acc:86.89%
Epoch [46/300], Step [90/391],                 Loss: 0.38431, Train_Acc:86.80%
Epoch [46/300], Step [100/391],                 Loss: 0.38412, Train_Acc:86.83%
Epoch [46/300], Step [110/391],                 Loss: 0.38924, Train_Acc:86.63%
Epoch [46/300], Step [120/391],                 Loss: 0.39160, Train_Acc:86.51%
Epoch [46/300], Step [130/391],                 Loss: 0.39297, Train_Acc:86.53%
Epoch [46/300], Step [140/391],                 Loss: 0.39177, Train_Acc:86.57%
Epoch [46/300], Step [150/391],                 Loss: 0.39200, Train_Acc:86.62%
Epoch [46/300], Step [160/391],                 Loss: 0.38930, Train_Acc:86.74%
Epoch [46/300], Step [170/391],                 Loss: 0.38958, Train_Acc:86.76%
Epoch [46/300], Step [180/391],                 Loss: 0.38899, Train_Acc:86.80%
Epoch [46/300], Step [190/391],                 Loss: 0.38924, Train_Acc:86.73%
Epoch [46/300], Step [200/391],                 Loss: 0.38883, Train_Acc:86.77%
Epoch [46/300], Step [210/391],                 Loss: 0.38819, Train_Acc:86.79%
Epoch [46/300], Step [220/391],                 Loss: 0.38806, Train_Acc:86.79%
Epoch [46/300], Step [230/391],                 Loss: 0.38673, Train_Acc:86.85%
Epoch [46/300], Step [240/391],                 Loss: 0.38592, Train_Acc:86.88%
Epoch [46/300], Step [250/391],                 Loss: 0.38534, Train_Acc:86.89%
Epoch [46/300], Step [260/391],                 Loss: 0.38604, Train_Acc:86.88%
Epoch [46/300], Step [270/391],                 Loss: 0.38644, Train_Acc:86.84%
Epoch [46/300], Step [280/391],                 Loss: 0.38625, Train_Acc:86.85%
Epoch [46/300], Step [290/391],                 Loss: 0.38497, Train_Acc:86.89%
Epoch [46/300], Step [300/391],                 Loss: 0.38415, Train_Acc:86.93%
Epoch [46/300], Step [310/391],                 Loss: 0.38274, Train_Acc:86.98%
Epoch [46/300], Step [320/391],                 Loss: 0.38302, Train_Acc:86.96%
Epoch [46/300], Step [330/391],                 Loss: 0.38227, Train_Acc:86.97%
Epoch [46/300], Step [340/391],                 Loss: 0.38221, Train_Acc:86.99%
Epoch [46/300], Step [350/391],                 Loss: 0.38234, Train_Acc:87.01%
Epoch [46/300], Step [360/391],                 Loss: 0.38260, Train_Acc:86.98%
Epoch [46/300], Step [370/391],                 Loss: 0.38297, Train_Acc:86.96%
Epoch [46/300], Step [380/391],                 Loss: 0.38280, Train_Acc:86.96%
Epoch [46/300], Step [390/391],                 Loss: 0.38204, Train_Acc:87.00%
Accuary on test images:73.88%
Epoch [47/300], Step [10/391],                 Loss: 0.41759, Train_Acc:85.39%
Epoch [47/300], Step [20/391],                 Loss: 0.40335, Train_Acc:86.09%
Epoch [47/300], Step [30/391],                 Loss: 0.39623, Train_Acc:86.25%
Epoch [47/300], Step [40/391],                 Loss: 0.39247, Train_Acc:86.48%
Epoch [47/300], Step [50/391],                 Loss: 0.39101, Train_Acc:86.34%
Epoch [47/300], Step [60/391],                 Loss: 0.39499, Train_Acc:86.05%
Epoch [47/300], Step [70/391],                 Loss: 0.39214, Train_Acc:86.34%
Epoch [47/300], Step [80/391],                 Loss: 0.39265, Train_Acc:86.33%
Epoch [47/300], Step [90/391],                 Loss: 0.39274, Train_Acc:86.35%
Epoch [47/300], Step [100/391],                 Loss: 0.38821, Train_Acc:86.58%
Epoch [47/300], Step [110/391],                 Loss: 0.38601, Train_Acc:86.73%
Epoch [47/300], Step [120/391],                 Loss: 0.38338, Train_Acc:86.76%
Epoch [47/300], Step [130/391],                 Loss: 0.38179, Train_Acc:86.81%
Epoch [47/300], Step [140/391],                 Loss: 0.37957, Train_Acc:86.90%
Epoch [47/300], Step [150/391],                 Loss: 0.37924, Train_Acc:86.95%
Epoch [47/300], Step [160/391],                 Loss: 0.37940, Train_Acc:86.95%
Epoch [47/300], Step [170/391],                 Loss: 0.38135, Train_Acc:86.87%
Epoch [47/300], Step [180/391],                 Loss: 0.38054, Train_Acc:86.90%
Epoch [47/300], Step [190/391],                 Loss: 0.38098, Train_Acc:86.93%
Epoch [47/300], Step [200/391],                 Loss: 0.38151, Train_Acc:86.92%
Epoch [47/300], Step [210/391],                 Loss: 0.38043, Train_Acc:86.98%
Epoch [47/300], Step [220/391],                 Loss: 0.37965, Train_Acc:87.02%
Epoch [47/300], Step [230/391],                 Loss: 0.37953, Train_Acc:87.02%
Epoch [47/300], Step [240/391],                 Loss: 0.37847, Train_Acc:87.02%
Epoch [47/300], Step [250/391],                 Loss: 0.37839, Train_Acc:87.03%
Epoch [47/300], Step [260/391],                 Loss: 0.38084, Train_Acc:86.97%
Epoch [47/300], Step [270/391],                 Loss: 0.38344, Train_Acc:86.89%
Epoch [47/300], Step [280/391],                 Loss: 0.38406, Train_Acc:86.86%
Epoch [47/300], Step [290/391],                 Loss: 0.38408, Train_Acc:86.89%
Epoch [47/300], Step [300/391],                 Loss: 0.38406, Train_Acc:86.86%
Epoch [47/300], Step [310/391],                 Loss: 0.38372, Train_Acc:86.88%
Epoch [47/300], Step [320/391],                 Loss: 0.38399, Train_Acc:86.87%
Epoch [47/300], Step [330/391],                 Loss: 0.38283, Train_Acc:86.93%
Epoch [47/300], Step [340/391],                 Loss: 0.38229, Train_Acc:86.96%
Epoch [47/300], Step [350/391],                 Loss: 0.38189, Train_Acc:87.00%
Epoch [47/300], Step [360/391],                 Loss: 0.38166, Train_Acc:87.01%
Epoch [47/300], Step [370/391],                 Loss: 0.38202, Train_Acc:86.98%
Epoch [47/300], Step [380/391],                 Loss: 0.38194, Train_Acc:86.98%
Epoch [47/300], Step [390/391],                 Loss: 0.38185, Train_Acc:86.98%
Accuary on test images:76.38%
Epoch [48/300], Step [10/391],                 Loss: 0.36068, Train_Acc:88.28%
Epoch [48/300], Step [20/391],                 Loss: 0.36646, Train_Acc:87.73%
Epoch [48/300], Step [30/391],                 Loss: 0.35934, Train_Acc:88.02%
Epoch [48/300], Step [40/391],                 Loss: 0.36797, Train_Acc:87.62%
Epoch [48/300], Step [50/391],                 Loss: 0.36877, Train_Acc:87.52%
Epoch [48/300], Step [60/391],                 Loss: 0.37161, Train_Acc:87.36%
Epoch [48/300], Step [70/391],                 Loss: 0.36883, Train_Acc:87.52%
Epoch [48/300], Step [80/391],                 Loss: 0.37190, Train_Acc:87.42%
Epoch [48/300], Step [90/391],                 Loss: 0.37902, Train_Acc:87.22%
Epoch [48/300], Step [100/391],                 Loss: 0.37934, Train_Acc:87.12%
Epoch [48/300], Step [110/391],                 Loss: 0.37851, Train_Acc:87.22%
Epoch [48/300], Step [120/391],                 Loss: 0.37798, Train_Acc:87.29%
Epoch [48/300], Step [130/391],                 Loss: 0.37763, Train_Acc:87.27%
Epoch [48/300], Step [140/391],                 Loss: 0.37597, Train_Acc:87.27%
Epoch [48/300], Step [150/391],                 Loss: 0.37625, Train_Acc:87.27%
Epoch [48/300], Step [160/391],                 Loss: 0.37683, Train_Acc:87.25%
Epoch [48/300], Step [170/391],                 Loss: 0.37853, Train_Acc:87.16%
Epoch [48/300], Step [180/391],                 Loss: 0.38018, Train_Acc:87.12%
Epoch [48/300], Step [190/391],                 Loss: 0.38213, Train_Acc:87.01%
Epoch [48/300], Step [200/391],                 Loss: 0.38513, Train_Acc:86.89%
Epoch [48/300], Step [210/391],                 Loss: 0.38611, Train_Acc:86.88%
Epoch [48/300], Step [220/391],                 Loss: 0.38564, Train_Acc:86.91%
Epoch [48/300], Step [230/391],                 Loss: 0.38360, Train_Acc:87.00%
Epoch [48/300], Step [240/391],                 Loss: 0.38275, Train_Acc:87.03%
Epoch [48/300], Step [250/391],                 Loss: 0.38286, Train_Acc:87.04%
Epoch [48/300], Step [260/391],                 Loss: 0.38389, Train_Acc:87.04%
Epoch [48/300], Step [270/391],                 Loss: 0.38633, Train_Acc:86.95%
Epoch [48/300], Step [280/391],                 Loss: 0.38647, Train_Acc:86.95%
Epoch [48/300], Step [290/391],                 Loss: 0.38727, Train_Acc:86.92%
Epoch [48/300], Step [300/391],                 Loss: 0.38697, Train_Acc:86.91%
Epoch [48/300], Step [310/391],                 Loss: 0.38683, Train_Acc:86.94%
Epoch [48/300], Step [320/391],                 Loss: 0.38646, Train_Acc:86.97%
Epoch [48/300], Step [330/391],                 Loss: 0.38571, Train_Acc:86.99%
Epoch [48/300], Step [340/391],                 Loss: 0.38492, Train_Acc:87.02%
Epoch [48/300], Step [350/391],                 Loss: 0.38327, Train_Acc:87.06%
Epoch [48/300], Step [360/391],                 Loss: 0.38245, Train_Acc:87.08%
Epoch [48/300], Step [370/391],                 Loss: 0.38207, Train_Acc:87.07%
Epoch [48/300], Step [380/391],                 Loss: 0.38142, Train_Acc:87.06%
Epoch [48/300], Step [390/391],                 Loss: 0.38070, Train_Acc:87.06%
Accuary on test images:76.78%
Epoch [49/300], Step [10/391],                 Loss: 0.36755, Train_Acc:88.28%
Epoch [49/300], Step [20/391],                 Loss: 0.37010, Train_Acc:87.62%
Epoch [49/300], Step [30/391],                 Loss: 0.35877, Train_Acc:87.92%
Epoch [49/300], Step [40/391],                 Loss: 0.36123, Train_Acc:87.85%
Epoch [49/300], Step [50/391],                 Loss: 0.36124, Train_Acc:87.98%
Epoch [49/300], Step [60/391],                 Loss: 0.36727, Train_Acc:87.80%
Epoch [49/300], Step [70/391],                 Loss: 0.36925, Train_Acc:87.58%
Epoch [49/300], Step [80/391],                 Loss: 0.37222, Train_Acc:87.45%
Epoch [49/300], Step [90/391],                 Loss: 0.37404, Train_Acc:87.38%
Epoch [49/300], Step [100/391],                 Loss: 0.37173, Train_Acc:87.38%
Epoch [49/300], Step [110/391],                 Loss: 0.37441, Train_Acc:87.23%
Epoch [49/300], Step [120/391],                 Loss: 0.37608, Train_Acc:87.20%
Epoch [49/300], Step [130/391],                 Loss: 0.37814, Train_Acc:87.11%
Epoch [49/300], Step [140/391],                 Loss: 0.37680, Train_Acc:87.20%
Epoch [49/300], Step [150/391],                 Loss: 0.37833, Train_Acc:87.15%
Epoch [49/300], Step [160/391],                 Loss: 0.37728, Train_Acc:87.19%
Epoch [49/300], Step [170/391],                 Loss: 0.37897, Train_Acc:87.16%
Epoch [49/300], Step [180/391],                 Loss: 0.37949, Train_Acc:87.12%
Epoch [49/300], Step [190/391],                 Loss: 0.38013, Train_Acc:87.11%
Epoch [49/300], Step [200/391],                 Loss: 0.37922, Train_Acc:87.16%
Epoch [49/300], Step [210/391],                 Loss: 0.37908, Train_Acc:87.18%
Epoch [49/300], Step [220/391],                 Loss: 0.37647, Train_Acc:87.25%
Epoch [49/300], Step [230/391],                 Loss: 0.37529, Train_Acc:87.27%
Epoch [49/300], Step [240/391],                 Loss: 0.37483, Train_Acc:87.29%
Epoch [49/300], Step [250/391],                 Loss: 0.37451, Train_Acc:87.29%
Epoch [49/300], Step [260/391],                 Loss: 0.37589, Train_Acc:87.25%
Epoch [49/300], Step [270/391],                 Loss: 0.37681, Train_Acc:87.21%
Epoch [49/300], Step [280/391],                 Loss: 0.37747, Train_Acc:87.18%
Epoch [49/300], Step [290/391],                 Loss: 0.37728, Train_Acc:87.23%
Epoch [49/300], Step [300/391],                 Loss: 0.37660, Train_Acc:87.21%
Epoch [49/300], Step [310/391],                 Loss: 0.37628, Train_Acc:87.20%
Epoch [49/300], Step [320/391],                 Loss: 0.37721, Train_Acc:87.19%
Epoch [49/300], Step [330/391],                 Loss: 0.37671, Train_Acc:87.24%
Epoch [49/300], Step [340/391],                 Loss: 0.37667, Train_Acc:87.23%
Epoch [49/300], Step [350/391],                 Loss: 0.37717, Train_Acc:87.22%
Epoch [49/300], Step [360/391],                 Loss: 0.37771, Train_Acc:87.19%
Epoch [49/300], Step [370/391],                 Loss: 0.37781, Train_Acc:87.18%
Epoch [49/300], Step [380/391],                 Loss: 0.37740, Train_Acc:87.22%
Epoch [49/300], Step [390/391],                 Loss: 0.37611, Train_Acc:87.26%
Accuary on test images:76.86%
Epoch [50/300], Step [10/391],                 Loss: 0.36509, Train_Acc:87.81%
Epoch [50/300], Step [20/391],                 Loss: 0.36412, Train_Acc:87.62%
Epoch [50/300], Step [30/391],                 Loss: 0.36923, Train_Acc:87.32%
Epoch [50/300], Step [40/391],                 Loss: 0.37799, Train_Acc:87.15%
Epoch [50/300], Step [50/391],                 Loss: 0.38004, Train_Acc:87.08%
Epoch [50/300], Step [60/391],                 Loss: 0.38660, Train_Acc:86.85%
Epoch [50/300], Step [70/391],                 Loss: 0.38151, Train_Acc:87.11%
Epoch [50/300], Step [80/391],                 Loss: 0.38051, Train_Acc:87.11%
Epoch [50/300], Step [90/391],                 Loss: 0.38515, Train_Acc:86.99%
Epoch [50/300], Step [100/391],                 Loss: 0.38350, Train_Acc:87.02%
Epoch [50/300], Step [110/391],                 Loss: 0.38743, Train_Acc:86.89%
Epoch [50/300], Step [120/391],                 Loss: 0.39126, Train_Acc:86.74%
Epoch [50/300], Step [130/391],                 Loss: 0.39225, Train_Acc:86.74%
Epoch [50/300], Step [140/391],                 Loss: 0.39181, Train_Acc:86.76%
Epoch [50/300], Step [150/391],                 Loss: 0.39140, Train_Acc:86.74%
Epoch [50/300], Step [160/391],                 Loss: 0.38932, Train_Acc:86.85%
Epoch [50/300], Step [170/391],                 Loss: 0.39109, Train_Acc:86.77%
Epoch [50/300], Step [180/391],                 Loss: 0.39174, Train_Acc:86.72%
Epoch [50/300], Step [190/391],                 Loss: 0.39148, Train_Acc:86.67%
Epoch [50/300], Step [200/391],                 Loss: 0.39226, Train_Acc:86.66%
Epoch [50/300], Step [210/391],                 Loss: 0.39157, Train_Acc:86.69%
Epoch [50/300], Step [220/391],                 Loss: 0.38960, Train_Acc:86.76%
Epoch [50/300], Step [230/391],                 Loss: 0.38633, Train_Acc:86.83%
Epoch [50/300], Step [240/391],                 Loss: 0.38359, Train_Acc:86.93%
Epoch [50/300], Step [250/391],                 Loss: 0.38314, Train_Acc:86.94%
Epoch [50/300], Step [260/391],                 Loss: 0.38582, Train_Acc:86.89%
Epoch [50/300], Step [270/391],                 Loss: 0.38763, Train_Acc:86.80%
Epoch [50/300], Step [280/391],                 Loss: 0.38808, Train_Acc:86.78%
Epoch [50/300], Step [290/391],                 Loss: 0.38747, Train_Acc:86.78%
Epoch [50/300], Step [300/391],                 Loss: 0.38701, Train_Acc:86.80%
Epoch [50/300], Step [310/391],                 Loss: 0.38485, Train_Acc:86.90%
Epoch [50/300], Step [320/391],                 Loss: 0.38492, Train_Acc:86.90%
Epoch [50/300], Step [330/391],                 Loss: 0.38445, Train_Acc:86.93%
Epoch [50/300], Step [340/391],                 Loss: 0.38431, Train_Acc:86.93%
Epoch [50/300], Step [350/391],                 Loss: 0.38368, Train_Acc:86.98%
Epoch [50/300], Step [360/391],                 Loss: 0.38358, Train_Acc:87.00%
Epoch [50/300], Step [370/391],                 Loss: 0.38389, Train_Acc:86.97%
Epoch [50/300], Step [380/391],                 Loss: 0.38417, Train_Acc:86.94%
Epoch [50/300], Step [390/391],                 Loss: 0.38285, Train_Acc:86.98%
Accuary on test images:64.18%
Epoch [51/300], Step [10/391],                 Loss: 0.39107, Train_Acc:86.64%
Epoch [51/300], Step [20/391],                 Loss: 0.37736, Train_Acc:87.34%
Epoch [51/300], Step [30/391],                 Loss: 0.36513, Train_Acc:87.55%
Epoch [51/300], Step [40/391],                 Loss: 0.37297, Train_Acc:87.29%
Epoch [51/300], Step [50/391],                 Loss: 0.37089, Train_Acc:87.39%
Epoch [51/300], Step [60/391],                 Loss: 0.37817, Train_Acc:87.23%
Epoch [51/300], Step [70/391],                 Loss: 0.37522, Train_Acc:87.40%
Epoch [51/300], Step [80/391],                 Loss: 0.37964, Train_Acc:87.34%
Epoch [51/300], Step [90/391],                 Loss: 0.37937, Train_Acc:87.27%
Epoch [51/300], Step [100/391],                 Loss: 0.37801, Train_Acc:87.29%
Epoch [51/300], Step [110/391],                 Loss: 0.37976, Train_Acc:87.23%
Epoch [51/300], Step [120/391],                 Loss: 0.38151, Train_Acc:87.10%
Epoch [51/300], Step [130/391],                 Loss: 0.38469, Train_Acc:87.01%
Epoch [51/300], Step [140/391],                 Loss: 0.38469, Train_Acc:87.01%
Epoch [51/300], Step [150/391],                 Loss: 0.38304, Train_Acc:87.04%
Epoch [51/300], Step [160/391],                 Loss: 0.38305, Train_Acc:86.97%
Epoch [51/300], Step [170/391],                 Loss: 0.38269, Train_Acc:86.95%
Epoch [51/300], Step [180/391],                 Loss: 0.38411, Train_Acc:86.91%
Epoch [51/300], Step [190/391],                 Loss: 0.38349, Train_Acc:86.95%
Epoch [51/300], Step [200/391],                 Loss: 0.38523, Train_Acc:86.92%
Epoch [51/300], Step [210/391],                 Loss: 0.38553, Train_Acc:86.95%
Epoch [51/300], Step [220/391],                 Loss: 0.38491, Train_Acc:86.94%
Epoch [51/300], Step [230/391],                 Loss: 0.38526, Train_Acc:86.93%
Epoch [51/300], Step [240/391],                 Loss: 0.38386, Train_Acc:86.96%
Epoch [51/300], Step [250/391],                 Loss: 0.38257, Train_Acc:86.98%
Epoch [51/300], Step [260/391],                 Loss: 0.38419, Train_Acc:86.96%
Epoch [51/300], Step [270/391],                 Loss: 0.38430, Train_Acc:86.97%
Epoch [51/300], Step [280/391],                 Loss: 0.38362, Train_Acc:86.99%
Epoch [51/300], Step [290/391],                 Loss: 0.38291, Train_Acc:87.04%
Epoch [51/300], Step [300/391],                 Loss: 0.38153, Train_Acc:87.06%
Epoch [51/300], Step [310/391],                 Loss: 0.38141, Train_Acc:87.03%
Epoch [51/300], Step [320/391],                 Loss: 0.38141, Train_Acc:87.03%
Epoch [51/300], Step [330/391],                 Loss: 0.37990, Train_Acc:87.08%
Epoch [51/300], Step [340/391],                 Loss: 0.38008, Train_Acc:87.08%
Epoch [51/300], Step [350/391],                 Loss: 0.37965, Train_Acc:87.11%
Epoch [51/300], Step [360/391],                 Loss: 0.37856, Train_Acc:87.12%
Epoch [51/300], Step [370/391],                 Loss: 0.37820, Train_Acc:87.12%
Epoch [51/300], Step [380/391],                 Loss: 0.37775, Train_Acc:87.15%
Epoch [51/300], Step [390/391],                 Loss: 0.37680, Train_Acc:87.16%
Accuary on test images:74.76%
Epoch [52/300], Step [10/391],                 Loss: 0.37759, Train_Acc:86.64%
Epoch [52/300], Step [20/391],                 Loss: 0.37548, Train_Acc:86.84%
Epoch [52/300], Step [30/391],                 Loss: 0.37930, Train_Acc:87.01%
Epoch [52/300], Step [40/391],                 Loss: 0.39290, Train_Acc:86.86%
Epoch [52/300], Step [50/391],                 Loss: 0.38942, Train_Acc:86.88%
Epoch [52/300], Step [60/391],                 Loss: 0.39366, Train_Acc:86.60%
Epoch [52/300], Step [70/391],                 Loss: 0.39398, Train_Acc:86.67%
Epoch [52/300], Step [80/391],                 Loss: 0.39558, Train_Acc:86.58%
Epoch [52/300], Step [90/391],                 Loss: 0.39840, Train_Acc:86.51%
Epoch [52/300], Step [100/391],                 Loss: 0.39611, Train_Acc:86.50%
Epoch [52/300], Step [110/391],                 Loss: 0.39813, Train_Acc:86.44%
Epoch [52/300], Step [120/391],                 Loss: 0.39532, Train_Acc:86.48%
Epoch [52/300], Step [130/391],                 Loss: 0.39482, Train_Acc:86.52%
Epoch [52/300], Step [140/391],                 Loss: 0.39202, Train_Acc:86.62%
Epoch [52/300], Step [150/391],                 Loss: 0.39062, Train_Acc:86.69%
Epoch [52/300], Step [160/391],                 Loss: 0.38715, Train_Acc:86.83%
Epoch [52/300], Step [170/391],                 Loss: 0.38623, Train_Acc:86.88%
Epoch [52/300], Step [180/391],                 Loss: 0.38496, Train_Acc:86.88%
Epoch [52/300], Step [190/391],                 Loss: 0.38516, Train_Acc:86.90%
Epoch [52/300], Step [200/391],                 Loss: 0.38605, Train_Acc:86.82%
Epoch [52/300], Step [210/391],                 Loss: 0.38568, Train_Acc:86.87%
Epoch [52/300], Step [220/391],                 Loss: 0.38542, Train_Acc:86.88%
Epoch [52/300], Step [230/391],                 Loss: 0.38471, Train_Acc:86.89%
Epoch [52/300], Step [240/391],                 Loss: 0.38498, Train_Acc:86.89%
Epoch [52/300], Step [250/391],                 Loss: 0.38559, Train_Acc:86.88%
Epoch [52/300], Step [260/391],                 Loss: 0.38736, Train_Acc:86.86%
Epoch [52/300], Step [270/391],                 Loss: 0.38858, Train_Acc:86.82%
Epoch [52/300], Step [280/391],                 Loss: 0.38840, Train_Acc:86.82%
Epoch [52/300], Step [290/391],                 Loss: 0.38744, Train_Acc:86.84%
Epoch [52/300], Step [300/391],                 Loss: 0.38644, Train_Acc:86.89%
Epoch [52/300], Step [310/391],                 Loss: 0.38560, Train_Acc:86.89%
Epoch [52/300], Step [320/391],                 Loss: 0.38576, Train_Acc:86.90%
Epoch [52/300], Step [330/391],                 Loss: 0.38504, Train_Acc:86.93%
Epoch [52/300], Step [340/391],                 Loss: 0.38410, Train_Acc:86.97%
Epoch [52/300], Step [350/391],                 Loss: 0.38301, Train_Acc:87.01%
Epoch [52/300], Step [360/391],                 Loss: 0.38376, Train_Acc:86.96%
Epoch [52/300], Step [370/391],                 Loss: 0.38493, Train_Acc:86.92%
Epoch [52/300], Step [380/391],                 Loss: 0.38426, Train_Acc:86.95%
Epoch [52/300], Step [390/391],                 Loss: 0.38316, Train_Acc:87.01%
Accuary on test images:77.08%
Epoch [53/300], Step [10/391],                 Loss: 0.37542, Train_Acc:87.42%
Epoch [53/300], Step [20/391],                 Loss: 0.37058, Train_Acc:87.73%
Epoch [53/300], Step [30/391],                 Loss: 0.37480, Train_Acc:87.37%
Epoch [53/300], Step [40/391],                 Loss: 0.37083, Train_Acc:87.44%
Epoch [53/300], Step [50/391],                 Loss: 0.37277, Train_Acc:87.31%
Epoch [53/300], Step [60/391],                 Loss: 0.38077, Train_Acc:87.01%
Epoch [53/300], Step [70/391],                 Loss: 0.38424, Train_Acc:86.94%
Epoch [53/300], Step [80/391],                 Loss: 0.38362, Train_Acc:87.01%
Epoch [53/300], Step [90/391],                 Loss: 0.38267, Train_Acc:87.01%
Epoch [53/300], Step [100/391],                 Loss: 0.37824, Train_Acc:87.21%
Epoch [53/300], Step [110/391],                 Loss: 0.37756, Train_Acc:87.28%
Epoch [53/300], Step [120/391],                 Loss: 0.37771, Train_Acc:87.29%
Epoch [53/300], Step [130/391],                 Loss: 0.38042, Train_Acc:87.22%
Epoch [53/300], Step [140/391],                 Loss: 0.38035, Train_Acc:87.23%
Epoch [53/300], Step [150/391],                 Loss: 0.38199, Train_Acc:87.16%
Epoch [53/300], Step [160/391],                 Loss: 0.38168, Train_Acc:87.21%
Epoch [53/300], Step [170/391],                 Loss: 0.38323, Train_Acc:87.16%
Epoch [53/300], Step [180/391],                 Loss: 0.38447, Train_Acc:87.04%
Epoch [53/300], Step [190/391],                 Loss: 0.38576, Train_Acc:86.99%
Epoch [53/300], Step [200/391],                 Loss: 0.38596, Train_Acc:87.00%
Epoch [53/300], Step [210/391],                 Loss: 0.38592, Train_Acc:87.02%
Epoch [53/300], Step [220/391],                 Loss: 0.38459, Train_Acc:87.07%
Epoch [53/300], Step [230/391],                 Loss: 0.38145, Train_Acc:87.16%
Epoch [53/300], Step [240/391],                 Loss: 0.38039, Train_Acc:87.22%
Epoch [53/300], Step [250/391],                 Loss: 0.38035, Train_Acc:87.21%
Epoch [53/300], Step [260/391],                 Loss: 0.38218, Train_Acc:87.12%
Epoch [53/300], Step [270/391],                 Loss: 0.38308, Train_Acc:87.14%
Epoch [53/300], Step [280/391],                 Loss: 0.38309, Train_Acc:87.14%
Epoch [53/300], Step [290/391],                 Loss: 0.38238, Train_Acc:87.16%
Epoch [53/300], Step [300/391],                 Loss: 0.38146, Train_Acc:87.17%
Epoch [53/300], Step [310/391],                 Loss: 0.38071, Train_Acc:87.20%
Epoch [53/300], Step [320/391],                 Loss: 0.38033, Train_Acc:87.23%
Epoch [53/300], Step [330/391],                 Loss: 0.37934, Train_Acc:87.28%
Epoch [53/300], Step [340/391],                 Loss: 0.37858, Train_Acc:87.31%
Epoch [53/300], Step [350/391],                 Loss: 0.37790, Train_Acc:87.36%
Epoch [53/300], Step [360/391],                 Loss: 0.37835, Train_Acc:87.32%
Epoch [53/300], Step [370/391],                 Loss: 0.37889, Train_Acc:87.30%
Epoch [53/300], Step [380/391],                 Loss: 0.37892, Train_Acc:87.31%
Epoch [53/300], Step [390/391],                 Loss: 0.37920, Train_Acc:87.28%
Accuary on test images:71.64%
Epoch [54/300], Step [10/391],                 Loss: 0.35837, Train_Acc:87.81%
Epoch [54/300], Step [20/391],                 Loss: 0.36433, Train_Acc:87.70%
Epoch [54/300], Step [30/391],                 Loss: 0.36507, Train_Acc:87.55%
Epoch [54/300], Step [40/391],                 Loss: 0.35670, Train_Acc:88.07%
Epoch [54/300], Step [50/391],                 Loss: 0.35694, Train_Acc:88.11%
Epoch [54/300], Step [60/391],                 Loss: 0.36468, Train_Acc:87.75%
Epoch [54/300], Step [70/391],                 Loss: 0.36487, Train_Acc:87.66%
Epoch [54/300], Step [80/391],                 Loss: 0.36611, Train_Acc:87.64%
Epoch [54/300], Step [90/391],                 Loss: 0.36824, Train_Acc:87.54%
Epoch [54/300], Step [100/391],                 Loss: 0.36730, Train_Acc:87.55%
Epoch [54/300], Step [110/391],                 Loss: 0.37118, Train_Acc:87.36%
Epoch [54/300], Step [120/391],                 Loss: 0.37484, Train_Acc:87.19%
Epoch [54/300], Step [130/391],                 Loss: 0.37608, Train_Acc:87.13%
Epoch [54/300], Step [140/391],                 Loss: 0.37523, Train_Acc:87.21%
Epoch [54/300], Step [150/391],                 Loss: 0.37669, Train_Acc:87.12%
Epoch [54/300], Step [160/391],                 Loss: 0.37634, Train_Acc:87.12%
Epoch [54/300], Step [170/391],                 Loss: 0.37715, Train_Acc:87.07%
Epoch [54/300], Step [180/391],                 Loss: 0.37809, Train_Acc:87.04%
Epoch [54/300], Step [190/391],                 Loss: 0.37785, Train_Acc:87.04%
Epoch [54/300], Step [200/391],                 Loss: 0.37899, Train_Acc:87.01%
Epoch [54/300], Step [210/391],                 Loss: 0.37959, Train_Acc:86.98%
Epoch [54/300], Step [220/391],                 Loss: 0.37918, Train_Acc:86.97%
Epoch [54/300], Step [230/391],                 Loss: 0.37699, Train_Acc:87.02%
Epoch [54/300], Step [240/391],                 Loss: 0.37597, Train_Acc:87.09%
Epoch [54/300], Step [250/391],                 Loss: 0.37675, Train_Acc:87.06%
Epoch [54/300], Step [260/391],                 Loss: 0.38078, Train_Acc:86.91%
Epoch [54/300], Step [270/391],                 Loss: 0.38427, Train_Acc:86.80%
Epoch [54/300], Step [280/391],                 Loss: 0.38511, Train_Acc:86.81%
Epoch [54/300], Step [290/391],                 Loss: 0.38468, Train_Acc:86.83%
Epoch [54/300], Step [300/391],                 Loss: 0.38352, Train_Acc:86.88%
Epoch [54/300], Step [310/391],                 Loss: 0.38276, Train_Acc:86.91%
Epoch [54/300], Step [320/391],                 Loss: 0.38358, Train_Acc:86.88%
Epoch [54/300], Step [330/391],                 Loss: 0.38331, Train_Acc:86.89%
Epoch [54/300], Step [340/391],                 Loss: 0.38286, Train_Acc:86.90%
Epoch [54/300], Step [350/391],                 Loss: 0.38227, Train_Acc:86.90%
Epoch [54/300], Step [360/391],                 Loss: 0.38324, Train_Acc:86.89%
Epoch [54/300], Step [370/391],                 Loss: 0.38401, Train_Acc:86.88%
Epoch [54/300], Step [380/391],                 Loss: 0.38428, Train_Acc:86.87%
Epoch [54/300], Step [390/391],                 Loss: 0.38306, Train_Acc:86.92%
Accuary on test images:80.02%
Epoch [55/300], Step [10/391],                 Loss: 0.34821, Train_Acc:88.52%
Epoch [55/300], Step [20/391],                 Loss: 0.34667, Train_Acc:88.48%
Epoch [55/300], Step [30/391],                 Loss: 0.34389, Train_Acc:88.54%
Epoch [55/300], Step [40/391],                 Loss: 0.34886, Train_Acc:88.61%
Epoch [55/300], Step [50/391],                 Loss: 0.36051, Train_Acc:88.11%
Epoch [55/300], Step [60/391],                 Loss: 0.37013, Train_Acc:87.81%
Epoch [55/300], Step [70/391],                 Loss: 0.37142, Train_Acc:87.72%
Epoch [55/300], Step [80/391],                 Loss: 0.37689, Train_Acc:87.39%
Epoch [55/300], Step [90/391],                 Loss: 0.38162, Train_Acc:87.00%
Epoch [55/300], Step [100/391],                 Loss: 0.38123, Train_Acc:86.95%
Epoch [55/300], Step [110/391],                 Loss: 0.38388, Train_Acc:86.84%
Epoch [55/300], Step [120/391],                 Loss: 0.38234, Train_Acc:86.85%
Epoch [55/300], Step [130/391],                 Loss: 0.38310, Train_Acc:86.84%
Epoch [55/300], Step [140/391],                 Loss: 0.38182, Train_Acc:86.84%
Epoch [55/300], Step [150/391],                 Loss: 0.38337, Train_Acc:86.80%
Epoch [55/300], Step [160/391],                 Loss: 0.38353, Train_Acc:86.81%
Epoch [55/300], Step [170/391],                 Loss: 0.38345, Train_Acc:86.84%
Epoch [55/300], Step [180/391],                 Loss: 0.38268, Train_Acc:86.86%
Epoch [55/300], Step [190/391],                 Loss: 0.38200, Train_Acc:86.86%
Epoch [55/300], Step [200/391],                 Loss: 0.38223, Train_Acc:86.88%
Epoch [55/300], Step [210/391],                 Loss: 0.38097, Train_Acc:86.89%
Epoch [55/300], Step [220/391],                 Loss: 0.37915, Train_Acc:86.97%
Epoch [55/300], Step [230/391],                 Loss: 0.37640, Train_Acc:87.07%
Epoch [55/300], Step [240/391],                 Loss: 0.37384, Train_Acc:87.15%
Epoch [55/300], Step [250/391],                 Loss: 0.37392, Train_Acc:87.11%
Epoch [55/300], Step [260/391],                 Loss: 0.37612, Train_Acc:87.07%
Epoch [55/300], Step [270/391],                 Loss: 0.37784, Train_Acc:87.01%
Epoch [55/300], Step [280/391],                 Loss: 0.37698, Train_Acc:87.03%
Epoch [55/300], Step [290/391],                 Loss: 0.37712, Train_Acc:87.02%
Epoch [55/300], Step [300/391],                 Loss: 0.37694, Train_Acc:87.04%
Epoch [55/300], Step [310/391],                 Loss: 0.37721, Train_Acc:87.03%
Epoch [55/300], Step [320/391],                 Loss: 0.37763, Train_Acc:87.00%
Epoch [55/300], Step [330/391],                 Loss: 0.37775, Train_Acc:87.01%
Epoch [55/300], Step [340/391],                 Loss: 0.37854, Train_Acc:86.96%
Epoch [55/300], Step [350/391],                 Loss: 0.37750, Train_Acc:87.02%
Epoch [55/300], Step [360/391],                 Loss: 0.37733, Train_Acc:87.02%
Epoch [55/300], Step [370/391],                 Loss: 0.37743, Train_Acc:87.03%
Epoch [55/300], Step [380/391],                 Loss: 0.37718, Train_Acc:87.03%
Epoch [55/300], Step [390/391],                 Loss: 0.37685, Train_Acc:87.04%
Accuary on test images:75.82%
Epoch [56/300], Step [10/391],                 Loss: 0.40965, Train_Acc:87.27%
Epoch [56/300], Step [20/391],                 Loss: 0.39640, Train_Acc:87.07%
Epoch [56/300], Step [30/391],                 Loss: 0.38097, Train_Acc:87.14%
Epoch [56/300], Step [40/391],                 Loss: 0.38302, Train_Acc:87.05%
Epoch [56/300], Step [50/391],                 Loss: 0.38058, Train_Acc:87.08%
Epoch [56/300], Step [60/391],                 Loss: 0.38657, Train_Acc:86.84%
Epoch [56/300], Step [70/391],                 Loss: 0.38620, Train_Acc:86.95%
Epoch [56/300], Step [80/391],                 Loss: 0.38616, Train_Acc:86.85%
Epoch [56/300], Step [90/391],                 Loss: 0.38728, Train_Acc:86.78%
Epoch [56/300], Step [100/391],                 Loss: 0.38342, Train_Acc:86.92%
Epoch [56/300], Step [110/391],                 Loss: 0.38599, Train_Acc:86.80%
Epoch [56/300], Step [120/391],                 Loss: 0.38651, Train_Acc:86.73%
Epoch [56/300], Step [130/391],                 Loss: 0.38705, Train_Acc:86.71%
Epoch [56/300], Step [140/391],                 Loss: 0.38569, Train_Acc:86.77%
Epoch [56/300], Step [150/391],                 Loss: 0.38704, Train_Acc:86.71%
Epoch [56/300], Step [160/391],                 Loss: 0.38529, Train_Acc:86.72%
Epoch [56/300], Step [170/391],                 Loss: 0.38646, Train_Acc:86.65%
Epoch [56/300], Step [180/391],                 Loss: 0.38636, Train_Acc:86.61%
Epoch [56/300], Step [190/391],                 Loss: 0.38628, Train_Acc:86.61%
Epoch [56/300], Step [200/391],                 Loss: 0.38746, Train_Acc:86.60%
Epoch [56/300], Step [210/391],                 Loss: 0.38702, Train_Acc:86.61%
Epoch [56/300], Step [220/391],                 Loss: 0.38664, Train_Acc:86.64%
Epoch [56/300], Step [230/391],                 Loss: 0.38431, Train_Acc:86.73%
Epoch [56/300], Step [240/391],                 Loss: 0.38182, Train_Acc:86.83%
Epoch [56/300], Step [250/391],                 Loss: 0.38028, Train_Acc:86.89%
Epoch [56/300], Step [260/391],                 Loss: 0.38073, Train_Acc:86.91%
Epoch [56/300], Step [270/391],                 Loss: 0.38122, Train_Acc:86.90%
Epoch [56/300], Step [280/391],                 Loss: 0.38008, Train_Acc:86.95%
Epoch [56/300], Step [290/391],                 Loss: 0.37986, Train_Acc:86.98%
Epoch [56/300], Step [300/391],                 Loss: 0.37865, Train_Acc:86.99%
Epoch [56/300], Step [310/391],                 Loss: 0.37925, Train_Acc:87.01%
Epoch [56/300], Step [320/391],                 Loss: 0.37984, Train_Acc:86.98%
Epoch [56/300], Step [330/391],                 Loss: 0.37842, Train_Acc:87.03%
Epoch [56/300], Step [340/391],                 Loss: 0.37752, Train_Acc:87.03%
Epoch [56/300], Step [350/391],                 Loss: 0.37709, Train_Acc:87.04%
Epoch [56/300], Step [360/391],                 Loss: 0.37775, Train_Acc:87.01%
Epoch [56/300], Step [370/391],                 Loss: 0.37897, Train_Acc:86.98%
Epoch [56/300], Step [380/391],                 Loss: 0.37935, Train_Acc:86.99%
Epoch [56/300], Step [390/391],                 Loss: 0.37959, Train_Acc:86.98%
Accuary on test images:76.62%
Epoch [57/300], Step [10/391],                 Loss: 0.40669, Train_Acc:86.64%
Epoch [57/300], Step [20/391],                 Loss: 0.38555, Train_Acc:87.15%
Epoch [57/300], Step [30/391],                 Loss: 0.37344, Train_Acc:87.47%
Epoch [57/300], Step [40/391],                 Loss: 0.37115, Train_Acc:87.58%
Epoch [57/300], Step [50/391],                 Loss: 0.36822, Train_Acc:87.72%
Epoch [57/300], Step [60/391],                 Loss: 0.37100, Train_Acc:87.66%
Epoch [57/300], Step [70/391],                 Loss: 0.37306, Train_Acc:87.56%
Epoch [57/300], Step [80/391],                 Loss: 0.37232, Train_Acc:87.60%
Epoch [57/300], Step [90/391],                 Loss: 0.37391, Train_Acc:87.46%
Epoch [57/300], Step [100/391],                 Loss: 0.37141, Train_Acc:87.50%
Epoch [57/300], Step [110/391],                 Loss: 0.37336, Train_Acc:87.44%
Epoch [57/300], Step [120/391],                 Loss: 0.37422, Train_Acc:87.38%
Epoch [57/300], Step [130/391],                 Loss: 0.37479, Train_Acc:87.33%
Epoch [57/300], Step [140/391],                 Loss: 0.37444, Train_Acc:87.40%
Epoch [57/300], Step [150/391],                 Loss: 0.37384, Train_Acc:87.40%
Epoch [57/300], Step [160/391],                 Loss: 0.37314, Train_Acc:87.36%
Epoch [57/300], Step [170/391],                 Loss: 0.37465, Train_Acc:87.31%
Epoch [57/300], Step [180/391],                 Loss: 0.37665, Train_Acc:87.24%
Epoch [57/300], Step [190/391],                 Loss: 0.37777, Train_Acc:87.19%
Epoch [57/300], Step [200/391],                 Loss: 0.37865, Train_Acc:87.20%
Epoch [57/300], Step [210/391],                 Loss: 0.37785, Train_Acc:87.24%
Epoch [57/300], Step [220/391],                 Loss: 0.37785, Train_Acc:87.28%
Epoch [57/300], Step [230/391],                 Loss: 0.37840, Train_Acc:87.25%
Epoch [57/300], Step [240/391],                 Loss: 0.37755, Train_Acc:87.23%
Epoch [57/300], Step [250/391],                 Loss: 0.37724, Train_Acc:87.21%
Epoch [57/300], Step [260/391],                 Loss: 0.37886, Train_Acc:87.14%
Epoch [57/300], Step [270/391],                 Loss: 0.38137, Train_Acc:87.03%
Epoch [57/300], Step [280/391],                 Loss: 0.38138, Train_Acc:87.04%
Epoch [57/300], Step [290/391],                 Loss: 0.38150, Train_Acc:87.08%
Epoch [57/300], Step [300/391],                 Loss: 0.38096, Train_Acc:87.07%
Epoch [57/300], Step [310/391],                 Loss: 0.37990, Train_Acc:87.12%
Epoch [57/300], Step [320/391],                 Loss: 0.37974, Train_Acc:87.14%
Epoch [57/300], Step [330/391],                 Loss: 0.37852, Train_Acc:87.16%
Epoch [57/300], Step [340/391],                 Loss: 0.37788, Train_Acc:87.18%
Epoch [57/300], Step [350/391],                 Loss: 0.37786, Train_Acc:87.18%
Epoch [57/300], Step [360/391],                 Loss: 0.37790, Train_Acc:87.14%
Epoch [57/300], Step [370/391],                 Loss: 0.37758, Train_Acc:87.18%
Epoch [57/300], Step [380/391],                 Loss: 0.37746, Train_Acc:87.18%
Epoch [57/300], Step [390/391],                 Loss: 0.37796, Train_Acc:87.16%
Accuary on test images:65.76%
Epoch [58/300], Step [10/391],                 Loss: 0.44715, Train_Acc:85.70%
Epoch [58/300], Step [20/391],                 Loss: 0.42161, Train_Acc:86.64%
Epoch [58/300], Step [30/391],                 Loss: 0.40080, Train_Acc:86.93%
Epoch [58/300], Step [40/391],                 Loss: 0.38754, Train_Acc:87.27%
Epoch [58/300], Step [50/391],                 Loss: 0.38008, Train_Acc:87.55%
Epoch [58/300], Step [60/391],                 Loss: 0.38811, Train_Acc:87.12%
Epoch [58/300], Step [70/391],                 Loss: 0.38109, Train_Acc:87.27%
Epoch [58/300], Step [80/391],                 Loss: 0.38221, Train_Acc:87.20%
Epoch [58/300], Step [90/391],                 Loss: 0.38575, Train_Acc:87.07%
Epoch [58/300], Step [100/391],                 Loss: 0.38294, Train_Acc:87.06%
Epoch [58/300], Step [110/391],                 Loss: 0.38341, Train_Acc:87.12%
Epoch [58/300], Step [120/391],                 Loss: 0.38177, Train_Acc:87.16%
Epoch [58/300], Step [130/391],                 Loss: 0.38003, Train_Acc:87.22%
Epoch [58/300], Step [140/391],                 Loss: 0.37592, Train_Acc:87.33%
Epoch [58/300], Step [150/391],                 Loss: 0.37554, Train_Acc:87.32%
Epoch [58/300], Step [160/391],                 Loss: 0.37646, Train_Acc:87.25%
Epoch [58/300], Step [170/391],                 Loss: 0.37757, Train_Acc:87.17%
Epoch [58/300], Step [180/391],                 Loss: 0.37799, Train_Acc:87.14%
Epoch [58/300], Step [190/391],                 Loss: 0.37985, Train_Acc:87.11%
Epoch [58/300], Step [200/391],                 Loss: 0.38130, Train_Acc:87.10%
Epoch [58/300], Step [210/391],                 Loss: 0.38184, Train_Acc:87.06%
Epoch [58/300], Step [220/391],                 Loss: 0.38110, Train_Acc:87.06%
Epoch [58/300], Step [230/391],                 Loss: 0.37828, Train_Acc:87.17%
Epoch [58/300], Step [240/391],                 Loss: 0.37661, Train_Acc:87.27%
Epoch [58/300], Step [250/391],                 Loss: 0.37603, Train_Acc:87.28%
Epoch [58/300], Step [260/391],                 Loss: 0.37772, Train_Acc:87.22%
Epoch [58/300], Step [270/391],                 Loss: 0.38023, Train_Acc:87.11%
Epoch [58/300], Step [280/391],                 Loss: 0.38142, Train_Acc:87.10%
Epoch [58/300], Step [290/391],                 Loss: 0.38023, Train_Acc:87.14%
Epoch [58/300], Step [300/391],                 Loss: 0.37904, Train_Acc:87.19%
Epoch [58/300], Step [310/391],                 Loss: 0.37736, Train_Acc:87.25%
Epoch [58/300], Step [320/391],                 Loss: 0.37597, Train_Acc:87.29%
Epoch [58/300], Step [330/391],                 Loss: 0.37374, Train_Acc:87.37%
Epoch [58/300], Step [340/391],                 Loss: 0.37309, Train_Acc:87.39%
Epoch [58/300], Step [350/391],                 Loss: 0.37311, Train_Acc:87.40%
Epoch [58/300], Step [360/391],                 Loss: 0.37302, Train_Acc:87.39%
Epoch [58/300], Step [370/391],                 Loss: 0.37391, Train_Acc:87.34%
Epoch [58/300], Step [380/391],                 Loss: 0.37379, Train_Acc:87.34%
Epoch [58/300], Step [390/391],                 Loss: 0.37315, Train_Acc:87.35%
Accuary on test images:71.66%
Epoch [59/300], Step [10/391],                 Loss: 0.40231, Train_Acc:85.08%
Epoch [59/300], Step [20/391],                 Loss: 0.40057, Train_Acc:85.78%
Epoch [59/300], Step [30/391],                 Loss: 0.38422, Train_Acc:86.46%
Epoch [59/300], Step [40/391],                 Loss: 0.38537, Train_Acc:86.74%
Epoch [59/300], Step [50/391],                 Loss: 0.37690, Train_Acc:86.88%
Epoch [59/300], Step [60/391],                 Loss: 0.38318, Train_Acc:86.69%
Epoch [59/300], Step [70/391],                 Loss: 0.37661, Train_Acc:86.95%
Epoch [59/300], Step [80/391],                 Loss: 0.37721, Train_Acc:86.91%
Epoch [59/300], Step [90/391],                 Loss: 0.37912, Train_Acc:86.93%
Epoch [59/300], Step [100/391],                 Loss: 0.37659, Train_Acc:86.90%
Epoch [59/300], Step [110/391],                 Loss: 0.37643, Train_Acc:87.02%
Epoch [59/300], Step [120/391],                 Loss: 0.37505, Train_Acc:86.97%
Epoch [59/300], Step [130/391],                 Loss: 0.37590, Train_Acc:87.01%
Epoch [59/300], Step [140/391],                 Loss: 0.37383, Train_Acc:87.06%
Epoch [59/300], Step [150/391],                 Loss: 0.37334, Train_Acc:87.10%
Epoch [59/300], Step [160/391],                 Loss: 0.37298, Train_Acc:87.10%
Epoch [59/300], Step [170/391],                 Loss: 0.37546, Train_Acc:87.04%
Epoch [59/300], Step [180/391],                 Loss: 0.37552, Train_Acc:87.06%
Epoch [59/300], Step [190/391],                 Loss: 0.37539, Train_Acc:87.07%
Epoch [59/300], Step [200/391],                 Loss: 0.37741, Train_Acc:87.02%
Epoch [59/300], Step [210/391],                 Loss: 0.37768, Train_Acc:87.01%
Epoch [59/300], Step [220/391],                 Loss: 0.37964, Train_Acc:86.93%
Epoch [59/300], Step [230/391],                 Loss: 0.37797, Train_Acc:86.99%
Epoch [59/300], Step [240/391],                 Loss: 0.37628, Train_Acc:87.05%
Epoch [59/300], Step [250/391],                 Loss: 0.37779, Train_Acc:86.96%
Epoch [59/300], Step [260/391],                 Loss: 0.37987, Train_Acc:86.93%
Epoch [59/300], Step [270/391],                 Loss: 0.38146, Train_Acc:86.84%
Epoch [59/300], Step [280/391],                 Loss: 0.38046, Train_Acc:86.86%
Epoch [59/300], Step [290/391],                 Loss: 0.37981, Train_Acc:86.90%
Epoch [59/300], Step [300/391],                 Loss: 0.37874, Train_Acc:86.94%
Epoch [59/300], Step [310/391],                 Loss: 0.37830, Train_Acc:86.96%
Epoch [59/300], Step [320/391],                 Loss: 0.37717, Train_Acc:87.00%
Epoch [59/300], Step [330/391],                 Loss: 0.37709, Train_Acc:87.04%
Epoch [59/300], Step [340/391],                 Loss: 0.37678, Train_Acc:87.03%
Epoch [59/300], Step [350/391],                 Loss: 0.37620, Train_Acc:87.06%
Epoch [59/300], Step [360/391],                 Loss: 0.37530, Train_Acc:87.09%
Epoch [59/300], Step [370/391],                 Loss: 0.37575, Train_Acc:87.07%
Epoch [59/300], Step [380/391],                 Loss: 0.37595, Train_Acc:87.07%
Epoch [59/300], Step [390/391],                 Loss: 0.37596, Train_Acc:87.09%
Accuary on test images:73.10%
Epoch [60/300], Step [10/391],                 Loss: 0.35879, Train_Acc:87.34%
Epoch [60/300], Step [20/391],                 Loss: 0.38434, Train_Acc:86.56%
Epoch [60/300], Step [30/391],                 Loss: 0.37319, Train_Acc:86.98%
Epoch [60/300], Step [40/391],                 Loss: 0.37263, Train_Acc:87.25%
Epoch [60/300], Step [50/391],                 Loss: 0.37813, Train_Acc:87.22%
Epoch [60/300], Step [60/391],                 Loss: 0.38109, Train_Acc:87.19%
Epoch [60/300], Step [70/391],                 Loss: 0.37911, Train_Acc:87.19%
Epoch [60/300], Step [80/391],                 Loss: 0.38079, Train_Acc:87.06%
Epoch [60/300], Step [90/391],                 Loss: 0.38247, Train_Acc:86.93%
Epoch [60/300], Step [100/391],                 Loss: 0.37971, Train_Acc:87.04%
Epoch [60/300], Step [110/391],                 Loss: 0.38113, Train_Acc:87.04%
Epoch [60/300], Step [120/391],                 Loss: 0.37846, Train_Acc:87.13%
Epoch [60/300], Step [130/391],                 Loss: 0.37933, Train_Acc:87.15%
Epoch [60/300], Step [140/391],                 Loss: 0.37819, Train_Acc:87.23%
Epoch [60/300], Step [150/391],                 Loss: 0.37715, Train_Acc:87.32%
Epoch [60/300], Step [160/391],                 Loss: 0.37526, Train_Acc:87.31%
Epoch [60/300], Step [170/391],                 Loss: 0.37644, Train_Acc:87.23%
Epoch [60/300], Step [180/391],                 Loss: 0.37893, Train_Acc:87.15%
Epoch [60/300], Step [190/391],                 Loss: 0.37839, Train_Acc:87.21%
Epoch [60/300], Step [200/391],                 Loss: 0.37996, Train_Acc:87.15%
Epoch [60/300], Step [210/391],                 Loss: 0.37915, Train_Acc:87.17%
Epoch [60/300], Step [220/391],                 Loss: 0.37740, Train_Acc:87.23%
Epoch [60/300], Step [230/391],                 Loss: 0.37614, Train_Acc:87.24%
Epoch [60/300], Step [240/391],                 Loss: 0.37551, Train_Acc:87.23%
Epoch [60/300], Step [250/391],                 Loss: 0.37576, Train_Acc:87.23%
Epoch [60/300], Step [260/391],                 Loss: 0.37764, Train_Acc:87.17%
Epoch [60/300], Step [270/391],                 Loss: 0.37952, Train_Acc:87.14%
Epoch [60/300], Step [280/391],                 Loss: 0.37997, Train_Acc:87.14%
Epoch [60/300], Step [290/391],                 Loss: 0.38020, Train_Acc:87.12%
Epoch [60/300], Step [300/391],                 Loss: 0.38026, Train_Acc:87.08%
Epoch [60/300], Step [310/391],                 Loss: 0.37958, Train_Acc:87.11%
Epoch [60/300], Step [320/391],                 Loss: 0.38041, Train_Acc:87.09%
Epoch [60/300], Step [330/391],                 Loss: 0.37922, Train_Acc:87.12%
Epoch [60/300], Step [340/391],                 Loss: 0.37882, Train_Acc:87.16%
Epoch [60/300], Step [350/391],                 Loss: 0.37797, Train_Acc:87.20%
Epoch [60/300], Step [360/391],                 Loss: 0.37842, Train_Acc:87.17%
Epoch [60/300], Step [370/391],                 Loss: 0.37799, Train_Acc:87.19%
Epoch [60/300], Step [380/391],                 Loss: 0.37672, Train_Acc:87.25%
Epoch [60/300], Step [390/391],                 Loss: 0.37574, Train_Acc:87.29%
Accuary on test images:75.68%
Epoch [61/300], Step [10/391],                 Loss: 0.34280, Train_Acc:88.36%
Epoch [61/300], Step [20/391],                 Loss: 0.34445, Train_Acc:88.20%
Epoch [61/300], Step [30/391],                 Loss: 0.35002, Train_Acc:88.12%
Epoch [61/300], Step [40/391],                 Loss: 0.35710, Train_Acc:88.01%
Epoch [61/300], Step [50/391],                 Loss: 0.36566, Train_Acc:87.55%
Epoch [61/300], Step [60/391],                 Loss: 0.38151, Train_Acc:86.76%
Epoch [61/300], Step [70/391],                 Loss: 0.38346, Train_Acc:86.90%
Epoch [61/300], Step [80/391],                 Loss: 0.38861, Train_Acc:86.69%
Epoch [61/300], Step [90/391],                 Loss: 0.39339, Train_Acc:86.51%
Epoch [61/300], Step [100/391],                 Loss: 0.38945, Train_Acc:86.70%
Epoch [61/300], Step [110/391],                 Loss: 0.38808, Train_Acc:86.66%
Epoch [61/300], Step [120/391],                 Loss: 0.38610, Train_Acc:86.76%
Epoch [61/300], Step [130/391],                 Loss: 0.38800, Train_Acc:86.71%
Epoch [61/300], Step [140/391],                 Loss: 0.38469, Train_Acc:86.85%
Epoch [61/300], Step [150/391],                 Loss: 0.38424, Train_Acc:86.79%
Epoch [61/300], Step [160/391],                 Loss: 0.38111, Train_Acc:86.94%
Epoch [61/300], Step [170/391],                 Loss: 0.38247, Train_Acc:86.90%
Epoch [61/300], Step [180/391],                 Loss: 0.38278, Train_Acc:86.88%
Epoch [61/300], Step [190/391],                 Loss: 0.38281, Train_Acc:86.81%
Epoch [61/300], Step [200/391],                 Loss: 0.38307, Train_Acc:86.79%
Epoch [61/300], Step [210/391],                 Loss: 0.38313, Train_Acc:86.80%
Epoch [61/300], Step [220/391],                 Loss: 0.38480, Train_Acc:86.73%
Epoch [61/300], Step [230/391],                 Loss: 0.38457, Train_Acc:86.73%
Epoch [61/300], Step [240/391],                 Loss: 0.38297, Train_Acc:86.79%
Epoch [61/300], Step [250/391],                 Loss: 0.38274, Train_Acc:86.77%
Epoch [61/300], Step [260/391],                 Loss: 0.38471, Train_Acc:86.72%
Epoch [61/300], Step [270/391],                 Loss: 0.38544, Train_Acc:86.70%
Epoch [61/300], Step [280/391],                 Loss: 0.38424, Train_Acc:86.77%
Epoch [61/300], Step [290/391],                 Loss: 0.38368, Train_Acc:86.81%
Epoch [61/300], Step [300/391],                 Loss: 0.38282, Train_Acc:86.83%
Epoch [61/300], Step [310/391],                 Loss: 0.38182, Train_Acc:86.87%
Epoch [61/300], Step [320/391],                 Loss: 0.38136, Train_Acc:86.89%
Epoch [61/300], Step [330/391],                 Loss: 0.38006, Train_Acc:86.93%
Epoch [61/300], Step [340/391],                 Loss: 0.37869, Train_Acc:87.00%
Epoch [61/300], Step [350/391],                 Loss: 0.37755, Train_Acc:87.04%
Epoch [61/300], Step [360/391],                 Loss: 0.37701, Train_Acc:87.05%
Epoch [61/300], Step [370/391],                 Loss: 0.37690, Train_Acc:87.05%
Epoch [61/300], Step [380/391],                 Loss: 0.37746, Train_Acc:87.05%
Epoch [61/300], Step [390/391],                 Loss: 0.37742, Train_Acc:87.03%
Accuary on test images:76.26%
Epoch [62/300], Step [10/391],                 Loss: 0.35965, Train_Acc:87.66%
Epoch [62/300], Step [20/391],                 Loss: 0.35498, Train_Acc:87.89%
Epoch [62/300], Step [30/391],                 Loss: 0.35023, Train_Acc:88.20%
Epoch [62/300], Step [40/391],                 Loss: 0.35764, Train_Acc:87.85%
Epoch [62/300], Step [50/391],                 Loss: 0.36284, Train_Acc:87.70%
Epoch [62/300], Step [60/391],                 Loss: 0.37135, Train_Acc:87.32%
Epoch [62/300], Step [70/391],                 Loss: 0.36912, Train_Acc:87.44%
Epoch [62/300], Step [80/391],                 Loss: 0.36920, Train_Acc:87.41%
Epoch [62/300], Step [90/391],                 Loss: 0.37341, Train_Acc:87.25%
Epoch [62/300], Step [100/391],                 Loss: 0.37273, Train_Acc:87.36%
Epoch [62/300], Step [110/391],                 Loss: 0.37455, Train_Acc:87.39%
Epoch [62/300], Step [120/391],                 Loss: 0.37699, Train_Acc:87.31%
Epoch [62/300], Step [130/391],                 Loss: 0.37787, Train_Acc:87.32%
Epoch [62/300], Step [140/391],                 Loss: 0.37467, Train_Acc:87.41%
Epoch [62/300], Step [150/391],                 Loss: 0.37474, Train_Acc:87.34%
Epoch [62/300], Step [160/391],                 Loss: 0.37499, Train_Acc:87.34%
Epoch [62/300], Step [170/391],                 Loss: 0.37676, Train_Acc:87.29%
Epoch [62/300], Step [180/391],                 Loss: 0.37775, Train_Acc:87.25%
Epoch [62/300], Step [190/391],                 Loss: 0.37852, Train_Acc:87.20%
Epoch [62/300], Step [200/391],                 Loss: 0.37997, Train_Acc:87.19%
Epoch [62/300], Step [210/391],                 Loss: 0.38097, Train_Acc:87.16%
Epoch [62/300], Step [220/391],                 Loss: 0.38011, Train_Acc:87.20%
Epoch [62/300], Step [230/391],                 Loss: 0.37849, Train_Acc:87.26%
Epoch [62/300], Step [240/391],                 Loss: 0.37799, Train_Acc:87.26%
Epoch [62/300], Step [250/391],                 Loss: 0.37828, Train_Acc:87.22%
Epoch [62/300], Step [260/391],                 Loss: 0.38101, Train_Acc:87.13%
Epoch [62/300], Step [270/391],                 Loss: 0.38219, Train_Acc:87.07%
Epoch [62/300], Step [280/391],                 Loss: 0.38122, Train_Acc:87.08%
Epoch [62/300], Step [290/391],                 Loss: 0.38124, Train_Acc:87.09%
Epoch [62/300], Step [300/391],                 Loss: 0.38078, Train_Acc:87.11%
Epoch [62/300], Step [310/391],                 Loss: 0.38002, Train_Acc:87.13%
Epoch [62/300], Step [320/391],                 Loss: 0.38066, Train_Acc:87.11%
Epoch [62/300], Step [330/391],                 Loss: 0.37979, Train_Acc:87.12%
Epoch [62/300], Step [340/391],                 Loss: 0.38016, Train_Acc:87.11%
Epoch [62/300], Step [350/391],                 Loss: 0.38023, Train_Acc:87.12%
Epoch [62/300], Step [360/391],                 Loss: 0.37974, Train_Acc:87.15%
Epoch [62/300], Step [370/391],                 Loss: 0.38000, Train_Acc:87.13%
Epoch [62/300], Step [380/391],                 Loss: 0.37982, Train_Acc:87.13%
Epoch [62/300], Step [390/391],                 Loss: 0.37881, Train_Acc:87.15%
Accuary on test images:73.24%
Epoch [63/300], Step [10/391],                 Loss: 0.37912, Train_Acc:87.50%
Epoch [63/300], Step [20/391],                 Loss: 0.38752, Train_Acc:87.27%
Epoch [63/300], Step [30/391],                 Loss: 0.37504, Train_Acc:87.79%
Epoch [63/300], Step [40/391],                 Loss: 0.37138, Train_Acc:87.87%
Epoch [63/300], Step [50/391],                 Loss: 0.37808, Train_Acc:87.52%
Epoch [63/300], Step [60/391],                 Loss: 0.38511, Train_Acc:87.20%
Epoch [63/300], Step [70/391],                 Loss: 0.38555, Train_Acc:87.12%
Epoch [63/300], Step [80/391],                 Loss: 0.38112, Train_Acc:87.36%
Epoch [63/300], Step [90/391],                 Loss: 0.38145, Train_Acc:87.39%
Epoch [63/300], Step [100/391],                 Loss: 0.37646, Train_Acc:87.56%
Epoch [63/300], Step [110/391],                 Loss: 0.37936, Train_Acc:87.37%
Epoch [63/300], Step [120/391],                 Loss: 0.37886, Train_Acc:87.36%
Epoch [63/300], Step [130/391],                 Loss: 0.37952, Train_Acc:87.32%
Epoch [63/300], Step [140/391],                 Loss: 0.37858, Train_Acc:87.34%
Epoch [63/300], Step [150/391],                 Loss: 0.37784, Train_Acc:87.35%
Epoch [63/300], Step [160/391],                 Loss: 0.37862, Train_Acc:87.32%
Epoch [63/300], Step [170/391],                 Loss: 0.37847, Train_Acc:87.30%
Epoch [63/300], Step [180/391],                 Loss: 0.37937, Train_Acc:87.21%
Epoch [63/300], Step [190/391],                 Loss: 0.37967, Train_Acc:87.21%
Epoch [63/300], Step [200/391],                 Loss: 0.38069, Train_Acc:87.17%
Epoch [63/300], Step [210/391],                 Loss: 0.37971, Train_Acc:87.18%
Epoch [63/300], Step [220/391],                 Loss: 0.37960, Train_Acc:87.22%
Epoch [63/300], Step [230/391],                 Loss: 0.37744, Train_Acc:87.28%
Epoch [63/300], Step [240/391],                 Loss: 0.37497, Train_Acc:87.32%
Epoch [63/300], Step [250/391],                 Loss: 0.37323, Train_Acc:87.39%
Epoch [63/300], Step [260/391],                 Loss: 0.37645, Train_Acc:87.30%
Epoch [63/300], Step [270/391],                 Loss: 0.37849, Train_Acc:87.23%
Epoch [63/300], Step [280/391],                 Loss: 0.37865, Train_Acc:87.25%
Epoch [63/300], Step [290/391],                 Loss: 0.37856, Train_Acc:87.24%
Epoch [63/300], Step [300/391],                 Loss: 0.37677, Train_Acc:87.28%
Epoch [63/300], Step [310/391],                 Loss: 0.37586, Train_Acc:87.33%
Epoch [63/300], Step [320/391],                 Loss: 0.37569, Train_Acc:87.35%
Epoch [63/300], Step [330/391],                 Loss: 0.37415, Train_Acc:87.40%
Epoch [63/300], Step [340/391],                 Loss: 0.37361, Train_Acc:87.40%
Epoch [63/300], Step [350/391],                 Loss: 0.37353, Train_Acc:87.37%
Epoch [63/300], Step [360/391],                 Loss: 0.37406, Train_Acc:87.36%
Epoch [63/300], Step [370/391],                 Loss: 0.37502, Train_Acc:87.34%
Epoch [63/300], Step [380/391],                 Loss: 0.37459, Train_Acc:87.36%
Epoch [63/300], Step [390/391],                 Loss: 0.37416, Train_Acc:87.39%
Accuary on test images:69.46%
Epoch [64/300], Step [10/391],                 Loss: 0.40324, Train_Acc:86.56%
Epoch [64/300], Step [20/391],                 Loss: 0.38665, Train_Acc:87.15%
Epoch [64/300], Step [30/391],                 Loss: 0.38052, Train_Acc:87.34%
Epoch [64/300], Step [40/391],                 Loss: 0.38129, Train_Acc:87.30%
Epoch [64/300], Step [50/391],                 Loss: 0.38070, Train_Acc:87.23%
Epoch [64/300], Step [60/391],                 Loss: 0.38746, Train_Acc:86.85%
Epoch [64/300], Step [70/391],                 Loss: 0.38822, Train_Acc:86.85%
Epoch [64/300], Step [80/391],                 Loss: 0.38797, Train_Acc:86.87%
Epoch [64/300], Step [90/391],                 Loss: 0.38724, Train_Acc:87.01%
Epoch [64/300], Step [100/391],                 Loss: 0.38652, Train_Acc:87.07%
Epoch [64/300], Step [110/391],                 Loss: 0.38545, Train_Acc:87.07%
Epoch [64/300], Step [120/391],                 Loss: 0.38289, Train_Acc:87.12%
Epoch [64/300], Step [130/391],                 Loss: 0.38055, Train_Acc:87.18%
Epoch [64/300], Step [140/391],                 Loss: 0.37817, Train_Acc:87.29%
Epoch [64/300], Step [150/391],                 Loss: 0.37801, Train_Acc:87.30%
Epoch [64/300], Step [160/391],                 Loss: 0.37708, Train_Acc:87.31%
Epoch [64/300], Step [170/391],                 Loss: 0.37839, Train_Acc:87.25%
Epoch [64/300], Step [180/391],                 Loss: 0.37923, Train_Acc:87.17%
Epoch [64/300], Step [190/391],                 Loss: 0.37770, Train_Acc:87.24%
Epoch [64/300], Step [200/391],                 Loss: 0.37797, Train_Acc:87.21%
Epoch [64/300], Step [210/391],                 Loss: 0.37775, Train_Acc:87.22%
Epoch [64/300], Step [220/391],                 Loss: 0.37641, Train_Acc:87.29%
Epoch [64/300], Step [230/391],                 Loss: 0.37458, Train_Acc:87.34%
Epoch [64/300], Step [240/391],                 Loss: 0.37317, Train_Acc:87.40%
Epoch [64/300], Step [250/391],                 Loss: 0.37446, Train_Acc:87.36%
Epoch [64/300], Step [260/391],                 Loss: 0.37639, Train_Acc:87.32%
Epoch [64/300], Step [270/391],                 Loss: 0.37772, Train_Acc:87.28%
Epoch [64/300], Step [280/391],                 Loss: 0.37716, Train_Acc:87.28%
Epoch [64/300], Step [290/391],                 Loss: 0.37540, Train_Acc:87.34%
Epoch [64/300], Step [300/391],                 Loss: 0.37383, Train_Acc:87.40%
Epoch [64/300], Step [310/391],                 Loss: 0.37219, Train_Acc:87.45%
Epoch [64/300], Step [320/391],                 Loss: 0.37186, Train_Acc:87.45%
Epoch [64/300], Step [330/391],                 Loss: 0.37113, Train_Acc:87.45%
Epoch [64/300], Step [340/391],                 Loss: 0.37157, Train_Acc:87.42%
Epoch [64/300], Step [350/391],                 Loss: 0.37163, Train_Acc:87.42%
Epoch [64/300], Step [360/391],                 Loss: 0.37153, Train_Acc:87.44%
Epoch [64/300], Step [370/391],                 Loss: 0.37156, Train_Acc:87.42%
Epoch [64/300], Step [380/391],                 Loss: 0.37091, Train_Acc:87.43%
Epoch [64/300], Step [390/391],                 Loss: 0.37067, Train_Acc:87.45%
Accuary on test images:72.00%
Epoch [65/300], Step [10/391],                 Loss: 0.37781, Train_Acc:86.64%
Epoch [65/300], Step [20/391],                 Loss: 0.37761, Train_Acc:86.99%
Epoch [65/300], Step [30/391],                 Loss: 0.38580, Train_Acc:86.93%
Epoch [65/300], Step [40/391],                 Loss: 0.38559, Train_Acc:87.19%
Epoch [65/300], Step [50/391],                 Loss: 0.38498, Train_Acc:87.06%
Epoch [65/300], Step [60/391],                 Loss: 0.39463, Train_Acc:86.72%
Epoch [65/300], Step [70/391],                 Loss: 0.39211, Train_Acc:86.84%
Epoch [65/300], Step [80/391],                 Loss: 0.39294, Train_Acc:86.84%
Epoch [65/300], Step [90/391],                 Loss: 0.39475, Train_Acc:86.77%
Epoch [65/300], Step [100/391],                 Loss: 0.39341, Train_Acc:86.76%
Epoch [65/300], Step [110/391],                 Loss: 0.39217, Train_Acc:86.81%
Epoch [65/300], Step [120/391],                 Loss: 0.38931, Train_Acc:86.87%
Epoch [65/300], Step [130/391],                 Loss: 0.38979, Train_Acc:86.91%
Epoch [65/300], Step [140/391],                 Loss: 0.38708, Train_Acc:86.96%
Epoch [65/300], Step [150/391],                 Loss: 0.38713, Train_Acc:86.95%
Epoch [65/300], Step [160/391],                 Loss: 0.38796, Train_Acc:86.88%
Epoch [65/300], Step [170/391],                 Loss: 0.38809, Train_Acc:86.86%
Epoch [65/300], Step [180/391],                 Loss: 0.38805, Train_Acc:86.87%
Epoch [65/300], Step [190/391],                 Loss: 0.38811, Train_Acc:86.87%
Epoch [65/300], Step [200/391],                 Loss: 0.38897, Train_Acc:86.81%
Epoch [65/300], Step [210/391],                 Loss: 0.38858, Train_Acc:86.83%
Epoch [65/300], Step [220/391],                 Loss: 0.38747, Train_Acc:86.84%
Epoch [65/300], Step [230/391],                 Loss: 0.38621, Train_Acc:86.85%
Epoch [65/300], Step [240/391],                 Loss: 0.38396, Train_Acc:86.93%
Epoch [65/300], Step [250/391],                 Loss: 0.38370, Train_Acc:86.92%
Epoch [65/300], Step [260/391],                 Loss: 0.38520, Train_Acc:86.88%
Epoch [65/300], Step [270/391],                 Loss: 0.38650, Train_Acc:86.83%
Epoch [65/300], Step [280/391],                 Loss: 0.38665, Train_Acc:86.81%
Epoch [65/300], Step [290/391],                 Loss: 0.38731, Train_Acc:86.78%
Epoch [65/300], Step [300/391],                 Loss: 0.38651, Train_Acc:86.82%
Epoch [65/300], Step [310/391],                 Loss: 0.38570, Train_Acc:86.86%
Epoch [65/300], Step [320/391],                 Loss: 0.38601, Train_Acc:86.87%
Epoch [65/300], Step [330/391],                 Loss: 0.38475, Train_Acc:86.92%
Epoch [65/300], Step [340/391],                 Loss: 0.38309, Train_Acc:86.96%
Epoch [65/300], Step [350/391],                 Loss: 0.38212, Train_Acc:86.98%
Epoch [65/300], Step [360/391],                 Loss: 0.38182, Train_Acc:86.98%
Epoch [65/300], Step [370/391],                 Loss: 0.38193, Train_Acc:86.96%
Epoch [65/300], Step [380/391],                 Loss: 0.38242, Train_Acc:86.96%
Epoch [65/300], Step [390/391],                 Loss: 0.38176, Train_Acc:86.97%
Accuary on test images:76.10%
Epoch [66/300], Step [10/391],                 Loss: 0.38887, Train_Acc:87.42%
Epoch [66/300], Step [20/391],                 Loss: 0.39608, Train_Acc:86.52%
Epoch [66/300], Step [30/391],                 Loss: 0.38632, Train_Acc:86.69%
Epoch [66/300], Step [40/391],                 Loss: 0.38289, Train_Acc:86.89%
Epoch [66/300], Step [50/391],                 Loss: 0.37843, Train_Acc:86.78%
Epoch [66/300], Step [60/391],                 Loss: 0.38681, Train_Acc:86.43%
Epoch [66/300], Step [70/391],                 Loss: 0.38881, Train_Acc:86.37%
Epoch [66/300], Step [80/391],                 Loss: 0.39123, Train_Acc:86.28%
Epoch [66/300], Step [90/391],                 Loss: 0.39052, Train_Acc:86.29%
Epoch [66/300], Step [100/391],                 Loss: 0.38479, Train_Acc:86.62%
Epoch [66/300], Step [110/391],                 Loss: 0.38192, Train_Acc:86.75%
Epoch [66/300], Step [120/391],                 Loss: 0.37873, Train_Acc:86.90%
Epoch [66/300], Step [130/391],                 Loss: 0.37865, Train_Acc:86.94%
Epoch [66/300], Step [140/391],                 Loss: 0.37693, Train_Acc:87.04%
Epoch [66/300], Step [150/391],                 Loss: 0.37587, Train_Acc:87.07%
Epoch [66/300], Step [160/391],                 Loss: 0.37699, Train_Acc:87.02%
Epoch [66/300], Step [170/391],                 Loss: 0.37880, Train_Acc:86.99%
Epoch [66/300], Step [180/391],                 Loss: 0.37864, Train_Acc:86.96%
Epoch [66/300], Step [190/391],                 Loss: 0.37947, Train_Acc:86.91%
Epoch [66/300], Step [200/391],                 Loss: 0.38065, Train_Acc:86.86%
Epoch [66/300], Step [210/391],                 Loss: 0.37947, Train_Acc:86.89%
Epoch [66/300], Step [220/391],                 Loss: 0.37907, Train_Acc:86.93%
Epoch [66/300], Step [230/391],                 Loss: 0.37697, Train_Acc:86.99%
Epoch [66/300], Step [240/391],                 Loss: 0.37397, Train_Acc:87.09%
Epoch [66/300], Step [250/391],                 Loss: 0.37348, Train_Acc:87.09%
Epoch [66/300], Step [260/391],                 Loss: 0.37550, Train_Acc:87.06%
Epoch [66/300], Step [270/391],                 Loss: 0.37746, Train_Acc:86.99%
Epoch [66/300], Step [280/391],                 Loss: 0.37783, Train_Acc:86.98%
Epoch [66/300], Step [290/391],                 Loss: 0.37823, Train_Acc:86.97%
Epoch [66/300], Step [300/391],                 Loss: 0.37880, Train_Acc:86.95%
Epoch [66/300], Step [310/391],                 Loss: 0.37728, Train_Acc:87.01%
Epoch [66/300], Step [320/391],                 Loss: 0.37872, Train_Acc:86.99%
Epoch [66/300], Step [330/391],                 Loss: 0.37725, Train_Acc:87.07%
Epoch [66/300], Step [340/391],                 Loss: 0.37692, Train_Acc:87.07%
Epoch [66/300], Step [350/391],                 Loss: 0.37636, Train_Acc:87.09%
Epoch [66/300], Step [360/391],                 Loss: 0.37663, Train_Acc:87.07%
Epoch [66/300], Step [370/391],                 Loss: 0.37726, Train_Acc:87.06%
Epoch [66/300], Step [380/391],                 Loss: 0.37718, Train_Acc:87.06%
Epoch [66/300], Step [390/391],                 Loss: 0.37671, Train_Acc:87.07%
Accuary on test images:71.72%
Epoch [67/300], Step [10/391],                 Loss: 0.42718, Train_Acc:85.23%
Epoch [67/300], Step [20/391],                 Loss: 0.39520, Train_Acc:86.64%
Epoch [67/300], Step [30/391],                 Loss: 0.38862, Train_Acc:86.88%
Epoch [67/300], Step [40/391],                 Loss: 0.38652, Train_Acc:87.19%
Epoch [67/300], Step [50/391],                 Loss: 0.38675, Train_Acc:87.09%
Epoch [67/300], Step [60/391],                 Loss: 0.38794, Train_Acc:86.82%
Epoch [67/300], Step [70/391],                 Loss: 0.38786, Train_Acc:86.94%
Epoch [67/300], Step [80/391],                 Loss: 0.38555, Train_Acc:86.92%
Epoch [67/300], Step [90/391],                 Loss: 0.38607, Train_Acc:86.93%
Epoch [67/300], Step [100/391],                 Loss: 0.38402, Train_Acc:87.00%
Epoch [67/300], Step [110/391],                 Loss: 0.38209, Train_Acc:87.12%
Epoch [67/300], Step [120/391],                 Loss: 0.38181, Train_Acc:87.16%
Epoch [67/300], Step [130/391],                 Loss: 0.38141, Train_Acc:87.13%
Epoch [67/300], Step [140/391],                 Loss: 0.37912, Train_Acc:87.20%
Epoch [67/300], Step [150/391],                 Loss: 0.37804, Train_Acc:87.14%
Epoch [67/300], Step [160/391],                 Loss: 0.37850, Train_Acc:87.13%
Epoch [67/300], Step [170/391],                 Loss: 0.38048, Train_Acc:87.07%
Epoch [67/300], Step [180/391],                 Loss: 0.38069, Train_Acc:87.03%
Epoch [67/300], Step [190/391],                 Loss: 0.38019, Train_Acc:87.06%
Epoch [67/300], Step [200/391],                 Loss: 0.38174, Train_Acc:86.98%
Epoch [67/300], Step [210/391],                 Loss: 0.38069, Train_Acc:87.00%
Epoch [67/300], Step [220/391],                 Loss: 0.37828, Train_Acc:87.08%
Epoch [67/300], Step [230/391],                 Loss: 0.37726, Train_Acc:87.14%
Epoch [67/300], Step [240/391],                 Loss: 0.37449, Train_Acc:87.23%
Epoch [67/300], Step [250/391],                 Loss: 0.37314, Train_Acc:87.26%
Epoch [67/300], Step [260/391],                 Loss: 0.37489, Train_Acc:87.24%
Epoch [67/300], Step [270/391],                 Loss: 0.37668, Train_Acc:87.16%
Epoch [67/300], Step [280/391],                 Loss: 0.37726, Train_Acc:87.10%
Epoch [67/300], Step [290/391],                 Loss: 0.37872, Train_Acc:87.09%
Epoch [67/300], Step [300/391],                 Loss: 0.37841, Train_Acc:87.09%
Epoch [67/300], Step [310/391],                 Loss: 0.37830, Train_Acc:87.10%
Epoch [67/300], Step [320/391],                 Loss: 0.37912, Train_Acc:87.09%
Epoch [67/300], Step [330/391],                 Loss: 0.37765, Train_Acc:87.14%
Epoch [67/300], Step [340/391],                 Loss: 0.37666, Train_Acc:87.18%
Epoch [67/300], Step [350/391],                 Loss: 0.37575, Train_Acc:87.23%
Epoch [67/300], Step [360/391],                 Loss: 0.37548, Train_Acc:87.19%
Epoch [67/300], Step [370/391],                 Loss: 0.37535, Train_Acc:87.20%
Epoch [67/300], Step [380/391],                 Loss: 0.37512, Train_Acc:87.19%
Epoch [67/300], Step [390/391],                 Loss: 0.37462, Train_Acc:87.20%
Accuary on test images:72.96%
Epoch [68/300], Step [10/391],                 Loss: 0.39532, Train_Acc:86.95%
Epoch [68/300], Step [20/391],                 Loss: 0.39127, Train_Acc:86.88%
Epoch [68/300], Step [30/391],                 Loss: 0.38434, Train_Acc:87.03%
Epoch [68/300], Step [40/391],                 Loss: 0.38307, Train_Acc:87.01%
Epoch [68/300], Step [50/391],                 Loss: 0.38501, Train_Acc:87.02%
Epoch [68/300], Step [60/391],                 Loss: 0.38598, Train_Acc:87.11%
Epoch [68/300], Step [70/391],                 Loss: 0.38374, Train_Acc:87.25%
Epoch [68/300], Step [80/391],                 Loss: 0.37983, Train_Acc:87.33%
Epoch [68/300], Step [90/391],                 Loss: 0.37876, Train_Acc:87.20%
Epoch [68/300], Step [100/391],                 Loss: 0.37624, Train_Acc:87.31%
Epoch [68/300], Step [110/391],                 Loss: 0.37977, Train_Acc:87.26%
Epoch [68/300], Step [120/391],                 Loss: 0.38090, Train_Acc:87.15%
Epoch [68/300], Step [130/391],                 Loss: 0.38161, Train_Acc:87.13%
Epoch [68/300], Step [140/391],                 Loss: 0.38109, Train_Acc:87.17%
Epoch [68/300], Step [150/391],                 Loss: 0.38018, Train_Acc:87.18%
Epoch [68/300], Step [160/391],                 Loss: 0.37931, Train_Acc:87.24%
Epoch [68/300], Step [170/391],                 Loss: 0.38158, Train_Acc:87.14%
Epoch [68/300], Step [180/391],                 Loss: 0.38258, Train_Acc:87.09%
Epoch [68/300], Step [190/391],                 Loss: 0.38339, Train_Acc:87.06%
Epoch [68/300], Step [200/391],                 Loss: 0.38475, Train_Acc:87.05%
Epoch [68/300], Step [210/391],                 Loss: 0.38543, Train_Acc:87.04%
Epoch [68/300], Step [220/391],                 Loss: 0.38515, Train_Acc:87.09%
Epoch [68/300], Step [230/391],                 Loss: 0.38300, Train_Acc:87.16%
Epoch [68/300], Step [240/391],                 Loss: 0.38069, Train_Acc:87.23%
Epoch [68/300], Step [250/391],                 Loss: 0.38015, Train_Acc:87.27%
Epoch [68/300], Step [260/391],                 Loss: 0.38132, Train_Acc:87.23%
Epoch [68/300], Step [270/391],                 Loss: 0.38178, Train_Acc:87.24%
Epoch [68/300], Step [280/391],                 Loss: 0.38225, Train_Acc:87.21%
Epoch [68/300], Step [290/391],                 Loss: 0.38272, Train_Acc:87.21%
Epoch [68/300], Step [300/391],                 Loss: 0.38157, Train_Acc:87.22%
Epoch [68/300], Step [310/391],                 Loss: 0.38062, Train_Acc:87.24%
Epoch [68/300], Step [320/391],                 Loss: 0.37990, Train_Acc:87.24%
Epoch [68/300], Step [330/391],                 Loss: 0.37917, Train_Acc:87.28%
Epoch [68/300], Step [340/391],                 Loss: 0.37806, Train_Acc:87.32%
Epoch [68/300], Step [350/391],                 Loss: 0.37760, Train_Acc:87.33%
Epoch [68/300], Step [360/391],                 Loss: 0.37816, Train_Acc:87.30%
Epoch [68/300], Step [370/391],                 Loss: 0.37933, Train_Acc:87.26%
Epoch [68/300], Step [380/391],                 Loss: 0.37922, Train_Acc:87.27%
Epoch [68/300], Step [390/391],                 Loss: 0.37905, Train_Acc:87.28%
Accuary on test images:66.28%
Epoch [69/300], Step [10/391],                 Loss: 0.39674, Train_Acc:86.48%
Epoch [69/300], Step [20/391],                 Loss: 0.38396, Train_Acc:86.95%
Epoch [69/300], Step [30/391],                 Loss: 0.37292, Train_Acc:87.60%
Epoch [69/300], Step [40/391],                 Loss: 0.37214, Train_Acc:87.71%
Epoch [69/300], Step [50/391],                 Loss: 0.36579, Train_Acc:87.95%
Epoch [69/300], Step [60/391],                 Loss: 0.37023, Train_Acc:87.76%
Epoch [69/300], Step [70/391],                 Loss: 0.36806, Train_Acc:87.81%
Epoch [69/300], Step [80/391],                 Loss: 0.37354, Train_Acc:87.51%
Epoch [69/300], Step [90/391],                 Loss: 0.37530, Train_Acc:87.29%
Epoch [69/300], Step [100/391],                 Loss: 0.37247, Train_Acc:87.30%
Epoch [69/300], Step [110/391],                 Loss: 0.37267, Train_Acc:87.27%
Epoch [69/300], Step [120/391],                 Loss: 0.37176, Train_Acc:87.29%
Epoch [69/300], Step [130/391],                 Loss: 0.37384, Train_Acc:87.18%
Epoch [69/300], Step [140/391],                 Loss: 0.37081, Train_Acc:87.29%
Epoch [69/300], Step [150/391],                 Loss: 0.37215, Train_Acc:87.25%
Epoch [69/300], Step [160/391],                 Loss: 0.37083, Train_Acc:87.31%
Epoch [69/300], Step [170/391],                 Loss: 0.37286, Train_Acc:87.24%
Epoch [69/300], Step [180/391],                 Loss: 0.37231, Train_Acc:87.25%
Epoch [69/300], Step [190/391],                 Loss: 0.37182, Train_Acc:87.29%
Epoch [69/300], Step [200/391],                 Loss: 0.37220, Train_Acc:87.28%
Epoch [69/300], Step [210/391],                 Loss: 0.37207, Train_Acc:87.29%
Epoch [69/300], Step [220/391],                 Loss: 0.37231, Train_Acc:87.29%
Epoch [69/300], Step [230/391],                 Loss: 0.37203, Train_Acc:87.31%
Epoch [69/300], Step [240/391],                 Loss: 0.37024, Train_Acc:87.36%
Epoch [69/300], Step [250/391],                 Loss: 0.37121, Train_Acc:87.36%
Epoch [69/300], Step [260/391],                 Loss: 0.37325, Train_Acc:87.30%
Epoch [69/300], Step [270/391],                 Loss: 0.37458, Train_Acc:87.28%
Epoch [69/300], Step [280/391],                 Loss: 0.37368, Train_Acc:87.29%
Epoch [69/300], Step [290/391],                 Loss: 0.37252, Train_Acc:87.37%
Epoch [69/300], Step [300/391],                 Loss: 0.37117, Train_Acc:87.39%
Epoch [69/300], Step [310/391],                 Loss: 0.37094, Train_Acc:87.40%
Epoch [69/300], Step [320/391],                 Loss: 0.37186, Train_Acc:87.39%
Epoch [69/300], Step [330/391],                 Loss: 0.37176, Train_Acc:87.37%
Epoch [69/300], Step [340/391],                 Loss: 0.37207, Train_Acc:87.35%
Epoch [69/300], Step [350/391],                 Loss: 0.37216, Train_Acc:87.33%
Epoch [69/300], Step [360/391],                 Loss: 0.37256, Train_Acc:87.32%
Epoch [69/300], Step [370/391],                 Loss: 0.37237, Train_Acc:87.31%
Epoch [69/300], Step [380/391],                 Loss: 0.37142, Train_Acc:87.35%
Epoch [69/300], Step [390/391],                 Loss: 0.37023, Train_Acc:87.38%
Accuary on test images:69.42%
Epoch [70/300], Step [10/391],                 Loss: 0.38057, Train_Acc:86.25%
Epoch [70/300], Step [20/391],                 Loss: 0.38079, Train_Acc:86.60%
Epoch [70/300], Step [30/391],                 Loss: 0.37674, Train_Acc:87.06%
Epoch [70/300], Step [40/391],                 Loss: 0.37646, Train_Acc:87.09%
Epoch [70/300], Step [50/391],                 Loss: 0.37485, Train_Acc:86.98%
Epoch [70/300], Step [60/391],                 Loss: 0.38404, Train_Acc:86.73%
Epoch [70/300], Step [70/391],                 Loss: 0.38356, Train_Acc:86.89%
Epoch [70/300], Step [80/391],                 Loss: 0.38534, Train_Acc:86.90%
Epoch [70/300], Step [90/391],                 Loss: 0.38832, Train_Acc:86.84%
Epoch [70/300], Step [100/391],                 Loss: 0.38378, Train_Acc:86.99%
Epoch [70/300], Step [110/391],                 Loss: 0.38274, Train_Acc:87.09%
Epoch [70/300], Step [120/391],                 Loss: 0.38259, Train_Acc:87.09%
Epoch [70/300], Step [130/391],                 Loss: 0.38237, Train_Acc:87.18%
Epoch [70/300], Step [140/391],                 Loss: 0.37962, Train_Acc:87.28%
Epoch [70/300], Step [150/391],                 Loss: 0.37893, Train_Acc:87.28%
Epoch [70/300], Step [160/391],                 Loss: 0.37882, Train_Acc:87.31%
Epoch [70/300], Step [170/391],                 Loss: 0.37904, Train_Acc:87.29%
Epoch [70/300], Step [180/391],                 Loss: 0.38039, Train_Acc:87.17%
Epoch [70/300], Step [190/391],                 Loss: 0.38160, Train_Acc:87.11%
Epoch [70/300], Step [200/391],                 Loss: 0.38402, Train_Acc:87.04%
Epoch [70/300], Step [210/391],                 Loss: 0.38412, Train_Acc:87.03%
Epoch [70/300], Step [220/391],                 Loss: 0.38475, Train_Acc:87.03%
Epoch [70/300], Step [230/391],                 Loss: 0.38212, Train_Acc:87.09%
Epoch [70/300], Step [240/391],                 Loss: 0.38008, Train_Acc:87.16%
Epoch [70/300], Step [250/391],                 Loss: 0.37892, Train_Acc:87.19%
Epoch [70/300], Step [260/391],                 Loss: 0.38105, Train_Acc:87.13%
Epoch [70/300], Step [270/391],                 Loss: 0.38373, Train_Acc:87.05%
Epoch [70/300], Step [280/391],                 Loss: 0.38344, Train_Acc:87.04%
Epoch [70/300], Step [290/391],                 Loss: 0.38212, Train_Acc:87.09%
Epoch [70/300], Step [300/391],                 Loss: 0.38127, Train_Acc:87.10%
Epoch [70/300], Step [310/391],                 Loss: 0.38039, Train_Acc:87.13%
Epoch [70/300], Step [320/391],                 Loss: 0.38019, Train_Acc:87.15%
Epoch [70/300], Step [330/391],                 Loss: 0.37947, Train_Acc:87.16%
Epoch [70/300], Step [340/391],                 Loss: 0.37830, Train_Acc:87.19%
Epoch [70/300], Step [350/391],                 Loss: 0.37692, Train_Acc:87.25%
Epoch [70/300], Step [360/391],                 Loss: 0.37831, Train_Acc:87.21%
Epoch [70/300], Step [370/391],                 Loss: 0.37832, Train_Acc:87.20%
Epoch [70/300], Step [380/391],                 Loss: 0.37732, Train_Acc:87.23%
Epoch [70/300], Step [390/391],                 Loss: 0.37624, Train_Acc:87.27%
Accuary on test images:75.38%
Epoch [71/300], Step [10/391],                 Loss: 0.37712, Train_Acc:86.72%
Epoch [71/300], Step [20/391],                 Loss: 0.35327, Train_Acc:87.81%
Epoch [71/300], Step [30/391],                 Loss: 0.34458, Train_Acc:88.07%
Epoch [71/300], Step [40/391],                 Loss: 0.34618, Train_Acc:88.12%
Epoch [71/300], Step [50/391],                 Loss: 0.35649, Train_Acc:87.72%
Epoch [71/300], Step [60/391],                 Loss: 0.37251, Train_Acc:87.19%
Epoch [71/300], Step [70/391],                 Loss: 0.37421, Train_Acc:87.09%
Epoch [71/300], Step [80/391],                 Loss: 0.38046, Train_Acc:86.86%
Epoch [71/300], Step [90/391],                 Loss: 0.38644, Train_Acc:86.73%
Epoch [71/300], Step [100/391],                 Loss: 0.38515, Train_Acc:86.80%
Epoch [71/300], Step [110/391],                 Loss: 0.38602, Train_Acc:86.85%
Epoch [71/300], Step [120/391],                 Loss: 0.38489, Train_Acc:86.89%
Epoch [71/300], Step [130/391],                 Loss: 0.38450, Train_Acc:86.91%
Epoch [71/300], Step [140/391],                 Loss: 0.38204, Train_Acc:87.01%
Epoch [71/300], Step [150/391],                 Loss: 0.38252, Train_Acc:86.97%
Epoch [71/300], Step [160/391],                 Loss: 0.37931, Train_Acc:87.09%
Epoch [71/300], Step [170/391],                 Loss: 0.38168, Train_Acc:87.01%
Epoch [71/300], Step [180/391],                 Loss: 0.38290, Train_Acc:86.96%
Epoch [71/300], Step [190/391],                 Loss: 0.38329, Train_Acc:86.93%
Epoch [71/300], Step [200/391],                 Loss: 0.38486, Train_Acc:86.89%
Epoch [71/300], Step [210/391],                 Loss: 0.38511, Train_Acc:86.86%
Epoch [71/300], Step [220/391],                 Loss: 0.38307, Train_Acc:86.99%
Epoch [71/300], Step [230/391],                 Loss: 0.38109, Train_Acc:87.06%
Epoch [71/300], Step [240/391],                 Loss: 0.37917, Train_Acc:87.15%
Epoch [71/300], Step [250/391],                 Loss: 0.37920, Train_Acc:87.15%
Epoch [71/300], Step [260/391],                 Loss: 0.38227, Train_Acc:87.06%
Epoch [71/300], Step [270/391],                 Loss: 0.38399, Train_Acc:86.99%
Epoch [71/300], Step [280/391],                 Loss: 0.38463, Train_Acc:86.94%
Epoch [71/300], Step [290/391],                 Loss: 0.38393, Train_Acc:86.97%
Epoch [71/300], Step [300/391],                 Loss: 0.38359, Train_Acc:86.97%
Epoch [71/300], Step [310/391],                 Loss: 0.38224, Train_Acc:87.03%
Epoch [71/300], Step [320/391],                 Loss: 0.38171, Train_Acc:87.05%
Epoch [71/300], Step [330/391],                 Loss: 0.38082, Train_Acc:87.10%
Epoch [71/300], Step [340/391],                 Loss: 0.38061, Train_Acc:87.09%
Epoch [71/300], Step [350/391],                 Loss: 0.38060, Train_Acc:87.12%
Epoch [71/300], Step [360/391],                 Loss: 0.38050, Train_Acc:87.12%
Epoch [71/300], Step [370/391],                 Loss: 0.38116, Train_Acc:87.09%
Epoch [71/300], Step [380/391],                 Loss: 0.38117, Train_Acc:87.08%
Epoch [71/300], Step [390/391],                 Loss: 0.37996, Train_Acc:87.13%
Accuary on test images:74.28%
Epoch [72/300], Step [10/391],                 Loss: 0.36673, Train_Acc:87.97%
Epoch [72/300], Step [20/391],                 Loss: 0.36618, Train_Acc:87.89%
Epoch [72/300], Step [30/391],                 Loss: 0.36457, Train_Acc:87.86%
Epoch [72/300], Step [40/391],                 Loss: 0.36964, Train_Acc:87.83%
Epoch [72/300], Step [50/391],                 Loss: 0.37055, Train_Acc:87.55%
Epoch [72/300], Step [60/391],                 Loss: 0.37169, Train_Acc:87.38%
Epoch [72/300], Step [70/391],                 Loss: 0.37552, Train_Acc:87.33%
Epoch [72/300], Step [80/391],                 Loss: 0.37438, Train_Acc:87.36%
Epoch [72/300], Step [90/391],                 Loss: 0.37768, Train_Acc:87.28%
Epoch [72/300], Step [100/391],                 Loss: 0.37265, Train_Acc:87.46%
Epoch [72/300], Step [110/391],                 Loss: 0.37464, Train_Acc:87.40%
Epoch [72/300], Step [120/391],                 Loss: 0.37265, Train_Acc:87.40%
Epoch [72/300], Step [130/391],                 Loss: 0.37293, Train_Acc:87.35%
Epoch [72/300], Step [140/391],                 Loss: 0.37136, Train_Acc:87.42%
Epoch [72/300], Step [150/391],                 Loss: 0.37171, Train_Acc:87.42%
Epoch [72/300], Step [160/391],                 Loss: 0.37273, Train_Acc:87.36%
Epoch [72/300], Step [170/391],                 Loss: 0.37338, Train_Acc:87.34%
Epoch [72/300], Step [180/391],                 Loss: 0.37317, Train_Acc:87.33%
Epoch [72/300], Step [190/391],                 Loss: 0.37349, Train_Acc:87.29%
Epoch [72/300], Step [200/391],                 Loss: 0.37292, Train_Acc:87.28%
Epoch [72/300], Step [210/391],                 Loss: 0.37317, Train_Acc:87.25%
Epoch [72/300], Step [220/391],                 Loss: 0.37399, Train_Acc:87.22%
Epoch [72/300], Step [230/391],                 Loss: 0.37364, Train_Acc:87.23%
Epoch [72/300], Step [240/391],                 Loss: 0.37301, Train_Acc:87.30%
Epoch [72/300], Step [250/391],                 Loss: 0.37263, Train_Acc:87.31%
Epoch [72/300], Step [260/391],                 Loss: 0.37492, Train_Acc:87.24%
Epoch [72/300], Step [270/391],                 Loss: 0.37590, Train_Acc:87.21%
Epoch [72/300], Step [280/391],                 Loss: 0.37511, Train_Acc:87.23%
Epoch [72/300], Step [290/391],                 Loss: 0.37513, Train_Acc:87.24%
Epoch [72/300], Step [300/391],                 Loss: 0.37517, Train_Acc:87.24%
Epoch [72/300], Step [310/391],                 Loss: 0.37472, Train_Acc:87.28%
Epoch [72/300], Step [320/391],                 Loss: 0.37515, Train_Acc:87.25%
Epoch [72/300], Step [330/391],                 Loss: 0.37484, Train_Acc:87.27%
Epoch [72/300], Step [340/391],                 Loss: 0.37526, Train_Acc:87.24%
Epoch [72/300], Step [350/391],                 Loss: 0.37574, Train_Acc:87.25%
Epoch [72/300], Step [360/391],                 Loss: 0.37643, Train_Acc:87.21%
Epoch [72/300], Step [370/391],                 Loss: 0.37683, Train_Acc:87.22%
Epoch [72/300], Step [380/391],                 Loss: 0.37621, Train_Acc:87.25%
Epoch [72/300], Step [390/391],                 Loss: 0.37623, Train_Acc:87.25%
Accuary on test images:76.80%
Epoch [73/300], Step [10/391],                 Loss: 0.34481, Train_Acc:88.36%
Epoch [73/300], Step [20/391],                 Loss: 0.34118, Train_Acc:88.44%
Epoch [73/300], Step [30/391],                 Loss: 0.33544, Train_Acc:88.72%
Epoch [73/300], Step [40/391],                 Loss: 0.33597, Train_Acc:88.61%
Epoch [73/300], Step [50/391],                 Loss: 0.34432, Train_Acc:88.58%
Epoch [73/300], Step [60/391],                 Loss: 0.35886, Train_Acc:88.11%
Epoch [73/300], Step [70/391],                 Loss: 0.36519, Train_Acc:87.75%
Epoch [73/300], Step [80/391],                 Loss: 0.36618, Train_Acc:87.68%
Epoch [73/300], Step [90/391],                 Loss: 0.36901, Train_Acc:87.56%
Epoch [73/300], Step [100/391],                 Loss: 0.37299, Train_Acc:87.45%
Epoch [73/300], Step [110/391],                 Loss: 0.37645, Train_Acc:87.29%
Epoch [73/300], Step [120/391],                 Loss: 0.37852, Train_Acc:87.15%
Epoch [73/300], Step [130/391],                 Loss: 0.38024, Train_Acc:87.09%
Epoch [73/300], Step [140/391],                 Loss: 0.37858, Train_Acc:87.17%
Epoch [73/300], Step [150/391],                 Loss: 0.37868, Train_Acc:87.17%
Epoch [73/300], Step [160/391],                 Loss: 0.37899, Train_Acc:87.08%
Epoch [73/300], Step [170/391],                 Loss: 0.37852, Train_Acc:87.10%
Epoch [73/300], Step [180/391],                 Loss: 0.37944, Train_Acc:87.06%
Epoch [73/300], Step [190/391],                 Loss: 0.38074, Train_Acc:86.97%
Epoch [73/300], Step [200/391],                 Loss: 0.38199, Train_Acc:86.92%
Epoch [73/300], Step [210/391],                 Loss: 0.38321, Train_Acc:86.90%
Epoch [73/300], Step [220/391],                 Loss: 0.38291, Train_Acc:86.86%
Epoch [73/300], Step [230/391],                 Loss: 0.38206, Train_Acc:86.93%
Epoch [73/300], Step [240/391],                 Loss: 0.38124, Train_Acc:86.98%
Epoch [73/300], Step [250/391],                 Loss: 0.38160, Train_Acc:86.98%
Epoch [73/300], Step [260/391],                 Loss: 0.38333, Train_Acc:86.95%
Epoch [73/300], Step [270/391],                 Loss: 0.38484, Train_Acc:86.91%
Epoch [73/300], Step [280/391],                 Loss: 0.38456, Train_Acc:86.93%
Epoch [73/300], Step [290/391],                 Loss: 0.38373, Train_Acc:86.97%
Epoch [73/300], Step [300/391],                 Loss: 0.38337, Train_Acc:86.98%
Epoch [73/300], Step [310/391],                 Loss: 0.38255, Train_Acc:87.00%
Epoch [73/300], Step [320/391],                 Loss: 0.38182, Train_Acc:87.03%
Epoch [73/300], Step [330/391],                 Loss: 0.38038, Train_Acc:87.08%
Epoch [73/300], Step [340/391],                 Loss: 0.37915, Train_Acc:87.09%
Epoch [73/300], Step [350/391],                 Loss: 0.37838, Train_Acc:87.10%
Epoch [73/300], Step [360/391],                 Loss: 0.37790, Train_Acc:87.13%
Epoch [73/300], Step [370/391],                 Loss: 0.37746, Train_Acc:87.13%
Epoch [73/300], Step [380/391],                 Loss: 0.37694, Train_Acc:87.17%
Epoch [73/300], Step [390/391],                 Loss: 0.37598, Train_Acc:87.20%
Accuary on test images:70.34%
Epoch [74/300], Step [10/391],                 Loss: 0.39728, Train_Acc:86.64%
Epoch [74/300], Step [20/391],                 Loss: 0.37918, Train_Acc:87.70%
Epoch [74/300], Step [30/391],                 Loss: 0.37591, Train_Acc:87.55%
Epoch [74/300], Step [40/391],                 Loss: 0.38578, Train_Acc:87.23%
Epoch [74/300], Step [50/391],                 Loss: 0.38116, Train_Acc:87.23%
Epoch [74/300], Step [60/391],                 Loss: 0.38409, Train_Acc:87.14%
Epoch [74/300], Step [70/391],                 Loss: 0.38291, Train_Acc:87.12%
Epoch [74/300], Step [80/391],                 Loss: 0.38449, Train_Acc:87.02%
Epoch [74/300], Step [90/391],                 Loss: 0.38513, Train_Acc:86.91%
Epoch [74/300], Step [100/391],                 Loss: 0.38454, Train_Acc:86.88%
Epoch [74/300], Step [110/391],                 Loss: 0.38441, Train_Acc:86.97%
Epoch [74/300], Step [120/391],                 Loss: 0.38293, Train_Acc:86.97%
Epoch [74/300], Step [130/391],                 Loss: 0.38189, Train_Acc:86.98%
Epoch [74/300], Step [140/391],                 Loss: 0.38038, Train_Acc:87.06%
Epoch [74/300], Step [150/391],                 Loss: 0.37928, Train_Acc:87.09%
Epoch [74/300], Step [160/391],                 Loss: 0.37698, Train_Acc:87.15%
Epoch [74/300], Step [170/391],                 Loss: 0.37628, Train_Acc:87.18%
Epoch [74/300], Step [180/391],                 Loss: 0.37618, Train_Acc:87.15%
Epoch [74/300], Step [190/391],                 Loss: 0.37659, Train_Acc:87.09%
Epoch [74/300], Step [200/391],                 Loss: 0.37845, Train_Acc:87.05%
Epoch [74/300], Step [210/391],                 Loss: 0.38005, Train_Acc:87.00%
Epoch [74/300], Step [220/391],                 Loss: 0.37985, Train_Acc:86.99%
Epoch [74/300], Step [230/391],                 Loss: 0.37937, Train_Acc:87.02%
Epoch [74/300], Step [240/391],                 Loss: 0.37689, Train_Acc:87.13%
Epoch [74/300], Step [250/391],                 Loss: 0.37631, Train_Acc:87.13%
Epoch [74/300], Step [260/391],                 Loss: 0.37721, Train_Acc:87.13%
Epoch [74/300], Step [270/391],                 Loss: 0.37980, Train_Acc:87.02%
Epoch [74/300], Step [280/391],                 Loss: 0.38042, Train_Acc:86.98%
Epoch [74/300], Step [290/391],                 Loss: 0.38083, Train_Acc:86.95%
Epoch [74/300], Step [300/391],                 Loss: 0.37962, Train_Acc:87.00%
Epoch [74/300], Step [310/391],                 Loss: 0.37905, Train_Acc:87.03%
Epoch [74/300], Step [320/391],                 Loss: 0.37834, Train_Acc:87.03%
Epoch [74/300], Step [330/391],                 Loss: 0.37624, Train_Acc:87.10%
Epoch [74/300], Step [340/391],                 Loss: 0.37501, Train_Acc:87.15%
Epoch [74/300], Step [350/391],                 Loss: 0.37532, Train_Acc:87.16%
Epoch [74/300], Step [360/391],                 Loss: 0.37507, Train_Acc:87.16%
Epoch [74/300], Step [370/391],                 Loss: 0.37538, Train_Acc:87.13%
Epoch [74/300], Step [380/391],                 Loss: 0.37522, Train_Acc:87.15%
Epoch [74/300], Step [390/391],                 Loss: 0.37519, Train_Acc:87.14%
Accuary on test images:65.36%
Epoch [75/300], Step [10/391],                 Loss: 0.36707, Train_Acc:87.89%
Epoch [75/300], Step [20/391],                 Loss: 0.35399, Train_Acc:88.05%
Epoch [75/300], Step [30/391],                 Loss: 0.34593, Train_Acc:88.10%
Epoch [75/300], Step [40/391],                 Loss: 0.35812, Train_Acc:87.81%
Epoch [75/300], Step [50/391],                 Loss: 0.35906, Train_Acc:87.70%
Epoch [75/300], Step [60/391],                 Loss: 0.36861, Train_Acc:87.33%
Epoch [75/300], Step [70/391],                 Loss: 0.36745, Train_Acc:87.42%
Epoch [75/300], Step [80/391],                 Loss: 0.36699, Train_Acc:87.52%
Epoch [75/300], Step [90/391],                 Loss: 0.36968, Train_Acc:87.45%
Epoch [75/300], Step [100/391],                 Loss: 0.36959, Train_Acc:87.43%
Epoch [75/300], Step [110/391],                 Loss: 0.37146, Train_Acc:87.34%
Epoch [75/300], Step [120/391],                 Loss: 0.37043, Train_Acc:87.40%
Epoch [75/300], Step [130/391],                 Loss: 0.37317, Train_Acc:87.33%
Epoch [75/300], Step [140/391],                 Loss: 0.37183, Train_Acc:87.42%
Epoch [75/300], Step [150/391],                 Loss: 0.37239, Train_Acc:87.39%
Epoch [75/300], Step [160/391],                 Loss: 0.37000, Train_Acc:87.50%
Epoch [75/300], Step [170/391],                 Loss: 0.37006, Train_Acc:87.53%
Epoch [75/300], Step [180/391],                 Loss: 0.37158, Train_Acc:87.48%
Epoch [75/300], Step [190/391],                 Loss: 0.37313, Train_Acc:87.46%
Epoch [75/300], Step [200/391],                 Loss: 0.37429, Train_Acc:87.45%
Epoch [75/300], Step [210/391],                 Loss: 0.37448, Train_Acc:87.41%
Epoch [75/300], Step [220/391],                 Loss: 0.37422, Train_Acc:87.43%
Epoch [75/300], Step [230/391],                 Loss: 0.37310, Train_Acc:87.48%
Epoch [75/300], Step [240/391],                 Loss: 0.37283, Train_Acc:87.47%
Epoch [75/300], Step [250/391],                 Loss: 0.37351, Train_Acc:87.45%
Epoch [75/300], Step [260/391],                 Loss: 0.37611, Train_Acc:87.36%
Epoch [75/300], Step [270/391],                 Loss: 0.37644, Train_Acc:87.34%
Epoch [75/300], Step [280/391],                 Loss: 0.37591, Train_Acc:87.37%
Epoch [75/300], Step [290/391],                 Loss: 0.37468, Train_Acc:87.40%
Epoch [75/300], Step [300/391],                 Loss: 0.37383, Train_Acc:87.43%
Epoch [75/300], Step [310/391],                 Loss: 0.37294, Train_Acc:87.46%
Epoch [75/300], Step [320/391],                 Loss: 0.37242, Train_Acc:87.48%
Epoch [75/300], Step [330/391],                 Loss: 0.37196, Train_Acc:87.48%
Epoch [75/300], Step [340/391],                 Loss: 0.37214, Train_Acc:87.45%
Epoch [75/300], Step [350/391],                 Loss: 0.37211, Train_Acc:87.43%
Epoch [75/300], Step [360/391],                 Loss: 0.37202, Train_Acc:87.42%
Epoch [75/300], Step [370/391],                 Loss: 0.37255, Train_Acc:87.38%
Epoch [75/300], Step [380/391],                 Loss: 0.37253, Train_Acc:87.38%
Epoch [75/300], Step [390/391],                 Loss: 0.37264, Train_Acc:87.35%
Accuary on test images:75.96%
Epoch [76/300], Step [10/391],                 Loss: 0.36930, Train_Acc:86.48%
Epoch [76/300], Step [20/391],                 Loss: 0.37176, Train_Acc:86.76%
Epoch [76/300], Step [30/391],                 Loss: 0.37077, Train_Acc:87.16%
Epoch [76/300], Step [40/391],                 Loss: 0.38076, Train_Acc:87.07%
Epoch [76/300], Step [50/391],                 Loss: 0.38606, Train_Acc:86.69%
Epoch [76/300], Step [60/391],                 Loss: 0.38973, Train_Acc:86.61%
Epoch [76/300], Step [70/391],                 Loss: 0.38595, Train_Acc:86.69%
Epoch [76/300], Step [80/391],                 Loss: 0.38313, Train_Acc:86.89%
Epoch [76/300], Step [90/391],                 Loss: 0.38717, Train_Acc:86.71%
Epoch [76/300], Step [100/391],                 Loss: 0.38808, Train_Acc:86.65%
Epoch [76/300], Step [110/391],                 Loss: 0.38956, Train_Acc:86.68%
Epoch [76/300], Step [120/391],                 Loss: 0.39019, Train_Acc:86.63%
Epoch [76/300], Step [130/391],                 Loss: 0.39045, Train_Acc:86.66%
Epoch [76/300], Step [140/391],                 Loss: 0.38934, Train_Acc:86.67%
Epoch [76/300], Step [150/391],                 Loss: 0.38910, Train_Acc:86.74%
Epoch [76/300], Step [160/391],                 Loss: 0.38680, Train_Acc:86.85%
Epoch [76/300], Step [170/391],                 Loss: 0.38617, Train_Acc:86.82%
Epoch [76/300], Step [180/391],                 Loss: 0.38484, Train_Acc:86.84%
Epoch [76/300], Step [190/391],                 Loss: 0.38404, Train_Acc:86.87%
Epoch [76/300], Step [200/391],                 Loss: 0.38344, Train_Acc:86.85%
Epoch [76/300], Step [210/391],                 Loss: 0.38227, Train_Acc:86.89%
Epoch [76/300], Step [220/391],                 Loss: 0.38047, Train_Acc:86.97%
Epoch [76/300], Step [230/391],                 Loss: 0.37742, Train_Acc:87.06%
Epoch [76/300], Step [240/391],                 Loss: 0.37675, Train_Acc:87.06%
Epoch [76/300], Step [250/391],                 Loss: 0.37602, Train_Acc:87.07%
Epoch [76/300], Step [260/391],                 Loss: 0.37890, Train_Acc:86.98%
Epoch [76/300], Step [270/391],                 Loss: 0.38051, Train_Acc:86.92%
Epoch [76/300], Step [280/391],                 Loss: 0.38026, Train_Acc:86.92%
Epoch [76/300], Step [290/391],                 Loss: 0.38013, Train_Acc:86.94%
Epoch [76/300], Step [300/391],                 Loss: 0.38008, Train_Acc:86.98%
Epoch [76/300], Step [310/391],                 Loss: 0.37927, Train_Acc:86.98%
Epoch [76/300], Step [320/391],                 Loss: 0.37947, Train_Acc:86.98%
Epoch [76/300], Step [330/391],                 Loss: 0.37821, Train_Acc:87.03%
Epoch [76/300], Step [340/391],                 Loss: 0.37757, Train_Acc:87.05%
Epoch [76/300], Step [350/391],                 Loss: 0.37681, Train_Acc:87.06%
Epoch [76/300], Step [360/391],                 Loss: 0.37641, Train_Acc:87.07%
Epoch [76/300], Step [370/391],                 Loss: 0.37654, Train_Acc:87.04%
Epoch [76/300], Step [380/391],                 Loss: 0.37627, Train_Acc:87.06%
Epoch [76/300], Step [390/391],                 Loss: 0.37578, Train_Acc:87.12%
Accuary on test images:71.52%
Epoch [77/300], Step [10/391],                 Loss: 0.37456, Train_Acc:86.95%
Epoch [77/300], Step [20/391],                 Loss: 0.36597, Train_Acc:87.66%
Epoch [77/300], Step [30/391],                 Loss: 0.36187, Train_Acc:88.12%
Epoch [77/300], Step [40/391],                 Loss: 0.36160, Train_Acc:88.05%
Epoch [77/300], Step [50/391],                 Loss: 0.36724, Train_Acc:87.78%
Epoch [77/300], Step [60/391],                 Loss: 0.38020, Train_Acc:87.23%
Epoch [77/300], Step [70/391],                 Loss: 0.37638, Train_Acc:87.32%
Epoch [77/300], Step [80/391],                 Loss: 0.37688, Train_Acc:87.34%
Epoch [77/300], Step [90/391],                 Loss: 0.38061, Train_Acc:87.16%
Epoch [77/300], Step [100/391],                 Loss: 0.38147, Train_Acc:87.16%
Epoch [77/300], Step [110/391],                 Loss: 0.38464, Train_Acc:87.07%
Epoch [77/300], Step [120/391],                 Loss: 0.38556, Train_Acc:87.04%
Epoch [77/300], Step [130/391],                 Loss: 0.38874, Train_Acc:86.91%
Epoch [77/300], Step [140/391],                 Loss: 0.38692, Train_Acc:87.01%
Epoch [77/300], Step [150/391],                 Loss: 0.38749, Train_Acc:86.96%
Epoch [77/300], Step [160/391],                 Loss: 0.38667, Train_Acc:87.06%
Epoch [77/300], Step [170/391],                 Loss: 0.38733, Train_Acc:87.06%
Epoch [77/300], Step [180/391],                 Loss: 0.38545, Train_Acc:87.07%
Epoch [77/300], Step [190/391],                 Loss: 0.38356, Train_Acc:87.14%
Epoch [77/300], Step [200/391],                 Loss: 0.38343, Train_Acc:87.12%
Epoch [77/300], Step [210/391],                 Loss: 0.38337, Train_Acc:87.11%
Epoch [77/300], Step [220/391],                 Loss: 0.38316, Train_Acc:87.16%
Epoch [77/300], Step [230/391],                 Loss: 0.38190, Train_Acc:87.18%
Epoch [77/300], Step [240/391],                 Loss: 0.38049, Train_Acc:87.19%
Epoch [77/300], Step [250/391],                 Loss: 0.38021, Train_Acc:87.17%
Epoch [77/300], Step [260/391],                 Loss: 0.38174, Train_Acc:87.14%
Epoch [77/300], Step [270/391],                 Loss: 0.38243, Train_Acc:87.14%
Epoch [77/300], Step [280/391],                 Loss: 0.38259, Train_Acc:87.11%
Epoch [77/300], Step [290/391],                 Loss: 0.38453, Train_Acc:87.03%
Epoch [77/300], Step [300/391],                 Loss: 0.38470, Train_Acc:87.01%
Epoch [77/300], Step [310/391],                 Loss: 0.38462, Train_Acc:87.02%
Epoch [77/300], Step [320/391],                 Loss: 0.38491, Train_Acc:87.01%
Epoch [77/300], Step [330/391],                 Loss: 0.38298, Train_Acc:87.10%
Epoch [77/300], Step [340/391],                 Loss: 0.38256, Train_Acc:87.11%
Epoch [77/300], Step [350/391],                 Loss: 0.38115, Train_Acc:87.17%
Epoch [77/300], Step [360/391],                 Loss: 0.38012, Train_Acc:87.22%
Epoch [77/300], Step [370/391],                 Loss: 0.38013, Train_Acc:87.21%
Epoch [77/300], Step [380/391],                 Loss: 0.37932, Train_Acc:87.25%
Epoch [77/300], Step [390/391],                 Loss: 0.37930, Train_Acc:87.26%
Accuary on test images:75.02%
Epoch [78/300], Step [10/391],                 Loss: 0.36935, Train_Acc:87.66%
Epoch [78/300], Step [20/391],                 Loss: 0.35232, Train_Acc:88.44%
Epoch [78/300], Step [30/391],                 Loss: 0.34653, Train_Acc:88.57%
Epoch [78/300], Step [40/391],                 Loss: 0.34116, Train_Acc:88.75%
Epoch [78/300], Step [50/391],                 Loss: 0.34254, Train_Acc:88.73%
Epoch [78/300], Step [60/391],                 Loss: 0.34872, Train_Acc:88.53%
Epoch [78/300], Step [70/391],                 Loss: 0.35025, Train_Acc:88.46%
Epoch [78/300], Step [80/391],                 Loss: 0.35289, Train_Acc:88.33%
Epoch [78/300], Step [90/391],                 Loss: 0.35548, Train_Acc:88.33%
Epoch [78/300], Step [100/391],                 Loss: 0.35254, Train_Acc:88.33%
Epoch [78/300], Step [110/391],                 Loss: 0.35295, Train_Acc:88.28%
Epoch [78/300], Step [120/391],                 Loss: 0.35460, Train_Acc:88.22%
Epoch [78/300], Step [130/391],                 Loss: 0.35720, Train_Acc:88.13%
Epoch [78/300], Step [140/391],                 Loss: 0.35895, Train_Acc:88.00%
Epoch [78/300], Step [150/391],                 Loss: 0.35920, Train_Acc:87.95%
Epoch [78/300], Step [160/391],                 Loss: 0.36060, Train_Acc:87.87%
Epoch [78/300], Step [170/391],                 Loss: 0.36331, Train_Acc:87.72%
Epoch [78/300], Step [180/391],                 Loss: 0.36451, Train_Acc:87.64%
Epoch [78/300], Step [190/391],                 Loss: 0.36477, Train_Acc:87.64%
Epoch [78/300], Step [200/391],                 Loss: 0.36620, Train_Acc:87.58%
Epoch [78/300], Step [210/391],                 Loss: 0.36573, Train_Acc:87.60%
Epoch [78/300], Step [220/391],                 Loss: 0.36549, Train_Acc:87.66%
Epoch [78/300], Step [230/391],                 Loss: 0.36431, Train_Acc:87.68%
Epoch [78/300], Step [240/391],                 Loss: 0.36327, Train_Acc:87.74%
Epoch [78/300], Step [250/391],                 Loss: 0.36235, Train_Acc:87.78%
Epoch [78/300], Step [260/391],                 Loss: 0.36561, Train_Acc:87.68%
Epoch [78/300], Step [270/391],                 Loss: 0.36893, Train_Acc:87.59%
Epoch [78/300], Step [280/391],                 Loss: 0.36908, Train_Acc:87.55%
Epoch [78/300], Step [290/391],                 Loss: 0.36958, Train_Acc:87.56%
Epoch [78/300], Step [300/391],                 Loss: 0.36885, Train_Acc:87.57%
Epoch [78/300], Step [310/391],                 Loss: 0.36798, Train_Acc:87.62%
Epoch [78/300], Step [320/391],                 Loss: 0.36919, Train_Acc:87.57%
Epoch [78/300], Step [330/391],                 Loss: 0.36922, Train_Acc:87.56%
Epoch [78/300], Step [340/391],                 Loss: 0.36952, Train_Acc:87.52%
Epoch [78/300], Step [350/391],                 Loss: 0.36875, Train_Acc:87.55%
Epoch [78/300], Step [360/391],                 Loss: 0.36867, Train_Acc:87.55%
Epoch [78/300], Step [370/391],                 Loss: 0.36864, Train_Acc:87.56%
Epoch [78/300], Step [380/391],                 Loss: 0.36860, Train_Acc:87.56%
Epoch [78/300], Step [390/391],                 Loss: 0.36789, Train_Acc:87.59%
Accuary on test images:78.50%
Epoch [79/300], Step [10/391],                 Loss: 0.37233, Train_Acc:87.50%
Epoch [79/300], Step [20/391],                 Loss: 0.37116, Train_Acc:87.27%
Epoch [79/300], Step [30/391],                 Loss: 0.37135, Train_Acc:87.21%
Epoch [79/300], Step [40/391],                 Loss: 0.36893, Train_Acc:87.32%
Epoch [79/300], Step [50/391],                 Loss: 0.36763, Train_Acc:87.41%
Epoch [79/300], Step [60/391],                 Loss: 0.37422, Train_Acc:87.14%
Epoch [79/300], Step [70/391],                 Loss: 0.37355, Train_Acc:87.28%
Epoch [79/300], Step [80/391],                 Loss: 0.36968, Train_Acc:87.33%
Epoch [79/300], Step [90/391],                 Loss: 0.37577, Train_Acc:87.25%
Epoch [79/300], Step [100/391],                 Loss: 0.37455, Train_Acc:87.23%
Epoch [79/300], Step [110/391],                 Loss: 0.37809, Train_Acc:87.11%
Epoch [79/300], Step [120/391],                 Loss: 0.37948, Train_Acc:86.98%
Epoch [79/300], Step [130/391],                 Loss: 0.38061, Train_Acc:86.97%
Epoch [79/300], Step [140/391],                 Loss: 0.37850, Train_Acc:87.03%
Epoch [79/300], Step [150/391],                 Loss: 0.37840, Train_Acc:87.04%
Epoch [79/300], Step [160/391],                 Loss: 0.37705, Train_Acc:87.11%
Epoch [79/300], Step [170/391],                 Loss: 0.37637, Train_Acc:87.22%
Epoch [79/300], Step [180/391],                 Loss: 0.37582, Train_Acc:87.21%
Epoch [79/300], Step [190/391],                 Loss: 0.37552, Train_Acc:87.15%
Epoch [79/300], Step [200/391],                 Loss: 0.37701, Train_Acc:87.11%
Epoch [79/300], Step [210/391],                 Loss: 0.37629, Train_Acc:87.11%
Epoch [79/300], Step [220/391],                 Loss: 0.37721, Train_Acc:87.11%
Epoch [79/300], Step [230/391],                 Loss: 0.37607, Train_Acc:87.17%
Epoch [79/300], Step [240/391],                 Loss: 0.37497, Train_Acc:87.19%
Epoch [79/300], Step [250/391],                 Loss: 0.37425, Train_Acc:87.19%
Epoch [79/300], Step [260/391],                 Loss: 0.37653, Train_Acc:87.11%
Epoch [79/300], Step [270/391],                 Loss: 0.37791, Train_Acc:87.07%
Epoch [79/300], Step [280/391],                 Loss: 0.37892, Train_Acc:87.05%
Epoch [79/300], Step [290/391],                 Loss: 0.37840, Train_Acc:87.06%
Epoch [79/300], Step [300/391],                 Loss: 0.37862, Train_Acc:87.04%
Epoch [79/300], Step [310/391],                 Loss: 0.37844, Train_Acc:87.06%
Epoch [79/300], Step [320/391],                 Loss: 0.37979, Train_Acc:87.05%
Epoch [79/300], Step [330/391],                 Loss: 0.37784, Train_Acc:87.13%
Epoch [79/300], Step [340/391],                 Loss: 0.37713, Train_Acc:87.14%
Epoch [79/300], Step [350/391],                 Loss: 0.37639, Train_Acc:87.16%
Epoch [79/300], Step [360/391],                 Loss: 0.37543, Train_Acc:87.17%
Epoch [79/300], Step [370/391],                 Loss: 0.37565, Train_Acc:87.17%
Epoch [79/300], Step [380/391],                 Loss: 0.37518, Train_Acc:87.20%
Epoch [79/300], Step [390/391],                 Loss: 0.37474, Train_Acc:87.20%
Accuary on test images:78.40%
Epoch [80/300], Step [10/391],                 Loss: 0.38509, Train_Acc:85.94%
Epoch [80/300], Step [20/391],                 Loss: 0.36917, Train_Acc:86.95%
Epoch [80/300], Step [30/391],                 Loss: 0.36658, Train_Acc:87.21%
Epoch [80/300], Step [40/391],                 Loss: 0.36785, Train_Acc:87.07%
Epoch [80/300], Step [50/391],                 Loss: 0.37094, Train_Acc:87.00%
Epoch [80/300], Step [60/391],                 Loss: 0.37765, Train_Acc:86.69%
Epoch [80/300], Step [70/391],                 Loss: 0.37304, Train_Acc:86.99%
Epoch [80/300], Step [80/391],                 Loss: 0.37171, Train_Acc:87.12%
Epoch [80/300], Step [90/391],                 Loss: 0.37554, Train_Acc:86.92%
Epoch [80/300], Step [100/391],                 Loss: 0.37468, Train_Acc:87.02%
Epoch [80/300], Step [110/391],                 Loss: 0.37679, Train_Acc:87.05%
Epoch [80/300], Step [120/391],                 Loss: 0.37948, Train_Acc:86.97%
Epoch [80/300], Step [130/391],                 Loss: 0.37906, Train_Acc:87.00%
Epoch [80/300], Step [140/391],                 Loss: 0.37808, Train_Acc:87.03%
Epoch [80/300], Step [150/391],                 Loss: 0.37929, Train_Acc:86.95%
Epoch [80/300], Step [160/391],                 Loss: 0.37733, Train_Acc:87.04%
Epoch [80/300], Step [170/391],                 Loss: 0.37532, Train_Acc:87.07%
Epoch [80/300], Step [180/391],                 Loss: 0.37536, Train_Acc:87.03%
Epoch [80/300], Step [190/391],                 Loss: 0.37585, Train_Acc:87.04%
Epoch [80/300], Step [200/391],                 Loss: 0.37889, Train_Acc:86.94%
Epoch [80/300], Step [210/391],                 Loss: 0.37936, Train_Acc:86.90%
Epoch [80/300], Step [220/391],                 Loss: 0.37934, Train_Acc:86.90%
Epoch [80/300], Step [230/391],                 Loss: 0.38019, Train_Acc:86.86%
Epoch [80/300], Step [240/391],                 Loss: 0.37959, Train_Acc:86.91%
Epoch [80/300], Step [250/391],                 Loss: 0.37947, Train_Acc:86.91%
Epoch [80/300], Step [260/391],                 Loss: 0.38084, Train_Acc:86.89%
Epoch [80/300], Step [270/391],                 Loss: 0.38274, Train_Acc:86.84%
Epoch [80/300], Step [280/391],                 Loss: 0.38256, Train_Acc:86.83%
Epoch [80/300], Step [290/391],                 Loss: 0.38214, Train_Acc:86.90%
Epoch [80/300], Step [300/391],                 Loss: 0.38051, Train_Acc:86.95%
Epoch [80/300], Step [310/391],                 Loss: 0.38143, Train_Acc:86.92%
Epoch [80/300], Step [320/391],                 Loss: 0.38138, Train_Acc:86.96%
Epoch [80/300], Step [330/391],                 Loss: 0.37991, Train_Acc:87.00%
Epoch [80/300], Step [340/391],                 Loss: 0.37990, Train_Acc:87.04%
Epoch [80/300], Step [350/391],                 Loss: 0.37976, Train_Acc:87.06%
Epoch [80/300], Step [360/391],                 Loss: 0.37987, Train_Acc:87.05%
Epoch [80/300], Step [370/391],                 Loss: 0.38080, Train_Acc:87.00%
Epoch [80/300], Step [380/391],                 Loss: 0.37952, Train_Acc:87.06%
Epoch [80/300], Step [390/391],                 Loss: 0.37800, Train_Acc:87.12%
Accuary on test images:79.92%
Epoch [81/300], Step [10/391],                 Loss: 0.34487, Train_Acc:88.52%
Epoch [81/300], Step [20/391],                 Loss: 0.35192, Train_Acc:88.36%
Epoch [81/300], Step [30/391],                 Loss: 0.36440, Train_Acc:87.94%
Epoch [81/300], Step [40/391],                 Loss: 0.36753, Train_Acc:87.62%
Epoch [81/300], Step [50/391],                 Loss: 0.37526, Train_Acc:87.19%
Epoch [81/300], Step [60/391],                 Loss: 0.38187, Train_Acc:87.11%
Epoch [81/300], Step [70/391],                 Loss: 0.38019, Train_Acc:87.15%
Epoch [81/300], Step [80/391],                 Loss: 0.38145, Train_Acc:87.17%
Epoch [81/300], Step [90/391],                 Loss: 0.38803, Train_Acc:86.97%
Epoch [81/300], Step [100/391],                 Loss: 0.38902, Train_Acc:86.91%
Epoch [81/300], Step [110/391],                 Loss: 0.39316, Train_Acc:86.78%
Epoch [81/300], Step [120/391],                 Loss: 0.39079, Train_Acc:86.81%
Epoch [81/300], Step [130/391],                 Loss: 0.38981, Train_Acc:86.86%
Epoch [81/300], Step [140/391],                 Loss: 0.38521, Train_Acc:87.07%
Epoch [81/300], Step [150/391],                 Loss: 0.38578, Train_Acc:87.06%
Epoch [81/300], Step [160/391],                 Loss: 0.38465, Train_Acc:87.09%
Epoch [81/300], Step [170/391],                 Loss: 0.38416, Train_Acc:87.10%
Epoch [81/300], Step [180/391],                 Loss: 0.38392, Train_Acc:87.10%
Epoch [81/300], Step [190/391],                 Loss: 0.38251, Train_Acc:87.08%
Epoch [81/300], Step [200/391],                 Loss: 0.38177, Train_Acc:87.12%
Epoch [81/300], Step [210/391],                 Loss: 0.38166, Train_Acc:87.10%
Epoch [81/300], Step [220/391],                 Loss: 0.38082, Train_Acc:87.12%
Epoch [81/300], Step [230/391],                 Loss: 0.37888, Train_Acc:87.18%
Epoch [81/300], Step [240/391],                 Loss: 0.37736, Train_Acc:87.21%
Epoch [81/300], Step [250/391],                 Loss: 0.37628, Train_Acc:87.24%
Epoch [81/300], Step [260/391],                 Loss: 0.37761, Train_Acc:87.24%
Epoch [81/300], Step [270/391],                 Loss: 0.37889, Train_Acc:87.17%
Epoch [81/300], Step [280/391],                 Loss: 0.37865, Train_Acc:87.18%
Epoch [81/300], Step [290/391],                 Loss: 0.37827, Train_Acc:87.21%
Epoch [81/300], Step [300/391],                 Loss: 0.37680, Train_Acc:87.23%
Epoch [81/300], Step [310/391],                 Loss: 0.37525, Train_Acc:87.30%
Epoch [81/300], Step [320/391],                 Loss: 0.37450, Train_Acc:87.33%
Epoch [81/300], Step [330/391],                 Loss: 0.37357, Train_Acc:87.35%
Epoch [81/300], Step [340/391],                 Loss: 0.37321, Train_Acc:87.34%
Epoch [81/300], Step [350/391],                 Loss: 0.37320, Train_Acc:87.35%
Epoch [81/300], Step [360/391],                 Loss: 0.37303, Train_Acc:87.34%
Epoch [81/300], Step [370/391],                 Loss: 0.37360, Train_Acc:87.30%
Epoch [81/300], Step [380/391],                 Loss: 0.37340, Train_Acc:87.34%
Epoch [81/300], Step [390/391],                 Loss: 0.37283, Train_Acc:87.37%
Accuary on test images:74.40%
Epoch [82/300], Step [10/391],                 Loss: 0.33941, Train_Acc:87.97%
Epoch [82/300], Step [20/391],                 Loss: 0.33847, Train_Acc:88.16%
Epoch [82/300], Step [30/391],                 Loss: 0.34416, Train_Acc:87.81%
Epoch [82/300], Step [40/391],                 Loss: 0.35452, Train_Acc:87.40%
Epoch [82/300], Step [50/391],                 Loss: 0.36411, Train_Acc:87.22%
Epoch [82/300], Step [60/391],                 Loss: 0.37219, Train_Acc:87.07%
Epoch [82/300], Step [70/391],                 Loss: 0.36760, Train_Acc:87.38%
Epoch [82/300], Step [80/391],                 Loss: 0.36697, Train_Acc:87.49%
Epoch [82/300], Step [90/391],                 Loss: 0.37261, Train_Acc:87.27%
Epoch [82/300], Step [100/391],                 Loss: 0.37834, Train_Acc:87.04%
Epoch [82/300], Step [110/391],                 Loss: 0.38073, Train_Acc:87.02%
Epoch [82/300], Step [120/391],                 Loss: 0.38236, Train_Acc:86.91%
Epoch [82/300], Step [130/391],                 Loss: 0.38299, Train_Acc:86.90%
Epoch [82/300], Step [140/391],                 Loss: 0.37937, Train_Acc:87.00%
Epoch [82/300], Step [150/391],                 Loss: 0.37986, Train_Acc:87.00%
Epoch [82/300], Step [160/391],                 Loss: 0.37883, Train_Acc:87.05%
Epoch [82/300], Step [170/391],                 Loss: 0.37982, Train_Acc:87.05%
Epoch [82/300], Step [180/391],                 Loss: 0.37751, Train_Acc:87.11%
Epoch [82/300], Step [190/391],                 Loss: 0.37548, Train_Acc:87.16%
Epoch [82/300], Step [200/391],                 Loss: 0.37454, Train_Acc:87.18%
Epoch [82/300], Step [210/391],                 Loss: 0.37530, Train_Acc:87.19%
Epoch [82/300], Step [220/391],                 Loss: 0.37495, Train_Acc:87.24%
Epoch [82/300], Step [230/391],                 Loss: 0.37440, Train_Acc:87.26%
Epoch [82/300], Step [240/391],                 Loss: 0.37337, Train_Acc:87.28%
Epoch [82/300], Step [250/391],                 Loss: 0.37387, Train_Acc:87.24%
Epoch [82/300], Step [260/391],                 Loss: 0.37553, Train_Acc:87.19%
Epoch [82/300], Step [270/391],                 Loss: 0.37888, Train_Acc:87.08%
Epoch [82/300], Step [280/391],                 Loss: 0.38030, Train_Acc:87.04%
Epoch [82/300], Step [290/391],                 Loss: 0.38039, Train_Acc:87.07%
Epoch [82/300], Step [300/391],                 Loss: 0.38064, Train_Acc:87.05%
Epoch [82/300], Step [310/391],                 Loss: 0.37898, Train_Acc:87.11%
Epoch [82/300], Step [320/391],                 Loss: 0.37839, Train_Acc:87.15%
Epoch [82/300], Step [330/391],                 Loss: 0.37755, Train_Acc:87.18%
Epoch [82/300], Step [340/391],                 Loss: 0.37719, Train_Acc:87.18%
Epoch [82/300], Step [350/391],                 Loss: 0.37751, Train_Acc:87.17%
Epoch [82/300], Step [360/391],                 Loss: 0.37730, Train_Acc:87.17%
Epoch [82/300], Step [370/391],                 Loss: 0.37780, Train_Acc:87.14%
Epoch [82/300], Step [380/391],                 Loss: 0.37810, Train_Acc:87.13%
Epoch [82/300], Step [390/391],                 Loss: 0.37782, Train_Acc:87.14%
Accuary on test images:78.24%
Epoch [83/300], Step [10/391],                 Loss: 0.37298, Train_Acc:87.34%
Epoch [83/300], Step [20/391],                 Loss: 0.34905, Train_Acc:88.12%
Epoch [83/300], Step [30/391],                 Loss: 0.34893, Train_Acc:87.94%
Epoch [83/300], Step [40/391],                 Loss: 0.34810, Train_Acc:88.24%
Epoch [83/300], Step [50/391],                 Loss: 0.34724, Train_Acc:88.25%
Epoch [83/300], Step [60/391],                 Loss: 0.35640, Train_Acc:87.76%
Epoch [83/300], Step [70/391],                 Loss: 0.35801, Train_Acc:87.87%
Epoch [83/300], Step [80/391],                 Loss: 0.36173, Train_Acc:87.75%
Epoch [83/300], Step [90/391],                 Loss: 0.36574, Train_Acc:87.63%
Epoch [83/300], Step [100/391],                 Loss: 0.36708, Train_Acc:87.54%
Epoch [83/300], Step [110/391],                 Loss: 0.37071, Train_Acc:87.40%
Epoch [83/300], Step [120/391],                 Loss: 0.37083, Train_Acc:87.42%
Epoch [83/300], Step [130/391],                 Loss: 0.37189, Train_Acc:87.48%
Epoch [83/300], Step [140/391],                 Loss: 0.37073, Train_Acc:87.50%
Epoch [83/300], Step [150/391],                 Loss: 0.37347, Train_Acc:87.33%
Epoch [83/300], Step [160/391],                 Loss: 0.37559, Train_Acc:87.27%
Epoch [83/300], Step [170/391],                 Loss: 0.37633, Train_Acc:87.27%
Epoch [83/300], Step [180/391],                 Loss: 0.37635, Train_Acc:87.26%
Epoch [83/300], Step [190/391],                 Loss: 0.37589, Train_Acc:87.31%
Epoch [83/300], Step [200/391],                 Loss: 0.37664, Train_Acc:87.28%
Epoch [83/300], Step [210/391],                 Loss: 0.37697, Train_Acc:87.28%
Epoch [83/300], Step [220/391],                 Loss: 0.37826, Train_Acc:87.23%
Epoch [83/300], Step [230/391],                 Loss: 0.37718, Train_Acc:87.24%
Epoch [83/300], Step [240/391],                 Loss: 0.37563, Train_Acc:87.27%
Epoch [83/300], Step [250/391],                 Loss: 0.37420, Train_Acc:87.28%
Epoch [83/300], Step [260/391],                 Loss: 0.37554, Train_Acc:87.24%
Epoch [83/300], Step [270/391],                 Loss: 0.37605, Train_Acc:87.20%
Epoch [83/300], Step [280/391],                 Loss: 0.37486, Train_Acc:87.24%
Epoch [83/300], Step [290/391],                 Loss: 0.37555, Train_Acc:87.23%
Epoch [83/300], Step [300/391],                 Loss: 0.37392, Train_Acc:87.28%
Epoch [83/300], Step [310/391],                 Loss: 0.37296, Train_Acc:87.30%
Epoch [83/300], Step [320/391],                 Loss: 0.37105, Train_Acc:87.36%
Epoch [83/300], Step [330/391],                 Loss: 0.36931, Train_Acc:87.41%
Epoch [83/300], Step [340/391],                 Loss: 0.36927, Train_Acc:87.42%
Epoch [83/300], Step [350/391],                 Loss: 0.36955, Train_Acc:87.43%
Epoch [83/300], Step [360/391],                 Loss: 0.37013, Train_Acc:87.42%
Epoch [83/300], Step [370/391],                 Loss: 0.37111, Train_Acc:87.36%
Epoch [83/300], Step [380/391],                 Loss: 0.37086, Train_Acc:87.36%
Epoch [83/300], Step [390/391],                 Loss: 0.37034, Train_Acc:87.37%
Accuary on test images:68.04%
Epoch [84/300], Step [10/391],                 Loss: 0.36330, Train_Acc:87.89%
Epoch [84/300], Step [20/391],                 Loss: 0.35118, Train_Acc:88.20%
Epoch [84/300], Step [30/391],                 Loss: 0.34472, Train_Acc:88.26%
Epoch [84/300], Step [40/391],                 Loss: 0.35044, Train_Acc:88.32%
Epoch [84/300], Step [50/391],                 Loss: 0.35439, Train_Acc:87.92%
Epoch [84/300], Step [60/391],                 Loss: 0.37162, Train_Acc:87.43%
Epoch [84/300], Step [70/391],                 Loss: 0.37065, Train_Acc:87.43%
Epoch [84/300], Step [80/391],                 Loss: 0.37474, Train_Acc:87.26%
Epoch [84/300], Step [90/391],                 Loss: 0.37888, Train_Acc:87.19%
Epoch [84/300], Step [100/391],                 Loss: 0.37680, Train_Acc:87.25%
Epoch [84/300], Step [110/391],                 Loss: 0.37706, Train_Acc:87.27%
Epoch [84/300], Step [120/391],                 Loss: 0.37551, Train_Acc:87.27%
Epoch [84/300], Step [130/391],                 Loss: 0.37754, Train_Acc:87.22%
Epoch [84/300], Step [140/391],                 Loss: 0.37621, Train_Acc:87.29%
Epoch [84/300], Step [150/391],                 Loss: 0.37397, Train_Acc:87.33%
Epoch [84/300], Step [160/391],                 Loss: 0.37145, Train_Acc:87.36%
Epoch [84/300], Step [170/391],                 Loss: 0.37137, Train_Acc:87.37%
Epoch [84/300], Step [180/391],                 Loss: 0.37103, Train_Acc:87.35%
Epoch [84/300], Step [190/391],                 Loss: 0.37214, Train_Acc:87.30%
Epoch [84/300], Step [200/391],                 Loss: 0.37394, Train_Acc:87.22%
Epoch [84/300], Step [210/391],                 Loss: 0.37702, Train_Acc:87.18%
Epoch [84/300], Step [220/391],                 Loss: 0.37752, Train_Acc:87.18%
Epoch [84/300], Step [230/391],                 Loss: 0.37706, Train_Acc:87.19%
Epoch [84/300], Step [240/391],                 Loss: 0.37530, Train_Acc:87.28%
Epoch [84/300], Step [250/391],                 Loss: 0.37562, Train_Acc:87.26%
Epoch [84/300], Step [260/391],                 Loss: 0.37719, Train_Acc:87.20%
Epoch [84/300], Step [270/391],                 Loss: 0.37843, Train_Acc:87.16%
Epoch [84/300], Step [280/391],                 Loss: 0.37810, Train_Acc:87.16%
Epoch [84/300], Step [290/391],                 Loss: 0.37782, Train_Acc:87.14%
Epoch [84/300], Step [300/391],                 Loss: 0.37626, Train_Acc:87.19%
Epoch [84/300], Step [310/391],                 Loss: 0.37441, Train_Acc:87.22%
Epoch [84/300], Step [320/391],                 Loss: 0.37508, Train_Acc:87.20%
Epoch [84/300], Step [330/391],                 Loss: 0.37396, Train_Acc:87.23%
Epoch [84/300], Step [340/391],                 Loss: 0.37368, Train_Acc:87.26%
Epoch [84/300], Step [350/391],                 Loss: 0.37323, Train_Acc:87.30%
Epoch [84/300], Step [360/391],                 Loss: 0.37395, Train_Acc:87.27%
Epoch [84/300], Step [370/391],                 Loss: 0.37563, Train_Acc:87.24%
Epoch [84/300], Step [380/391],                 Loss: 0.37569, Train_Acc:87.23%
Epoch [84/300], Step [390/391],                 Loss: 0.37540, Train_Acc:87.25%
Accuary on test images:77.48%
Epoch [85/300], Step [10/391],                 Loss: 0.37627, Train_Acc:87.03%
Epoch [85/300], Step [20/391],                 Loss: 0.35905, Train_Acc:87.54%
Epoch [85/300], Step [30/391],                 Loss: 0.35632, Train_Acc:87.60%
Epoch [85/300], Step [40/391],                 Loss: 0.35831, Train_Acc:87.87%
Epoch [85/300], Step [50/391],                 Loss: 0.35688, Train_Acc:87.86%
Epoch [85/300], Step [60/391],                 Loss: 0.36779, Train_Acc:87.50%
Epoch [85/300], Step [70/391],                 Loss: 0.36485, Train_Acc:87.59%
Epoch [85/300], Step [80/391],                 Loss: 0.36901, Train_Acc:87.45%
Epoch [85/300], Step [90/391],                 Loss: 0.37197, Train_Acc:87.29%
Epoch [85/300], Step [100/391],                 Loss: 0.37227, Train_Acc:87.33%
Epoch [85/300], Step [110/391],                 Loss: 0.37336, Train_Acc:87.20%
Epoch [85/300], Step [120/391],                 Loss: 0.37357, Train_Acc:87.09%
Epoch [85/300], Step [130/391],                 Loss: 0.37375, Train_Acc:87.07%
Epoch [85/300], Step [140/391],                 Loss: 0.37282, Train_Acc:87.17%
Epoch [85/300], Step [150/391],                 Loss: 0.37386, Train_Acc:87.10%
Epoch [85/300], Step [160/391],                 Loss: 0.37180, Train_Acc:87.15%
Epoch [85/300], Step [170/391],                 Loss: 0.37187, Train_Acc:87.20%
Epoch [85/300], Step [180/391],                 Loss: 0.37206, Train_Acc:87.17%
Epoch [85/300], Step [190/391],                 Loss: 0.37269, Train_Acc:87.13%
Epoch [85/300], Step [200/391],                 Loss: 0.37446, Train_Acc:87.10%
Epoch [85/300], Step [210/391],                 Loss: 0.37458, Train_Acc:87.12%
Epoch [85/300], Step [220/391],                 Loss: 0.37492, Train_Acc:87.16%
Epoch [85/300], Step [230/391],                 Loss: 0.37236, Train_Acc:87.23%
Epoch [85/300], Step [240/391],                 Loss: 0.37023, Train_Acc:87.33%
Epoch [85/300], Step [250/391],                 Loss: 0.36991, Train_Acc:87.33%
Epoch [85/300], Step [260/391],                 Loss: 0.37120, Train_Acc:87.30%
Epoch [85/300], Step [270/391],                 Loss: 0.37304, Train_Acc:87.26%
Epoch [85/300], Step [280/391],                 Loss: 0.37369, Train_Acc:87.25%
Epoch [85/300], Step [290/391],                 Loss: 0.37476, Train_Acc:87.21%
Epoch [85/300], Step [300/391],                 Loss: 0.37402, Train_Acc:87.21%
Epoch [85/300], Step [310/391],                 Loss: 0.37353, Train_Acc:87.25%
Epoch [85/300], Step [320/391],                 Loss: 0.37286, Train_Acc:87.29%
Epoch [85/300], Step [330/391],                 Loss: 0.37211, Train_Acc:87.35%
Epoch [85/300], Step [340/391],                 Loss: 0.37099, Train_Acc:87.37%
Epoch [85/300], Step [350/391],                 Loss: 0.37110, Train_Acc:87.34%
Epoch [85/300], Step [360/391],                 Loss: 0.37154, Train_Acc:87.33%
Epoch [85/300], Step [370/391],                 Loss: 0.37205, Train_Acc:87.29%
Epoch [85/300], Step [380/391],                 Loss: 0.37180, Train_Acc:87.30%
Epoch [85/300], Step [390/391],                 Loss: 0.37084, Train_Acc:87.34%
Accuary on test images:78.62%
Epoch [86/300], Step [10/391],                 Loss: 0.38604, Train_Acc:86.72%
Epoch [86/300], Step [20/391],                 Loss: 0.37142, Train_Acc:87.97%
Epoch [86/300], Step [30/391],                 Loss: 0.36347, Train_Acc:88.15%
Epoch [86/300], Step [40/391],                 Loss: 0.37348, Train_Acc:87.64%
Epoch [86/300], Step [50/391],                 Loss: 0.37111, Train_Acc:87.64%
Epoch [86/300], Step [60/391],                 Loss: 0.37710, Train_Acc:87.15%
Epoch [86/300], Step [70/391],                 Loss: 0.37347, Train_Acc:87.41%
Epoch [86/300], Step [80/391],                 Loss: 0.37614, Train_Acc:87.35%
Epoch [86/300], Step [90/391],                 Loss: 0.37926, Train_Acc:87.19%
Epoch [86/300], Step [100/391],                 Loss: 0.37723, Train_Acc:87.25%
Epoch [86/300], Step [110/391],                 Loss: 0.37873, Train_Acc:87.23%
Epoch [86/300], Step [120/391],                 Loss: 0.37823, Train_Acc:87.20%
Epoch [86/300], Step [130/391],                 Loss: 0.37924, Train_Acc:87.15%
Epoch [86/300], Step [140/391],                 Loss: 0.37703, Train_Acc:87.24%
Epoch [86/300], Step [150/391],                 Loss: 0.37478, Train_Acc:87.29%
Epoch [86/300], Step [160/391],                 Loss: 0.37497, Train_Acc:87.28%
Epoch [86/300], Step [170/391],                 Loss: 0.37719, Train_Acc:87.15%
Epoch [86/300], Step [180/391],                 Loss: 0.37990, Train_Acc:87.07%
Epoch [86/300], Step [190/391],                 Loss: 0.38176, Train_Acc:87.00%
Epoch [86/300], Step [200/391],                 Loss: 0.38513, Train_Acc:86.86%
Epoch [86/300], Step [210/391],                 Loss: 0.38514, Train_Acc:86.89%
Epoch [86/300], Step [220/391],                 Loss: 0.38591, Train_Acc:86.83%
Epoch [86/300], Step [230/391],                 Loss: 0.38403, Train_Acc:86.88%
Epoch [86/300], Step [240/391],                 Loss: 0.38324, Train_Acc:86.89%
Epoch [86/300], Step [250/391],                 Loss: 0.38189, Train_Acc:86.92%
Epoch [86/300], Step [260/391],                 Loss: 0.38305, Train_Acc:86.88%
Epoch [86/300], Step [270/391],                 Loss: 0.38391, Train_Acc:86.81%
Epoch [86/300], Step [280/391],                 Loss: 0.38298, Train_Acc:86.86%
Epoch [86/300], Step [290/391],                 Loss: 0.38276, Train_Acc:86.83%
Epoch [86/300], Step [300/391],                 Loss: 0.38243, Train_Acc:86.79%
Epoch [86/300], Step [310/391],                 Loss: 0.38156, Train_Acc:86.83%
Epoch [86/300], Step [320/391],                 Loss: 0.38195, Train_Acc:86.84%
Epoch [86/300], Step [330/391],                 Loss: 0.38074, Train_Acc:86.87%
Epoch [86/300], Step [340/391],                 Loss: 0.38053, Train_Acc:86.87%
Epoch [86/300], Step [350/391],                 Loss: 0.38065, Train_Acc:86.88%
Epoch [86/300], Step [360/391],                 Loss: 0.38060, Train_Acc:86.86%
Epoch [86/300], Step [370/391],                 Loss: 0.38052, Train_Acc:86.86%
Epoch [86/300], Step [380/391],                 Loss: 0.38010, Train_Acc:86.89%
Epoch [86/300], Step [390/391],                 Loss: 0.37905, Train_Acc:86.95%
Accuary on test images:74.12%
Epoch [87/300], Step [10/391],                 Loss: 0.38612, Train_Acc:86.41%
Epoch [87/300], Step [20/391],                 Loss: 0.36088, Train_Acc:87.54%
Epoch [87/300], Step [30/391],                 Loss: 0.35022, Train_Acc:88.31%
Epoch [87/300], Step [40/391],                 Loss: 0.35763, Train_Acc:88.01%
Epoch [87/300], Step [50/391],                 Loss: 0.35419, Train_Acc:88.11%
Epoch [87/300], Step [60/391],                 Loss: 0.36079, Train_Acc:87.83%
Epoch [87/300], Step [70/391],                 Loss: 0.35899, Train_Acc:87.86%
Epoch [87/300], Step [80/391],                 Loss: 0.36482, Train_Acc:87.78%
Epoch [87/300], Step [90/391],                 Loss: 0.37131, Train_Acc:87.51%
Epoch [87/300], Step [100/391],                 Loss: 0.37159, Train_Acc:87.38%
Epoch [87/300], Step [110/391],                 Loss: 0.37737, Train_Acc:87.17%
Epoch [87/300], Step [120/391],                 Loss: 0.37837, Train_Acc:87.13%
Epoch [87/300], Step [130/391],                 Loss: 0.37861, Train_Acc:87.14%
Epoch [87/300], Step [140/391],                 Loss: 0.37435, Train_Acc:87.30%
Epoch [87/300], Step [150/391],                 Loss: 0.37200, Train_Acc:87.36%
Epoch [87/300], Step [160/391],                 Loss: 0.37068, Train_Acc:87.46%
Epoch [87/300], Step [170/391],                 Loss: 0.37149, Train_Acc:87.47%
Epoch [87/300], Step [180/391],                 Loss: 0.37298, Train_Acc:87.35%
Epoch [87/300], Step [190/391],                 Loss: 0.37441, Train_Acc:87.31%
Epoch [87/300], Step [200/391],                 Loss: 0.37625, Train_Acc:87.25%
Epoch [87/300], Step [210/391],                 Loss: 0.37752, Train_Acc:87.21%
Epoch [87/300], Step [220/391],                 Loss: 0.37806, Train_Acc:87.19%
Epoch [87/300], Step [230/391],                 Loss: 0.37778, Train_Acc:87.20%
Epoch [87/300], Step [240/391],                 Loss: 0.37516, Train_Acc:87.27%
Epoch [87/300], Step [250/391],                 Loss: 0.37447, Train_Acc:87.29%
Epoch [87/300], Step [260/391],                 Loss: 0.37527, Train_Acc:87.32%
Epoch [87/300], Step [270/391],                 Loss: 0.37515, Train_Acc:87.31%
Epoch [87/300], Step [280/391],                 Loss: 0.37519, Train_Acc:87.30%
Epoch [87/300], Step [290/391],                 Loss: 0.37496, Train_Acc:87.30%
Epoch [87/300], Step [300/391],                 Loss: 0.37470, Train_Acc:87.33%
Epoch [87/300], Step [310/391],                 Loss: 0.37520, Train_Acc:87.34%
Epoch [87/300], Step [320/391],                 Loss: 0.37506, Train_Acc:87.34%
Epoch [87/300], Step [330/391],                 Loss: 0.37403, Train_Acc:87.41%
Epoch [87/300], Step [340/391],                 Loss: 0.37287, Train_Acc:87.46%
Epoch [87/300], Step [350/391],                 Loss: 0.37189, Train_Acc:87.46%
Epoch [87/300], Step [360/391],                 Loss: 0.37188, Train_Acc:87.43%
Epoch [87/300], Step [370/391],                 Loss: 0.37242, Train_Acc:87.40%
Epoch [87/300], Step [380/391],                 Loss: 0.37205, Train_Acc:87.40%
Epoch [87/300], Step [390/391],                 Loss: 0.37146, Train_Acc:87.43%
Accuary on test images:78.58%
Epoch [88/300], Step [10/391],                 Loss: 0.40193, Train_Acc:86.33%
Epoch [88/300], Step [20/391],                 Loss: 0.39898, Train_Acc:86.72%
Epoch [88/300], Step [30/391],                 Loss: 0.39291, Train_Acc:87.01%
Epoch [88/300], Step [40/391],                 Loss: 0.39117, Train_Acc:87.09%
Epoch [88/300], Step [50/391],                 Loss: 0.39363, Train_Acc:86.94%
Epoch [88/300], Step [60/391],                 Loss: 0.39580, Train_Acc:86.85%
Epoch [88/300], Step [70/391],                 Loss: 0.39978, Train_Acc:86.67%
Epoch [88/300], Step [80/391],                 Loss: 0.39911, Train_Acc:86.64%
Epoch [88/300], Step [90/391],                 Loss: 0.39780, Train_Acc:86.63%
Epoch [88/300], Step [100/391],                 Loss: 0.39696, Train_Acc:86.59%
Epoch [88/300], Step [110/391],                 Loss: 0.39732, Train_Acc:86.59%
Epoch [88/300], Step [120/391],                 Loss: 0.39779, Train_Acc:86.56%
Epoch [88/300], Step [130/391],                 Loss: 0.39617, Train_Acc:86.56%
Epoch [88/300], Step [140/391],                 Loss: 0.39172, Train_Acc:86.71%
Epoch [88/300], Step [150/391],                 Loss: 0.38802, Train_Acc:86.89%
Epoch [88/300], Step [160/391],                 Loss: 0.38591, Train_Acc:86.96%
Epoch [88/300], Step [170/391],                 Loss: 0.38567, Train_Acc:86.94%
Epoch [88/300], Step [180/391],                 Loss: 0.38339, Train_Acc:87.04%
Epoch [88/300], Step [190/391],                 Loss: 0.38309, Train_Acc:87.04%
Epoch [88/300], Step [200/391],                 Loss: 0.38349, Train_Acc:87.04%
Epoch [88/300], Step [210/391],                 Loss: 0.38244, Train_Acc:87.09%
Epoch [88/300], Step [220/391],                 Loss: 0.38106, Train_Acc:87.16%
Epoch [88/300], Step [230/391],                 Loss: 0.38031, Train_Acc:87.17%
Epoch [88/300], Step [240/391],                 Loss: 0.37750, Train_Acc:87.26%
Epoch [88/300], Step [250/391],                 Loss: 0.37719, Train_Acc:87.28%
Epoch [88/300], Step [260/391],                 Loss: 0.37877, Train_Acc:87.20%
Epoch [88/300], Step [270/391],                 Loss: 0.38062, Train_Acc:87.14%
Epoch [88/300], Step [280/391],                 Loss: 0.38066, Train_Acc:87.13%
Epoch [88/300], Step [290/391],                 Loss: 0.38047, Train_Acc:87.16%
Epoch [88/300], Step [300/391],                 Loss: 0.38133, Train_Acc:87.10%
Epoch [88/300], Step [310/391],                 Loss: 0.38054, Train_Acc:87.16%
Epoch [88/300], Step [320/391],                 Loss: 0.38072, Train_Acc:87.14%
Epoch [88/300], Step [330/391],                 Loss: 0.37958, Train_Acc:87.21%
Epoch [88/300], Step [340/391],                 Loss: 0.37846, Train_Acc:87.22%
Epoch [88/300], Step [350/391],                 Loss: 0.37744, Train_Acc:87.26%
Epoch [88/300], Step [360/391],                 Loss: 0.37675, Train_Acc:87.29%
Epoch [88/300], Step [370/391],                 Loss: 0.37728, Train_Acc:87.24%
Epoch [88/300], Step [380/391],                 Loss: 0.37719, Train_Acc:87.25%
Epoch [88/300], Step [390/391],                 Loss: 0.37731, Train_Acc:87.22%
Accuary on test images:73.44%
Epoch [89/300], Step [10/391],                 Loss: 0.37378, Train_Acc:87.81%
Epoch [89/300], Step [20/391],                 Loss: 0.35518, Train_Acc:88.40%
Epoch [89/300], Step [30/391],                 Loss: 0.35058, Train_Acc:88.10%
Epoch [89/300], Step [40/391],                 Loss: 0.35160, Train_Acc:88.12%
Epoch [89/300], Step [50/391],                 Loss: 0.36012, Train_Acc:87.95%
Epoch [89/300], Step [60/391],                 Loss: 0.37555, Train_Acc:87.49%
Epoch [89/300], Step [70/391],                 Loss: 0.37081, Train_Acc:87.71%
Epoch [89/300], Step [80/391],                 Loss: 0.37843, Train_Acc:87.40%
Epoch [89/300], Step [90/391],                 Loss: 0.38040, Train_Acc:87.24%
Epoch [89/300], Step [100/391],                 Loss: 0.38073, Train_Acc:87.25%
Epoch [89/300], Step [110/391],                 Loss: 0.38379, Train_Acc:87.18%
Epoch [89/300], Step [120/391],                 Loss: 0.38397, Train_Acc:87.15%
Epoch [89/300], Step [130/391],                 Loss: 0.38656, Train_Acc:87.06%
Epoch [89/300], Step [140/391],                 Loss: 0.38581, Train_Acc:87.08%
Epoch [89/300], Step [150/391],                 Loss: 0.38640, Train_Acc:87.04%
Epoch [89/300], Step [160/391],                 Loss: 0.38499, Train_Acc:87.16%
Epoch [89/300], Step [170/391],                 Loss: 0.38420, Train_Acc:87.18%
Epoch [89/300], Step [180/391],                 Loss: 0.38301, Train_Acc:87.23%
Epoch [89/300], Step [190/391],                 Loss: 0.38354, Train_Acc:87.18%
Epoch [89/300], Step [200/391],                 Loss: 0.38436, Train_Acc:87.16%
Epoch [89/300], Step [210/391],                 Loss: 0.38441, Train_Acc:87.09%
Epoch [89/300], Step [220/391],                 Loss: 0.38412, Train_Acc:87.11%
Epoch [89/300], Step [230/391],                 Loss: 0.38278, Train_Acc:87.15%
Epoch [89/300], Step [240/391],                 Loss: 0.37979, Train_Acc:87.29%
Epoch [89/300], Step [250/391],                 Loss: 0.37975, Train_Acc:87.26%
Epoch [89/300], Step [260/391],                 Loss: 0.38081, Train_Acc:87.24%
Epoch [89/300], Step [270/391],                 Loss: 0.38293, Train_Acc:87.16%
Epoch [89/300], Step [280/391],                 Loss: 0.38252, Train_Acc:87.17%
Epoch [89/300], Step [290/391],                 Loss: 0.38188, Train_Acc:87.19%
Epoch [89/300], Step [300/391],                 Loss: 0.38105, Train_Acc:87.21%
Epoch [89/300], Step [310/391],                 Loss: 0.38012, Train_Acc:87.25%
Epoch [89/300], Step [320/391],                 Loss: 0.37960, Train_Acc:87.27%
Epoch [89/300], Step [330/391],                 Loss: 0.37825, Train_Acc:87.32%
Epoch [89/300], Step [340/391],                 Loss: 0.37742, Train_Acc:87.37%
Epoch [89/300], Step [350/391],                 Loss: 0.37636, Train_Acc:87.42%
Epoch [89/300], Step [360/391],                 Loss: 0.37576, Train_Acc:87.40%
Epoch [89/300], Step [370/391],                 Loss: 0.37586, Train_Acc:87.39%
Epoch [89/300], Step [380/391],                 Loss: 0.37605, Train_Acc:87.37%
Epoch [89/300], Step [390/391],                 Loss: 0.37616, Train_Acc:87.39%
Accuary on test images:71.98%
Epoch [90/300], Step [10/391],                 Loss: 0.37606, Train_Acc:88.20%
Epoch [90/300], Step [20/391],                 Loss: 0.37040, Train_Acc:87.89%
Epoch [90/300], Step [30/391],                 Loss: 0.37008, Train_Acc:87.45%
Epoch [90/300], Step [40/391],                 Loss: 0.37958, Train_Acc:87.36%
Epoch [90/300], Step [50/391],                 Loss: 0.37141, Train_Acc:87.56%
Epoch [90/300], Step [60/391],                 Loss: 0.37198, Train_Acc:87.46%
Epoch [90/300], Step [70/391],                 Loss: 0.37195, Train_Acc:87.52%
Epoch [90/300], Step [80/391],                 Loss: 0.37260, Train_Acc:87.55%
Epoch [90/300], Step [90/391],                 Loss: 0.37434, Train_Acc:87.43%
Epoch [90/300], Step [100/391],                 Loss: 0.37065, Train_Acc:87.52%
Epoch [90/300], Step [110/391],                 Loss: 0.37476, Train_Acc:87.44%
Epoch [90/300], Step [120/391],                 Loss: 0.37493, Train_Acc:87.40%
Epoch [90/300], Step [130/391],                 Loss: 0.37630, Train_Acc:87.46%
Epoch [90/300], Step [140/391],                 Loss: 0.37485, Train_Acc:87.44%
Epoch [90/300], Step [150/391],                 Loss: 0.37573, Train_Acc:87.43%
Epoch [90/300], Step [160/391],                 Loss: 0.37408, Train_Acc:87.43%
Epoch [90/300], Step [170/391],                 Loss: 0.37583, Train_Acc:87.40%
Epoch [90/300], Step [180/391],                 Loss: 0.37677, Train_Acc:87.33%
Epoch [90/300], Step [190/391],                 Loss: 0.37679, Train_Acc:87.29%
Epoch [90/300], Step [200/391],                 Loss: 0.37782, Train_Acc:87.25%
Epoch [90/300], Step [210/391],                 Loss: 0.37851, Train_Acc:87.23%
Epoch [90/300], Step [220/391],                 Loss: 0.38001, Train_Acc:87.12%
Epoch [90/300], Step [230/391],                 Loss: 0.37989, Train_Acc:87.14%
Epoch [90/300], Step [240/391],                 Loss: 0.37870, Train_Acc:87.18%
Epoch [90/300], Step [250/391],                 Loss: 0.37837, Train_Acc:87.22%
Epoch [90/300], Step [260/391],                 Loss: 0.37906, Train_Acc:87.24%
Epoch [90/300], Step [270/391],                 Loss: 0.37931, Train_Acc:87.20%
Epoch [90/300], Step [280/391],                 Loss: 0.37884, Train_Acc:87.22%
Epoch [90/300], Step [290/391],                 Loss: 0.37813, Train_Acc:87.24%
Epoch [90/300], Step [300/391],                 Loss: 0.37733, Train_Acc:87.27%
Epoch [90/300], Step [310/391],                 Loss: 0.37576, Train_Acc:87.32%
Epoch [90/300], Step [320/391],                 Loss: 0.37541, Train_Acc:87.33%
Epoch [90/300], Step [330/391],                 Loss: 0.37429, Train_Acc:87.39%
Epoch [90/300], Step [340/391],                 Loss: 0.37358, Train_Acc:87.42%
Epoch [90/300], Step [350/391],                 Loss: 0.37269, Train_Acc:87.46%
Epoch [90/300], Step [360/391],                 Loss: 0.37329, Train_Acc:87.45%
Epoch [90/300], Step [370/391],                 Loss: 0.37321, Train_Acc:87.45%
Epoch [90/300], Step [380/391],                 Loss: 0.37296, Train_Acc:87.44%
Epoch [90/300], Step [390/391],                 Loss: 0.37237, Train_Acc:87.46%
Accuary on test images:75.26%
Epoch [91/300], Step [10/391],                 Loss: 0.37450, Train_Acc:87.27%
Epoch [91/300], Step [20/391],                 Loss: 0.36243, Train_Acc:87.62%
Epoch [91/300], Step [30/391],                 Loss: 0.35699, Train_Acc:87.42%
Epoch [91/300], Step [40/391],                 Loss: 0.37070, Train_Acc:87.03%
Epoch [91/300], Step [50/391],                 Loss: 0.37518, Train_Acc:86.92%
Epoch [91/300], Step [60/391],                 Loss: 0.37837, Train_Acc:86.69%
Epoch [91/300], Step [70/391],                 Loss: 0.37458, Train_Acc:86.99%
Epoch [91/300], Step [80/391],                 Loss: 0.37352, Train_Acc:87.09%
Epoch [91/300], Step [90/391],                 Loss: 0.37517, Train_Acc:87.08%
Epoch [91/300], Step [100/391],                 Loss: 0.37444, Train_Acc:87.21%
Epoch [91/300], Step [110/391],                 Loss: 0.37567, Train_Acc:87.16%
Epoch [91/300], Step [120/391],                 Loss: 0.37485, Train_Acc:87.17%
Epoch [91/300], Step [130/391],                 Loss: 0.37744, Train_Acc:87.10%
Epoch [91/300], Step [140/391],                 Loss: 0.37768, Train_Acc:87.14%
Epoch [91/300], Step [150/391],                 Loss: 0.37914, Train_Acc:87.07%
Epoch [91/300], Step [160/391],                 Loss: 0.37830, Train_Acc:87.13%
Epoch [91/300], Step [170/391],                 Loss: 0.37843, Train_Acc:87.17%
Epoch [91/300], Step [180/391],                 Loss: 0.37939, Train_Acc:87.10%
Epoch [91/300], Step [190/391],                 Loss: 0.38038, Train_Acc:87.09%
Epoch [91/300], Step [200/391],                 Loss: 0.38229, Train_Acc:87.00%
Epoch [91/300], Step [210/391],                 Loss: 0.38097, Train_Acc:87.08%
Epoch [91/300], Step [220/391],                 Loss: 0.38114, Train_Acc:87.08%
Epoch [91/300], Step [230/391],                 Loss: 0.37895, Train_Acc:87.17%
Epoch [91/300], Step [240/391],                 Loss: 0.37697, Train_Acc:87.26%
Epoch [91/300], Step [250/391],                 Loss: 0.37622, Train_Acc:87.27%
Epoch [91/300], Step [260/391],                 Loss: 0.37734, Train_Acc:87.25%
Epoch [91/300], Step [270/391],                 Loss: 0.37972, Train_Acc:87.19%
Epoch [91/300], Step [280/391],                 Loss: 0.38007, Train_Acc:87.18%
Epoch [91/300], Step [290/391],                 Loss: 0.37996, Train_Acc:87.22%
Epoch [91/300], Step [300/391],                 Loss: 0.37964, Train_Acc:87.21%
Epoch [91/300], Step [310/391],                 Loss: 0.37946, Train_Acc:87.22%
Epoch [91/300], Step [320/391],                 Loss: 0.37978, Train_Acc:87.20%
Epoch [91/300], Step [330/391],                 Loss: 0.37861, Train_Acc:87.24%
Epoch [91/300], Step [340/391],                 Loss: 0.37748, Train_Acc:87.26%
Epoch [91/300], Step [350/391],                 Loss: 0.37699, Train_Acc:87.27%
Epoch [91/300], Step [360/391],                 Loss: 0.37699, Train_Acc:87.28%
Epoch [91/300], Step [370/391],                 Loss: 0.37809, Train_Acc:87.22%
Epoch [91/300], Step [380/391],                 Loss: 0.37677, Train_Acc:87.27%
Epoch [91/300], Step [390/391],                 Loss: 0.37648, Train_Acc:87.28%
Accuary on test images:76.34%
Epoch [92/300], Step [10/391],                 Loss: 0.37840, Train_Acc:87.50%
Epoch [92/300], Step [20/391],                 Loss: 0.36695, Train_Acc:87.93%
Epoch [92/300], Step [30/391],                 Loss: 0.36115, Train_Acc:88.23%
Epoch [92/300], Step [40/391],                 Loss: 0.36256, Train_Acc:88.03%
Epoch [92/300], Step [50/391],                 Loss: 0.36708, Train_Acc:87.64%
Epoch [92/300], Step [60/391],                 Loss: 0.37792, Train_Acc:87.08%
Epoch [92/300], Step [70/391],                 Loss: 0.38028, Train_Acc:87.19%
Epoch [92/300], Step [80/391],                 Loss: 0.38184, Train_Acc:87.23%
Epoch [92/300], Step [90/391],                 Loss: 0.38254, Train_Acc:87.17%
Epoch [92/300], Step [100/391],                 Loss: 0.38120, Train_Acc:87.20%
Epoch [92/300], Step [110/391],                 Loss: 0.38367, Train_Acc:87.12%
Epoch [92/300], Step [120/391],                 Loss: 0.38136, Train_Acc:87.15%
Epoch [92/300], Step [130/391],                 Loss: 0.38121, Train_Acc:87.16%
Epoch [92/300], Step [140/391],                 Loss: 0.37742, Train_Acc:87.28%
Epoch [92/300], Step [150/391],                 Loss: 0.37741, Train_Acc:87.30%
Epoch [92/300], Step [160/391],                 Loss: 0.37639, Train_Acc:87.28%
Epoch [92/300], Step [170/391],                 Loss: 0.37841, Train_Acc:87.13%
Epoch [92/300], Step [180/391],                 Loss: 0.37947, Train_Acc:87.04%
Epoch [92/300], Step [190/391],                 Loss: 0.38105, Train_Acc:87.01%
Epoch [92/300], Step [200/391],                 Loss: 0.38330, Train_Acc:86.97%
Epoch [92/300], Step [210/391],                 Loss: 0.38347, Train_Acc:86.95%
Epoch [92/300], Step [220/391],                 Loss: 0.38447, Train_Acc:86.95%
Epoch [92/300], Step [230/391],                 Loss: 0.38273, Train_Acc:87.00%
Epoch [92/300], Step [240/391],                 Loss: 0.38107, Train_Acc:87.08%
Epoch [92/300], Step [250/391],                 Loss: 0.38005, Train_Acc:87.10%
Epoch [92/300], Step [260/391],                 Loss: 0.38094, Train_Acc:87.09%
Epoch [92/300], Step [270/391],                 Loss: 0.38052, Train_Acc:87.10%
Epoch [92/300], Step [280/391],                 Loss: 0.37974, Train_Acc:87.11%
Epoch [92/300], Step [290/391],                 Loss: 0.37966, Train_Acc:87.13%
Epoch [92/300], Step [300/391],                 Loss: 0.37926, Train_Acc:87.14%
Epoch [92/300], Step [310/391],                 Loss: 0.37790, Train_Acc:87.18%
Epoch [92/300], Step [320/391],                 Loss: 0.37861, Train_Acc:87.16%
Epoch [92/300], Step [330/391],                 Loss: 0.37784, Train_Acc:87.20%
Epoch [92/300], Step [340/391],                 Loss: 0.37774, Train_Acc:87.18%
Epoch [92/300], Step [350/391],                 Loss: 0.37645, Train_Acc:87.23%
Epoch [92/300], Step [360/391],                 Loss: 0.37626, Train_Acc:87.22%
Epoch [92/300], Step [370/391],                 Loss: 0.37645, Train_Acc:87.20%
Epoch [92/300], Step [380/391],                 Loss: 0.37629, Train_Acc:87.20%
Epoch [92/300], Step [390/391],                 Loss: 0.37622, Train_Acc:87.20%
Accuary on test images:70.62%
Epoch [93/300], Step [10/391],                 Loss: 0.39223, Train_Acc:86.17%
Epoch [93/300], Step [20/391],                 Loss: 0.38349, Train_Acc:86.80%
Epoch [93/300], Step [30/391],                 Loss: 0.38044, Train_Acc:86.80%
Epoch [93/300], Step [40/391],                 Loss: 0.38113, Train_Acc:87.09%
Epoch [93/300], Step [50/391],                 Loss: 0.38377, Train_Acc:87.08%
Epoch [93/300], Step [60/391],                 Loss: 0.38763, Train_Acc:86.80%
Epoch [93/300], Step [70/391],                 Loss: 0.38711, Train_Acc:86.93%
Epoch [93/300], Step [80/391],                 Loss: 0.38988, Train_Acc:86.80%
Epoch [93/300], Step [90/391],                 Loss: 0.39085, Train_Acc:86.71%
Epoch [93/300], Step [100/391],                 Loss: 0.38921, Train_Acc:86.69%
Epoch [93/300], Step [110/391],                 Loss: 0.39000, Train_Acc:86.66%
Epoch [93/300], Step [120/391],                 Loss: 0.38796, Train_Acc:86.76%
Epoch [93/300], Step [130/391],                 Loss: 0.38823, Train_Acc:86.78%
Epoch [93/300], Step [140/391],                 Loss: 0.38630, Train_Acc:86.87%
Epoch [93/300], Step [150/391],                 Loss: 0.38740, Train_Acc:86.82%
Epoch [93/300], Step [160/391],                 Loss: 0.38682, Train_Acc:86.87%
Epoch [93/300], Step [170/391],                 Loss: 0.38465, Train_Acc:86.95%
Epoch [93/300], Step [180/391],                 Loss: 0.38405, Train_Acc:86.94%
Epoch [93/300], Step [190/391],                 Loss: 0.38273, Train_Acc:86.92%
Epoch [93/300], Step [200/391],                 Loss: 0.38366, Train_Acc:86.91%
Epoch [93/300], Step [210/391],                 Loss: 0.38429, Train_Acc:86.93%
Epoch [93/300], Step [220/391],                 Loss: 0.38519, Train_Acc:86.91%
Epoch [93/300], Step [230/391],                 Loss: 0.38538, Train_Acc:86.90%
Epoch [93/300], Step [240/391],                 Loss: 0.38466, Train_Acc:86.90%
Epoch [93/300], Step [250/391],                 Loss: 0.38365, Train_Acc:86.95%
Epoch [93/300], Step [260/391],                 Loss: 0.38464, Train_Acc:86.94%
Epoch [93/300], Step [270/391],                 Loss: 0.38667, Train_Acc:86.92%
Epoch [93/300], Step [280/391],                 Loss: 0.38551, Train_Acc:86.97%
Epoch [93/300], Step [290/391],                 Loss: 0.38496, Train_Acc:87.00%
Epoch [93/300], Step [300/391],                 Loss: 0.38408, Train_Acc:87.04%
Epoch [93/300], Step [310/391],                 Loss: 0.38110, Train_Acc:87.15%
Epoch [93/300], Step [320/391],                 Loss: 0.37994, Train_Acc:87.19%
Epoch [93/300], Step [330/391],                 Loss: 0.37872, Train_Acc:87.23%
Epoch [93/300], Step [340/391],                 Loss: 0.37814, Train_Acc:87.24%
Epoch [93/300], Step [350/391],                 Loss: 0.37764, Train_Acc:87.27%
Epoch [93/300], Step [360/391],                 Loss: 0.37833, Train_Acc:87.22%
Epoch [93/300], Step [370/391],                 Loss: 0.37909, Train_Acc:87.17%
Epoch [93/300], Step [380/391],                 Loss: 0.37895, Train_Acc:87.18%
Epoch [93/300], Step [390/391],                 Loss: 0.37847, Train_Acc:87.19%
Accuary on test images:74.70%
Epoch [94/300], Step [10/391],                 Loss: 0.35953, Train_Acc:88.05%
Epoch [94/300], Step [20/391],                 Loss: 0.35241, Train_Acc:88.12%
Epoch [94/300], Step [30/391],                 Loss: 0.34460, Train_Acc:88.52%
Epoch [94/300], Step [40/391],                 Loss: 0.34961, Train_Acc:88.40%
Epoch [94/300], Step [50/391],                 Loss: 0.35975, Train_Acc:87.86%
Epoch [94/300], Step [60/391],                 Loss: 0.36990, Train_Acc:87.45%
Epoch [94/300], Step [70/391],                 Loss: 0.36853, Train_Acc:87.50%
Epoch [94/300], Step [80/391],                 Loss: 0.36480, Train_Acc:87.64%
Epoch [94/300], Step [90/391],                 Loss: 0.36829, Train_Acc:87.50%
Epoch [94/300], Step [100/391],                 Loss: 0.36635, Train_Acc:87.55%
Epoch [94/300], Step [110/391],                 Loss: 0.36893, Train_Acc:87.41%
Epoch [94/300], Step [120/391],                 Loss: 0.37130, Train_Acc:87.30%
Epoch [94/300], Step [130/391],                 Loss: 0.37280, Train_Acc:87.28%
Epoch [94/300], Step [140/391],                 Loss: 0.37199, Train_Acc:87.32%
Epoch [94/300], Step [150/391],                 Loss: 0.37181, Train_Acc:87.36%
Epoch [94/300], Step [160/391],                 Loss: 0.37206, Train_Acc:87.32%
Epoch [94/300], Step [170/391],                 Loss: 0.37221, Train_Acc:87.30%
Epoch [94/300], Step [180/391],                 Loss: 0.37107, Train_Acc:87.33%
Epoch [94/300], Step [190/391],                 Loss: 0.37249, Train_Acc:87.31%
Epoch [94/300], Step [200/391],                 Loss: 0.37245, Train_Acc:87.30%
Epoch [94/300], Step [210/391],                 Loss: 0.37412, Train_Acc:87.27%
Epoch [94/300], Step [220/391],                 Loss: 0.37459, Train_Acc:87.23%
Epoch [94/300], Step [230/391],                 Loss: 0.37344, Train_Acc:87.24%
Epoch [94/300], Step [240/391],                 Loss: 0.37242, Train_Acc:87.26%
Epoch [94/300], Step [250/391],                 Loss: 0.37134, Train_Acc:87.29%
Epoch [94/300], Step [260/391],                 Loss: 0.37313, Train_Acc:87.24%
Epoch [94/300], Step [270/391],                 Loss: 0.37407, Train_Acc:87.21%
Epoch [94/300], Step [280/391],                 Loss: 0.37424, Train_Acc:87.17%
Epoch [94/300], Step [290/391],                 Loss: 0.37451, Train_Acc:87.16%
Epoch [94/300], Step [300/391],                 Loss: 0.37339, Train_Acc:87.20%
Epoch [94/300], Step [310/391],                 Loss: 0.37273, Train_Acc:87.25%
Epoch [94/300], Step [320/391],                 Loss: 0.37316, Train_Acc:87.24%
Epoch [94/300], Step [330/391],                 Loss: 0.37301, Train_Acc:87.24%
Epoch [94/300], Step [340/391],                 Loss: 0.37228, Train_Acc:87.25%
Epoch [94/300], Step [350/391],                 Loss: 0.37251, Train_Acc:87.25%
Epoch [94/300], Step [360/391],                 Loss: 0.37153, Train_Acc:87.29%
Epoch [94/300], Step [370/391],                 Loss: 0.37158, Train_Acc:87.28%
Epoch [94/300], Step [380/391],                 Loss: 0.37136, Train_Acc:87.31%
Epoch [94/300], Step [390/391],                 Loss: 0.37079, Train_Acc:87.32%
Accuary on test images:72.20%
Epoch [95/300], Step [10/391],                 Loss: 0.40475, Train_Acc:86.48%
Epoch [95/300], Step [20/391],                 Loss: 0.39218, Train_Acc:86.68%
Epoch [95/300], Step [30/391],                 Loss: 0.39471, Train_Acc:86.88%
Epoch [95/300], Step [40/391],                 Loss: 0.38357, Train_Acc:87.23%
Epoch [95/300], Step [50/391],                 Loss: 0.37978, Train_Acc:87.28%
Epoch [95/300], Step [60/391],                 Loss: 0.37664, Train_Acc:87.37%
Epoch [95/300], Step [70/391],                 Loss: 0.37253, Train_Acc:87.52%
Epoch [95/300], Step [80/391],                 Loss: 0.37194, Train_Acc:87.52%
Epoch [95/300], Step [90/391],                 Loss: 0.37869, Train_Acc:87.25%
Epoch [95/300], Step [100/391],                 Loss: 0.38076, Train_Acc:87.16%
Epoch [95/300], Step [110/391],                 Loss: 0.38559, Train_Acc:86.94%
Epoch [95/300], Step [120/391],                 Loss: 0.38507, Train_Acc:86.97%
Epoch [95/300], Step [130/391],                 Loss: 0.38934, Train_Acc:86.81%
Epoch [95/300], Step [140/391],                 Loss: 0.38673, Train_Acc:86.89%
Epoch [95/300], Step [150/391],                 Loss: 0.38796, Train_Acc:86.84%
Epoch [95/300], Step [160/391],                 Loss: 0.38617, Train_Acc:86.91%
Epoch [95/300], Step [170/391],                 Loss: 0.38434, Train_Acc:86.94%
Epoch [95/300], Step [180/391],                 Loss: 0.38344, Train_Acc:86.95%
Epoch [95/300], Step [190/391],                 Loss: 0.38148, Train_Acc:86.95%
Epoch [95/300], Step [200/391],                 Loss: 0.38195, Train_Acc:86.98%
Epoch [95/300], Step [210/391],                 Loss: 0.38202, Train_Acc:86.95%
Epoch [95/300], Step [220/391],                 Loss: 0.38260, Train_Acc:86.95%
Epoch [95/300], Step [230/391],                 Loss: 0.38031, Train_Acc:87.06%
Epoch [95/300], Step [240/391],                 Loss: 0.37754, Train_Acc:87.12%
Epoch [95/300], Step [250/391],                 Loss: 0.37724, Train_Acc:87.11%
Epoch [95/300], Step [260/391],                 Loss: 0.37918, Train_Acc:87.10%
Epoch [95/300], Step [270/391],                 Loss: 0.38135, Train_Acc:86.99%
Epoch [95/300], Step [280/391],                 Loss: 0.38260, Train_Acc:86.96%
Epoch [95/300], Step [290/391],                 Loss: 0.38375, Train_Acc:86.93%
Epoch [95/300], Step [300/391],                 Loss: 0.38329, Train_Acc:86.93%
Epoch [95/300], Step [310/391],                 Loss: 0.38204, Train_Acc:87.00%
Epoch [95/300], Step [320/391],                 Loss: 0.38177, Train_Acc:87.00%
Epoch [95/300], Step [330/391],                 Loss: 0.38088, Train_Acc:87.05%
Epoch [95/300], Step [340/391],                 Loss: 0.38013, Train_Acc:87.08%
Epoch [95/300], Step [350/391],                 Loss: 0.37880, Train_Acc:87.15%
Epoch [95/300], Step [360/391],                 Loss: 0.37906, Train_Acc:87.14%
Epoch [95/300], Step [370/391],                 Loss: 0.37961, Train_Acc:87.10%
Epoch [95/300], Step [380/391],                 Loss: 0.37847, Train_Acc:87.16%
Epoch [95/300], Step [390/391],                 Loss: 0.37777, Train_Acc:87.18%
Accuary on test images:78.62%
Epoch [96/300], Step [10/391],                 Loss: 0.36914, Train_Acc:87.27%
Epoch [96/300], Step [20/391],                 Loss: 0.36079, Train_Acc:87.93%
Epoch [96/300], Step [30/391],                 Loss: 0.34999, Train_Acc:88.05%
Epoch [96/300], Step [40/391],                 Loss: 0.35453, Train_Acc:88.01%
Epoch [96/300], Step [50/391],                 Loss: 0.35963, Train_Acc:87.83%
Epoch [96/300], Step [60/391],                 Loss: 0.37113, Train_Acc:87.46%
Epoch [96/300], Step [70/391],                 Loss: 0.37156, Train_Acc:87.50%
Epoch [96/300], Step [80/391],                 Loss: 0.37100, Train_Acc:87.58%
Epoch [96/300], Step [90/391],                 Loss: 0.37301, Train_Acc:87.48%
Epoch [96/300], Step [100/391],                 Loss: 0.37126, Train_Acc:87.52%
Epoch [96/300], Step [110/391],                 Loss: 0.37597, Train_Acc:87.37%
Epoch [96/300], Step [120/391],                 Loss: 0.37646, Train_Acc:87.29%
Epoch [96/300], Step [130/391],                 Loss: 0.37850, Train_Acc:87.26%
Epoch [96/300], Step [140/391],                 Loss: 0.37854, Train_Acc:87.17%
Epoch [96/300], Step [150/391],                 Loss: 0.37795, Train_Acc:87.23%
Epoch [96/300], Step [160/391],                 Loss: 0.37713, Train_Acc:87.25%
Epoch [96/300], Step [170/391],                 Loss: 0.37636, Train_Acc:87.26%
Epoch [96/300], Step [180/391],                 Loss: 0.37632, Train_Acc:87.26%
Epoch [96/300], Step [190/391],                 Loss: 0.37701, Train_Acc:87.25%
Epoch [96/300], Step [200/391],                 Loss: 0.37842, Train_Acc:87.24%
Epoch [96/300], Step [210/391],                 Loss: 0.37920, Train_Acc:87.21%
Epoch [96/300], Step [220/391],                 Loss: 0.37888, Train_Acc:87.25%
Epoch [96/300], Step [230/391],                 Loss: 0.37700, Train_Acc:87.27%
Epoch [96/300], Step [240/391],                 Loss: 0.37639, Train_Acc:87.26%
Epoch [96/300], Step [250/391],                 Loss: 0.37678, Train_Acc:87.23%
Epoch [96/300], Step [260/391],                 Loss: 0.37784, Train_Acc:87.21%
Epoch [96/300], Step [270/391],                 Loss: 0.38012, Train_Acc:87.10%
Epoch [96/300], Step [280/391],                 Loss: 0.38058, Train_Acc:87.06%
Epoch [96/300], Step [290/391],                 Loss: 0.38034, Train_Acc:87.06%
Epoch [96/300], Step [300/391],                 Loss: 0.37978, Train_Acc:87.06%
Epoch [96/300], Step [310/391],                 Loss: 0.37810, Train_Acc:87.10%
Epoch [96/300], Step [320/391],                 Loss: 0.37698, Train_Acc:87.08%
Epoch [96/300], Step [330/391],                 Loss: 0.37582, Train_Acc:87.13%
Epoch [96/300], Step [340/391],                 Loss: 0.37504, Train_Acc:87.16%
Epoch [96/300], Step [350/391],                 Loss: 0.37419, Train_Acc:87.20%
Epoch [96/300], Step [360/391],                 Loss: 0.37412, Train_Acc:87.20%
Epoch [96/300], Step [370/391],                 Loss: 0.37411, Train_Acc:87.19%
Epoch [96/300], Step [380/391],                 Loss: 0.37422, Train_Acc:87.21%
Epoch [96/300], Step [390/391],                 Loss: 0.37350, Train_Acc:87.24%
Accuary on test images:79.58%
Epoch [97/300], Step [10/391],                 Loss: 0.35639, Train_Acc:88.75%
Epoch [97/300], Step [20/391],                 Loss: 0.34497, Train_Acc:88.28%
Epoch [97/300], Step [30/391],                 Loss: 0.35282, Train_Acc:87.81%
Epoch [97/300], Step [40/391],                 Loss: 0.35642, Train_Acc:87.99%
Epoch [97/300], Step [50/391],                 Loss: 0.35876, Train_Acc:87.86%
Epoch [97/300], Step [60/391],                 Loss: 0.36422, Train_Acc:87.73%
Epoch [97/300], Step [70/391],                 Loss: 0.36497, Train_Acc:87.66%
Epoch [97/300], Step [80/391],                 Loss: 0.36589, Train_Acc:87.64%
Epoch [97/300], Step [90/391],                 Loss: 0.36753, Train_Acc:87.62%
Epoch [97/300], Step [100/391],                 Loss: 0.36601, Train_Acc:87.75%
Epoch [97/300], Step [110/391],                 Loss: 0.36621, Train_Acc:87.76%
Epoch [97/300], Step [120/391],                 Loss: 0.36799, Train_Acc:87.58%
Epoch [97/300], Step [130/391],                 Loss: 0.36783, Train_Acc:87.61%
Epoch [97/300], Step [140/391],                 Loss: 0.36836, Train_Acc:87.57%
Epoch [97/300], Step [150/391],                 Loss: 0.37065, Train_Acc:87.42%
Epoch [97/300], Step [160/391],                 Loss: 0.37082, Train_Acc:87.46%
Epoch [97/300], Step [170/391],                 Loss: 0.37433, Train_Acc:87.33%
Epoch [97/300], Step [180/391],                 Loss: 0.37567, Train_Acc:87.30%
Epoch [97/300], Step [190/391],                 Loss: 0.37547, Train_Acc:87.32%
Epoch [97/300], Step [200/391],                 Loss: 0.37676, Train_Acc:87.25%
Epoch [97/300], Step [210/391],                 Loss: 0.37655, Train_Acc:87.21%
Epoch [97/300], Step [220/391],                 Loss: 0.37634, Train_Acc:87.25%
Epoch [97/300], Step [230/391],                 Loss: 0.37512, Train_Acc:87.25%
Epoch [97/300], Step [240/391],                 Loss: 0.37298, Train_Acc:87.34%
Epoch [97/300], Step [250/391],                 Loss: 0.37302, Train_Acc:87.33%
Epoch [97/300], Step [260/391],                 Loss: 0.37401, Train_Acc:87.30%
Epoch [97/300], Step [270/391],                 Loss: 0.37521, Train_Acc:87.26%
Epoch [97/300], Step [280/391],                 Loss: 0.37524, Train_Acc:87.25%
Epoch [97/300], Step [290/391],                 Loss: 0.37551, Train_Acc:87.24%
Epoch [97/300], Step [300/391],                 Loss: 0.37499, Train_Acc:87.21%
Epoch [97/300], Step [310/391],                 Loss: 0.37412, Train_Acc:87.24%
Epoch [97/300], Step [320/391],                 Loss: 0.37449, Train_Acc:87.20%
Epoch [97/300], Step [330/391],                 Loss: 0.37431, Train_Acc:87.21%
Epoch [97/300], Step [340/391],                 Loss: 0.37335, Train_Acc:87.22%
Epoch [97/300], Step [350/391],                 Loss: 0.37375, Train_Acc:87.20%
Epoch [97/300], Step [360/391],                 Loss: 0.37407, Train_Acc:87.19%
Epoch [97/300], Step [370/391],                 Loss: 0.37357, Train_Acc:87.22%
Epoch [97/300], Step [380/391],                 Loss: 0.37327, Train_Acc:87.22%
Epoch [97/300], Step [390/391],                 Loss: 0.37297, Train_Acc:87.22%
Accuary on test images:72.70%
Epoch [98/300], Step [10/391],                 Loss: 0.39257, Train_Acc:85.86%
Epoch [98/300], Step [20/391],                 Loss: 0.36660, Train_Acc:87.54%
Epoch [98/300], Step [30/391],                 Loss: 0.35151, Train_Acc:87.99%
Epoch [98/300], Step [40/391],                 Loss: 0.36063, Train_Acc:87.75%
Epoch [98/300], Step [50/391],                 Loss: 0.36746, Train_Acc:87.55%
Epoch [98/300], Step [60/391],                 Loss: 0.37896, Train_Acc:87.07%
Epoch [98/300], Step [70/391],                 Loss: 0.37840, Train_Acc:87.17%
Epoch [98/300], Step [80/391],                 Loss: 0.37804, Train_Acc:87.02%
Epoch [98/300], Step [90/391],                 Loss: 0.38261, Train_Acc:86.90%
Epoch [98/300], Step [100/391],                 Loss: 0.37676, Train_Acc:87.10%
Epoch [98/300], Step [110/391],                 Loss: 0.37769, Train_Acc:87.09%
Epoch [98/300], Step [120/391],                 Loss: 0.37918, Train_Acc:87.12%
Epoch [98/300], Step [130/391],                 Loss: 0.38164, Train_Acc:87.01%
Epoch [98/300], Step [140/391],                 Loss: 0.38086, Train_Acc:86.94%
Epoch [98/300], Step [150/391],                 Loss: 0.38204, Train_Acc:86.90%
Epoch [98/300], Step [160/391],                 Loss: 0.38073, Train_Acc:86.87%
Epoch [98/300], Step [170/391],                 Loss: 0.37841, Train_Acc:86.94%
Epoch [98/300], Step [180/391],                 Loss: 0.37673, Train_Acc:86.99%
Epoch [98/300], Step [190/391],                 Loss: 0.37620, Train_Acc:87.01%
Epoch [98/300], Step [200/391],                 Loss: 0.37734, Train_Acc:87.02%
Epoch [98/300], Step [210/391],                 Loss: 0.37605, Train_Acc:87.06%
Epoch [98/300], Step [220/391],                 Loss: 0.37686, Train_Acc:87.10%
Epoch [98/300], Step [230/391],                 Loss: 0.37632, Train_Acc:87.16%
Epoch [98/300], Step [240/391],                 Loss: 0.37425, Train_Acc:87.22%
Epoch [98/300], Step [250/391],                 Loss: 0.37361, Train_Acc:87.25%
Epoch [98/300], Step [260/391],                 Loss: 0.37528, Train_Acc:87.23%
Epoch [98/300], Step [270/391],                 Loss: 0.37665, Train_Acc:87.16%
Epoch [98/300], Step [280/391],                 Loss: 0.37639, Train_Acc:87.19%
Epoch [98/300], Step [290/391],                 Loss: 0.37643, Train_Acc:87.21%
Epoch [98/300], Step [300/391],                 Loss: 0.37524, Train_Acc:87.22%
Epoch [98/300], Step [310/391],                 Loss: 0.37403, Train_Acc:87.27%
Epoch [98/300], Step [320/391],                 Loss: 0.37460, Train_Acc:87.27%
Epoch [98/300], Step [330/391],                 Loss: 0.37329, Train_Acc:87.34%
Epoch [98/300], Step [340/391],                 Loss: 0.37301, Train_Acc:87.35%
Epoch [98/300], Step [350/391],                 Loss: 0.37256, Train_Acc:87.39%
Epoch [98/300], Step [360/391],                 Loss: 0.37278, Train_Acc:87.37%
Epoch [98/300], Step [370/391],                 Loss: 0.37359, Train_Acc:87.34%
Epoch [98/300], Step [380/391],                 Loss: 0.37363, Train_Acc:87.35%
Epoch [98/300], Step [390/391],                 Loss: 0.37356, Train_Acc:87.33%
Accuary on test images:74.94%
Epoch [99/300], Step [10/391],                 Loss: 0.36467, Train_Acc:87.73%
Epoch [99/300], Step [20/391],                 Loss: 0.36382, Train_Acc:87.66%
Epoch [99/300], Step [30/391],                 Loss: 0.35449, Train_Acc:88.02%
Epoch [99/300], Step [40/391],                 Loss: 0.35955, Train_Acc:87.93%
Epoch [99/300], Step [50/391],                 Loss: 0.36338, Train_Acc:87.81%
Epoch [99/300], Step [60/391],                 Loss: 0.36233, Train_Acc:87.81%
Epoch [99/300], Step [70/391],                 Loss: 0.36154, Train_Acc:87.82%
Epoch [99/300], Step [80/391],                 Loss: 0.36069, Train_Acc:87.85%
Epoch [99/300], Step [90/391],                 Loss: 0.36668, Train_Acc:87.61%
Epoch [99/300], Step [100/391],                 Loss: 0.36687, Train_Acc:87.45%
Epoch [99/300], Step [110/391],                 Loss: 0.37144, Train_Acc:87.39%
Epoch [99/300], Step [120/391],                 Loss: 0.37399, Train_Acc:87.29%
Epoch [99/300], Step [130/391],                 Loss: 0.37758, Train_Acc:87.14%
Epoch [99/300], Step [140/391],                 Loss: 0.38025, Train_Acc:87.08%
Epoch [99/300], Step [150/391],                 Loss: 0.38095, Train_Acc:87.08%
Epoch [99/300], Step [160/391],                 Loss: 0.37895, Train_Acc:87.18%
Epoch [99/300], Step [170/391],                 Loss: 0.37865, Train_Acc:87.18%
Epoch [99/300], Step [180/391],                 Loss: 0.37650, Train_Acc:87.25%
Epoch [99/300], Step [190/391],                 Loss: 0.37537, Train_Acc:87.31%
Epoch [99/300], Step [200/391],                 Loss: 0.37613, Train_Acc:87.27%
Epoch [99/300], Step [210/391],                 Loss: 0.37641, Train_Acc:87.24%
Epoch [99/300], Step [220/391],                 Loss: 0.37591, Train_Acc:87.30%
Epoch [99/300], Step [230/391],                 Loss: 0.37451, Train_Acc:87.32%
Epoch [99/300], Step [240/391],                 Loss: 0.37381, Train_Acc:87.34%
Epoch [99/300], Step [250/391],                 Loss: 0.37378, Train_Acc:87.32%
Epoch [99/300], Step [260/391],                 Loss: 0.37685, Train_Acc:87.22%
Epoch [99/300], Step [270/391],                 Loss: 0.37894, Train_Acc:87.16%
Epoch [99/300], Step [280/391],                 Loss: 0.37827, Train_Acc:87.17%
Epoch [99/300], Step [290/391],                 Loss: 0.37753, Train_Acc:87.21%
Epoch [99/300], Step [300/391],                 Loss: 0.37732, Train_Acc:87.21%
Epoch [99/300], Step [310/391],                 Loss: 0.37697, Train_Acc:87.22%
Epoch [99/300], Step [320/391],                 Loss: 0.37742, Train_Acc:87.19%
Epoch [99/300], Step [330/391],                 Loss: 0.37631, Train_Acc:87.23%
Epoch [99/300], Step [340/391],                 Loss: 0.37631, Train_Acc:87.24%
Epoch [99/300], Step [350/391],                 Loss: 0.37640, Train_Acc:87.23%
Epoch [99/300], Step [360/391],                 Loss: 0.37699, Train_Acc:87.21%
Epoch [99/300], Step [370/391],                 Loss: 0.37765, Train_Acc:87.19%
Epoch [99/300], Step [380/391],                 Loss: 0.37746, Train_Acc:87.22%
Epoch [99/300], Step [390/391],                 Loss: 0.37676, Train_Acc:87.26%
Accuary on test images:73.10%
Epoch [100/300], Step [10/391],                 Loss: 0.39061, Train_Acc:86.72%
Epoch [100/300], Step [20/391],                 Loss: 0.37753, Train_Acc:87.38%
Epoch [100/300], Step [30/391],                 Loss: 0.37230, Train_Acc:87.55%
Epoch [100/300], Step [40/391],                 Loss: 0.37079, Train_Acc:87.48%
Epoch [100/300], Step [50/391],                 Loss: 0.37316, Train_Acc:87.31%
Epoch [100/300], Step [60/391],                 Loss: 0.37695, Train_Acc:87.19%
Epoch [100/300], Step [70/391],                 Loss: 0.37513, Train_Acc:87.23%
Epoch [100/300], Step [80/391],                 Loss: 0.37446, Train_Acc:87.27%
Epoch [100/300], Step [90/391],                 Loss: 0.37643, Train_Acc:87.20%
Epoch [100/300], Step [100/391],                 Loss: 0.37516, Train_Acc:87.27%
Epoch [100/300], Step [110/391],                 Loss: 0.37857, Train_Acc:87.17%
Epoch [100/300], Step [120/391],                 Loss: 0.38043, Train_Acc:87.13%
Epoch [100/300], Step [130/391],                 Loss: 0.38150, Train_Acc:87.17%
Epoch [100/300], Step [140/391],                 Loss: 0.37899, Train_Acc:87.28%
Epoch [100/300], Step [150/391],                 Loss: 0.38084, Train_Acc:87.23%
Epoch [100/300], Step [160/391],                 Loss: 0.37890, Train_Acc:87.32%
Epoch [100/300], Step [170/391],                 Loss: 0.37761, Train_Acc:87.35%
Epoch [100/300], Step [180/391],                 Loss: 0.37547, Train_Acc:87.40%
Epoch [100/300], Step [190/391],                 Loss: 0.37619, Train_Acc:87.36%
Epoch [100/300], Step [200/391],                 Loss: 0.37709, Train_Acc:87.33%
Epoch [100/300], Step [210/391],                 Loss: 0.37666, Train_Acc:87.33%
Epoch [100/300], Step [220/391],                 Loss: 0.37624, Train_Acc:87.38%
Epoch [100/300], Step [230/391],                 Loss: 0.37548, Train_Acc:87.42%
Epoch [100/300], Step [240/391],                 Loss: 0.37375, Train_Acc:87.44%
Epoch [100/300], Step [250/391],                 Loss: 0.37304, Train_Acc:87.45%
Epoch [100/300], Step [260/391],                 Loss: 0.37360, Train_Acc:87.45%
Epoch [100/300], Step [270/391],                 Loss: 0.37390, Train_Acc:87.42%
Epoch [100/300], Step [280/391],                 Loss: 0.37333, Train_Acc:87.43%
Epoch [100/300], Step [290/391],                 Loss: 0.37302, Train_Acc:87.43%
Epoch [100/300], Step [300/391],                 Loss: 0.37322, Train_Acc:87.44%
Epoch [100/300], Step [310/391],                 Loss: 0.37315, Train_Acc:87.45%
Epoch [100/300], Step [320/391],                 Loss: 0.37298, Train_Acc:87.47%
Epoch [100/300], Step [330/391],                 Loss: 0.37200, Train_Acc:87.52%
Epoch [100/300], Step [340/391],                 Loss: 0.37136, Train_Acc:87.52%
Epoch [100/300], Step [350/391],                 Loss: 0.37070, Train_Acc:87.57%
Epoch [100/300], Step [360/391],                 Loss: 0.37082, Train_Acc:87.54%
Epoch [100/300], Step [370/391],                 Loss: 0.37262, Train_Acc:87.46%
Epoch [100/300], Step [380/391],                 Loss: 0.37320, Train_Acc:87.45%
Epoch [100/300], Step [390/391],                 Loss: 0.37362, Train_Acc:87.46%
Accuary on test images:75.18%
Epoch [101/300], Step [10/391],                 Loss: 0.38982, Train_Acc:86.56%
Epoch [101/300], Step [20/391],                 Loss: 0.37927, Train_Acc:86.60%
Epoch [101/300], Step [30/391],                 Loss: 0.36830, Train_Acc:86.98%
Epoch [101/300], Step [40/391],                 Loss: 0.37259, Train_Acc:87.09%
Epoch [101/300], Step [50/391],                 Loss: 0.37751, Train_Acc:86.98%
Epoch [101/300], Step [60/391],                 Loss: 0.37919, Train_Acc:86.97%
Epoch [101/300], Step [70/391],                 Loss: 0.37405, Train_Acc:87.20%
Epoch [101/300], Step [80/391],                 Loss: 0.37511, Train_Acc:87.28%
Epoch [101/300], Step [90/391],                 Loss: 0.37274, Train_Acc:87.46%
Epoch [101/300], Step [100/391],                 Loss: 0.36640, Train_Acc:87.69%
Epoch [101/300], Step [110/391],                 Loss: 0.36752, Train_Acc:87.66%
Epoch [101/300], Step [120/391],                 Loss: 0.36860, Train_Acc:87.60%
Epoch [101/300], Step [130/391],                 Loss: 0.37093, Train_Acc:87.52%
Epoch [101/300], Step [140/391],                 Loss: 0.36984, Train_Acc:87.53%
Epoch [101/300], Step [150/391],                 Loss: 0.37203, Train_Acc:87.49%
Epoch [101/300], Step [160/391],                 Loss: 0.37249, Train_Acc:87.48%
Epoch [101/300], Step [170/391],                 Loss: 0.37382, Train_Acc:87.51%
Epoch [101/300], Step [180/391],                 Loss: 0.37648, Train_Acc:87.32%
Epoch [101/300], Step [190/391],                 Loss: 0.38037, Train_Acc:87.17%
Epoch [101/300], Step [200/391],                 Loss: 0.38217, Train_Acc:87.12%
Epoch [101/300], Step [210/391],                 Loss: 0.38227, Train_Acc:87.11%
Epoch [101/300], Step [220/391],                 Loss: 0.38116, Train_Acc:87.17%
Epoch [101/300], Step [230/391],                 Loss: 0.37816, Train_Acc:87.25%
Epoch [101/300], Step [240/391],                 Loss: 0.37601, Train_Acc:87.30%
Epoch [101/300], Step [250/391],                 Loss: 0.37532, Train_Acc:87.28%
Epoch [101/300], Step [260/391],                 Loss: 0.37719, Train_Acc:87.24%
Epoch [101/300], Step [270/391],                 Loss: 0.37861, Train_Acc:87.20%
Epoch [101/300], Step [280/391],                 Loss: 0.37830, Train_Acc:87.20%
Epoch [101/300], Step [290/391],                 Loss: 0.37812, Train_Acc:87.19%
Epoch [101/300], Step [300/391],                 Loss: 0.37775, Train_Acc:87.21%
Epoch [101/300], Step [310/391],                 Loss: 0.37711, Train_Acc:87.27%
Epoch [101/300], Step [320/391],                 Loss: 0.37723, Train_Acc:87.24%
Epoch [101/300], Step [330/391],                 Loss: 0.37634, Train_Acc:87.29%
Epoch [101/300], Step [340/391],                 Loss: 0.37606, Train_Acc:87.28%
Epoch [101/300], Step [350/391],                 Loss: 0.37507, Train_Acc:87.34%
Epoch [101/300], Step [360/391],                 Loss: 0.37403, Train_Acc:87.37%
Epoch [101/300], Step [370/391],                 Loss: 0.37353, Train_Acc:87.40%
Epoch [101/300], Step [380/391],                 Loss: 0.37364, Train_Acc:87.38%
Epoch [101/300], Step [390/391],                 Loss: 0.37351, Train_Acc:87.39%
Accuary on test images:75.14%
Epoch [102/300], Step [10/391],                 Loss: 0.39469, Train_Acc:86.02%
Epoch [102/300], Step [20/391],                 Loss: 0.37961, Train_Acc:86.68%
Epoch [102/300], Step [30/391],                 Loss: 0.37177, Train_Acc:87.42%
Epoch [102/300], Step [40/391],                 Loss: 0.37314, Train_Acc:87.32%
Epoch [102/300], Step [50/391],                 Loss: 0.36759, Train_Acc:87.50%
Epoch [102/300], Step [60/391],                 Loss: 0.36874, Train_Acc:87.42%
Epoch [102/300], Step [70/391],                 Loss: 0.36810, Train_Acc:87.40%
Epoch [102/300], Step [80/391],                 Loss: 0.36773, Train_Acc:87.49%
Epoch [102/300], Step [90/391],                 Loss: 0.37097, Train_Acc:87.35%
Epoch [102/300], Step [100/391],                 Loss: 0.37757, Train_Acc:87.13%
Epoch [102/300], Step [110/391],                 Loss: 0.38113, Train_Acc:87.00%
Epoch [102/300], Step [120/391],                 Loss: 0.38309, Train_Acc:86.90%
Epoch [102/300], Step [130/391],                 Loss: 0.38278, Train_Acc:86.97%
Epoch [102/300], Step [140/391],                 Loss: 0.38067, Train_Acc:86.98%
Epoch [102/300], Step [150/391],                 Loss: 0.38173, Train_Acc:86.94%
Epoch [102/300], Step [160/391],                 Loss: 0.38089, Train_Acc:86.98%
Epoch [102/300], Step [170/391],                 Loss: 0.38169, Train_Acc:86.94%
Epoch [102/300], Step [180/391],                 Loss: 0.38156, Train_Acc:86.97%
Epoch [102/300], Step [190/391],                 Loss: 0.38133, Train_Acc:86.93%
Epoch [102/300], Step [200/391],                 Loss: 0.38253, Train_Acc:86.91%
Epoch [102/300], Step [210/391],                 Loss: 0.38373, Train_Acc:86.89%
Epoch [102/300], Step [220/391],                 Loss: 0.38374, Train_Acc:86.85%
Epoch [102/300], Step [230/391],                 Loss: 0.38130, Train_Acc:86.90%
Epoch [102/300], Step [240/391],                 Loss: 0.37861, Train_Acc:87.03%
Epoch [102/300], Step [250/391],                 Loss: 0.37694, Train_Acc:87.04%
Epoch [102/300], Step [260/391],                 Loss: 0.37833, Train_Acc:87.00%
Epoch [102/300], Step [270/391],                 Loss: 0.37981, Train_Acc:86.95%
Epoch [102/300], Step [280/391],                 Loss: 0.37931, Train_Acc:86.97%
Epoch [102/300], Step [290/391],                 Loss: 0.37817, Train_Acc:87.00%
Epoch [102/300], Step [300/391],                 Loss: 0.37743, Train_Acc:87.04%
Epoch [102/300], Step [310/391],                 Loss: 0.37710, Train_Acc:87.06%
Epoch [102/300], Step [320/391],                 Loss: 0.37854, Train_Acc:86.99%
Epoch [102/300], Step [330/391],                 Loss: 0.37873, Train_Acc:87.00%
Epoch [102/300], Step [340/391],                 Loss: 0.37819, Train_Acc:87.02%
Epoch [102/300], Step [350/391],                 Loss: 0.37775, Train_Acc:87.02%
Epoch [102/300], Step [360/391],                 Loss: 0.37765, Train_Acc:87.03%
Epoch [102/300], Step [370/391],                 Loss: 0.37811, Train_Acc:87.03%
Epoch [102/300], Step [380/391],                 Loss: 0.37819, Train_Acc:87.05%
Epoch [102/300], Step [390/391],                 Loss: 0.37789, Train_Acc:87.07%
Accuary on test images:73.94%
Epoch [103/300], Step [10/391],                 Loss: 0.41295, Train_Acc:85.62%
Epoch [103/300], Step [20/391],                 Loss: 0.38389, Train_Acc:86.99%
Epoch [103/300], Step [30/391],                 Loss: 0.36897, Train_Acc:87.73%
Epoch [103/300], Step [40/391],                 Loss: 0.37582, Train_Acc:87.56%
Epoch [103/300], Step [50/391],                 Loss: 0.37617, Train_Acc:87.47%
Epoch [103/300], Step [60/391],                 Loss: 0.38083, Train_Acc:87.20%
Epoch [103/300], Step [70/391],                 Loss: 0.37512, Train_Acc:87.39%
Epoch [103/300], Step [80/391],                 Loss: 0.37375, Train_Acc:87.40%
Epoch [103/300], Step [90/391],                 Loss: 0.37372, Train_Acc:87.47%
Epoch [103/300], Step [100/391],                 Loss: 0.37043, Train_Acc:87.63%
Epoch [103/300], Step [110/391],                 Loss: 0.37338, Train_Acc:87.41%
Epoch [103/300], Step [120/391],                 Loss: 0.37508, Train_Acc:87.34%
Epoch [103/300], Step [130/391],                 Loss: 0.37777, Train_Acc:87.31%
Epoch [103/300], Step [140/391],                 Loss: 0.37650, Train_Acc:87.32%
Epoch [103/300], Step [150/391],                 Loss: 0.37719, Train_Acc:87.28%
Epoch [103/300], Step [160/391],                 Loss: 0.37669, Train_Acc:87.28%
Epoch [103/300], Step [170/391],                 Loss: 0.37705, Train_Acc:87.28%
Epoch [103/300], Step [180/391],                 Loss: 0.37682, Train_Acc:87.30%
Epoch [103/300], Step [190/391],                 Loss: 0.37872, Train_Acc:87.25%
Epoch [103/300], Step [200/391],                 Loss: 0.38149, Train_Acc:87.15%
Epoch [103/300], Step [210/391],                 Loss: 0.38127, Train_Acc:87.17%
Epoch [103/300], Step [220/391],                 Loss: 0.38044, Train_Acc:87.20%
Epoch [103/300], Step [230/391],                 Loss: 0.37955, Train_Acc:87.21%
Epoch [103/300], Step [240/391],                 Loss: 0.37751, Train_Acc:87.29%
Epoch [103/300], Step [250/391],                 Loss: 0.37621, Train_Acc:87.33%
Epoch [103/300], Step [260/391],                 Loss: 0.37813, Train_Acc:87.25%
Epoch [103/300], Step [270/391],                 Loss: 0.37859, Train_Acc:87.26%
Epoch [103/300], Step [280/391],                 Loss: 0.37986, Train_Acc:87.22%
Epoch [103/300], Step [290/391],                 Loss: 0.37913, Train_Acc:87.25%
Epoch [103/300], Step [300/391],                 Loss: 0.37888, Train_Acc:87.24%
Epoch [103/300], Step [310/391],                 Loss: 0.37781, Train_Acc:87.29%
Epoch [103/300], Step [320/391],                 Loss: 0.37761, Train_Acc:87.27%
Epoch [103/300], Step [330/391],                 Loss: 0.37660, Train_Acc:87.28%
Epoch [103/300], Step [340/391],                 Loss: 0.37540, Train_Acc:87.33%
Epoch [103/300], Step [350/391],                 Loss: 0.37481, Train_Acc:87.35%
Epoch [103/300], Step [360/391],                 Loss: 0.37522, Train_Acc:87.31%
Epoch [103/300], Step [370/391],                 Loss: 0.37610, Train_Acc:87.26%
Epoch [103/300], Step [380/391],                 Loss: 0.37623, Train_Acc:87.26%
Epoch [103/300], Step [390/391],                 Loss: 0.37641, Train_Acc:87.23%
Accuary on test images:69.84%
Epoch [104/300], Step [10/391],                 Loss: 0.40985, Train_Acc:86.41%
Epoch [104/300], Step [20/391],                 Loss: 0.38262, Train_Acc:87.27%
Epoch [104/300], Step [30/391],                 Loss: 0.37243, Train_Acc:87.66%
Epoch [104/300], Step [40/391],                 Loss: 0.36373, Train_Acc:88.11%
Epoch [104/300], Step [50/391],                 Loss: 0.36691, Train_Acc:87.88%
Epoch [104/300], Step [60/391],                 Loss: 0.37568, Train_Acc:87.50%
Epoch [104/300], Step [70/391],                 Loss: 0.37557, Train_Acc:87.46%
Epoch [104/300], Step [80/391],                 Loss: 0.37663, Train_Acc:87.27%
Epoch [104/300], Step [90/391],                 Loss: 0.37605, Train_Acc:87.19%
Epoch [104/300], Step [100/391],                 Loss: 0.37491, Train_Acc:87.30%
Epoch [104/300], Step [110/391],                 Loss: 0.37473, Train_Acc:87.29%
Epoch [104/300], Step [120/391],                 Loss: 0.37364, Train_Acc:87.34%
Epoch [104/300], Step [130/391],                 Loss: 0.37511, Train_Acc:87.28%
Epoch [104/300], Step [140/391],                 Loss: 0.37413, Train_Acc:87.35%
Epoch [104/300], Step [150/391],                 Loss: 0.37393, Train_Acc:87.38%
Epoch [104/300], Step [160/391],                 Loss: 0.37299, Train_Acc:87.38%
Epoch [104/300], Step [170/391],                 Loss: 0.37289, Train_Acc:87.34%
Epoch [104/300], Step [180/391],                 Loss: 0.37362, Train_Acc:87.32%
Epoch [104/300], Step [190/391],                 Loss: 0.37381, Train_Acc:87.31%
Epoch [104/300], Step [200/391],                 Loss: 0.37640, Train_Acc:87.23%
Epoch [104/300], Step [210/391],                 Loss: 0.37796, Train_Acc:87.13%
Epoch [104/300], Step [220/391],                 Loss: 0.37800, Train_Acc:87.13%
Epoch [104/300], Step [230/391],                 Loss: 0.37578, Train_Acc:87.24%
Epoch [104/300], Step [240/391],                 Loss: 0.37343, Train_Acc:87.32%
Epoch [104/300], Step [250/391],                 Loss: 0.37240, Train_Acc:87.38%
Epoch [104/300], Step [260/391],                 Loss: 0.37616, Train_Acc:87.27%
Epoch [104/300], Step [270/391],                 Loss: 0.37920, Train_Acc:87.12%
Epoch [104/300], Step [280/391],                 Loss: 0.37981, Train_Acc:87.09%
Epoch [104/300], Step [290/391],                 Loss: 0.38000, Train_Acc:87.09%
Epoch [104/300], Step [300/391],                 Loss: 0.37941, Train_Acc:87.10%
Epoch [104/300], Step [310/391],                 Loss: 0.37851, Train_Acc:87.15%
Epoch [104/300], Step [320/391],                 Loss: 0.37785, Train_Acc:87.18%
Epoch [104/300], Step [330/391],                 Loss: 0.37722, Train_Acc:87.21%
Epoch [104/300], Step [340/391],                 Loss: 0.37616, Train_Acc:87.27%
Epoch [104/300], Step [350/391],                 Loss: 0.37567, Train_Acc:87.30%
Epoch [104/300], Step [360/391],                 Loss: 0.37515, Train_Acc:87.32%
Epoch [104/300], Step [370/391],                 Loss: 0.37586, Train_Acc:87.30%
Epoch [104/300], Step [380/391],                 Loss: 0.37531, Train_Acc:87.32%
Epoch [104/300], Step [390/391],                 Loss: 0.37503, Train_Acc:87.33%
Accuary on test images:74.74%
Epoch [105/300], Step [10/391],                 Loss: 0.39184, Train_Acc:86.02%
Epoch [105/300], Step [20/391],                 Loss: 0.37961, Train_Acc:86.88%
Epoch [105/300], Step [30/391],                 Loss: 0.36373, Train_Acc:87.47%
Epoch [105/300], Step [40/391],                 Loss: 0.35376, Train_Acc:87.81%
Epoch [105/300], Step [50/391],                 Loss: 0.35347, Train_Acc:87.67%
Epoch [105/300], Step [60/391],                 Loss: 0.35950, Train_Acc:87.66%
Epoch [105/300], Step [70/391],                 Loss: 0.36089, Train_Acc:87.50%
Epoch [105/300], Step [80/391],                 Loss: 0.36535, Train_Acc:87.30%
Epoch [105/300], Step [90/391],                 Loss: 0.37346, Train_Acc:87.18%
Epoch [105/300], Step [100/391],                 Loss: 0.37163, Train_Acc:87.30%
Epoch [105/300], Step [110/391],                 Loss: 0.37272, Train_Acc:87.24%
Epoch [105/300], Step [120/391],                 Loss: 0.37378, Train_Acc:87.19%
Epoch [105/300], Step [130/391],                 Loss: 0.37529, Train_Acc:87.17%
Epoch [105/300], Step [140/391],                 Loss: 0.37510, Train_Acc:87.23%
Epoch [105/300], Step [150/391],                 Loss: 0.37584, Train_Acc:87.22%
Epoch [105/300], Step [160/391],                 Loss: 0.37587, Train_Acc:87.18%
Epoch [105/300], Step [170/391],                 Loss: 0.37647, Train_Acc:87.13%
Epoch [105/300], Step [180/391],                 Loss: 0.37817, Train_Acc:87.10%
Epoch [105/300], Step [190/391],                 Loss: 0.37795, Train_Acc:87.12%
Epoch [105/300], Step [200/391],                 Loss: 0.37853, Train_Acc:87.11%
Epoch [105/300], Step [210/391],                 Loss: 0.37843, Train_Acc:87.11%
Epoch [105/300], Step [220/391],                 Loss: 0.37764, Train_Acc:87.20%
Epoch [105/300], Step [230/391],                 Loss: 0.37489, Train_Acc:87.32%
Epoch [105/300], Step [240/391],                 Loss: 0.37338, Train_Acc:87.38%
Epoch [105/300], Step [250/391],                 Loss: 0.37263, Train_Acc:87.40%
Epoch [105/300], Step [260/391],                 Loss: 0.37386, Train_Acc:87.39%
Epoch [105/300], Step [270/391],                 Loss: 0.37474, Train_Acc:87.35%
Epoch [105/300], Step [280/391],                 Loss: 0.37443, Train_Acc:87.32%
Epoch [105/300], Step [290/391],                 Loss: 0.37496, Train_Acc:87.32%
Epoch [105/300], Step [300/391],                 Loss: 0.37486, Train_Acc:87.29%
Epoch [105/300], Step [310/391],                 Loss: 0.37442, Train_Acc:87.29%
Epoch [105/300], Step [320/391],                 Loss: 0.37431, Train_Acc:87.28%
Epoch [105/300], Step [330/391],                 Loss: 0.37421, Train_Acc:87.30%
Epoch [105/300], Step [340/391],                 Loss: 0.37467, Train_Acc:87.29%
Epoch [105/300], Step [350/391],                 Loss: 0.37491, Train_Acc:87.29%
Epoch [105/300], Step [360/391],                 Loss: 0.37436, Train_Acc:87.30%
Epoch [105/300], Step [370/391],                 Loss: 0.37355, Train_Acc:87.30%
Epoch [105/300], Step [380/391],                 Loss: 0.37248, Train_Acc:87.35%
Epoch [105/300], Step [390/391],                 Loss: 0.37146, Train_Acc:87.40%
Accuary on test images:74.28%
Epoch [106/300], Step [10/391],                 Loss: 0.39419, Train_Acc:86.95%
Epoch [106/300], Step [20/391],                 Loss: 0.38114, Train_Acc:87.07%
Epoch [106/300], Step [30/391],                 Loss: 0.37617, Train_Acc:87.34%
Epoch [106/300], Step [40/391],                 Loss: 0.36720, Train_Acc:87.77%
Epoch [106/300], Step [50/391],                 Loss: 0.36142, Train_Acc:87.84%
Epoch [106/300], Step [60/391],                 Loss: 0.36117, Train_Acc:87.93%
Epoch [106/300], Step [70/391],                 Loss: 0.36234, Train_Acc:87.92%
Epoch [106/300], Step [80/391],                 Loss: 0.36607, Train_Acc:87.73%
Epoch [106/300], Step [90/391],                 Loss: 0.37235, Train_Acc:87.40%
Epoch [106/300], Step [100/391],                 Loss: 0.37115, Train_Acc:87.51%
Epoch [106/300], Step [110/391],                 Loss: 0.37304, Train_Acc:87.47%
Epoch [106/300], Step [120/391],                 Loss: 0.37345, Train_Acc:87.41%
Epoch [106/300], Step [130/391],                 Loss: 0.37636, Train_Acc:87.38%
Epoch [106/300], Step [140/391],                 Loss: 0.37629, Train_Acc:87.37%
Epoch [106/300], Step [150/391],                 Loss: 0.37878, Train_Acc:87.23%
Epoch [106/300], Step [160/391],                 Loss: 0.37699, Train_Acc:87.28%
Epoch [106/300], Step [170/391],                 Loss: 0.37579, Train_Acc:87.27%
Epoch [106/300], Step [180/391],                 Loss: 0.37603, Train_Acc:87.18%
Epoch [106/300], Step [190/391],                 Loss: 0.37627, Train_Acc:87.18%
Epoch [106/300], Step [200/391],                 Loss: 0.37761, Train_Acc:87.12%
Epoch [106/300], Step [210/391],                 Loss: 0.37813, Train_Acc:87.10%
Epoch [106/300], Step [220/391],                 Loss: 0.37860, Train_Acc:87.12%
Epoch [106/300], Step [230/391],                 Loss: 0.37814, Train_Acc:87.18%
Epoch [106/300], Step [240/391],                 Loss: 0.37680, Train_Acc:87.24%
Epoch [106/300], Step [250/391],                 Loss: 0.37517, Train_Acc:87.31%
Epoch [106/300], Step [260/391],                 Loss: 0.37699, Train_Acc:87.27%
Epoch [106/300], Step [270/391],                 Loss: 0.37773, Train_Acc:87.26%
Epoch [106/300], Step [280/391],                 Loss: 0.37750, Train_Acc:87.23%
Epoch [106/300], Step [290/391],                 Loss: 0.37882, Train_Acc:87.19%
Epoch [106/300], Step [300/391],                 Loss: 0.37878, Train_Acc:87.16%
Epoch [106/300], Step [310/391],                 Loss: 0.37860, Train_Acc:87.15%
Epoch [106/300], Step [320/391],                 Loss: 0.37869, Train_Acc:87.16%
Epoch [106/300], Step [330/391],                 Loss: 0.37784, Train_Acc:87.21%
Epoch [106/300], Step [340/391],                 Loss: 0.37725, Train_Acc:87.24%
Epoch [106/300], Step [350/391],                 Loss: 0.37688, Train_Acc:87.24%
Epoch [106/300], Step [360/391],                 Loss: 0.37678, Train_Acc:87.20%
Epoch [106/300], Step [370/391],                 Loss: 0.37741, Train_Acc:87.16%
Epoch [106/300], Step [380/391],                 Loss: 0.37686, Train_Acc:87.18%
Epoch [106/300], Step [390/391],                 Loss: 0.37629, Train_Acc:87.21%
Accuary on test images:74.58%
Epoch [107/300], Step [10/391],                 Loss: 0.37243, Train_Acc:87.03%
Epoch [107/300], Step [20/391],                 Loss: 0.36844, Train_Acc:87.27%
Epoch [107/300], Step [30/391],                 Loss: 0.35911, Train_Acc:87.68%
Epoch [107/300], Step [40/391],                 Loss: 0.36465, Train_Acc:87.68%
Epoch [107/300], Step [50/391],                 Loss: 0.36727, Train_Acc:87.56%
Epoch [107/300], Step [60/391],                 Loss: 0.37532, Train_Acc:87.32%
Epoch [107/300], Step [70/391],                 Loss: 0.37229, Train_Acc:87.43%
Epoch [107/300], Step [80/391],                 Loss: 0.37295, Train_Acc:87.37%
Epoch [107/300], Step [90/391],                 Loss: 0.37822, Train_Acc:87.24%
Epoch [107/300], Step [100/391],                 Loss: 0.37857, Train_Acc:87.15%
Epoch [107/300], Step [110/391],                 Loss: 0.37882, Train_Acc:87.16%
Epoch [107/300], Step [120/391],                 Loss: 0.37942, Train_Acc:87.17%
Epoch [107/300], Step [130/391],                 Loss: 0.37894, Train_Acc:87.21%
Epoch [107/300], Step [140/391],                 Loss: 0.37777, Train_Acc:87.24%
Epoch [107/300], Step [150/391],                 Loss: 0.38010, Train_Acc:87.16%
Epoch [107/300], Step [160/391],                 Loss: 0.38046, Train_Acc:87.16%
Epoch [107/300], Step [170/391],                 Loss: 0.38253, Train_Acc:87.10%
Epoch [107/300], Step [180/391],                 Loss: 0.38137, Train_Acc:87.14%
Epoch [107/300], Step [190/391],                 Loss: 0.38034, Train_Acc:87.20%
Epoch [107/300], Step [200/391],                 Loss: 0.38143, Train_Acc:87.13%
Epoch [107/300], Step [210/391],                 Loss: 0.38129, Train_Acc:87.12%
Epoch [107/300], Step [220/391],                 Loss: 0.38208, Train_Acc:87.08%
Epoch [107/300], Step [230/391],                 Loss: 0.38034, Train_Acc:87.15%
Epoch [107/300], Step [240/391],                 Loss: 0.37802, Train_Acc:87.20%
Epoch [107/300], Step [250/391],                 Loss: 0.37832, Train_Acc:87.21%
Epoch [107/300], Step [260/391],                 Loss: 0.37937, Train_Acc:87.17%
Epoch [107/300], Step [270/391],                 Loss: 0.38054, Train_Acc:87.15%
Epoch [107/300], Step [280/391],                 Loss: 0.38027, Train_Acc:87.13%
Epoch [107/300], Step [290/391],                 Loss: 0.38050, Train_Acc:87.11%
Epoch [107/300], Step [300/391],                 Loss: 0.38008, Train_Acc:87.08%
Epoch [107/300], Step [310/391],                 Loss: 0.37965, Train_Acc:87.09%
Epoch [107/300], Step [320/391],                 Loss: 0.37955, Train_Acc:87.11%
Epoch [107/300], Step [330/391],                 Loss: 0.37859, Train_Acc:87.13%
Epoch [107/300], Step [340/391],                 Loss: 0.37723, Train_Acc:87.18%
Epoch [107/300], Step [350/391],                 Loss: 0.37620, Train_Acc:87.22%
Epoch [107/300], Step [360/391],                 Loss: 0.37689, Train_Acc:87.18%
Epoch [107/300], Step [370/391],                 Loss: 0.37760, Train_Acc:87.16%
Epoch [107/300], Step [380/391],                 Loss: 0.37785, Train_Acc:87.19%
Epoch [107/300], Step [390/391],                 Loss: 0.37711, Train_Acc:87.21%
Accuary on test images:76.90%
Epoch [108/300], Step [10/391],                 Loss: 0.37428, Train_Acc:86.95%
Epoch [108/300], Step [20/391],                 Loss: 0.36382, Train_Acc:87.50%
Epoch [108/300], Step [30/391],                 Loss: 0.35556, Train_Acc:88.05%
Epoch [108/300], Step [40/391],                 Loss: 0.35424, Train_Acc:88.28%
Epoch [108/300], Step [50/391],                 Loss: 0.35659, Train_Acc:88.25%
Epoch [108/300], Step [60/391],                 Loss: 0.36235, Train_Acc:88.11%
Epoch [108/300], Step [70/391],                 Loss: 0.36166, Train_Acc:88.16%
Epoch [108/300], Step [80/391],                 Loss: 0.36305, Train_Acc:87.96%
Epoch [108/300], Step [90/391],                 Loss: 0.37267, Train_Acc:87.60%
Epoch [108/300], Step [100/391],                 Loss: 0.37119, Train_Acc:87.64%
Epoch [108/300], Step [110/391],                 Loss: 0.37394, Train_Acc:87.63%
Epoch [108/300], Step [120/391],                 Loss: 0.37283, Train_Acc:87.58%
Epoch [108/300], Step [130/391],                 Loss: 0.37508, Train_Acc:87.51%
Epoch [108/300], Step [140/391],                 Loss: 0.37440, Train_Acc:87.49%
Epoch [108/300], Step [150/391],                 Loss: 0.37403, Train_Acc:87.48%
Epoch [108/300], Step [160/391],                 Loss: 0.37411, Train_Acc:87.44%
Epoch [108/300], Step [170/391],                 Loss: 0.37427, Train_Acc:87.42%
Epoch [108/300], Step [180/391],                 Loss: 0.37444, Train_Acc:87.40%
Epoch [108/300], Step [190/391],                 Loss: 0.37330, Train_Acc:87.46%
Epoch [108/300], Step [200/391],                 Loss: 0.37375, Train_Acc:87.43%
Epoch [108/300], Step [210/391],                 Loss: 0.37350, Train_Acc:87.49%
Epoch [108/300], Step [220/391],                 Loss: 0.37357, Train_Acc:87.47%
Epoch [108/300], Step [230/391],                 Loss: 0.37253, Train_Acc:87.48%
Epoch [108/300], Step [240/391],                 Loss: 0.37249, Train_Acc:87.45%
Epoch [108/300], Step [250/391],                 Loss: 0.37303, Train_Acc:87.43%
Epoch [108/300], Step [260/391],                 Loss: 0.37396, Train_Acc:87.41%
Epoch [108/300], Step [270/391],                 Loss: 0.37550, Train_Acc:87.38%
Epoch [108/300], Step [280/391],                 Loss: 0.37593, Train_Acc:87.34%
Epoch [108/300], Step [290/391],                 Loss: 0.37585, Train_Acc:87.32%
Epoch [108/300], Step [300/391],                 Loss: 0.37644, Train_Acc:87.28%
Epoch [108/300], Step [310/391],                 Loss: 0.37605, Train_Acc:87.29%
Epoch [108/300], Step [320/391],                 Loss: 0.37634, Train_Acc:87.29%
Epoch [108/300], Step [330/391],                 Loss: 0.37570, Train_Acc:87.31%
Epoch [108/300], Step [340/391],                 Loss: 0.37474, Train_Acc:87.33%
Epoch [108/300], Step [350/391],                 Loss: 0.37505, Train_Acc:87.34%
Epoch [108/300], Step [360/391],                 Loss: 0.37552, Train_Acc:87.30%
Epoch [108/300], Step [370/391],                 Loss: 0.37577, Train_Acc:87.32%
Epoch [108/300], Step [380/391],                 Loss: 0.37554, Train_Acc:87.32%
Epoch [108/300], Step [390/391],                 Loss: 0.37489, Train_Acc:87.35%
Accuary on test images:80.46%
Epoch [109/300], Step [10/391],                 Loss: 0.35165, Train_Acc:87.73%
Epoch [109/300], Step [20/391],                 Loss: 0.35980, Train_Acc:88.05%
Epoch [109/300], Step [30/391],                 Loss: 0.35526, Train_Acc:88.26%
Epoch [109/300], Step [40/391],                 Loss: 0.35869, Train_Acc:88.26%
Epoch [109/300], Step [50/391],                 Loss: 0.36033, Train_Acc:88.16%
Epoch [109/300], Step [60/391],                 Loss: 0.37082, Train_Acc:87.70%
Epoch [109/300], Step [70/391],                 Loss: 0.37285, Train_Acc:87.67%
Epoch [109/300], Step [80/391],                 Loss: 0.37281, Train_Acc:87.75%
Epoch [109/300], Step [90/391],                 Loss: 0.37404, Train_Acc:87.62%
Epoch [109/300], Step [100/391],                 Loss: 0.37271, Train_Acc:87.59%
Epoch [109/300], Step [110/391],                 Loss: 0.37696, Train_Acc:87.46%
Epoch [109/300], Step [120/391],                 Loss: 0.37954, Train_Acc:87.29%
Epoch [109/300], Step [130/391],                 Loss: 0.38254, Train_Acc:87.24%
Epoch [109/300], Step [140/391],                 Loss: 0.38090, Train_Acc:87.23%
Epoch [109/300], Step [150/391],                 Loss: 0.37988, Train_Acc:87.27%
Epoch [109/300], Step [160/391],                 Loss: 0.37889, Train_Acc:87.29%
Epoch [109/300], Step [170/391],                 Loss: 0.37720, Train_Acc:87.28%
Epoch [109/300], Step [180/391],                 Loss: 0.37817, Train_Acc:87.29%
Epoch [109/300], Step [190/391],                 Loss: 0.37983, Train_Acc:87.21%
Epoch [109/300], Step [200/391],                 Loss: 0.38134, Train_Acc:87.15%
Epoch [109/300], Step [210/391],                 Loss: 0.38172, Train_Acc:87.15%
Epoch [109/300], Step [220/391],                 Loss: 0.38236, Train_Acc:87.13%
Epoch [109/300], Step [230/391],                 Loss: 0.37969, Train_Acc:87.19%
Epoch [109/300], Step [240/391],                 Loss: 0.37790, Train_Acc:87.27%
Epoch [109/300], Step [250/391],                 Loss: 0.37695, Train_Acc:87.30%
Epoch [109/300], Step [260/391],                 Loss: 0.37806, Train_Acc:87.26%
Epoch [109/300], Step [270/391],                 Loss: 0.37861, Train_Acc:87.22%
Epoch [109/300], Step [280/391],                 Loss: 0.37950, Train_Acc:87.20%
Epoch [109/300], Step [290/391],                 Loss: 0.37836, Train_Acc:87.21%
Epoch [109/300], Step [300/391],                 Loss: 0.37743, Train_Acc:87.24%
Epoch [109/300], Step [310/391],                 Loss: 0.37634, Train_Acc:87.29%
Epoch [109/300], Step [320/391],                 Loss: 0.37658, Train_Acc:87.29%
Epoch [109/300], Step [330/391],                 Loss: 0.37634, Train_Acc:87.31%
Epoch [109/300], Step [340/391],                 Loss: 0.37503, Train_Acc:87.34%
Epoch [109/300], Step [350/391],                 Loss: 0.37557, Train_Acc:87.32%
Epoch [109/300], Step [360/391],                 Loss: 0.37562, Train_Acc:87.33%
Epoch [109/300], Step [370/391],                 Loss: 0.37612, Train_Acc:87.31%
Epoch [109/300], Step [380/391],                 Loss: 0.37698, Train_Acc:87.27%
Epoch [109/300], Step [390/391],                 Loss: 0.37675, Train_Acc:87.27%
Accuary on test images:75.38%
Epoch [110/300], Step [10/391],                 Loss: 0.38460, Train_Acc:87.42%
Epoch [110/300], Step [20/391],                 Loss: 0.36375, Train_Acc:88.01%
Epoch [110/300], Step [30/391],                 Loss: 0.35975, Train_Acc:88.02%
Epoch [110/300], Step [40/391],                 Loss: 0.35497, Train_Acc:88.14%
Epoch [110/300], Step [50/391],                 Loss: 0.35651, Train_Acc:87.92%
Epoch [110/300], Step [60/391],                 Loss: 0.36550, Train_Acc:87.29%
Epoch [110/300], Step [70/391],                 Loss: 0.37019, Train_Acc:87.13%
Epoch [110/300], Step [80/391],                 Loss: 0.37308, Train_Acc:87.02%
Epoch [110/300], Step [90/391],                 Loss: 0.37825, Train_Acc:86.76%
Epoch [110/300], Step [100/391],                 Loss: 0.38165, Train_Acc:86.71%
Epoch [110/300], Step [110/391],                 Loss: 0.38747, Train_Acc:86.56%
Epoch [110/300], Step [120/391],                 Loss: 0.38635, Train_Acc:86.61%
Epoch [110/300], Step [130/391],                 Loss: 0.39001, Train_Acc:86.56%
Epoch [110/300], Step [140/391],                 Loss: 0.38888, Train_Acc:86.65%
Epoch [110/300], Step [150/391],                 Loss: 0.39165, Train_Acc:86.60%
Epoch [110/300], Step [160/391],                 Loss: 0.39149, Train_Acc:86.58%
Epoch [110/300], Step [170/391],                 Loss: 0.39065, Train_Acc:86.64%
Epoch [110/300], Step [180/391],                 Loss: 0.38866, Train_Acc:86.68%
Epoch [110/300], Step [190/391],                 Loss: 0.38743, Train_Acc:86.72%
Epoch [110/300], Step [200/391],                 Loss: 0.38654, Train_Acc:86.77%
Epoch [110/300], Step [210/391],                 Loss: 0.38639, Train_Acc:86.80%
Epoch [110/300], Step [220/391],                 Loss: 0.38561, Train_Acc:86.79%
Epoch [110/300], Step [230/391],                 Loss: 0.38431, Train_Acc:86.82%
Epoch [110/300], Step [240/391],                 Loss: 0.38252, Train_Acc:86.87%
Epoch [110/300], Step [250/391],                 Loss: 0.38171, Train_Acc:86.91%
Epoch [110/300], Step [260/391],                 Loss: 0.38239, Train_Acc:86.90%
Epoch [110/300], Step [270/391],                 Loss: 0.38331, Train_Acc:86.88%
Epoch [110/300], Step [280/391],                 Loss: 0.38333, Train_Acc:86.88%
Epoch [110/300], Step [290/391],                 Loss: 0.38298, Train_Acc:86.89%
Epoch [110/300], Step [300/391],                 Loss: 0.38303, Train_Acc:86.87%
Epoch [110/300], Step [310/391],                 Loss: 0.38205, Train_Acc:86.93%
Epoch [110/300], Step [320/391],                 Loss: 0.38131, Train_Acc:86.97%
Epoch [110/300], Step [330/391],                 Loss: 0.37965, Train_Acc:87.03%
Epoch [110/300], Step [340/391],                 Loss: 0.37880, Train_Acc:87.08%
Epoch [110/300], Step [350/391],                 Loss: 0.37800, Train_Acc:87.09%
Epoch [110/300], Step [360/391],                 Loss: 0.37941, Train_Acc:87.05%
Epoch [110/300], Step [370/391],                 Loss: 0.37949, Train_Acc:87.05%
Epoch [110/300], Step [380/391],                 Loss: 0.37871, Train_Acc:87.07%
Epoch [110/300], Step [390/391],                 Loss: 0.37757, Train_Acc:87.12%
Accuary on test images:79.08%
Epoch [111/300], Step [10/391],                 Loss: 0.36025, Train_Acc:87.73%
Epoch [111/300], Step [20/391],                 Loss: 0.35636, Train_Acc:88.09%
Epoch [111/300], Step [30/391],                 Loss: 0.35394, Train_Acc:88.28%
Epoch [111/300], Step [40/391],                 Loss: 0.35769, Train_Acc:88.05%
Epoch [111/300], Step [50/391],                 Loss: 0.35877, Train_Acc:87.95%
Epoch [111/300], Step [60/391],                 Loss: 0.36718, Train_Acc:87.59%
Epoch [111/300], Step [70/391],                 Loss: 0.36387, Train_Acc:87.76%
Epoch [111/300], Step [80/391],                 Loss: 0.36712, Train_Acc:87.60%
Epoch [111/300], Step [90/391],                 Loss: 0.37190, Train_Acc:87.55%
Epoch [111/300], Step [100/391],                 Loss: 0.37346, Train_Acc:87.44%
Epoch [111/300], Step [110/391],                 Loss: 0.37657, Train_Acc:87.29%
Epoch [111/300], Step [120/391],                 Loss: 0.37620, Train_Acc:87.26%
Epoch [111/300], Step [130/391],                 Loss: 0.37742, Train_Acc:87.26%
Epoch [111/300], Step [140/391],                 Loss: 0.37465, Train_Acc:87.39%
Epoch [111/300], Step [150/391],                 Loss: 0.37225, Train_Acc:87.47%
Epoch [111/300], Step [160/391],                 Loss: 0.37296, Train_Acc:87.42%
Epoch [111/300], Step [170/391],                 Loss: 0.37355, Train_Acc:87.35%
Epoch [111/300], Step [180/391],                 Loss: 0.37282, Train_Acc:87.35%
Epoch [111/300], Step [190/391],                 Loss: 0.37321, Train_Acc:87.32%
Epoch [111/300], Step [200/391],                 Loss: 0.37588, Train_Acc:87.21%
Epoch [111/300], Step [210/391],                 Loss: 0.37609, Train_Acc:87.16%
Epoch [111/300], Step [220/391],                 Loss: 0.37520, Train_Acc:87.25%
Epoch [111/300], Step [230/391],                 Loss: 0.37389, Train_Acc:87.26%
Epoch [111/300], Step [240/391],                 Loss: 0.37186, Train_Acc:87.33%
Epoch [111/300], Step [250/391],                 Loss: 0.37267, Train_Acc:87.31%
Epoch [111/300], Step [260/391],                 Loss: 0.37570, Train_Acc:87.24%
Epoch [111/300], Step [270/391],                 Loss: 0.37690, Train_Acc:87.19%
Epoch [111/300], Step [280/391],                 Loss: 0.37717, Train_Acc:87.16%
Epoch [111/300], Step [290/391],                 Loss: 0.37697, Train_Acc:87.17%
Epoch [111/300], Step [300/391],                 Loss: 0.37607, Train_Acc:87.19%
Epoch [111/300], Step [310/391],                 Loss: 0.37428, Train_Acc:87.26%
Epoch [111/300], Step [320/391],                 Loss: 0.37399, Train_Acc:87.29%
Epoch [111/300], Step [330/391],                 Loss: 0.37361, Train_Acc:87.30%
Epoch [111/300], Step [340/391],                 Loss: 0.37312, Train_Acc:87.35%
Epoch [111/300], Step [350/391],                 Loss: 0.37200, Train_Acc:87.40%
Epoch [111/300], Step [360/391],                 Loss: 0.37261, Train_Acc:87.35%
Epoch [111/300], Step [370/391],                 Loss: 0.37316, Train_Acc:87.31%
Epoch [111/300], Step [380/391],                 Loss: 0.37295, Train_Acc:87.31%
Epoch [111/300], Step [390/391],                 Loss: 0.37216, Train_Acc:87.34%
Accuary on test images:69.40%
Epoch [112/300], Step [10/391],                 Loss: 0.36857, Train_Acc:86.64%
Epoch [112/300], Step [20/391],                 Loss: 0.37219, Train_Acc:87.30%
Epoch [112/300], Step [30/391],                 Loss: 0.36497, Train_Acc:87.29%
Epoch [112/300], Step [40/391],                 Loss: 0.36211, Train_Acc:87.30%
Epoch [112/300], Step [50/391],                 Loss: 0.36720, Train_Acc:87.11%
Epoch [112/300], Step [60/391],                 Loss: 0.36795, Train_Acc:87.34%
Epoch [112/300], Step [70/391],                 Loss: 0.36939, Train_Acc:87.35%
Epoch [112/300], Step [80/391],                 Loss: 0.37126, Train_Acc:87.36%
Epoch [112/300], Step [90/391],                 Loss: 0.37960, Train_Acc:87.07%
Epoch [112/300], Step [100/391],                 Loss: 0.37989, Train_Acc:86.96%
Epoch [112/300], Step [110/391],                 Loss: 0.37813, Train_Acc:87.04%
Epoch [112/300], Step [120/391],                 Loss: 0.37785, Train_Acc:87.06%
Epoch [112/300], Step [130/391],                 Loss: 0.38021, Train_Acc:86.96%
Epoch [112/300], Step [140/391],                 Loss: 0.37808, Train_Acc:87.05%
Epoch [112/300], Step [150/391],                 Loss: 0.37668, Train_Acc:87.14%
Epoch [112/300], Step [160/391],                 Loss: 0.37578, Train_Acc:87.15%
Epoch [112/300], Step [170/391],                 Loss: 0.37651, Train_Acc:87.15%
Epoch [112/300], Step [180/391],                 Loss: 0.37631, Train_Acc:87.13%
Epoch [112/300], Step [190/391],                 Loss: 0.37688, Train_Acc:87.10%
Epoch [112/300], Step [200/391],                 Loss: 0.37891, Train_Acc:87.02%
Epoch [112/300], Step [210/391],                 Loss: 0.37838, Train_Acc:87.06%
Epoch [112/300], Step [220/391],                 Loss: 0.37721, Train_Acc:87.12%
Epoch [112/300], Step [230/391],                 Loss: 0.37510, Train_Acc:87.21%
Epoch [112/300], Step [240/391],                 Loss: 0.37278, Train_Acc:87.32%
Epoch [112/300], Step [250/391],                 Loss: 0.37159, Train_Acc:87.33%
Epoch [112/300], Step [260/391],                 Loss: 0.37332, Train_Acc:87.29%
Epoch [112/300], Step [270/391],                 Loss: 0.37381, Train_Acc:87.27%
Epoch [112/300], Step [280/391],                 Loss: 0.37420, Train_Acc:87.28%
Epoch [112/300], Step [290/391],                 Loss: 0.37461, Train_Acc:87.27%
Epoch [112/300], Step [300/391],                 Loss: 0.37495, Train_Acc:87.29%
Epoch [112/300], Step [310/391],                 Loss: 0.37441, Train_Acc:87.31%
Epoch [112/300], Step [320/391],                 Loss: 0.37399, Train_Acc:87.33%
Epoch [112/300], Step [330/391],                 Loss: 0.37230, Train_Acc:87.38%
Epoch [112/300], Step [340/391],                 Loss: 0.37074, Train_Acc:87.45%
Epoch [112/300], Step [350/391],                 Loss: 0.37019, Train_Acc:87.50%
Epoch [112/300], Step [360/391],                 Loss: 0.36949, Train_Acc:87.50%
Epoch [112/300], Step [370/391],                 Loss: 0.36988, Train_Acc:87.46%
Epoch [112/300], Step [380/391],                 Loss: 0.37045, Train_Acc:87.44%
Epoch [112/300], Step [390/391],                 Loss: 0.37068, Train_Acc:87.43%
Accuary on test images:74.84%
Epoch [113/300], Step [10/391],                 Loss: 0.38466, Train_Acc:86.64%
Epoch [113/300], Step [20/391],                 Loss: 0.37488, Train_Acc:87.30%
Epoch [113/300], Step [30/391],                 Loss: 0.36545, Train_Acc:87.68%
Epoch [113/300], Step [40/391],                 Loss: 0.36475, Train_Acc:87.58%
Epoch [113/300], Step [50/391],                 Loss: 0.35904, Train_Acc:87.75%
Epoch [113/300], Step [60/391],                 Loss: 0.36802, Train_Acc:87.37%
Epoch [113/300], Step [70/391],                 Loss: 0.36894, Train_Acc:87.27%
Epoch [113/300], Step [80/391],                 Loss: 0.37093, Train_Acc:87.16%
Epoch [113/300], Step [90/391],                 Loss: 0.37963, Train_Acc:86.85%
Epoch [113/300], Step [100/391],                 Loss: 0.37952, Train_Acc:86.80%
Epoch [113/300], Step [110/391],                 Loss: 0.38211, Train_Acc:86.79%
Epoch [113/300], Step [120/391],                 Loss: 0.38343, Train_Acc:86.82%
Epoch [113/300], Step [130/391],                 Loss: 0.38643, Train_Acc:86.78%
Epoch [113/300], Step [140/391],                 Loss: 0.38660, Train_Acc:86.73%
Epoch [113/300], Step [150/391],                 Loss: 0.38517, Train_Acc:86.80%
Epoch [113/300], Step [160/391],                 Loss: 0.38194, Train_Acc:86.90%
Epoch [113/300], Step [170/391],                 Loss: 0.38207, Train_Acc:86.89%
Epoch [113/300], Step [180/391],                 Loss: 0.38107, Train_Acc:86.90%
Epoch [113/300], Step [190/391],                 Loss: 0.38065, Train_Acc:86.94%
Epoch [113/300], Step [200/391],                 Loss: 0.38225, Train_Acc:86.91%
Epoch [113/300], Step [210/391],                 Loss: 0.38326, Train_Acc:86.89%
Epoch [113/300], Step [220/391],                 Loss: 0.38359, Train_Acc:86.88%
Epoch [113/300], Step [230/391],                 Loss: 0.38317, Train_Acc:86.86%
Epoch [113/300], Step [240/391],                 Loss: 0.38075, Train_Acc:86.93%
Epoch [113/300], Step [250/391],                 Loss: 0.37938, Train_Acc:86.94%
Epoch [113/300], Step [260/391],                 Loss: 0.38013, Train_Acc:86.95%
Epoch [113/300], Step [270/391],                 Loss: 0.38160, Train_Acc:86.91%
Epoch [113/300], Step [280/391],                 Loss: 0.38171, Train_Acc:86.93%
Epoch [113/300], Step [290/391],                 Loss: 0.38196, Train_Acc:86.93%
Epoch [113/300], Step [300/391],                 Loss: 0.38156, Train_Acc:86.93%
Epoch [113/300], Step [310/391],                 Loss: 0.38062, Train_Acc:86.98%
Epoch [113/300], Step [320/391],                 Loss: 0.38063, Train_Acc:86.98%
Epoch [113/300], Step [330/391],                 Loss: 0.37991, Train_Acc:87.01%
Epoch [113/300], Step [340/391],                 Loss: 0.37955, Train_Acc:87.01%
Epoch [113/300], Step [350/391],                 Loss: 0.37962, Train_Acc:87.01%
Epoch [113/300], Step [360/391],                 Loss: 0.37951, Train_Acc:87.01%
Epoch [113/300], Step [370/391],                 Loss: 0.38064, Train_Acc:86.97%
Epoch [113/300], Step [380/391],                 Loss: 0.38043, Train_Acc:87.01%
Epoch [113/300], Step [390/391],                 Loss: 0.38009, Train_Acc:87.03%
Accuary on test images:75.00%
Epoch [114/300], Step [10/391],                 Loss: 0.35137, Train_Acc:87.66%
Epoch [114/300], Step [20/391],                 Loss: 0.34087, Train_Acc:88.24%
Epoch [114/300], Step [30/391],                 Loss: 0.34354, Train_Acc:88.05%
Epoch [114/300], Step [40/391],                 Loss: 0.35023, Train_Acc:88.14%
Epoch [114/300], Step [50/391],                 Loss: 0.35935, Train_Acc:87.94%
Epoch [114/300], Step [60/391],                 Loss: 0.36844, Train_Acc:87.50%
Epoch [114/300], Step [70/391],                 Loss: 0.36903, Train_Acc:87.32%
Epoch [114/300], Step [80/391],                 Loss: 0.37079, Train_Acc:87.15%
Epoch [114/300], Step [90/391],                 Loss: 0.37804, Train_Acc:86.96%
Epoch [114/300], Step [100/391],                 Loss: 0.37388, Train_Acc:87.17%
Epoch [114/300], Step [110/391],                 Loss: 0.37410, Train_Acc:87.18%
Epoch [114/300], Step [120/391],                 Loss: 0.37481, Train_Acc:87.14%
Epoch [114/300], Step [130/391],                 Loss: 0.37529, Train_Acc:87.19%
Epoch [114/300], Step [140/391],                 Loss: 0.37324, Train_Acc:87.28%
Epoch [114/300], Step [150/391],                 Loss: 0.37472, Train_Acc:87.26%
Epoch [114/300], Step [160/391],                 Loss: 0.37390, Train_Acc:87.33%
Epoch [114/300], Step [170/391],                 Loss: 0.37247, Train_Acc:87.41%
Epoch [114/300], Step [180/391],                 Loss: 0.37311, Train_Acc:87.38%
Epoch [114/300], Step [190/391],                 Loss: 0.37299, Train_Acc:87.38%
Epoch [114/300], Step [200/391],                 Loss: 0.37410, Train_Acc:87.36%
Epoch [114/300], Step [210/391],                 Loss: 0.37314, Train_Acc:87.39%
Epoch [114/300], Step [220/391],                 Loss: 0.37431, Train_Acc:87.33%
Epoch [114/300], Step [230/391],                 Loss: 0.37445, Train_Acc:87.32%
Epoch [114/300], Step [240/391],                 Loss: 0.37338, Train_Acc:87.31%
Epoch [114/300], Step [250/391],                 Loss: 0.37263, Train_Acc:87.31%
Epoch [114/300], Step [260/391],                 Loss: 0.37436, Train_Acc:87.27%
Epoch [114/300], Step [270/391],                 Loss: 0.37679, Train_Acc:87.13%
Epoch [114/300], Step [280/391],                 Loss: 0.37808, Train_Acc:87.11%
Epoch [114/300], Step [290/391],                 Loss: 0.37918, Train_Acc:87.09%
Epoch [114/300], Step [300/391],                 Loss: 0.37903, Train_Acc:87.13%
Epoch [114/300], Step [310/391],                 Loss: 0.37882, Train_Acc:87.18%
Epoch [114/300], Step [320/391],                 Loss: 0.37854, Train_Acc:87.18%
Epoch [114/300], Step [330/391],                 Loss: 0.37751, Train_Acc:87.23%
Epoch [114/300], Step [340/391],                 Loss: 0.37633, Train_Acc:87.27%
Epoch [114/300], Step [350/391],                 Loss: 0.37582, Train_Acc:87.28%
Epoch [114/300], Step [360/391],                 Loss: 0.37551, Train_Acc:87.29%
Epoch [114/300], Step [370/391],                 Loss: 0.37599, Train_Acc:87.30%
Epoch [114/300], Step [380/391],                 Loss: 0.37579, Train_Acc:87.29%
Epoch [114/300], Step [390/391],                 Loss: 0.37578, Train_Acc:87.28%
Accuary on test images:76.98%
Epoch [115/300], Step [10/391],                 Loss: 0.38705, Train_Acc:87.66%
Epoch [115/300], Step [20/391],                 Loss: 0.37864, Train_Acc:87.54%
Epoch [115/300], Step [30/391],                 Loss: 0.36741, Train_Acc:87.97%
Epoch [115/300], Step [40/391],                 Loss: 0.37158, Train_Acc:87.91%
Epoch [115/300], Step [50/391],                 Loss: 0.37169, Train_Acc:87.72%
Epoch [115/300], Step [60/391],                 Loss: 0.38030, Train_Acc:87.50%
Epoch [115/300], Step [70/391],                 Loss: 0.37920, Train_Acc:87.52%
Epoch [115/300], Step [80/391],                 Loss: 0.38178, Train_Acc:87.41%
Epoch [115/300], Step [90/391],                 Loss: 0.38249, Train_Acc:87.29%
Epoch [115/300], Step [100/391],                 Loss: 0.37997, Train_Acc:87.31%
Epoch [115/300], Step [110/391],                 Loss: 0.38147, Train_Acc:87.34%
Epoch [115/300], Step [120/391],                 Loss: 0.38241, Train_Acc:87.24%
Epoch [115/300], Step [130/391],                 Loss: 0.38527, Train_Acc:87.17%
Epoch [115/300], Step [140/391],                 Loss: 0.38279, Train_Acc:87.24%
Epoch [115/300], Step [150/391],                 Loss: 0.38303, Train_Acc:87.22%
Epoch [115/300], Step [160/391],                 Loss: 0.38212, Train_Acc:87.28%
Epoch [115/300], Step [170/391],                 Loss: 0.38087, Train_Acc:87.34%
Epoch [115/300], Step [180/391],                 Loss: 0.37995, Train_Acc:87.30%
Epoch [115/300], Step [190/391],                 Loss: 0.37926, Train_Acc:87.33%
Epoch [115/300], Step [200/391],                 Loss: 0.37940, Train_Acc:87.25%
Epoch [115/300], Step [210/391],                 Loss: 0.37858, Train_Acc:87.25%
Epoch [115/300], Step [220/391],                 Loss: 0.37757, Train_Acc:87.30%
Epoch [115/300], Step [230/391],                 Loss: 0.37669, Train_Acc:87.32%
Epoch [115/300], Step [240/391],                 Loss: 0.37550, Train_Acc:87.36%
Epoch [115/300], Step [250/391],                 Loss: 0.37512, Train_Acc:87.38%
Epoch [115/300], Step [260/391],                 Loss: 0.37661, Train_Acc:87.40%
Epoch [115/300], Step [270/391],                 Loss: 0.37719, Train_Acc:87.38%
Epoch [115/300], Step [280/391],                 Loss: 0.37619, Train_Acc:87.41%
Epoch [115/300], Step [290/391],                 Loss: 0.37626, Train_Acc:87.44%
Epoch [115/300], Step [300/391],                 Loss: 0.37637, Train_Acc:87.40%
Epoch [115/300], Step [310/391],                 Loss: 0.37573, Train_Acc:87.42%
Epoch [115/300], Step [320/391],                 Loss: 0.37543, Train_Acc:87.41%
Epoch [115/300], Step [330/391],                 Loss: 0.37468, Train_Acc:87.43%
Epoch [115/300], Step [340/391],                 Loss: 0.37384, Train_Acc:87.45%
Epoch [115/300], Step [350/391],                 Loss: 0.37299, Train_Acc:87.46%
Epoch [115/300], Step [360/391],                 Loss: 0.37271, Train_Acc:87.45%
Epoch [115/300], Step [370/391],                 Loss: 0.37280, Train_Acc:87.42%
Epoch [115/300], Step [380/391],                 Loss: 0.37298, Train_Acc:87.45%
Epoch [115/300], Step [390/391],                 Loss: 0.37238, Train_Acc:87.47%
Accuary on test images:78.30%
Epoch [116/300], Step [10/391],                 Loss: 0.39129, Train_Acc:86.25%
Epoch [116/300], Step [20/391],                 Loss: 0.37273, Train_Acc:86.99%
Epoch [116/300], Step [30/391],                 Loss: 0.36002, Train_Acc:87.71%
Epoch [116/300], Step [40/391],                 Loss: 0.35885, Train_Acc:87.62%
Epoch [116/300], Step [50/391],                 Loss: 0.36035, Train_Acc:87.59%
Epoch [116/300], Step [60/391],                 Loss: 0.37319, Train_Acc:87.15%
Epoch [116/300], Step [70/391],                 Loss: 0.38088, Train_Acc:86.89%
Epoch [116/300], Step [80/391],                 Loss: 0.38598, Train_Acc:86.60%
Epoch [116/300], Step [90/391],                 Loss: 0.38941, Train_Acc:86.48%
Epoch [116/300], Step [100/391],                 Loss: 0.38612, Train_Acc:86.60%
Epoch [116/300], Step [110/391],                 Loss: 0.38595, Train_Acc:86.61%
Epoch [116/300], Step [120/391],                 Loss: 0.38555, Train_Acc:86.60%
Epoch [116/300], Step [130/391],                 Loss: 0.38666, Train_Acc:86.54%
Epoch [116/300], Step [140/391],                 Loss: 0.38360, Train_Acc:86.68%
Epoch [116/300], Step [150/391],                 Loss: 0.38243, Train_Acc:86.78%
Epoch [116/300], Step [160/391],                 Loss: 0.38226, Train_Acc:86.79%
Epoch [116/300], Step [170/391],                 Loss: 0.38122, Train_Acc:86.87%
Epoch [116/300], Step [180/391],                 Loss: 0.38196, Train_Acc:86.82%
Epoch [116/300], Step [190/391],                 Loss: 0.38139, Train_Acc:86.85%
Epoch [116/300], Step [200/391],                 Loss: 0.38323, Train_Acc:86.77%
Epoch [116/300], Step [210/391],                 Loss: 0.38305, Train_Acc:86.82%
Epoch [116/300], Step [220/391],                 Loss: 0.38200, Train_Acc:86.89%
Epoch [116/300], Step [230/391],                 Loss: 0.38054, Train_Acc:86.90%
Epoch [116/300], Step [240/391],                 Loss: 0.37884, Train_Acc:86.95%
Epoch [116/300], Step [250/391],                 Loss: 0.37895, Train_Acc:86.97%
Epoch [116/300], Step [260/391],                 Loss: 0.37970, Train_Acc:86.94%
Epoch [116/300], Step [270/391],                 Loss: 0.38051, Train_Acc:86.92%
Epoch [116/300], Step [280/391],                 Loss: 0.38058, Train_Acc:86.91%
Epoch [116/300], Step [290/391],                 Loss: 0.38143, Train_Acc:86.92%
Epoch [116/300], Step [300/391],                 Loss: 0.38079, Train_Acc:86.92%
Epoch [116/300], Step [310/391],                 Loss: 0.37972, Train_Acc:86.97%
Epoch [116/300], Step [320/391],                 Loss: 0.37858, Train_Acc:87.03%
Epoch [116/300], Step [330/391],                 Loss: 0.37787, Train_Acc:87.05%
Epoch [116/300], Step [340/391],                 Loss: 0.37664, Train_Acc:87.09%
Epoch [116/300], Step [350/391],                 Loss: 0.37644, Train_Acc:87.12%
Epoch [116/300], Step [360/391],                 Loss: 0.37679, Train_Acc:87.10%
Epoch [116/300], Step [370/391],                 Loss: 0.37762, Train_Acc:87.03%
Epoch [116/300], Step [380/391],                 Loss: 0.37726, Train_Acc:87.05%
Epoch [116/300], Step [390/391],                 Loss: 0.37658, Train_Acc:87.09%
Accuary on test images:75.34%
Epoch [117/300], Step [10/391],                 Loss: 0.37367, Train_Acc:87.27%
Epoch [117/300], Step [20/391],                 Loss: 0.37126, Train_Acc:87.38%
Epoch [117/300], Step [30/391],                 Loss: 0.37653, Train_Acc:87.24%
Epoch [117/300], Step [40/391],                 Loss: 0.37422, Train_Acc:87.42%
Epoch [117/300], Step [50/391],                 Loss: 0.37749, Train_Acc:87.22%
Epoch [117/300], Step [60/391],                 Loss: 0.37948, Train_Acc:87.02%
Epoch [117/300], Step [70/391],                 Loss: 0.37507, Train_Acc:87.31%
Epoch [117/300], Step [80/391],                 Loss: 0.37762, Train_Acc:87.18%
Epoch [117/300], Step [90/391],                 Loss: 0.38052, Train_Acc:87.07%
Epoch [117/300], Step [100/391],                 Loss: 0.37820, Train_Acc:87.11%
Epoch [117/300], Step [110/391],                 Loss: 0.37789, Train_Acc:87.08%
Epoch [117/300], Step [120/391],                 Loss: 0.37810, Train_Acc:87.00%
Epoch [117/300], Step [130/391],                 Loss: 0.37892, Train_Acc:87.04%
Epoch [117/300], Step [140/391],                 Loss: 0.37717, Train_Acc:87.18%
Epoch [117/300], Step [150/391],                 Loss: 0.37754, Train_Acc:87.16%
Epoch [117/300], Step [160/391],                 Loss: 0.37532, Train_Acc:87.23%
Epoch [117/300], Step [170/391],                 Loss: 0.37449, Train_Acc:87.22%
Epoch [117/300], Step [180/391],                 Loss: 0.37446, Train_Acc:87.26%
Epoch [117/300], Step [190/391],                 Loss: 0.37563, Train_Acc:87.21%
Epoch [117/300], Step [200/391],                 Loss: 0.37777, Train_Acc:87.16%
Epoch [117/300], Step [210/391],                 Loss: 0.37842, Train_Acc:87.14%
Epoch [117/300], Step [220/391],                 Loss: 0.37910, Train_Acc:87.12%
Epoch [117/300], Step [230/391],                 Loss: 0.37716, Train_Acc:87.16%
Epoch [117/300], Step [240/391],                 Loss: 0.37487, Train_Acc:87.23%
Epoch [117/300], Step [250/391],                 Loss: 0.37326, Train_Acc:87.27%
Epoch [117/300], Step [260/391],                 Loss: 0.37434, Train_Acc:87.26%
Epoch [117/300], Step [270/391],                 Loss: 0.37448, Train_Acc:87.23%
Epoch [117/300], Step [280/391],                 Loss: 0.37559, Train_Acc:87.21%
Epoch [117/300], Step [290/391],                 Loss: 0.37552, Train_Acc:87.23%
Epoch [117/300], Step [300/391],                 Loss: 0.37526, Train_Acc:87.25%
Epoch [117/300], Step [310/391],                 Loss: 0.37503, Train_Acc:87.27%
Epoch [117/300], Step [320/391],                 Loss: 0.37507, Train_Acc:87.27%
Epoch [117/300], Step [330/391],                 Loss: 0.37371, Train_Acc:87.31%
Epoch [117/300], Step [340/391],                 Loss: 0.37356, Train_Acc:87.32%
Epoch [117/300], Step [350/391],                 Loss: 0.37283, Train_Acc:87.33%
Epoch [117/300], Step [360/391],                 Loss: 0.37287, Train_Acc:87.30%
Epoch [117/300], Step [370/391],                 Loss: 0.37307, Train_Acc:87.29%
Epoch [117/300], Step [380/391],                 Loss: 0.37239, Train_Acc:87.31%
Epoch [117/300], Step [390/391],                 Loss: 0.37197, Train_Acc:87.30%
Accuary on test images:70.00%
Epoch [118/300], Step [10/391],                 Loss: 0.36534, Train_Acc:87.89%
Epoch [118/300], Step [20/391],                 Loss: 0.37171, Train_Acc:87.46%
Epoch [118/300], Step [30/391],                 Loss: 0.36734, Train_Acc:87.84%
Epoch [118/300], Step [40/391],                 Loss: 0.37386, Train_Acc:87.60%
Epoch [118/300], Step [50/391],                 Loss: 0.37522, Train_Acc:87.45%
Epoch [118/300], Step [60/391],                 Loss: 0.38243, Train_Acc:87.30%
Epoch [118/300], Step [70/391],                 Loss: 0.38036, Train_Acc:87.28%
Epoch [118/300], Step [80/391],                 Loss: 0.37991, Train_Acc:87.35%
Epoch [118/300], Step [90/391],                 Loss: 0.38494, Train_Acc:87.17%
Epoch [118/300], Step [100/391],                 Loss: 0.38604, Train_Acc:87.11%
Epoch [118/300], Step [110/391],                 Loss: 0.38924, Train_Acc:87.05%
Epoch [118/300], Step [120/391],                 Loss: 0.38718, Train_Acc:87.09%
Epoch [118/300], Step [130/391],                 Loss: 0.38590, Train_Acc:87.10%
Epoch [118/300], Step [140/391],                 Loss: 0.38316, Train_Acc:87.24%
Epoch [118/300], Step [150/391],                 Loss: 0.38154, Train_Acc:87.22%
Epoch [118/300], Step [160/391],                 Loss: 0.38066, Train_Acc:87.31%
Epoch [118/300], Step [170/391],                 Loss: 0.38031, Train_Acc:87.29%
Epoch [118/300], Step [180/391],                 Loss: 0.37802, Train_Acc:87.35%
Epoch [118/300], Step [190/391],                 Loss: 0.37808, Train_Acc:87.30%
Epoch [118/300], Step [200/391],                 Loss: 0.38061, Train_Acc:87.16%
Epoch [118/300], Step [210/391],                 Loss: 0.38174, Train_Acc:87.10%
Epoch [118/300], Step [220/391],                 Loss: 0.38334, Train_Acc:87.06%
Epoch [118/300], Step [230/391],                 Loss: 0.38145, Train_Acc:87.10%
Epoch [118/300], Step [240/391],                 Loss: 0.38036, Train_Acc:87.16%
Epoch [118/300], Step [250/391],                 Loss: 0.37889, Train_Acc:87.22%
Epoch [118/300], Step [260/391],                 Loss: 0.38010, Train_Acc:87.18%
Epoch [118/300], Step [270/391],                 Loss: 0.38141, Train_Acc:87.14%
Epoch [118/300], Step [280/391],                 Loss: 0.38116, Train_Acc:87.11%
Epoch [118/300], Step [290/391],                 Loss: 0.38121, Train_Acc:87.12%
Epoch [118/300], Step [300/391],                 Loss: 0.38009, Train_Acc:87.14%
Epoch [118/300], Step [310/391],                 Loss: 0.37969, Train_Acc:87.13%
Epoch [118/300], Step [320/391],                 Loss: 0.38052, Train_Acc:87.09%
Epoch [118/300], Step [330/391],                 Loss: 0.38069, Train_Acc:87.11%
Epoch [118/300], Step [340/391],                 Loss: 0.37965, Train_Acc:87.13%
Epoch [118/300], Step [350/391],                 Loss: 0.37997, Train_Acc:87.10%
Epoch [118/300], Step [360/391],                 Loss: 0.37971, Train_Acc:87.10%
Epoch [118/300], Step [370/391],                 Loss: 0.37997, Train_Acc:87.07%
Epoch [118/300], Step [380/391],                 Loss: 0.37927, Train_Acc:87.10%
Epoch [118/300], Step [390/391],                 Loss: 0.37895, Train_Acc:87.12%
Accuary on test images:77.86%
Epoch [119/300], Step [10/391],                 Loss: 0.34026, Train_Acc:88.36%
Epoch [119/300], Step [20/391],                 Loss: 0.34450, Train_Acc:88.09%
Epoch [119/300], Step [30/391],                 Loss: 0.35013, Train_Acc:87.92%
Epoch [119/300], Step [40/391],                 Loss: 0.35464, Train_Acc:87.97%
Epoch [119/300], Step [50/391],                 Loss: 0.35958, Train_Acc:87.72%
Epoch [119/300], Step [60/391],                 Loss: 0.36798, Train_Acc:87.38%
Epoch [119/300], Step [70/391],                 Loss: 0.36880, Train_Acc:87.44%
Epoch [119/300], Step [80/391],                 Loss: 0.37197, Train_Acc:87.30%
Epoch [119/300], Step [90/391],                 Loss: 0.37777, Train_Acc:87.05%
Epoch [119/300], Step [100/391],                 Loss: 0.37638, Train_Acc:87.16%
Epoch [119/300], Step [110/391],                 Loss: 0.37571, Train_Acc:87.16%
Epoch [119/300], Step [120/391],                 Loss: 0.37455, Train_Acc:87.22%
Epoch [119/300], Step [130/391],                 Loss: 0.37597, Train_Acc:87.19%
Epoch [119/300], Step [140/391],                 Loss: 0.37432, Train_Acc:87.15%
Epoch [119/300], Step [150/391],                 Loss: 0.37605, Train_Acc:87.09%
Epoch [119/300], Step [160/391],                 Loss: 0.37666, Train_Acc:87.03%
Epoch [119/300], Step [170/391],                 Loss: 0.37754, Train_Acc:87.03%
Epoch [119/300], Step [180/391],                 Loss: 0.37632, Train_Acc:87.09%
Epoch [119/300], Step [190/391],                 Loss: 0.37631, Train_Acc:87.13%
Epoch [119/300], Step [200/391],                 Loss: 0.37572, Train_Acc:87.18%
Epoch [119/300], Step [210/391],                 Loss: 0.37471, Train_Acc:87.22%
Epoch [119/300], Step [220/391],                 Loss: 0.37427, Train_Acc:87.20%
Epoch [119/300], Step [230/391],                 Loss: 0.37389, Train_Acc:87.21%
Epoch [119/300], Step [240/391],                 Loss: 0.37345, Train_Acc:87.21%
Epoch [119/300], Step [250/391],                 Loss: 0.37303, Train_Acc:87.23%
Epoch [119/300], Step [260/391],                 Loss: 0.37527, Train_Acc:87.17%
Epoch [119/300], Step [270/391],                 Loss: 0.37759, Train_Acc:87.08%
Epoch [119/300], Step [280/391],                 Loss: 0.37864, Train_Acc:87.04%
Epoch [119/300], Step [290/391],                 Loss: 0.37902, Train_Acc:87.05%
Epoch [119/300], Step [300/391],                 Loss: 0.37912, Train_Acc:87.05%
Epoch [119/300], Step [310/391],                 Loss: 0.37772, Train_Acc:87.12%
Epoch [119/300], Step [320/391],                 Loss: 0.37783, Train_Acc:87.12%
Epoch [119/300], Step [330/391],                 Loss: 0.37694, Train_Acc:87.18%
Epoch [119/300], Step [340/391],                 Loss: 0.37637, Train_Acc:87.19%
Epoch [119/300], Step [350/391],                 Loss: 0.37635, Train_Acc:87.19%
Epoch [119/300], Step [360/391],                 Loss: 0.37624, Train_Acc:87.19%
Epoch [119/300], Step [370/391],                 Loss: 0.37597, Train_Acc:87.19%
Epoch [119/300], Step [380/391],                 Loss: 0.37604, Train_Acc:87.19%
Epoch [119/300], Step [390/391],                 Loss: 0.37569, Train_Acc:87.20%
Accuary on test images:77.14%
Epoch [120/300], Step [10/391],                 Loss: 0.33975, Train_Acc:88.28%
Epoch [120/300], Step [20/391],                 Loss: 0.33723, Train_Acc:88.36%
Epoch [120/300], Step [30/391],                 Loss: 0.33550, Train_Acc:88.26%
Epoch [120/300], Step [40/391],                 Loss: 0.33856, Train_Acc:88.26%
Epoch [120/300], Step [50/391],                 Loss: 0.34567, Train_Acc:88.06%
Epoch [120/300], Step [60/391],                 Loss: 0.35422, Train_Acc:87.83%
Epoch [120/300], Step [70/391],                 Loss: 0.35075, Train_Acc:88.07%
Epoch [120/300], Step [80/391],                 Loss: 0.35339, Train_Acc:87.87%
Epoch [120/300], Step [90/391],                 Loss: 0.35611, Train_Acc:87.80%
Epoch [120/300], Step [100/391],                 Loss: 0.35656, Train_Acc:87.73%
Epoch [120/300], Step [110/391],                 Loss: 0.35947, Train_Acc:87.68%
Epoch [120/300], Step [120/391],                 Loss: 0.36074, Train_Acc:87.51%
Epoch [120/300], Step [130/391],                 Loss: 0.36361, Train_Acc:87.44%
Epoch [120/300], Step [140/391],                 Loss: 0.36439, Train_Acc:87.45%
Epoch [120/300], Step [150/391],                 Loss: 0.36443, Train_Acc:87.46%
Epoch [120/300], Step [160/391],                 Loss: 0.36434, Train_Acc:87.51%
Epoch [120/300], Step [170/391],                 Loss: 0.36423, Train_Acc:87.56%
Epoch [120/300], Step [180/391],                 Loss: 0.36394, Train_Acc:87.56%
Epoch [120/300], Step [190/391],                 Loss: 0.36546, Train_Acc:87.47%
Epoch [120/300], Step [200/391],                 Loss: 0.36768, Train_Acc:87.40%
Epoch [120/300], Step [210/391],                 Loss: 0.36931, Train_Acc:87.34%
Epoch [120/300], Step [220/391],                 Loss: 0.36983, Train_Acc:87.34%
Epoch [120/300], Step [230/391],                 Loss: 0.36926, Train_Acc:87.35%
Epoch [120/300], Step [240/391],                 Loss: 0.36769, Train_Acc:87.39%
Epoch [120/300], Step [250/391],                 Loss: 0.36806, Train_Acc:87.36%
Epoch [120/300], Step [260/391],                 Loss: 0.37122, Train_Acc:87.27%
Epoch [120/300], Step [270/391],                 Loss: 0.37505, Train_Acc:87.13%
Epoch [120/300], Step [280/391],                 Loss: 0.37543, Train_Acc:87.11%
Epoch [120/300], Step [290/391],                 Loss: 0.37660, Train_Acc:87.09%
Epoch [120/300], Step [300/391],                 Loss: 0.37666, Train_Acc:87.07%
Epoch [120/300], Step [310/391],                 Loss: 0.37571, Train_Acc:87.12%
Epoch [120/300], Step [320/391],                 Loss: 0.37566, Train_Acc:87.13%
Epoch [120/300], Step [330/391],                 Loss: 0.37426, Train_Acc:87.19%
Epoch [120/300], Step [340/391],                 Loss: 0.37274, Train_Acc:87.24%
Epoch [120/300], Step [350/391],                 Loss: 0.37266, Train_Acc:87.26%
Epoch [120/300], Step [360/391],                 Loss: 0.37211, Train_Acc:87.26%
Epoch [120/300], Step [370/391],                 Loss: 0.37303, Train_Acc:87.22%
Epoch [120/300], Step [380/391],                 Loss: 0.37275, Train_Acc:87.22%
Epoch [120/300], Step [390/391],                 Loss: 0.37170, Train_Acc:87.26%
Accuary on test images:73.22%
Epoch [121/300], Step [10/391],                 Loss: 0.39160, Train_Acc:86.25%
Epoch [121/300], Step [20/391],                 Loss: 0.37706, Train_Acc:87.27%
Epoch [121/300], Step [30/391],                 Loss: 0.36423, Train_Acc:87.63%
Epoch [121/300], Step [40/391],                 Loss: 0.35756, Train_Acc:88.03%
Epoch [121/300], Step [50/391],                 Loss: 0.35602, Train_Acc:88.05%
Epoch [121/300], Step [60/391],                 Loss: 0.36622, Train_Acc:87.71%
Epoch [121/300], Step [70/391],                 Loss: 0.36794, Train_Acc:87.82%
Epoch [121/300], Step [80/391],                 Loss: 0.37101, Train_Acc:87.66%
Epoch [121/300], Step [90/391],                 Loss: 0.37241, Train_Acc:87.54%
Epoch [121/300], Step [100/391],                 Loss: 0.37208, Train_Acc:87.48%
Epoch [121/300], Step [110/391],                 Loss: 0.37729, Train_Acc:87.37%
Epoch [121/300], Step [120/391],                 Loss: 0.37888, Train_Acc:87.33%
Epoch [121/300], Step [130/391],                 Loss: 0.37972, Train_Acc:87.37%
Epoch [121/300], Step [140/391],                 Loss: 0.37934, Train_Acc:87.38%
Epoch [121/300], Step [150/391],                 Loss: 0.38127, Train_Acc:87.30%
Epoch [121/300], Step [160/391],                 Loss: 0.38156, Train_Acc:87.30%
Epoch [121/300], Step [170/391],                 Loss: 0.38060, Train_Acc:87.32%
Epoch [121/300], Step [180/391],                 Loss: 0.38063, Train_Acc:87.32%
Epoch [121/300], Step [190/391],                 Loss: 0.37891, Train_Acc:87.38%
Epoch [121/300], Step [200/391],                 Loss: 0.37974, Train_Acc:87.31%
Epoch [121/300], Step [210/391],                 Loss: 0.37980, Train_Acc:87.30%
Epoch [121/300], Step [220/391],                 Loss: 0.37940, Train_Acc:87.35%
Epoch [121/300], Step [230/391],                 Loss: 0.37759, Train_Acc:87.42%
Epoch [121/300], Step [240/391],                 Loss: 0.37694, Train_Acc:87.38%
Epoch [121/300], Step [250/391],                 Loss: 0.37671, Train_Acc:87.34%
Epoch [121/300], Step [260/391],                 Loss: 0.37749, Train_Acc:87.33%
Epoch [121/300], Step [270/391],                 Loss: 0.37831, Train_Acc:87.32%
Epoch [121/300], Step [280/391],                 Loss: 0.37934, Train_Acc:87.26%
Epoch [121/300], Step [290/391],                 Loss: 0.37986, Train_Acc:87.25%
Epoch [121/300], Step [300/391],                 Loss: 0.38056, Train_Acc:87.21%
Epoch [121/300], Step [310/391],                 Loss: 0.37941, Train_Acc:87.26%
Epoch [121/300], Step [320/391],                 Loss: 0.37860, Train_Acc:87.30%
Epoch [121/300], Step [330/391],                 Loss: 0.37767, Train_Acc:87.34%
Epoch [121/300], Step [340/391],                 Loss: 0.37760, Train_Acc:87.38%
Epoch [121/300], Step [350/391],                 Loss: 0.37743, Train_Acc:87.38%
Epoch [121/300], Step [360/391],                 Loss: 0.37697, Train_Acc:87.36%
Epoch [121/300], Step [370/391],                 Loss: 0.37770, Train_Acc:87.33%
Epoch [121/300], Step [380/391],                 Loss: 0.37758, Train_Acc:87.34%
Epoch [121/300], Step [390/391],                 Loss: 0.37706, Train_Acc:87.35%
Accuary on test images:75.34%
Epoch [122/300], Step [10/391],                 Loss: 0.34450, Train_Acc:88.44%
Epoch [122/300], Step [20/391],                 Loss: 0.35308, Train_Acc:88.09%
Epoch [122/300], Step [30/391],                 Loss: 0.35539, Train_Acc:87.76%
Epoch [122/300], Step [40/391],                 Loss: 0.36081, Train_Acc:87.50%
Epoch [122/300], Step [50/391],                 Loss: 0.36609, Train_Acc:87.28%
Epoch [122/300], Step [60/391],                 Loss: 0.36647, Train_Acc:87.23%
Epoch [122/300], Step [70/391],                 Loss: 0.36401, Train_Acc:87.42%
Epoch [122/300], Step [80/391],                 Loss: 0.36838, Train_Acc:87.27%
Epoch [122/300], Step [90/391],                 Loss: 0.37295, Train_Acc:87.09%
Epoch [122/300], Step [100/391],                 Loss: 0.37509, Train_Acc:87.03%
Epoch [122/300], Step [110/391],                 Loss: 0.37517, Train_Acc:87.10%
Epoch [122/300], Step [120/391],                 Loss: 0.37560, Train_Acc:86.99%
Epoch [122/300], Step [130/391],                 Loss: 0.37660, Train_Acc:86.98%
Epoch [122/300], Step [140/391],                 Loss: 0.37513, Train_Acc:87.07%
Epoch [122/300], Step [150/391],                 Loss: 0.37371, Train_Acc:87.12%
Epoch [122/300], Step [160/391],                 Loss: 0.37370, Train_Acc:87.19%
Epoch [122/300], Step [170/391],                 Loss: 0.37574, Train_Acc:87.12%
Epoch [122/300], Step [180/391],                 Loss: 0.37680, Train_Acc:87.06%
Epoch [122/300], Step [190/391],                 Loss: 0.37644, Train_Acc:87.03%
Epoch [122/300], Step [200/391],                 Loss: 0.37874, Train_Acc:86.94%
Epoch [122/300], Step [210/391],                 Loss: 0.37880, Train_Acc:86.95%
Epoch [122/300], Step [220/391],                 Loss: 0.37847, Train_Acc:86.97%
Epoch [122/300], Step [230/391],                 Loss: 0.37633, Train_Acc:87.02%
Epoch [122/300], Step [240/391],                 Loss: 0.37573, Train_Acc:87.01%
Epoch [122/300], Step [250/391],                 Loss: 0.37734, Train_Acc:86.98%
Epoch [122/300], Step [260/391],                 Loss: 0.37924, Train_Acc:86.97%
Epoch [122/300], Step [270/391],                 Loss: 0.37955, Train_Acc:86.98%
Epoch [122/300], Step [280/391],                 Loss: 0.37907, Train_Acc:87.04%
Epoch [122/300], Step [290/391],                 Loss: 0.37899, Train_Acc:87.08%
Epoch [122/300], Step [300/391],                 Loss: 0.37796, Train_Acc:87.14%
Epoch [122/300], Step [310/391],                 Loss: 0.37689, Train_Acc:87.19%
Epoch [122/300], Step [320/391],                 Loss: 0.37755, Train_Acc:87.19%
Epoch [122/300], Step [330/391],                 Loss: 0.37676, Train_Acc:87.24%
Epoch [122/300], Step [340/391],                 Loss: 0.37649, Train_Acc:87.26%
Epoch [122/300], Step [350/391],                 Loss: 0.37640, Train_Acc:87.26%
Epoch [122/300], Step [360/391],                 Loss: 0.37621, Train_Acc:87.27%
Epoch [122/300], Step [370/391],                 Loss: 0.37566, Train_Acc:87.28%
Epoch [122/300], Step [380/391],                 Loss: 0.37440, Train_Acc:87.33%
Epoch [122/300], Step [390/391],                 Loss: 0.37280, Train_Acc:87.39%
Accuary on test images:80.60%
Epoch [123/300], Step [10/391],                 Loss: 0.36947, Train_Acc:88.12%
Epoch [123/300], Step [20/391],                 Loss: 0.36743, Train_Acc:88.09%
Epoch [123/300], Step [30/391],                 Loss: 0.36088, Train_Acc:88.33%
Epoch [123/300], Step [40/391],                 Loss: 0.36070, Train_Acc:88.12%
Epoch [123/300], Step [50/391],                 Loss: 0.36601, Train_Acc:87.88%
Epoch [123/300], Step [60/391],                 Loss: 0.36689, Train_Acc:87.75%
Epoch [123/300], Step [70/391],                 Loss: 0.36620, Train_Acc:87.91%
Epoch [123/300], Step [80/391],                 Loss: 0.36682, Train_Acc:87.92%
Epoch [123/300], Step [90/391],                 Loss: 0.36836, Train_Acc:87.85%
Epoch [123/300], Step [100/391],                 Loss: 0.36696, Train_Acc:87.76%
Epoch [123/300], Step [110/391],                 Loss: 0.36727, Train_Acc:87.74%
Epoch [123/300], Step [120/391],                 Loss: 0.36760, Train_Acc:87.71%
Epoch [123/300], Step [130/391],                 Loss: 0.37088, Train_Acc:87.65%
Epoch [123/300], Step [140/391],                 Loss: 0.37249, Train_Acc:87.53%
Epoch [123/300], Step [150/391],                 Loss: 0.37557, Train_Acc:87.39%
Epoch [123/300], Step [160/391],                 Loss: 0.37739, Train_Acc:87.33%
Epoch [123/300], Step [170/391],                 Loss: 0.37942, Train_Acc:87.22%
Epoch [123/300], Step [180/391],                 Loss: 0.38016, Train_Acc:87.18%
Epoch [123/300], Step [190/391],                 Loss: 0.37984, Train_Acc:87.18%
Epoch [123/300], Step [200/391],                 Loss: 0.38063, Train_Acc:87.15%
Epoch [123/300], Step [210/391],                 Loss: 0.38225, Train_Acc:87.11%
Epoch [123/300], Step [220/391],                 Loss: 0.38367, Train_Acc:87.08%
Epoch [123/300], Step [230/391],                 Loss: 0.38322, Train_Acc:87.08%
Epoch [123/300], Step [240/391],                 Loss: 0.38234, Train_Acc:87.11%
Epoch [123/300], Step [250/391],                 Loss: 0.38216, Train_Acc:87.12%
Epoch [123/300], Step [260/391],                 Loss: 0.38301, Train_Acc:87.10%
Epoch [123/300], Step [270/391],                 Loss: 0.38417, Train_Acc:87.04%
Epoch [123/300], Step [280/391],                 Loss: 0.38339, Train_Acc:87.05%
Epoch [123/300], Step [290/391],                 Loss: 0.38357, Train_Acc:87.05%
Epoch [123/300], Step [300/391],                 Loss: 0.38412, Train_Acc:87.02%
Epoch [123/300], Step [310/391],                 Loss: 0.38394, Train_Acc:87.02%
Epoch [123/300], Step [320/391],                 Loss: 0.38378, Train_Acc:87.03%
Epoch [123/300], Step [330/391],                 Loss: 0.38365, Train_Acc:87.05%
Epoch [123/300], Step [340/391],                 Loss: 0.38235, Train_Acc:87.10%
Epoch [123/300], Step [350/391],                 Loss: 0.38139, Train_Acc:87.15%
Epoch [123/300], Step [360/391],                 Loss: 0.37990, Train_Acc:87.17%
Epoch [123/300], Step [370/391],                 Loss: 0.37900, Train_Acc:87.20%
Epoch [123/300], Step [380/391],                 Loss: 0.37716, Train_Acc:87.26%
Epoch [123/300], Step [390/391],                 Loss: 0.37557, Train_Acc:87.32%
Accuary on test images:76.46%
Epoch [124/300], Step [10/391],                 Loss: 0.35191, Train_Acc:88.20%
Epoch [124/300], Step [20/391],                 Loss: 0.36345, Train_Acc:87.38%
Epoch [124/300], Step [30/391],                 Loss: 0.35998, Train_Acc:87.06%
Epoch [124/300], Step [40/391],                 Loss: 0.36689, Train_Acc:87.23%
Epoch [124/300], Step [50/391],                 Loss: 0.36548, Train_Acc:87.33%
Epoch [124/300], Step [60/391],                 Loss: 0.37114, Train_Acc:87.20%
Epoch [124/300], Step [70/391],                 Loss: 0.37222, Train_Acc:87.32%
Epoch [124/300], Step [80/391],                 Loss: 0.37616, Train_Acc:87.14%
Epoch [124/300], Step [90/391],                 Loss: 0.37821, Train_Acc:86.97%
Epoch [124/300], Step [100/391],                 Loss: 0.37440, Train_Acc:87.05%
Epoch [124/300], Step [110/391],                 Loss: 0.37436, Train_Acc:87.05%
Epoch [124/300], Step [120/391],                 Loss: 0.37519, Train_Acc:87.09%
Epoch [124/300], Step [130/391],                 Loss: 0.37686, Train_Acc:87.03%
Epoch [124/300], Step [140/391],                 Loss: 0.37631, Train_Acc:87.14%
Epoch [124/300], Step [150/391],                 Loss: 0.37720, Train_Acc:87.06%
Epoch [124/300], Step [160/391],                 Loss: 0.37651, Train_Acc:87.09%
Epoch [124/300], Step [170/391],                 Loss: 0.37574, Train_Acc:87.08%
Epoch [124/300], Step [180/391],                 Loss: 0.37595, Train_Acc:87.10%
Epoch [124/300], Step [190/391],                 Loss: 0.37826, Train_Acc:87.06%
Epoch [124/300], Step [200/391],                 Loss: 0.37953, Train_Acc:86.97%
Epoch [124/300], Step [210/391],                 Loss: 0.38061, Train_Acc:86.90%
Epoch [124/300], Step [220/391],                 Loss: 0.38217, Train_Acc:86.86%
Epoch [124/300], Step [230/391],                 Loss: 0.38148, Train_Acc:86.87%
Epoch [124/300], Step [240/391],                 Loss: 0.37901, Train_Acc:86.96%
Epoch [124/300], Step [250/391],                 Loss: 0.37793, Train_Acc:87.01%
Epoch [124/300], Step [260/391],                 Loss: 0.37949, Train_Acc:86.98%
Epoch [124/300], Step [270/391],                 Loss: 0.38032, Train_Acc:86.95%
Epoch [124/300], Step [280/391],                 Loss: 0.38119, Train_Acc:86.93%
Epoch [124/300], Step [290/391],                 Loss: 0.38096, Train_Acc:86.97%
Epoch [124/300], Step [300/391],                 Loss: 0.38090, Train_Acc:86.99%
Epoch [124/300], Step [310/391],                 Loss: 0.38086, Train_Acc:86.99%
Epoch [124/300], Step [320/391],                 Loss: 0.38118, Train_Acc:86.98%
Epoch [124/300], Step [330/391],                 Loss: 0.38021, Train_Acc:87.01%
Epoch [124/300], Step [340/391],                 Loss: 0.37945, Train_Acc:87.03%
Epoch [124/300], Step [350/391],                 Loss: 0.37801, Train_Acc:87.10%
Epoch [124/300], Step [360/391],                 Loss: 0.37697, Train_Acc:87.13%
Epoch [124/300], Step [370/391],                 Loss: 0.37575, Train_Acc:87.17%
Epoch [124/300], Step [380/391],                 Loss: 0.37459, Train_Acc:87.22%
Epoch [124/300], Step [390/391],                 Loss: 0.37424, Train_Acc:87.25%
Accuary on test images:76.94%
Epoch [125/300], Step [10/391],                 Loss: 0.34790, Train_Acc:87.58%
Epoch [125/300], Step [20/391],                 Loss: 0.34571, Train_Acc:88.05%
Epoch [125/300], Step [30/391],                 Loss: 0.35228, Train_Acc:88.07%
Epoch [125/300], Step [40/391],                 Loss: 0.36029, Train_Acc:87.83%
Epoch [125/300], Step [50/391],                 Loss: 0.36410, Train_Acc:87.62%
Epoch [125/300], Step [60/391],                 Loss: 0.37616, Train_Acc:87.23%
Epoch [125/300], Step [70/391],                 Loss: 0.37801, Train_Acc:87.24%
Epoch [125/300], Step [80/391],                 Loss: 0.38305, Train_Acc:87.09%
Epoch [125/300], Step [90/391],                 Loss: 0.38497, Train_Acc:87.08%
Epoch [125/300], Step [100/391],                 Loss: 0.38448, Train_Acc:87.09%
Epoch [125/300], Step [110/391],                 Loss: 0.38352, Train_Acc:87.14%
Epoch [125/300], Step [120/391],                 Loss: 0.38229, Train_Acc:87.14%
Epoch [125/300], Step [130/391],                 Loss: 0.38222, Train_Acc:87.09%
Epoch [125/300], Step [140/391],                 Loss: 0.37911, Train_Acc:87.19%
Epoch [125/300], Step [150/391],                 Loss: 0.37863, Train_Acc:87.17%
Epoch [125/300], Step [160/391],                 Loss: 0.37942, Train_Acc:87.14%
Epoch [125/300], Step [170/391],                 Loss: 0.38001, Train_Acc:87.21%
Epoch [125/300], Step [180/391],                 Loss: 0.37943, Train_Acc:87.20%
Epoch [125/300], Step [190/391],                 Loss: 0.37907, Train_Acc:87.23%
Epoch [125/300], Step [200/391],                 Loss: 0.37995, Train_Acc:87.17%
Epoch [125/300], Step [210/391],                 Loss: 0.37992, Train_Acc:87.14%
Epoch [125/300], Step [220/391],                 Loss: 0.38018, Train_Acc:87.18%
Epoch [125/300], Step [230/391],                 Loss: 0.37814, Train_Acc:87.24%
Epoch [125/300], Step [240/391],                 Loss: 0.37692, Train_Acc:87.28%
Epoch [125/300], Step [250/391],                 Loss: 0.37624, Train_Acc:87.26%
Epoch [125/300], Step [260/391],                 Loss: 0.37798, Train_Acc:87.22%
Epoch [125/300], Step [270/391],                 Loss: 0.37897, Train_Acc:87.20%
Epoch [125/300], Step [280/391],                 Loss: 0.37840, Train_Acc:87.22%
Epoch [125/300], Step [290/391],                 Loss: 0.37802, Train_Acc:87.24%
Epoch [125/300], Step [300/391],                 Loss: 0.37752, Train_Acc:87.22%
Epoch [125/300], Step [310/391],                 Loss: 0.37652, Train_Acc:87.27%
Epoch [125/300], Step [320/391],                 Loss: 0.37604, Train_Acc:87.30%
Epoch [125/300], Step [330/391],                 Loss: 0.37503, Train_Acc:87.33%
Epoch [125/300], Step [340/391],                 Loss: 0.37366, Train_Acc:87.38%
Epoch [125/300], Step [350/391],                 Loss: 0.37398, Train_Acc:87.35%
Epoch [125/300], Step [360/391],                 Loss: 0.37365, Train_Acc:87.34%
Epoch [125/300], Step [370/391],                 Loss: 0.37389, Train_Acc:87.33%
Epoch [125/300], Step [380/391],                 Loss: 0.37334, Train_Acc:87.35%
Epoch [125/300], Step [390/391],                 Loss: 0.37264, Train_Acc:87.38%
Accuary on test images:79.24%
Epoch [126/300], Step [10/391],                 Loss: 0.36369, Train_Acc:87.11%
Epoch [126/300], Step [20/391],                 Loss: 0.36887, Train_Acc:87.30%
Epoch [126/300], Step [30/391],                 Loss: 0.36563, Train_Acc:87.29%
Epoch [126/300], Step [40/391],                 Loss: 0.36237, Train_Acc:87.81%
Epoch [126/300], Step [50/391],                 Loss: 0.36857, Train_Acc:87.52%
Epoch [126/300], Step [60/391],                 Loss: 0.37273, Train_Acc:87.32%
Epoch [126/300], Step [70/391],                 Loss: 0.36669, Train_Acc:87.60%
Epoch [126/300], Step [80/391],                 Loss: 0.36633, Train_Acc:87.71%
Epoch [126/300], Step [90/391],                 Loss: 0.37053, Train_Acc:87.47%
Epoch [126/300], Step [100/391],                 Loss: 0.36955, Train_Acc:87.38%
Epoch [126/300], Step [110/391],                 Loss: 0.37271, Train_Acc:87.27%
Epoch [126/300], Step [120/391],                 Loss: 0.37326, Train_Acc:87.16%
Epoch [126/300], Step [130/391],                 Loss: 0.37443, Train_Acc:87.07%
Epoch [126/300], Step [140/391],                 Loss: 0.37412, Train_Acc:87.09%
Epoch [126/300], Step [150/391],                 Loss: 0.37500, Train_Acc:87.00%
Epoch [126/300], Step [160/391],                 Loss: 0.37603, Train_Acc:86.99%
Epoch [126/300], Step [170/391],                 Loss: 0.37621, Train_Acc:87.02%
Epoch [126/300], Step [180/391],                 Loss: 0.37808, Train_Acc:86.96%
Epoch [126/300], Step [190/391],                 Loss: 0.37904, Train_Acc:86.97%
Epoch [126/300], Step [200/391],                 Loss: 0.38100, Train_Acc:86.88%
Epoch [126/300], Step [210/391],                 Loss: 0.38019, Train_Acc:86.95%
Epoch [126/300], Step [220/391],                 Loss: 0.37988, Train_Acc:86.91%
Epoch [126/300], Step [230/391],                 Loss: 0.37918, Train_Acc:86.93%
Epoch [126/300], Step [240/391],                 Loss: 0.37778, Train_Acc:86.98%
Epoch [126/300], Step [250/391],                 Loss: 0.37762, Train_Acc:87.01%
Epoch [126/300], Step [260/391],                 Loss: 0.37993, Train_Acc:86.95%
Epoch [126/300], Step [270/391],                 Loss: 0.38102, Train_Acc:86.90%
Epoch [126/300], Step [280/391],                 Loss: 0.38108, Train_Acc:86.89%
Epoch [126/300], Step [290/391],                 Loss: 0.38033, Train_Acc:86.93%
Epoch [126/300], Step [300/391],                 Loss: 0.37965, Train_Acc:86.95%
Epoch [126/300], Step [310/391],                 Loss: 0.37953, Train_Acc:86.95%
Epoch [126/300], Step [320/391],                 Loss: 0.37878, Train_Acc:86.95%
Epoch [126/300], Step [330/391],                 Loss: 0.37694, Train_Acc:87.00%
Epoch [126/300], Step [340/391],                 Loss: 0.37666, Train_Acc:87.04%
Epoch [126/300], Step [350/391],                 Loss: 0.37501, Train_Acc:87.10%
Epoch [126/300], Step [360/391],                 Loss: 0.37437, Train_Acc:87.11%
Epoch [126/300], Step [370/391],                 Loss: 0.37482, Train_Acc:87.13%
Epoch [126/300], Step [380/391],                 Loss: 0.37485, Train_Acc:87.14%
Epoch [126/300], Step [390/391],                 Loss: 0.37428, Train_Acc:87.16%
Accuary on test images:77.62%
Epoch [127/300], Step [10/391],                 Loss: 0.36195, Train_Acc:86.80%
Epoch [127/300], Step [20/391],                 Loss: 0.35965, Train_Acc:87.46%
Epoch [127/300], Step [30/391],                 Loss: 0.35466, Train_Acc:87.76%
Epoch [127/300], Step [40/391],                 Loss: 0.36215, Train_Acc:87.73%
Epoch [127/300], Step [50/391],                 Loss: 0.36675, Train_Acc:87.50%
Epoch [127/300], Step [60/391],                 Loss: 0.37618, Train_Acc:87.08%
Epoch [127/300], Step [70/391],                 Loss: 0.37648, Train_Acc:87.06%
Epoch [127/300], Step [80/391],                 Loss: 0.37820, Train_Acc:87.07%
Epoch [127/300], Step [90/391],                 Loss: 0.37931, Train_Acc:87.07%
Epoch [127/300], Step [100/391],                 Loss: 0.37824, Train_Acc:87.07%
Epoch [127/300], Step [110/391],                 Loss: 0.38219, Train_Acc:86.97%
Epoch [127/300], Step [120/391],                 Loss: 0.38118, Train_Acc:87.02%
Epoch [127/300], Step [130/391],                 Loss: 0.38447, Train_Acc:86.89%
Epoch [127/300], Step [140/391],                 Loss: 0.38431, Train_Acc:86.91%
Epoch [127/300], Step [150/391],                 Loss: 0.38499, Train_Acc:86.90%
Epoch [127/300], Step [160/391],                 Loss: 0.38356, Train_Acc:86.91%
Epoch [127/300], Step [170/391],                 Loss: 0.38362, Train_Acc:86.92%
Epoch [127/300], Step [180/391],                 Loss: 0.38410, Train_Acc:86.93%
Epoch [127/300], Step [190/391],                 Loss: 0.38407, Train_Acc:86.94%
Epoch [127/300], Step [200/391],                 Loss: 0.38380, Train_Acc:86.96%
Epoch [127/300], Step [210/391],                 Loss: 0.38177, Train_Acc:86.99%
Epoch [127/300], Step [220/391],                 Loss: 0.38256, Train_Acc:87.00%
Epoch [127/300], Step [230/391],                 Loss: 0.38068, Train_Acc:87.02%
Epoch [127/300], Step [240/391],                 Loss: 0.37990, Train_Acc:87.05%
Epoch [127/300], Step [250/391],                 Loss: 0.38013, Train_Acc:87.01%
Epoch [127/300], Step [260/391],                 Loss: 0.38230, Train_Acc:86.97%
Epoch [127/300], Step [270/391],                 Loss: 0.38492, Train_Acc:86.83%
Epoch [127/300], Step [280/391],                 Loss: 0.38373, Train_Acc:86.86%
Epoch [127/300], Step [290/391],                 Loss: 0.38259, Train_Acc:86.92%
Epoch [127/300], Step [300/391],                 Loss: 0.38150, Train_Acc:86.93%
Epoch [127/300], Step [310/391],                 Loss: 0.37994, Train_Acc:86.99%
Epoch [127/300], Step [320/391],                 Loss: 0.37894, Train_Acc:87.04%
Epoch [127/300], Step [330/391],                 Loss: 0.37686, Train_Acc:87.11%
Epoch [127/300], Step [340/391],                 Loss: 0.37543, Train_Acc:87.18%
Epoch [127/300], Step [350/391],                 Loss: 0.37494, Train_Acc:87.21%
Epoch [127/300], Step [360/391],                 Loss: 0.37497, Train_Acc:87.20%
Epoch [127/300], Step [370/391],                 Loss: 0.37512, Train_Acc:87.20%
Epoch [127/300], Step [380/391],                 Loss: 0.37452, Train_Acc:87.23%
Epoch [127/300], Step [390/391],                 Loss: 0.37327, Train_Acc:87.27%
Accuary on test images:75.98%
Epoch [128/300], Step [10/391],                 Loss: 0.33800, Train_Acc:88.20%
Epoch [128/300], Step [20/391],                 Loss: 0.35359, Train_Acc:87.85%
Epoch [128/300], Step [30/391],                 Loss: 0.35119, Train_Acc:87.99%
Epoch [128/300], Step [40/391],                 Loss: 0.35674, Train_Acc:87.77%
Epoch [128/300], Step [50/391],                 Loss: 0.35802, Train_Acc:87.73%
Epoch [128/300], Step [60/391],                 Loss: 0.36649, Train_Acc:87.36%
Epoch [128/300], Step [70/391],                 Loss: 0.36371, Train_Acc:87.48%
Epoch [128/300], Step [80/391],                 Loss: 0.36634, Train_Acc:87.40%
Epoch [128/300], Step [90/391],                 Loss: 0.37234, Train_Acc:87.33%
Epoch [128/300], Step [100/391],                 Loss: 0.37205, Train_Acc:87.24%
Epoch [128/300], Step [110/391],                 Loss: 0.37351, Train_Acc:87.18%
Epoch [128/300], Step [120/391],                 Loss: 0.37583, Train_Acc:86.96%
Epoch [128/300], Step [130/391],                 Loss: 0.37889, Train_Acc:86.97%
Epoch [128/300], Step [140/391],                 Loss: 0.38106, Train_Acc:86.91%
Epoch [128/300], Step [150/391],                 Loss: 0.38369, Train_Acc:86.84%
Epoch [128/300], Step [160/391],                 Loss: 0.38372, Train_Acc:86.85%
Epoch [128/300], Step [170/391],                 Loss: 0.38418, Train_Acc:86.82%
Epoch [128/300], Step [180/391],                 Loss: 0.38402, Train_Acc:86.79%
Epoch [128/300], Step [190/391],                 Loss: 0.38482, Train_Acc:86.72%
Epoch [128/300], Step [200/391],                 Loss: 0.38579, Train_Acc:86.65%
Epoch [128/300], Step [210/391],                 Loss: 0.38623, Train_Acc:86.65%
Epoch [128/300], Step [220/391],                 Loss: 0.38592, Train_Acc:86.68%
Epoch [128/300], Step [230/391],                 Loss: 0.38549, Train_Acc:86.72%
Epoch [128/300], Step [240/391],                 Loss: 0.38495, Train_Acc:86.75%
Epoch [128/300], Step [250/391],                 Loss: 0.38469, Train_Acc:86.71%
Epoch [128/300], Step [260/391],                 Loss: 0.38552, Train_Acc:86.72%
Epoch [128/300], Step [270/391],                 Loss: 0.38662, Train_Acc:86.68%
Epoch [128/300], Step [280/391],                 Loss: 0.38499, Train_Acc:86.75%
Epoch [128/300], Step [290/391],                 Loss: 0.38490, Train_Acc:86.80%
Epoch [128/300], Step [300/391],                 Loss: 0.38453, Train_Acc:86.80%
Epoch [128/300], Step [310/391],                 Loss: 0.38335, Train_Acc:86.85%
Epoch [128/300], Step [320/391],                 Loss: 0.38338, Train_Acc:86.87%
Epoch [128/300], Step [330/391],                 Loss: 0.38212, Train_Acc:86.92%
Epoch [128/300], Step [340/391],                 Loss: 0.38112, Train_Acc:86.94%
Epoch [128/300], Step [350/391],                 Loss: 0.38074, Train_Acc:86.96%
Epoch [128/300], Step [360/391],                 Loss: 0.38039, Train_Acc:86.96%
Epoch [128/300], Step [370/391],                 Loss: 0.38019, Train_Acc:86.96%
Epoch [128/300], Step [380/391],                 Loss: 0.37906, Train_Acc:87.01%
Epoch [128/300], Step [390/391],                 Loss: 0.37836, Train_Acc:87.03%
Accuary on test images:74.82%
Epoch [129/300], Step [10/391],                 Loss: 0.40425, Train_Acc:86.09%
Epoch [129/300], Step [20/391],                 Loss: 0.39317, Train_Acc:86.60%
Epoch [129/300], Step [30/391],                 Loss: 0.37711, Train_Acc:87.32%
Epoch [129/300], Step [40/391],                 Loss: 0.36928, Train_Acc:87.68%
Epoch [129/300], Step [50/391],                 Loss: 0.36776, Train_Acc:87.53%
Epoch [129/300], Step [60/391],                 Loss: 0.36915, Train_Acc:87.43%
Epoch [129/300], Step [70/391],                 Loss: 0.36632, Train_Acc:87.48%
Epoch [129/300], Step [80/391],                 Loss: 0.36653, Train_Acc:87.45%
Epoch [129/300], Step [90/391],                 Loss: 0.36730, Train_Acc:87.44%
Epoch [129/300], Step [100/391],                 Loss: 0.37098, Train_Acc:87.23%
Epoch [129/300], Step [110/391],                 Loss: 0.37663, Train_Acc:87.17%
Epoch [129/300], Step [120/391],                 Loss: 0.37570, Train_Acc:87.16%
Epoch [129/300], Step [130/391],                 Loss: 0.37712, Train_Acc:87.12%
Epoch [129/300], Step [140/391],                 Loss: 0.37590, Train_Acc:87.17%
Epoch [129/300], Step [150/391],                 Loss: 0.37706, Train_Acc:87.13%
Epoch [129/300], Step [160/391],                 Loss: 0.37533, Train_Acc:87.25%
Epoch [129/300], Step [170/391],                 Loss: 0.37567, Train_Acc:87.27%
Epoch [129/300], Step [180/391],                 Loss: 0.37480, Train_Acc:87.29%
Epoch [129/300], Step [190/391],                 Loss: 0.37424, Train_Acc:87.28%
Epoch [129/300], Step [200/391],                 Loss: 0.37507, Train_Acc:87.28%
Epoch [129/300], Step [210/391],                 Loss: 0.37455, Train_Acc:87.30%
Epoch [129/300], Step [220/391],                 Loss: 0.37534, Train_Acc:87.30%
Epoch [129/300], Step [230/391],                 Loss: 0.37406, Train_Acc:87.32%
Epoch [129/300], Step [240/391],                 Loss: 0.37280, Train_Acc:87.33%
Epoch [129/300], Step [250/391],                 Loss: 0.37232, Train_Acc:87.32%
Epoch [129/300], Step [260/391],                 Loss: 0.37596, Train_Acc:87.17%
Epoch [129/300], Step [270/391],                 Loss: 0.37738, Train_Acc:87.09%
Epoch [129/300], Step [280/391],                 Loss: 0.37714, Train_Acc:87.14%
Epoch [129/300], Step [290/391],                 Loss: 0.37680, Train_Acc:87.20%
Epoch [129/300], Step [300/391],                 Loss: 0.37643, Train_Acc:87.18%
Epoch [129/300], Step [310/391],                 Loss: 0.37523, Train_Acc:87.21%
Epoch [129/300], Step [320/391],                 Loss: 0.37510, Train_Acc:87.23%
Epoch [129/300], Step [330/391],                 Loss: 0.37406, Train_Acc:87.30%
Epoch [129/300], Step [340/391],                 Loss: 0.37365, Train_Acc:87.33%
Epoch [129/300], Step [350/391],                 Loss: 0.37386, Train_Acc:87.33%
Epoch [129/300], Step [360/391],                 Loss: 0.37349, Train_Acc:87.33%
Epoch [129/300], Step [370/391],                 Loss: 0.37443, Train_Acc:87.29%
Epoch [129/300], Step [380/391],                 Loss: 0.37471, Train_Acc:87.29%
Epoch [129/300], Step [390/391],                 Loss: 0.37478, Train_Acc:87.30%
Accuary on test images:69.96%
Epoch [130/300], Step [10/391],                 Loss: 0.39698, Train_Acc:86.95%
Epoch [130/300], Step [20/391],                 Loss: 0.38136, Train_Acc:87.34%
Epoch [130/300], Step [30/391],                 Loss: 0.37086, Train_Acc:87.71%
Epoch [130/300], Step [40/391],                 Loss: 0.37056, Train_Acc:87.66%
Epoch [130/300], Step [50/391],                 Loss: 0.37006, Train_Acc:87.47%
Epoch [130/300], Step [60/391],                 Loss: 0.37498, Train_Acc:87.49%
Epoch [130/300], Step [70/391],                 Loss: 0.37413, Train_Acc:87.57%
Epoch [130/300], Step [80/391],                 Loss: 0.37798, Train_Acc:87.49%
Epoch [130/300], Step [90/391],                 Loss: 0.37823, Train_Acc:87.50%
Epoch [130/300], Step [100/391],                 Loss: 0.37420, Train_Acc:87.58%
Epoch [130/300], Step [110/391],                 Loss: 0.37954, Train_Acc:87.37%
Epoch [130/300], Step [120/391],                 Loss: 0.38149, Train_Acc:87.32%
Epoch [130/300], Step [130/391],                 Loss: 0.38254, Train_Acc:87.26%
Epoch [130/300], Step [140/391],                 Loss: 0.37972, Train_Acc:87.38%
Epoch [130/300], Step [150/391],                 Loss: 0.37904, Train_Acc:87.39%
Epoch [130/300], Step [160/391],                 Loss: 0.37702, Train_Acc:87.43%
Epoch [130/300], Step [170/391],                 Loss: 0.37727, Train_Acc:87.42%
Epoch [130/300], Step [180/391],                 Loss: 0.38017, Train_Acc:87.27%
Epoch [130/300], Step [190/391],                 Loss: 0.38158, Train_Acc:87.20%
Epoch [130/300], Step [200/391],                 Loss: 0.38291, Train_Acc:87.15%
Epoch [130/300], Step [210/391],                 Loss: 0.38191, Train_Acc:87.20%
Epoch [130/300], Step [220/391],                 Loss: 0.38204, Train_Acc:87.18%
Epoch [130/300], Step [230/391],                 Loss: 0.38060, Train_Acc:87.20%
Epoch [130/300], Step [240/391],                 Loss: 0.37825, Train_Acc:87.28%
Epoch [130/300], Step [250/391],                 Loss: 0.37640, Train_Acc:87.34%
Epoch [130/300], Step [260/391],                 Loss: 0.37865, Train_Acc:87.27%
Epoch [130/300], Step [270/391],                 Loss: 0.38093, Train_Acc:87.18%
Epoch [130/300], Step [280/391],                 Loss: 0.38162, Train_Acc:87.16%
Epoch [130/300], Step [290/391],                 Loss: 0.38225, Train_Acc:87.13%
Epoch [130/300], Step [300/391],                 Loss: 0.38193, Train_Acc:87.14%
Epoch [130/300], Step [310/391],                 Loss: 0.38183, Train_Acc:87.12%
Epoch [130/300], Step [320/391],                 Loss: 0.38184, Train_Acc:87.11%
Epoch [130/300], Step [330/391],                 Loss: 0.38079, Train_Acc:87.13%
Epoch [130/300], Step [340/391],                 Loss: 0.38025, Train_Acc:87.14%
Epoch [130/300], Step [350/391],                 Loss: 0.37938, Train_Acc:87.16%
Epoch [130/300], Step [360/391],                 Loss: 0.37958, Train_Acc:87.14%
Epoch [130/300], Step [370/391],                 Loss: 0.37950, Train_Acc:87.15%
Epoch [130/300], Step [380/391],                 Loss: 0.37883, Train_Acc:87.18%
Epoch [130/300], Step [390/391],                 Loss: 0.37853, Train_Acc:87.19%
Accuary on test images:76.32%
Epoch [131/300], Step [10/391],                 Loss: 0.38079, Train_Acc:87.19%
Epoch [131/300], Step [20/391],                 Loss: 0.37701, Train_Acc:87.58%
Epoch [131/300], Step [30/391],                 Loss: 0.37286, Train_Acc:87.53%
Epoch [131/300], Step [40/391],                 Loss: 0.36341, Train_Acc:88.05%
Epoch [131/300], Step [50/391],                 Loss: 0.36839, Train_Acc:87.64%
Epoch [131/300], Step [60/391],                 Loss: 0.37990, Train_Acc:87.33%
Epoch [131/300], Step [70/391],                 Loss: 0.37798, Train_Acc:87.44%
Epoch [131/300], Step [80/391],                 Loss: 0.38330, Train_Acc:87.29%
Epoch [131/300], Step [90/391],                 Loss: 0.38512, Train_Acc:87.26%
Epoch [131/300], Step [100/391],                 Loss: 0.38066, Train_Acc:87.41%
Epoch [131/300], Step [110/391],                 Loss: 0.37931, Train_Acc:87.47%
Epoch [131/300], Step [120/391],                 Loss: 0.37727, Train_Acc:87.51%
Epoch [131/300], Step [130/391],                 Loss: 0.37834, Train_Acc:87.46%
Epoch [131/300], Step [140/391],                 Loss: 0.37676, Train_Acc:87.59%
Epoch [131/300], Step [150/391],                 Loss: 0.37845, Train_Acc:87.47%
Epoch [131/300], Step [160/391],                 Loss: 0.37692, Train_Acc:87.51%
Epoch [131/300], Step [170/391],                 Loss: 0.37612, Train_Acc:87.53%
Epoch [131/300], Step [180/391],                 Loss: 0.37531, Train_Acc:87.49%
Epoch [131/300], Step [190/391],                 Loss: 0.37438, Train_Acc:87.50%
Epoch [131/300], Step [200/391],                 Loss: 0.37721, Train_Acc:87.35%
Epoch [131/300], Step [210/391],                 Loss: 0.37635, Train_Acc:87.33%
Epoch [131/300], Step [220/391],                 Loss: 0.37682, Train_Acc:87.35%
Epoch [131/300], Step [230/391],                 Loss: 0.37491, Train_Acc:87.40%
Epoch [131/300], Step [240/391],                 Loss: 0.37302, Train_Acc:87.44%
Epoch [131/300], Step [250/391],                 Loss: 0.37254, Train_Acc:87.47%
Epoch [131/300], Step [260/391],                 Loss: 0.37426, Train_Acc:87.42%
Epoch [131/300], Step [270/391],                 Loss: 0.37580, Train_Acc:87.36%
Epoch [131/300], Step [280/391],                 Loss: 0.37500, Train_Acc:87.38%
Epoch [131/300], Step [290/391],                 Loss: 0.37394, Train_Acc:87.42%
Epoch [131/300], Step [300/391],                 Loss: 0.37502, Train_Acc:87.39%
Epoch [131/300], Step [310/391],                 Loss: 0.37404, Train_Acc:87.39%
Epoch [131/300], Step [320/391],                 Loss: 0.37595, Train_Acc:87.30%
Epoch [131/300], Step [330/391],                 Loss: 0.37570, Train_Acc:87.29%
Epoch [131/300], Step [340/391],                 Loss: 0.37543, Train_Acc:87.33%
Epoch [131/300], Step [350/391],                 Loss: 0.37552, Train_Acc:87.33%
Epoch [131/300], Step [360/391],                 Loss: 0.37545, Train_Acc:87.32%
Epoch [131/300], Step [370/391],                 Loss: 0.37533, Train_Acc:87.28%
Epoch [131/300], Step [380/391],                 Loss: 0.37431, Train_Acc:87.33%
Epoch [131/300], Step [390/391],                 Loss: 0.37442, Train_Acc:87.33%
Accuary on test images:72.04%
Epoch [132/300], Step [10/391],                 Loss: 0.37320, Train_Acc:87.89%
Epoch [132/300], Step [20/391],                 Loss: 0.37258, Train_Acc:87.42%
Epoch [132/300], Step [30/391],                 Loss: 0.35703, Train_Acc:88.05%
Epoch [132/300], Step [40/391],                 Loss: 0.36156, Train_Acc:88.09%
Epoch [132/300], Step [50/391],                 Loss: 0.36231, Train_Acc:87.78%
Epoch [132/300], Step [60/391],                 Loss: 0.36823, Train_Acc:87.57%
Epoch [132/300], Step [70/391],                 Loss: 0.36438, Train_Acc:87.73%
Epoch [132/300], Step [80/391],                 Loss: 0.36498, Train_Acc:87.75%
Epoch [132/300], Step [90/391],                 Loss: 0.36810, Train_Acc:87.58%
Epoch [132/300], Step [100/391],                 Loss: 0.36834, Train_Acc:87.51%
Epoch [132/300], Step [110/391],                 Loss: 0.37397, Train_Acc:87.43%
Epoch [132/300], Step [120/391],                 Loss: 0.37592, Train_Acc:87.25%
Epoch [132/300], Step [130/391],                 Loss: 0.37856, Train_Acc:87.15%
Epoch [132/300], Step [140/391],                 Loss: 0.37691, Train_Acc:87.20%
Epoch [132/300], Step [150/391],                 Loss: 0.37610, Train_Acc:87.21%
Epoch [132/300], Step [160/391],                 Loss: 0.37500, Train_Acc:87.24%
Epoch [132/300], Step [170/391],                 Loss: 0.37500, Train_Acc:87.27%
Epoch [132/300], Step [180/391],                 Loss: 0.37534, Train_Acc:87.22%
Epoch [132/300], Step [190/391],                 Loss: 0.37650, Train_Acc:87.19%
Epoch [132/300], Step [200/391],                 Loss: 0.37763, Train_Acc:87.14%
Epoch [132/300], Step [210/391],                 Loss: 0.37754, Train_Acc:87.13%
Epoch [132/300], Step [220/391],                 Loss: 0.37655, Train_Acc:87.20%
Epoch [132/300], Step [230/391],                 Loss: 0.37446, Train_Acc:87.25%
Epoch [132/300], Step [240/391],                 Loss: 0.37213, Train_Acc:87.35%
Epoch [132/300], Step [250/391],                 Loss: 0.37154, Train_Acc:87.38%
Epoch [132/300], Step [260/391],                 Loss: 0.37399, Train_Acc:87.31%
Epoch [132/300], Step [270/391],                 Loss: 0.37576, Train_Acc:87.24%
Epoch [132/300], Step [280/391],                 Loss: 0.37569, Train_Acc:87.21%
Epoch [132/300], Step [290/391],                 Loss: 0.37607, Train_Acc:87.21%
Epoch [132/300], Step [300/391],                 Loss: 0.37505, Train_Acc:87.20%
Epoch [132/300], Step [310/391],                 Loss: 0.37300, Train_Acc:87.26%
Epoch [132/300], Step [320/391],                 Loss: 0.37215, Train_Acc:87.29%
Epoch [132/300], Step [330/391],                 Loss: 0.37157, Train_Acc:87.29%
Epoch [132/300], Step [340/391],                 Loss: 0.37102, Train_Acc:87.34%
Epoch [132/300], Step [350/391],                 Loss: 0.37216, Train_Acc:87.31%
Epoch [132/300], Step [360/391],                 Loss: 0.37223, Train_Acc:87.28%
Epoch [132/300], Step [370/391],                 Loss: 0.37274, Train_Acc:87.25%
Epoch [132/300], Step [380/391],                 Loss: 0.37240, Train_Acc:87.24%
Epoch [132/300], Step [390/391],                 Loss: 0.37217, Train_Acc:87.22%
Accuary on test images:76.64%
Epoch [133/300], Step [10/391],                 Loss: 0.38509, Train_Acc:87.66%
Epoch [133/300], Step [20/391],                 Loss: 0.38970, Train_Acc:87.07%
Epoch [133/300], Step [30/391],                 Loss: 0.38831, Train_Acc:86.88%
Epoch [133/300], Step [40/391],                 Loss: 0.38326, Train_Acc:86.97%
Epoch [133/300], Step [50/391],                 Loss: 0.37814, Train_Acc:87.09%
Epoch [133/300], Step [60/391],                 Loss: 0.38523, Train_Acc:86.88%
Epoch [133/300], Step [70/391],                 Loss: 0.38130, Train_Acc:87.09%
Epoch [133/300], Step [80/391],                 Loss: 0.37819, Train_Acc:87.19%
Epoch [133/300], Step [90/391],                 Loss: 0.37899, Train_Acc:87.21%
Epoch [133/300], Step [100/391],                 Loss: 0.37689, Train_Acc:87.31%
Epoch [133/300], Step [110/391],                 Loss: 0.37659, Train_Acc:87.20%
Epoch [133/300], Step [120/391],                 Loss: 0.37876, Train_Acc:87.05%
Epoch [133/300], Step [130/391],                 Loss: 0.38015, Train_Acc:87.00%
Epoch [133/300], Step [140/391],                 Loss: 0.37812, Train_Acc:87.10%
Epoch [133/300], Step [150/391],                 Loss: 0.37824, Train_Acc:87.10%
Epoch [133/300], Step [160/391],                 Loss: 0.37707, Train_Acc:87.14%
Epoch [133/300], Step [170/391],                 Loss: 0.37548, Train_Acc:87.20%
Epoch [133/300], Step [180/391],                 Loss: 0.37464, Train_Acc:87.23%
Epoch [133/300], Step [190/391],                 Loss: 0.37524, Train_Acc:87.26%
Epoch [133/300], Step [200/391],                 Loss: 0.37812, Train_Acc:87.17%
Epoch [133/300], Step [210/391],                 Loss: 0.37966, Train_Acc:87.14%
Epoch [133/300], Step [220/391],                 Loss: 0.38097, Train_Acc:87.04%
Epoch [133/300], Step [230/391],                 Loss: 0.38219, Train_Acc:86.98%
Epoch [133/300], Step [240/391],                 Loss: 0.38110, Train_Acc:87.04%
Epoch [133/300], Step [250/391],                 Loss: 0.38038, Train_Acc:87.06%
Epoch [133/300], Step [260/391],                 Loss: 0.38218, Train_Acc:87.00%
Epoch [133/300], Step [270/391],                 Loss: 0.38452, Train_Acc:86.91%
Epoch [133/300], Step [280/391],                 Loss: 0.38492, Train_Acc:86.92%
Epoch [133/300], Step [290/391],                 Loss: 0.38440, Train_Acc:86.93%
Epoch [133/300], Step [300/391],                 Loss: 0.38391, Train_Acc:86.96%
Epoch [133/300], Step [310/391],                 Loss: 0.38267, Train_Acc:87.02%
Epoch [133/300], Step [320/391],                 Loss: 0.38235, Train_Acc:87.02%
Epoch [133/300], Step [330/391],                 Loss: 0.38048, Train_Acc:87.09%
Epoch [133/300], Step [340/391],                 Loss: 0.37908, Train_Acc:87.12%
Epoch [133/300], Step [350/391],                 Loss: 0.37786, Train_Acc:87.16%
Epoch [133/300], Step [360/391],                 Loss: 0.37734, Train_Acc:87.16%
Epoch [133/300], Step [370/391],                 Loss: 0.37804, Train_Acc:87.11%
Epoch [133/300], Step [380/391],                 Loss: 0.37843, Train_Acc:87.09%
Epoch [133/300], Step [390/391],                 Loss: 0.37748, Train_Acc:87.15%
Accuary on test images:73.66%
Epoch [134/300], Step [10/391],                 Loss: 0.33583, Train_Acc:88.44%
Epoch [134/300], Step [20/391],                 Loss: 0.33615, Train_Acc:88.67%
Epoch [134/300], Step [30/391],                 Loss: 0.33286, Train_Acc:88.78%
Epoch [134/300], Step [40/391],                 Loss: 0.33924, Train_Acc:88.65%
Epoch [134/300], Step [50/391],                 Loss: 0.34893, Train_Acc:88.31%
Epoch [134/300], Step [60/391],                 Loss: 0.36654, Train_Acc:87.80%
Epoch [134/300], Step [70/391],                 Loss: 0.36890, Train_Acc:87.75%
Epoch [134/300], Step [80/391],                 Loss: 0.37164, Train_Acc:87.71%
Epoch [134/300], Step [90/391],                 Loss: 0.37447, Train_Acc:87.54%
Epoch [134/300], Step [100/391],                 Loss: 0.37241, Train_Acc:87.62%
Epoch [134/300], Step [110/391],                 Loss: 0.37192, Train_Acc:87.64%
Epoch [134/300], Step [120/391],                 Loss: 0.36959, Train_Acc:87.71%
Epoch [134/300], Step [130/391],                 Loss: 0.37081, Train_Acc:87.69%
Epoch [134/300], Step [140/391],                 Loss: 0.37057, Train_Acc:87.66%
Epoch [134/300], Step [150/391],                 Loss: 0.36987, Train_Acc:87.69%
Epoch [134/300], Step [160/391],                 Loss: 0.36887, Train_Acc:87.70%
Epoch [134/300], Step [170/391],                 Loss: 0.36975, Train_Acc:87.66%
Epoch [134/300], Step [180/391],                 Loss: 0.37091, Train_Acc:87.63%
Epoch [134/300], Step [190/391],                 Loss: 0.37094, Train_Acc:87.62%
Epoch [134/300], Step [200/391],                 Loss: 0.37388, Train_Acc:87.49%
Epoch [134/300], Step [210/391],                 Loss: 0.37478, Train_Acc:87.45%
Epoch [134/300], Step [220/391],                 Loss: 0.37466, Train_Acc:87.47%
Epoch [134/300], Step [230/391],                 Loss: 0.37454, Train_Acc:87.44%
Epoch [134/300], Step [240/391],                 Loss: 0.37499, Train_Acc:87.37%
Epoch [134/300], Step [250/391],                 Loss: 0.37574, Train_Acc:87.32%
Epoch [134/300], Step [260/391],                 Loss: 0.37783, Train_Acc:87.29%
Epoch [134/300], Step [270/391],                 Loss: 0.37899, Train_Acc:87.24%
Epoch [134/300], Step [280/391],                 Loss: 0.37891, Train_Acc:87.22%
Epoch [134/300], Step [290/391],                 Loss: 0.37867, Train_Acc:87.23%
Epoch [134/300], Step [300/391],                 Loss: 0.37766, Train_Acc:87.25%
Epoch [134/300], Step [310/391],                 Loss: 0.37611, Train_Acc:87.31%
Epoch [134/300], Step [320/391],                 Loss: 0.37629, Train_Acc:87.33%
Epoch [134/300], Step [330/391],                 Loss: 0.37552, Train_Acc:87.36%
Epoch [134/300], Step [340/391],                 Loss: 0.37512, Train_Acc:87.36%
Epoch [134/300], Step [350/391],                 Loss: 0.37392, Train_Acc:87.40%
Epoch [134/300], Step [360/391],                 Loss: 0.37422, Train_Acc:87.38%
Epoch [134/300], Step [370/391],                 Loss: 0.37353, Train_Acc:87.39%
Epoch [134/300], Step [380/391],                 Loss: 0.37259, Train_Acc:87.42%
Epoch [134/300], Step [390/391],                 Loss: 0.37214, Train_Acc:87.44%
Accuary on test images:71.40%
Epoch [135/300], Step [10/391],                 Loss: 0.38162, Train_Acc:86.64%
Epoch [135/300], Step [20/391],                 Loss: 0.37469, Train_Acc:87.03%
Epoch [135/300], Step [30/391],                 Loss: 0.37082, Train_Acc:87.21%
Epoch [135/300], Step [40/391],                 Loss: 0.36776, Train_Acc:87.40%
Epoch [135/300], Step [50/391],                 Loss: 0.37033, Train_Acc:87.52%
Epoch [135/300], Step [60/391],                 Loss: 0.37627, Train_Acc:87.27%
Epoch [135/300], Step [70/391],                 Loss: 0.37232, Train_Acc:87.41%
Epoch [135/300], Step [80/391],                 Loss: 0.37223, Train_Acc:87.43%
Epoch [135/300], Step [90/391],                 Loss: 0.37730, Train_Acc:87.20%
Epoch [135/300], Step [100/391],                 Loss: 0.37430, Train_Acc:87.27%
Epoch [135/300], Step [110/391],                 Loss: 0.37621, Train_Acc:87.15%
Epoch [135/300], Step [120/391],                 Loss: 0.37753, Train_Acc:87.15%
Epoch [135/300], Step [130/391],                 Loss: 0.37854, Train_Acc:87.21%
Epoch [135/300], Step [140/391],                 Loss: 0.37678, Train_Acc:87.28%
Epoch [135/300], Step [150/391],                 Loss: 0.37490, Train_Acc:87.29%
Epoch [135/300], Step [160/391],                 Loss: 0.37295, Train_Acc:87.33%
Epoch [135/300], Step [170/391],                 Loss: 0.37277, Train_Acc:87.32%
Epoch [135/300], Step [180/391],                 Loss: 0.37484, Train_Acc:87.26%
Epoch [135/300], Step [190/391],                 Loss: 0.37693, Train_Acc:87.15%
Epoch [135/300], Step [200/391],                 Loss: 0.37926, Train_Acc:87.07%
Epoch [135/300], Step [210/391],                 Loss: 0.37917, Train_Acc:87.03%
Epoch [135/300], Step [220/391],                 Loss: 0.37818, Train_Acc:87.09%
Epoch [135/300], Step [230/391],                 Loss: 0.37615, Train_Acc:87.13%
Epoch [135/300], Step [240/391],                 Loss: 0.37488, Train_Acc:87.21%
Epoch [135/300], Step [250/391],                 Loss: 0.37476, Train_Acc:87.20%
Epoch [135/300], Step [260/391],                 Loss: 0.37683, Train_Acc:87.11%
Epoch [135/300], Step [270/391],                 Loss: 0.37900, Train_Acc:87.05%
Epoch [135/300], Step [280/391],                 Loss: 0.37912, Train_Acc:87.06%
Epoch [135/300], Step [290/391],                 Loss: 0.38003, Train_Acc:87.03%
Epoch [135/300], Step [300/391],                 Loss: 0.38029, Train_Acc:87.01%
Epoch [135/300], Step [310/391],                 Loss: 0.38018, Train_Acc:87.01%
Epoch [135/300], Step [320/391],                 Loss: 0.38034, Train_Acc:87.03%
Epoch [135/300], Step [330/391],                 Loss: 0.37922, Train_Acc:87.07%
Epoch [135/300], Step [340/391],                 Loss: 0.37791, Train_Acc:87.11%
Epoch [135/300], Step [350/391],                 Loss: 0.37699, Train_Acc:87.14%
Epoch [135/300], Step [360/391],                 Loss: 0.37546, Train_Acc:87.17%
Epoch [135/300], Step [370/391],                 Loss: 0.37517, Train_Acc:87.14%
Epoch [135/300], Step [380/391],                 Loss: 0.37460, Train_Acc:87.18%
Epoch [135/300], Step [390/391],                 Loss: 0.37374, Train_Acc:87.20%
Accuary on test images:80.38%
Epoch [136/300], Step [10/391],                 Loss: 0.32893, Train_Acc:88.36%
Epoch [136/300], Step [20/391],                 Loss: 0.34206, Train_Acc:88.16%
Epoch [136/300], Step [30/391],                 Loss: 0.34385, Train_Acc:88.46%
Epoch [136/300], Step [40/391],                 Loss: 0.35898, Train_Acc:88.14%
Epoch [136/300], Step [50/391],                 Loss: 0.36419, Train_Acc:87.83%
Epoch [136/300], Step [60/391],                 Loss: 0.37378, Train_Acc:87.46%
Epoch [136/300], Step [70/391],                 Loss: 0.37725, Train_Acc:87.31%
Epoch [136/300], Step [80/391],                 Loss: 0.37630, Train_Acc:87.39%
Epoch [136/300], Step [90/391],                 Loss: 0.37914, Train_Acc:87.28%
Epoch [136/300], Step [100/391],                 Loss: 0.37972, Train_Acc:87.23%
Epoch [136/300], Step [110/391],                 Loss: 0.38048, Train_Acc:87.28%
Epoch [136/300], Step [120/391],                 Loss: 0.38000, Train_Acc:87.24%
Epoch [136/300], Step [130/391],                 Loss: 0.38111, Train_Acc:87.20%
Epoch [136/300], Step [140/391],                 Loss: 0.38070, Train_Acc:87.19%
Epoch [136/300], Step [150/391],                 Loss: 0.38123, Train_Acc:87.12%
Epoch [136/300], Step [160/391],                 Loss: 0.37880, Train_Acc:87.23%
Epoch [136/300], Step [170/391],                 Loss: 0.37952, Train_Acc:87.28%
Epoch [136/300], Step [180/391],                 Loss: 0.37823, Train_Acc:87.30%
Epoch [136/300], Step [190/391],                 Loss: 0.37712, Train_Acc:87.35%
Epoch [136/300], Step [200/391],                 Loss: 0.37765, Train_Acc:87.30%
Epoch [136/300], Step [210/391],                 Loss: 0.37646, Train_Acc:87.30%
Epoch [136/300], Step [220/391],                 Loss: 0.37635, Train_Acc:87.30%
Epoch [136/300], Step [230/391],                 Loss: 0.37351, Train_Acc:87.41%
Epoch [136/300], Step [240/391],                 Loss: 0.37305, Train_Acc:87.40%
Epoch [136/300], Step [250/391],                 Loss: 0.37214, Train_Acc:87.43%
Epoch [136/300], Step [260/391],                 Loss: 0.37358, Train_Acc:87.40%
Epoch [136/300], Step [270/391],                 Loss: 0.37428, Train_Acc:87.38%
Epoch [136/300], Step [280/391],                 Loss: 0.37363, Train_Acc:87.40%
Epoch [136/300], Step [290/391],                 Loss: 0.37255, Train_Acc:87.42%
Epoch [136/300], Step [300/391],                 Loss: 0.37158, Train_Acc:87.45%
Epoch [136/300], Step [310/391],                 Loss: 0.37079, Train_Acc:87.48%
Epoch [136/300], Step [320/391],                 Loss: 0.37106, Train_Acc:87.47%
Epoch [136/300], Step [330/391],                 Loss: 0.37042, Train_Acc:87.47%
Epoch [136/300], Step [340/391],                 Loss: 0.37021, Train_Acc:87.47%
Epoch [136/300], Step [350/391],                 Loss: 0.37049, Train_Acc:87.48%
Epoch [136/300], Step [360/391],                 Loss: 0.37089, Train_Acc:87.45%
Epoch [136/300], Step [370/391],                 Loss: 0.37153, Train_Acc:87.44%
Epoch [136/300], Step [380/391],                 Loss: 0.37107, Train_Acc:87.44%
Epoch [136/300], Step [390/391],                 Loss: 0.37087, Train_Acc:87.43%
Accuary on test images:73.00%
Epoch [137/300], Step [10/391],                 Loss: 0.39517, Train_Acc:86.72%
Epoch [137/300], Step [20/391],                 Loss: 0.38065, Train_Acc:87.27%
Epoch [137/300], Step [30/391],                 Loss: 0.35943, Train_Acc:87.81%
Epoch [137/300], Step [40/391],                 Loss: 0.36132, Train_Acc:87.93%
Epoch [137/300], Step [50/391],                 Loss: 0.36110, Train_Acc:87.95%
Epoch [137/300], Step [60/391],                 Loss: 0.37129, Train_Acc:87.45%
Epoch [137/300], Step [70/391],                 Loss: 0.37382, Train_Acc:87.39%
Epoch [137/300], Step [80/391],                 Loss: 0.37102, Train_Acc:87.44%
Epoch [137/300], Step [90/391],                 Loss: 0.37608, Train_Acc:87.28%
Epoch [137/300], Step [100/391],                 Loss: 0.37311, Train_Acc:87.38%
Epoch [137/300], Step [110/391],                 Loss: 0.37155, Train_Acc:87.38%
Epoch [137/300], Step [120/391],                 Loss: 0.37391, Train_Acc:87.30%
Epoch [137/300], Step [130/391],                 Loss: 0.37905, Train_Acc:87.12%
Epoch [137/300], Step [140/391],                 Loss: 0.37825, Train_Acc:87.16%
Epoch [137/300], Step [150/391],                 Loss: 0.38021, Train_Acc:87.03%
Epoch [137/300], Step [160/391],                 Loss: 0.37825, Train_Acc:87.16%
Epoch [137/300], Step [170/391],                 Loss: 0.37611, Train_Acc:87.24%
Epoch [137/300], Step [180/391],                 Loss: 0.37474, Train_Acc:87.24%
Epoch [137/300], Step [190/391],                 Loss: 0.37500, Train_Acc:87.24%
Epoch [137/300], Step [200/391],                 Loss: 0.37500, Train_Acc:87.23%
Epoch [137/300], Step [210/391],                 Loss: 0.37526, Train_Acc:87.22%
Epoch [137/300], Step [220/391],                 Loss: 0.37638, Train_Acc:87.23%
Epoch [137/300], Step [230/391],                 Loss: 0.37613, Train_Acc:87.24%
Epoch [137/300], Step [240/391],                 Loss: 0.37547, Train_Acc:87.25%
Epoch [137/300], Step [250/391],                 Loss: 0.37540, Train_Acc:87.22%
Epoch [137/300], Step [260/391],                 Loss: 0.37785, Train_Acc:87.15%
Epoch [137/300], Step [270/391],                 Loss: 0.37875, Train_Acc:87.13%
Epoch [137/300], Step [280/391],                 Loss: 0.37849, Train_Acc:87.11%
Epoch [137/300], Step [290/391],                 Loss: 0.37877, Train_Acc:87.09%
Epoch [137/300], Step [300/391],                 Loss: 0.37903, Train_Acc:87.06%
Epoch [137/300], Step [310/391],                 Loss: 0.37828, Train_Acc:87.07%
Epoch [137/300], Step [320/391],                 Loss: 0.37996, Train_Acc:87.03%
Epoch [137/300], Step [330/391],                 Loss: 0.37883, Train_Acc:87.08%
Epoch [137/300], Step [340/391],                 Loss: 0.37742, Train_Acc:87.14%
Epoch [137/300], Step [350/391],                 Loss: 0.37591, Train_Acc:87.22%
Epoch [137/300], Step [360/391],                 Loss: 0.37548, Train_Acc:87.23%
Epoch [137/300], Step [370/391],                 Loss: 0.37502, Train_Acc:87.24%
Epoch [137/300], Step [380/391],                 Loss: 0.37486, Train_Acc:87.26%
Epoch [137/300], Step [390/391],                 Loss: 0.37455, Train_Acc:87.27%
Accuary on test images:67.40%
Epoch [138/300], Step [10/391],                 Loss: 0.38512, Train_Acc:86.48%
Epoch [138/300], Step [20/391],                 Loss: 0.38403, Train_Acc:86.64%
Epoch [138/300], Step [30/391],                 Loss: 0.37358, Train_Acc:87.06%
Epoch [138/300], Step [40/391],                 Loss: 0.36744, Train_Acc:87.52%
Epoch [138/300], Step [50/391],                 Loss: 0.37002, Train_Acc:87.33%
Epoch [138/300], Step [60/391],                 Loss: 0.37271, Train_Acc:87.27%
Epoch [138/300], Step [70/391],                 Loss: 0.37071, Train_Acc:87.51%
Epoch [138/300], Step [80/391],                 Loss: 0.37323, Train_Acc:87.40%
Epoch [138/300], Step [90/391],                 Loss: 0.37846, Train_Acc:87.16%
Epoch [138/300], Step [100/391],                 Loss: 0.37704, Train_Acc:87.27%
Epoch [138/300], Step [110/391],                 Loss: 0.37792, Train_Acc:87.17%
Epoch [138/300], Step [120/391],                 Loss: 0.38000, Train_Acc:87.09%
Epoch [138/300], Step [130/391],                 Loss: 0.38062, Train_Acc:87.07%
Epoch [138/300], Step [140/391],                 Loss: 0.38012, Train_Acc:87.13%
Epoch [138/300], Step [150/391],                 Loss: 0.38044, Train_Acc:87.15%
Epoch [138/300], Step [160/391],                 Loss: 0.37963, Train_Acc:87.13%
Epoch [138/300], Step [170/391],                 Loss: 0.37909, Train_Acc:87.16%
Epoch [138/300], Step [180/391],                 Loss: 0.37805, Train_Acc:87.19%
Epoch [138/300], Step [190/391],                 Loss: 0.37714, Train_Acc:87.21%
Epoch [138/300], Step [200/391],                 Loss: 0.37832, Train_Acc:87.18%
Epoch [138/300], Step [210/391],                 Loss: 0.37872, Train_Acc:87.12%
Epoch [138/300], Step [220/391],                 Loss: 0.37821, Train_Acc:87.17%
Epoch [138/300], Step [230/391],                 Loss: 0.37699, Train_Acc:87.19%
Epoch [138/300], Step [240/391],                 Loss: 0.37519, Train_Acc:87.29%
Epoch [138/300], Step [250/391],                 Loss: 0.37452, Train_Acc:87.34%
Epoch [138/300], Step [260/391],                 Loss: 0.37642, Train_Acc:87.28%
Epoch [138/300], Step [270/391],                 Loss: 0.37813, Train_Acc:87.22%
Epoch [138/300], Step [280/391],                 Loss: 0.37860, Train_Acc:87.15%
Epoch [138/300], Step [290/391],                 Loss: 0.37841, Train_Acc:87.14%
Epoch [138/300], Step [300/391],                 Loss: 0.37898, Train_Acc:87.11%
Epoch [138/300], Step [310/391],                 Loss: 0.37856, Train_Acc:87.12%
Epoch [138/300], Step [320/391],                 Loss: 0.37875, Train_Acc:87.09%
Epoch [138/300], Step [330/391],                 Loss: 0.37733, Train_Acc:87.14%
Epoch [138/300], Step [340/391],                 Loss: 0.37668, Train_Acc:87.19%
Epoch [138/300], Step [350/391],                 Loss: 0.37593, Train_Acc:87.21%
Epoch [138/300], Step [360/391],                 Loss: 0.37601, Train_Acc:87.19%
Epoch [138/300], Step [370/391],                 Loss: 0.37516, Train_Acc:87.21%
Epoch [138/300], Step [380/391],                 Loss: 0.37475, Train_Acc:87.25%
Epoch [138/300], Step [390/391],                 Loss: 0.37456, Train_Acc:87.26%
Accuary on test images:78.70%
Epoch [139/300], Step [10/391],                 Loss: 0.35717, Train_Acc:88.20%
Epoch [139/300], Step [20/391],                 Loss: 0.35638, Train_Acc:88.09%
Epoch [139/300], Step [30/391],                 Loss: 0.34512, Train_Acc:88.39%
Epoch [139/300], Step [40/391],                 Loss: 0.35303, Train_Acc:87.95%
Epoch [139/300], Step [50/391],                 Loss: 0.35851, Train_Acc:87.59%
Epoch [139/300], Step [60/391],                 Loss: 0.36862, Train_Acc:87.17%
Epoch [139/300], Step [70/391],                 Loss: 0.37017, Train_Acc:87.22%
Epoch [139/300], Step [80/391],                 Loss: 0.37426, Train_Acc:87.14%
Epoch [139/300], Step [90/391],                 Loss: 0.37866, Train_Acc:86.94%
Epoch [139/300], Step [100/391],                 Loss: 0.37597, Train_Acc:87.13%
Epoch [139/300], Step [110/391],                 Loss: 0.37553, Train_Acc:87.19%
Epoch [139/300], Step [120/391],                 Loss: 0.37793, Train_Acc:87.05%
Epoch [139/300], Step [130/391],                 Loss: 0.37945, Train_Acc:87.07%
Epoch [139/300], Step [140/391],                 Loss: 0.38031, Train_Acc:87.05%
Epoch [139/300], Step [150/391],                 Loss: 0.37887, Train_Acc:87.10%
Epoch [139/300], Step [160/391],                 Loss: 0.37623, Train_Acc:87.24%
Epoch [139/300], Step [170/391],                 Loss: 0.37509, Train_Acc:87.27%
Epoch [139/300], Step [180/391],                 Loss: 0.37324, Train_Acc:87.36%
Epoch [139/300], Step [190/391],                 Loss: 0.37528, Train_Acc:87.31%
Epoch [139/300], Step [200/391],                 Loss: 0.37801, Train_Acc:87.23%
Epoch [139/300], Step [210/391],                 Loss: 0.37794, Train_Acc:87.22%
Epoch [139/300], Step [220/391],                 Loss: 0.37669, Train_Acc:87.28%
Epoch [139/300], Step [230/391],                 Loss: 0.37582, Train_Acc:87.29%
Epoch [139/300], Step [240/391],                 Loss: 0.37401, Train_Acc:87.37%
Epoch [139/300], Step [250/391],                 Loss: 0.37393, Train_Acc:87.34%
Epoch [139/300], Step [260/391],                 Loss: 0.37674, Train_Acc:87.26%
Epoch [139/300], Step [270/391],                 Loss: 0.37812, Train_Acc:87.20%
Epoch [139/300], Step [280/391],                 Loss: 0.37756, Train_Acc:87.20%
Epoch [139/300], Step [290/391],                 Loss: 0.37736, Train_Acc:87.21%
Epoch [139/300], Step [300/391],                 Loss: 0.37667, Train_Acc:87.19%
Epoch [139/300], Step [310/391],                 Loss: 0.37568, Train_Acc:87.22%
Epoch [139/300], Step [320/391],                 Loss: 0.37607, Train_Acc:87.20%
Epoch [139/300], Step [330/391],                 Loss: 0.37479, Train_Acc:87.28%
Epoch [139/300], Step [340/391],                 Loss: 0.37472, Train_Acc:87.28%
Epoch [139/300], Step [350/391],                 Loss: 0.37367, Train_Acc:87.32%
Epoch [139/300], Step [360/391],                 Loss: 0.37399, Train_Acc:87.26%
Epoch [139/300], Step [370/391],                 Loss: 0.37403, Train_Acc:87.25%
Epoch [139/300], Step [380/391],                 Loss: 0.37366, Train_Acc:87.26%
Epoch [139/300], Step [390/391],                 Loss: 0.37343, Train_Acc:87.26%
Accuary on test images:75.02%
Epoch [140/300], Step [10/391],                 Loss: 0.40017, Train_Acc:85.62%
Epoch [140/300], Step [20/391],                 Loss: 0.38917, Train_Acc:86.21%
Epoch [140/300], Step [30/391],                 Loss: 0.37875, Train_Acc:86.74%
Epoch [140/300], Step [40/391],                 Loss: 0.38619, Train_Acc:86.78%
Epoch [140/300], Step [50/391],                 Loss: 0.38378, Train_Acc:86.92%
Epoch [140/300], Step [60/391],                 Loss: 0.38487, Train_Acc:86.81%
Epoch [140/300], Step [70/391],                 Loss: 0.38376, Train_Acc:86.91%
Epoch [140/300], Step [80/391],                 Loss: 0.38174, Train_Acc:86.95%
Epoch [140/300], Step [90/391],                 Loss: 0.38464, Train_Acc:86.88%
Epoch [140/300], Step [100/391],                 Loss: 0.38132, Train_Acc:86.98%
Epoch [140/300], Step [110/391],                 Loss: 0.38464, Train_Acc:86.81%
Epoch [140/300], Step [120/391],                 Loss: 0.38283, Train_Acc:86.88%
Epoch [140/300], Step [130/391],                 Loss: 0.38258, Train_Acc:86.86%
Epoch [140/300], Step [140/391],                 Loss: 0.38240, Train_Acc:86.88%
Epoch [140/300], Step [150/391],                 Loss: 0.38210, Train_Acc:86.86%
Epoch [140/300], Step [160/391],                 Loss: 0.38059, Train_Acc:86.93%
Epoch [140/300], Step [170/391],                 Loss: 0.38138, Train_Acc:86.93%
Epoch [140/300], Step [180/391],                 Loss: 0.38026, Train_Acc:86.95%
Epoch [140/300], Step [190/391],                 Loss: 0.37990, Train_Acc:86.99%
Epoch [140/300], Step [200/391],                 Loss: 0.38047, Train_Acc:86.99%
Epoch [140/300], Step [210/391],                 Loss: 0.37975, Train_Acc:87.02%
Epoch [140/300], Step [220/391],                 Loss: 0.37844, Train_Acc:87.12%
Epoch [140/300], Step [230/391],                 Loss: 0.37632, Train_Acc:87.18%
Epoch [140/300], Step [240/391],                 Loss: 0.37495, Train_Acc:87.22%
Epoch [140/300], Step [250/391],                 Loss: 0.37511, Train_Acc:87.20%
Epoch [140/300], Step [260/391],                 Loss: 0.37693, Train_Acc:87.15%
Epoch [140/300], Step [270/391],                 Loss: 0.37880, Train_Acc:87.09%
Epoch [140/300], Step [280/391],                 Loss: 0.37950, Train_Acc:87.03%
Epoch [140/300], Step [290/391],                 Loss: 0.37932, Train_Acc:87.04%
Epoch [140/300], Step [300/391],                 Loss: 0.37971, Train_Acc:87.00%
Epoch [140/300], Step [310/391],                 Loss: 0.37849, Train_Acc:87.06%
Epoch [140/300], Step [320/391],                 Loss: 0.37930, Train_Acc:87.05%
Epoch [140/300], Step [330/391],                 Loss: 0.37845, Train_Acc:87.08%
Epoch [140/300], Step [340/391],                 Loss: 0.37813, Train_Acc:87.09%
Epoch [140/300], Step [350/391],                 Loss: 0.37741, Train_Acc:87.12%
Epoch [140/300], Step [360/391],                 Loss: 0.37686, Train_Acc:87.15%
Epoch [140/300], Step [370/391],                 Loss: 0.37720, Train_Acc:87.12%
Epoch [140/300], Step [380/391],                 Loss: 0.37707, Train_Acc:87.14%
Epoch [140/300], Step [390/391],                 Loss: 0.37584, Train_Acc:87.21%
Accuary on test images:74.84%
Epoch [141/300], Step [10/391],                 Loss: 0.35051, Train_Acc:89.45%
Epoch [141/300], Step [20/391],                 Loss: 0.34716, Train_Acc:89.18%
Epoch [141/300], Step [30/391],                 Loss: 0.33894, Train_Acc:89.14%
Epoch [141/300], Step [40/391],                 Loss: 0.34498, Train_Acc:88.89%
Epoch [141/300], Step [50/391],                 Loss: 0.34717, Train_Acc:88.66%
Epoch [141/300], Step [60/391],                 Loss: 0.35468, Train_Acc:88.39%
Epoch [141/300], Step [70/391],                 Loss: 0.35654, Train_Acc:88.31%
Epoch [141/300], Step [80/391],                 Loss: 0.35877, Train_Acc:88.24%
Epoch [141/300], Step [90/391],                 Loss: 0.36452, Train_Acc:88.05%
Epoch [141/300], Step [100/391],                 Loss: 0.36358, Train_Acc:87.96%
Epoch [141/300], Step [110/391],                 Loss: 0.36758, Train_Acc:87.91%
Epoch [141/300], Step [120/391],                 Loss: 0.37100, Train_Acc:87.70%
Epoch [141/300], Step [130/391],                 Loss: 0.37280, Train_Acc:87.61%
Epoch [141/300], Step [140/391],                 Loss: 0.37371, Train_Acc:87.55%
Epoch [141/300], Step [150/391],                 Loss: 0.37460, Train_Acc:87.52%
Epoch [141/300], Step [160/391],                 Loss: 0.37323, Train_Acc:87.58%
Epoch [141/300], Step [170/391],                 Loss: 0.37506, Train_Acc:87.58%
Epoch [141/300], Step [180/391],                 Loss: 0.37749, Train_Acc:87.51%
Epoch [141/300], Step [190/391],                 Loss: 0.37681, Train_Acc:87.53%
Epoch [141/300], Step [200/391],                 Loss: 0.37765, Train_Acc:87.45%
Epoch [141/300], Step [210/391],                 Loss: 0.37818, Train_Acc:87.41%
Epoch [141/300], Step [220/391],                 Loss: 0.37905, Train_Acc:87.36%
Epoch [141/300], Step [230/391],                 Loss: 0.37706, Train_Acc:87.46%
Epoch [141/300], Step [240/391],                 Loss: 0.37671, Train_Acc:87.48%
Epoch [141/300], Step [250/391],                 Loss: 0.37516, Train_Acc:87.51%
Epoch [141/300], Step [260/391],                 Loss: 0.37511, Train_Acc:87.52%
Epoch [141/300], Step [270/391],                 Loss: 0.37608, Train_Acc:87.47%
Epoch [141/300], Step [280/391],                 Loss: 0.37523, Train_Acc:87.49%
Epoch [141/300], Step [290/391],                 Loss: 0.37590, Train_Acc:87.48%
Epoch [141/300], Step [300/391],                 Loss: 0.37576, Train_Acc:87.46%
Epoch [141/300], Step [310/391],                 Loss: 0.37627, Train_Acc:87.43%
Epoch [141/300], Step [320/391],                 Loss: 0.37723, Train_Acc:87.40%
Epoch [141/300], Step [330/391],                 Loss: 0.37703, Train_Acc:87.39%
Epoch [141/300], Step [340/391],                 Loss: 0.37650, Train_Acc:87.40%
Epoch [141/300], Step [350/391],                 Loss: 0.37657, Train_Acc:87.39%
Epoch [141/300], Step [360/391],                 Loss: 0.37600, Train_Acc:87.40%
Epoch [141/300], Step [370/391],                 Loss: 0.37500, Train_Acc:87.43%
Epoch [141/300], Step [380/391],                 Loss: 0.37509, Train_Acc:87.41%
Epoch [141/300], Step [390/391],                 Loss: 0.37479, Train_Acc:87.42%
Accuary on test images:75.62%
Epoch [142/300], Step [10/391],                 Loss: 0.37692, Train_Acc:87.50%
Epoch [142/300], Step [20/391],                 Loss: 0.35553, Train_Acc:87.89%
Epoch [142/300], Step [30/391],                 Loss: 0.35132, Train_Acc:88.12%
Epoch [142/300], Step [40/391],                 Loss: 0.34950, Train_Acc:88.28%
Epoch [142/300], Step [50/391],                 Loss: 0.35565, Train_Acc:87.95%
Epoch [142/300], Step [60/391],                 Loss: 0.36594, Train_Acc:87.60%
Epoch [142/300], Step [70/391],                 Loss: 0.36915, Train_Acc:87.44%
Epoch [142/300], Step [80/391],                 Loss: 0.36954, Train_Acc:87.53%
Epoch [142/300], Step [90/391],                 Loss: 0.37486, Train_Acc:87.37%
Epoch [142/300], Step [100/391],                 Loss: 0.37474, Train_Acc:87.29%
Epoch [142/300], Step [110/391],                 Loss: 0.37615, Train_Acc:87.30%
Epoch [142/300], Step [120/391],                 Loss: 0.37737, Train_Acc:87.23%
Epoch [142/300], Step [130/391],                 Loss: 0.37799, Train_Acc:87.25%
Epoch [142/300], Step [140/391],                 Loss: 0.37724, Train_Acc:87.33%
Epoch [142/300], Step [150/391],                 Loss: 0.37736, Train_Acc:87.30%
Epoch [142/300], Step [160/391],                 Loss: 0.37614, Train_Acc:87.31%
Epoch [142/300], Step [170/391],                 Loss: 0.37633, Train_Acc:87.31%
Epoch [142/300], Step [180/391],                 Loss: 0.37584, Train_Acc:87.30%
Epoch [142/300], Step [190/391],                 Loss: 0.37564, Train_Acc:87.26%
Epoch [142/300], Step [200/391],                 Loss: 0.37608, Train_Acc:87.23%
Epoch [142/300], Step [210/391],                 Loss: 0.37616, Train_Acc:87.22%
Epoch [142/300], Step [220/391],                 Loss: 0.37738, Train_Acc:87.21%
Epoch [142/300], Step [230/391],                 Loss: 0.37553, Train_Acc:87.28%
Epoch [142/300], Step [240/391],                 Loss: 0.37493, Train_Acc:87.29%
Epoch [142/300], Step [250/391],                 Loss: 0.37497, Train_Acc:87.27%
Epoch [142/300], Step [260/391],                 Loss: 0.37751, Train_Acc:87.21%
Epoch [142/300], Step [270/391],                 Loss: 0.37826, Train_Acc:87.18%
Epoch [142/300], Step [280/391],                 Loss: 0.37869, Train_Acc:87.19%
Epoch [142/300], Step [290/391],                 Loss: 0.37911, Train_Acc:87.21%
Epoch [142/300], Step [300/391],                 Loss: 0.37900, Train_Acc:87.18%
Epoch [142/300], Step [310/391],                 Loss: 0.37823, Train_Acc:87.23%
Epoch [142/300], Step [320/391],                 Loss: 0.37864, Train_Acc:87.22%
Epoch [142/300], Step [330/391],                 Loss: 0.37812, Train_Acc:87.23%
Epoch [142/300], Step [340/391],                 Loss: 0.37790, Train_Acc:87.23%
Epoch [142/300], Step [350/391],                 Loss: 0.37746, Train_Acc:87.28%
Epoch [142/300], Step [360/391],                 Loss: 0.37797, Train_Acc:87.27%
Epoch [142/300], Step [370/391],                 Loss: 0.37863, Train_Acc:87.22%
Epoch [142/300], Step [380/391],                 Loss: 0.37792, Train_Acc:87.23%
Epoch [142/300], Step [390/391],                 Loss: 0.37767, Train_Acc:87.24%
Accuary on test images:79.98%
Epoch [143/300], Step [10/391],                 Loss: 0.37579, Train_Acc:86.95%
Epoch [143/300], Step [20/391],                 Loss: 0.38031, Train_Acc:86.80%
Epoch [143/300], Step [30/391],                 Loss: 0.37570, Train_Acc:87.03%
Epoch [143/300], Step [40/391],                 Loss: 0.37347, Train_Acc:87.23%
Epoch [143/300], Step [50/391],                 Loss: 0.37414, Train_Acc:87.25%
Epoch [143/300], Step [60/391],                 Loss: 0.38272, Train_Acc:86.93%
Epoch [143/300], Step [70/391],                 Loss: 0.38246, Train_Acc:87.10%
Epoch [143/300], Step [80/391],                 Loss: 0.38246, Train_Acc:87.04%
Epoch [143/300], Step [90/391],                 Loss: 0.38275, Train_Acc:86.96%
Epoch [143/300], Step [100/391],                 Loss: 0.38032, Train_Acc:87.05%
Epoch [143/300], Step [110/391],                 Loss: 0.37934, Train_Acc:87.04%
Epoch [143/300], Step [120/391],                 Loss: 0.37859, Train_Acc:86.99%
Epoch [143/300], Step [130/391],                 Loss: 0.38219, Train_Acc:86.95%
Epoch [143/300], Step [140/391],                 Loss: 0.38027, Train_Acc:86.98%
Epoch [143/300], Step [150/391],                 Loss: 0.37987, Train_Acc:87.02%
Epoch [143/300], Step [160/391],                 Loss: 0.38146, Train_Acc:86.99%
Epoch [143/300], Step [170/391],                 Loss: 0.38148, Train_Acc:86.98%
Epoch [143/300], Step [180/391],                 Loss: 0.38140, Train_Acc:86.95%
Epoch [143/300], Step [190/391],                 Loss: 0.38237, Train_Acc:86.91%
Epoch [143/300], Step [200/391],                 Loss: 0.38444, Train_Acc:86.88%
Epoch [143/300], Step [210/391],                 Loss: 0.38460, Train_Acc:86.87%
Epoch [143/300], Step [220/391],                 Loss: 0.38327, Train_Acc:86.95%
Epoch [143/300], Step [230/391],                 Loss: 0.38139, Train_Acc:87.00%
Epoch [143/300], Step [240/391],                 Loss: 0.37934, Train_Acc:87.10%
Epoch [143/300], Step [250/391],                 Loss: 0.37740, Train_Acc:87.16%
Epoch [143/300], Step [260/391],                 Loss: 0.37832, Train_Acc:87.17%
Epoch [143/300], Step [270/391],                 Loss: 0.37827, Train_Acc:87.14%
Epoch [143/300], Step [280/391],                 Loss: 0.37781, Train_Acc:87.19%
Epoch [143/300], Step [290/391],                 Loss: 0.37640, Train_Acc:87.24%
Epoch [143/300], Step [300/391],                 Loss: 0.37622, Train_Acc:87.25%
Epoch [143/300], Step [310/391],                 Loss: 0.37567, Train_Acc:87.27%
Epoch [143/300], Step [320/391],                 Loss: 0.37602, Train_Acc:87.22%
Epoch [143/300], Step [330/391],                 Loss: 0.37450, Train_Acc:87.27%
Epoch [143/300], Step [340/391],                 Loss: 0.37341, Train_Acc:87.30%
Epoch [143/300], Step [350/391],                 Loss: 0.37243, Train_Acc:87.36%
Epoch [143/300], Step [360/391],                 Loss: 0.37142, Train_Acc:87.39%
Epoch [143/300], Step [370/391],                 Loss: 0.37162, Train_Acc:87.39%
Epoch [143/300], Step [380/391],                 Loss: 0.37102, Train_Acc:87.41%
Epoch [143/300], Step [390/391],                 Loss: 0.37088, Train_Acc:87.43%
Accuary on test images:79.68%
Epoch [144/300], Step [10/391],                 Loss: 0.36808, Train_Acc:87.81%
Epoch [144/300], Step [20/391],                 Loss: 0.36348, Train_Acc:87.50%
Epoch [144/300], Step [30/391],                 Loss: 0.36685, Train_Acc:87.24%
Epoch [144/300], Step [40/391],                 Loss: 0.37329, Train_Acc:87.05%
Epoch [144/300], Step [50/391],                 Loss: 0.36966, Train_Acc:87.05%
Epoch [144/300], Step [60/391],                 Loss: 0.37509, Train_Acc:86.84%
Epoch [144/300], Step [70/391],                 Loss: 0.37344, Train_Acc:87.10%
Epoch [144/300], Step [80/391],                 Loss: 0.37112, Train_Acc:87.15%
Epoch [144/300], Step [90/391],                 Loss: 0.37692, Train_Acc:87.01%
Epoch [144/300], Step [100/391],                 Loss: 0.37846, Train_Acc:86.97%
Epoch [144/300], Step [110/391],                 Loss: 0.37911, Train_Acc:87.00%
Epoch [144/300], Step [120/391],                 Loss: 0.38003, Train_Acc:86.96%
Epoch [144/300], Step [130/391],                 Loss: 0.38047, Train_Acc:86.97%
Epoch [144/300], Step [140/391],                 Loss: 0.38131, Train_Acc:86.95%
Epoch [144/300], Step [150/391],                 Loss: 0.38174, Train_Acc:87.04%
Epoch [144/300], Step [160/391],                 Loss: 0.38161, Train_Acc:87.07%
Epoch [144/300], Step [170/391],                 Loss: 0.38288, Train_Acc:87.01%
Epoch [144/300], Step [180/391],                 Loss: 0.38259, Train_Acc:86.94%
Epoch [144/300], Step [190/391],                 Loss: 0.38482, Train_Acc:86.87%
Epoch [144/300], Step [200/391],                 Loss: 0.38598, Train_Acc:86.82%
Epoch [144/300], Step [210/391],                 Loss: 0.38418, Train_Acc:86.85%
Epoch [144/300], Step [220/391],                 Loss: 0.38226, Train_Acc:86.92%
Epoch [144/300], Step [230/391],                 Loss: 0.38091, Train_Acc:87.01%
Epoch [144/300], Step [240/391],                 Loss: 0.37873, Train_Acc:87.07%
Epoch [144/300], Step [250/391],                 Loss: 0.37733, Train_Acc:87.14%
Epoch [144/300], Step [260/391],                 Loss: 0.37855, Train_Acc:87.12%
Epoch [144/300], Step [270/391],                 Loss: 0.38008, Train_Acc:87.06%
Epoch [144/300], Step [280/391],                 Loss: 0.37956, Train_Acc:87.06%
Epoch [144/300], Step [290/391],                 Loss: 0.37842, Train_Acc:87.11%
Epoch [144/300], Step [300/391],                 Loss: 0.37869, Train_Acc:87.10%
Epoch [144/300], Step [310/391],                 Loss: 0.37735, Train_Acc:87.13%
Epoch [144/300], Step [320/391],                 Loss: 0.37595, Train_Acc:87.18%
Epoch [144/300], Step [330/391],                 Loss: 0.37424, Train_Acc:87.25%
Epoch [144/300], Step [340/391],                 Loss: 0.37404, Train_Acc:87.26%
Epoch [144/300], Step [350/391],                 Loss: 0.37412, Train_Acc:87.27%
Epoch [144/300], Step [360/391],                 Loss: 0.37389, Train_Acc:87.27%
Epoch [144/300], Step [370/391],                 Loss: 0.37393, Train_Acc:87.26%
Epoch [144/300], Step [380/391],                 Loss: 0.37434, Train_Acc:87.24%
Epoch [144/300], Step [390/391],                 Loss: 0.37373, Train_Acc:87.28%
Accuary on test images:68.04%
Epoch [145/300], Step [10/391],                 Loss: 0.39459, Train_Acc:86.02%
Epoch [145/300], Step [20/391],                 Loss: 0.38082, Train_Acc:86.56%
Epoch [145/300], Step [30/391],                 Loss: 0.36803, Train_Acc:87.06%
Epoch [145/300], Step [40/391],                 Loss: 0.36336, Train_Acc:87.68%
Epoch [145/300], Step [50/391],                 Loss: 0.36560, Train_Acc:87.72%
Epoch [145/300], Step [60/391],                 Loss: 0.36610, Train_Acc:87.51%
Epoch [145/300], Step [70/391],                 Loss: 0.36742, Train_Acc:87.41%
Epoch [145/300], Step [80/391],                 Loss: 0.36610, Train_Acc:87.54%
Epoch [145/300], Step [90/391],                 Loss: 0.37071, Train_Acc:87.34%
Epoch [145/300], Step [100/391],                 Loss: 0.37140, Train_Acc:87.39%
Epoch [145/300], Step [110/391],                 Loss: 0.37414, Train_Acc:87.20%
Epoch [145/300], Step [120/391],                 Loss: 0.37591, Train_Acc:87.12%
Epoch [145/300], Step [130/391],                 Loss: 0.37794, Train_Acc:87.05%
Epoch [145/300], Step [140/391],                 Loss: 0.37428, Train_Acc:87.24%
Epoch [145/300], Step [150/391],                 Loss: 0.37496, Train_Acc:87.20%
Epoch [145/300], Step [160/391],                 Loss: 0.37610, Train_Acc:87.17%
Epoch [145/300], Step [170/391],                 Loss: 0.37526, Train_Acc:87.23%
Epoch [145/300], Step [180/391],                 Loss: 0.37526, Train_Acc:87.23%
Epoch [145/300], Step [190/391],                 Loss: 0.37677, Train_Acc:87.19%
Epoch [145/300], Step [200/391],                 Loss: 0.37939, Train_Acc:87.07%
Epoch [145/300], Step [210/391],                 Loss: 0.38074, Train_Acc:87.02%
Epoch [145/300], Step [220/391],                 Loss: 0.38012, Train_Acc:87.08%
Epoch [145/300], Step [230/391],                 Loss: 0.37876, Train_Acc:87.13%
Epoch [145/300], Step [240/391],                 Loss: 0.37753, Train_Acc:87.15%
Epoch [145/300], Step [250/391],                 Loss: 0.37678, Train_Acc:87.17%
Epoch [145/300], Step [260/391],                 Loss: 0.37848, Train_Acc:87.15%
Epoch [145/300], Step [270/391],                 Loss: 0.38085, Train_Acc:87.04%
Epoch [145/300], Step [280/391],                 Loss: 0.38065, Train_Acc:87.04%
Epoch [145/300], Step [290/391],                 Loss: 0.38014, Train_Acc:87.07%
Epoch [145/300], Step [300/391],                 Loss: 0.37931, Train_Acc:87.09%
Epoch [145/300], Step [310/391],                 Loss: 0.37822, Train_Acc:87.13%
Epoch [145/300], Step [320/391],                 Loss: 0.37785, Train_Acc:87.14%
Epoch [145/300], Step [330/391],                 Loss: 0.37653, Train_Acc:87.20%
Epoch [145/300], Step [340/391],                 Loss: 0.37489, Train_Acc:87.24%
Epoch [145/300], Step [350/391],                 Loss: 0.37428, Train_Acc:87.27%
Epoch [145/300], Step [360/391],                 Loss: 0.37394, Train_Acc:87.26%
Epoch [145/300], Step [370/391],                 Loss: 0.37464, Train_Acc:87.23%
Epoch [145/300], Step [380/391],                 Loss: 0.37474, Train_Acc:87.23%
Epoch [145/300], Step [390/391],                 Loss: 0.37443, Train_Acc:87.24%
Accuary on test images:70.50%
Epoch [146/300], Step [10/391],                 Loss: 0.39418, Train_Acc:87.11%
Epoch [146/300], Step [20/391],                 Loss: 0.37924, Train_Acc:87.42%
Epoch [146/300], Step [30/391],                 Loss: 0.37001, Train_Acc:87.81%
Epoch [146/300], Step [40/391],                 Loss: 0.37644, Train_Acc:87.64%
Epoch [146/300], Step [50/391],                 Loss: 0.37671, Train_Acc:87.70%
Epoch [146/300], Step [60/391],                 Loss: 0.37953, Train_Acc:87.53%
Epoch [146/300], Step [70/391],                 Loss: 0.37858, Train_Acc:87.52%
Epoch [146/300], Step [80/391],                 Loss: 0.37922, Train_Acc:87.45%
Epoch [146/300], Step [90/391],                 Loss: 0.38098, Train_Acc:87.34%
Epoch [146/300], Step [100/391],                 Loss: 0.37886, Train_Acc:87.33%
Epoch [146/300], Step [110/391],                 Loss: 0.37941, Train_Acc:87.24%
Epoch [146/300], Step [120/391],                 Loss: 0.38113, Train_Acc:87.19%
Epoch [146/300], Step [130/391],                 Loss: 0.38326, Train_Acc:87.12%
Epoch [146/300], Step [140/391],                 Loss: 0.38134, Train_Acc:87.18%
Epoch [146/300], Step [150/391],                 Loss: 0.38133, Train_Acc:87.15%
Epoch [146/300], Step [160/391],                 Loss: 0.37934, Train_Acc:87.21%
Epoch [146/300], Step [170/391],                 Loss: 0.37818, Train_Acc:87.24%
Epoch [146/300], Step [180/391],                 Loss: 0.37621, Train_Acc:87.31%
Epoch [146/300], Step [190/391],                 Loss: 0.37718, Train_Acc:87.26%
Epoch [146/300], Step [200/391],                 Loss: 0.37718, Train_Acc:87.26%
Epoch [146/300], Step [210/391],                 Loss: 0.37723, Train_Acc:87.21%
Epoch [146/300], Step [220/391],                 Loss: 0.37745, Train_Acc:87.19%
Epoch [146/300], Step [230/391],                 Loss: 0.37678, Train_Acc:87.19%
Epoch [146/300], Step [240/391],                 Loss: 0.37521, Train_Acc:87.28%
Epoch [146/300], Step [250/391],                 Loss: 0.37556, Train_Acc:87.24%
Epoch [146/300], Step [260/391],                 Loss: 0.37728, Train_Acc:87.23%
Epoch [146/300], Step [270/391],                 Loss: 0.37848, Train_Acc:87.16%
Epoch [146/300], Step [280/391],                 Loss: 0.37940, Train_Acc:87.11%
Epoch [146/300], Step [290/391],                 Loss: 0.37873, Train_Acc:87.14%
Epoch [146/300], Step [300/391],                 Loss: 0.37814, Train_Acc:87.15%
Epoch [146/300], Step [310/391],                 Loss: 0.37749, Train_Acc:87.18%
Epoch [146/300], Step [320/391],                 Loss: 0.37811, Train_Acc:87.17%
Epoch [146/300], Step [330/391],                 Loss: 0.37769, Train_Acc:87.17%
Epoch [146/300], Step [340/391],                 Loss: 0.37773, Train_Acc:87.20%
Epoch [146/300], Step [350/391],                 Loss: 0.37707, Train_Acc:87.22%
Epoch [146/300], Step [360/391],                 Loss: 0.37722, Train_Acc:87.21%
Epoch [146/300], Step [370/391],                 Loss: 0.37773, Train_Acc:87.21%
Epoch [146/300], Step [380/391],                 Loss: 0.37751, Train_Acc:87.20%
Epoch [146/300], Step [390/391],                 Loss: 0.37711, Train_Acc:87.22%
Accuary on test images:64.50%
Epoch [147/300], Step [10/391],                 Loss: 0.36232, Train_Acc:88.28%
Epoch [147/300], Step [20/391],                 Loss: 0.36137, Train_Acc:88.36%
Epoch [147/300], Step [30/391],                 Loss: 0.35834, Train_Acc:88.65%
Epoch [147/300], Step [40/391],                 Loss: 0.36907, Train_Acc:88.07%
Epoch [147/300], Step [50/391],                 Loss: 0.36998, Train_Acc:87.75%
Epoch [147/300], Step [60/391],                 Loss: 0.37418, Train_Acc:87.53%
Epoch [147/300], Step [70/391],                 Loss: 0.36922, Train_Acc:87.68%
Epoch [147/300], Step [80/391],                 Loss: 0.37253, Train_Acc:87.54%
Epoch [147/300], Step [90/391],                 Loss: 0.37138, Train_Acc:87.56%
Epoch [147/300], Step [100/391],                 Loss: 0.36806, Train_Acc:87.66%
Epoch [147/300], Step [110/391],                 Loss: 0.36794, Train_Acc:87.71%
Epoch [147/300], Step [120/391],                 Loss: 0.36925, Train_Acc:87.71%
Epoch [147/300], Step [130/391],                 Loss: 0.37083, Train_Acc:87.61%
Epoch [147/300], Step [140/391],                 Loss: 0.37164, Train_Acc:87.63%
Epoch [147/300], Step [150/391],                 Loss: 0.37262, Train_Acc:87.60%
Epoch [147/300], Step [160/391],                 Loss: 0.37154, Train_Acc:87.67%
Epoch [147/300], Step [170/391],                 Loss: 0.37062, Train_Acc:87.70%
Epoch [147/300], Step [180/391],                 Loss: 0.36984, Train_Acc:87.70%
Epoch [147/300], Step [190/391],                 Loss: 0.37024, Train_Acc:87.66%
Epoch [147/300], Step [200/391],                 Loss: 0.37197, Train_Acc:87.60%
Epoch [147/300], Step [210/391],                 Loss: 0.37260, Train_Acc:87.56%
Epoch [147/300], Step [220/391],                 Loss: 0.37330, Train_Acc:87.57%
Epoch [147/300], Step [230/391],                 Loss: 0.37142, Train_Acc:87.61%
Epoch [147/300], Step [240/391],                 Loss: 0.36940, Train_Acc:87.67%
Epoch [147/300], Step [250/391],                 Loss: 0.36906, Train_Acc:87.66%
Epoch [147/300], Step [260/391],                 Loss: 0.37252, Train_Acc:87.54%
Epoch [147/300], Step [270/391],                 Loss: 0.37525, Train_Acc:87.47%
Epoch [147/300], Step [280/391],                 Loss: 0.37542, Train_Acc:87.44%
Epoch [147/300], Step [290/391],                 Loss: 0.37475, Train_Acc:87.48%
Epoch [147/300], Step [300/391],                 Loss: 0.37475, Train_Acc:87.46%
Epoch [147/300], Step [310/391],                 Loss: 0.37493, Train_Acc:87.48%
Epoch [147/300], Step [320/391],                 Loss: 0.37555, Train_Acc:87.43%
Epoch [147/300], Step [330/391],                 Loss: 0.37455, Train_Acc:87.45%
Epoch [147/300], Step [340/391],                 Loss: 0.37416, Train_Acc:87.46%
Epoch [147/300], Step [350/391],                 Loss: 0.37326, Train_Acc:87.52%
Epoch [147/300], Step [360/391],                 Loss: 0.37275, Train_Acc:87.52%
Epoch [147/300], Step [370/391],                 Loss: 0.37304, Train_Acc:87.52%
Epoch [147/300], Step [380/391],                 Loss: 0.37285, Train_Acc:87.52%
Epoch [147/300], Step [390/391],                 Loss: 0.37331, Train_Acc:87.52%
Accuary on test images:73.98%
Epoch [148/300], Step [10/391],                 Loss: 0.39065, Train_Acc:86.80%
Epoch [148/300], Step [20/391],                 Loss: 0.37608, Train_Acc:87.23%
Epoch [148/300], Step [30/391],                 Loss: 0.37352, Train_Acc:87.55%
Epoch [148/300], Step [40/391],                 Loss: 0.36978, Train_Acc:87.83%
Epoch [148/300], Step [50/391],                 Loss: 0.37124, Train_Acc:87.67%
Epoch [148/300], Step [60/391],                 Loss: 0.37824, Train_Acc:87.42%
Epoch [148/300], Step [70/391],                 Loss: 0.37777, Train_Acc:87.35%
Epoch [148/300], Step [80/391],                 Loss: 0.37795, Train_Acc:87.26%
Epoch [148/300], Step [90/391],                 Loss: 0.38353, Train_Acc:87.03%
Epoch [148/300], Step [100/391],                 Loss: 0.38228, Train_Acc:87.09%
Epoch [148/300], Step [110/391],                 Loss: 0.38446, Train_Acc:86.97%
Epoch [148/300], Step [120/391],                 Loss: 0.38338, Train_Acc:86.97%
Epoch [148/300], Step [130/391],                 Loss: 0.38331, Train_Acc:87.00%
Epoch [148/300], Step [140/391],                 Loss: 0.38244, Train_Acc:87.11%
Epoch [148/300], Step [150/391],                 Loss: 0.38290, Train_Acc:87.09%
Epoch [148/300], Step [160/391],                 Loss: 0.38239, Train_Acc:87.12%
Epoch [148/300], Step [170/391],                 Loss: 0.38186, Train_Acc:87.09%
Epoch [148/300], Step [180/391],                 Loss: 0.37901, Train_Acc:87.21%
Epoch [148/300], Step [190/391],                 Loss: 0.37736, Train_Acc:87.22%
Epoch [148/300], Step [200/391],                 Loss: 0.37627, Train_Acc:87.27%
Epoch [148/300], Step [210/391],                 Loss: 0.37463, Train_Acc:87.35%
Epoch [148/300], Step [220/391],                 Loss: 0.37488, Train_Acc:87.36%
Epoch [148/300], Step [230/391],                 Loss: 0.37521, Train_Acc:87.37%
Epoch [148/300], Step [240/391],                 Loss: 0.37394, Train_Acc:87.41%
Epoch [148/300], Step [250/391],                 Loss: 0.37391, Train_Acc:87.42%
Epoch [148/300], Step [260/391],                 Loss: 0.37730, Train_Acc:87.32%
Epoch [148/300], Step [270/391],                 Loss: 0.37790, Train_Acc:87.28%
Epoch [148/300], Step [280/391],                 Loss: 0.37716, Train_Acc:87.27%
Epoch [148/300], Step [290/391],                 Loss: 0.37668, Train_Acc:87.30%
Epoch [148/300], Step [300/391],                 Loss: 0.37637, Train_Acc:87.29%
Epoch [148/300], Step [310/391],                 Loss: 0.37612, Train_Acc:87.28%
Epoch [148/300], Step [320/391],                 Loss: 0.37801, Train_Acc:87.23%
Epoch [148/300], Step [330/391],                 Loss: 0.37773, Train_Acc:87.27%
Epoch [148/300], Step [340/391],                 Loss: 0.37688, Train_Acc:87.28%
Epoch [148/300], Step [350/391],                 Loss: 0.37592, Train_Acc:87.34%
Epoch [148/300], Step [360/391],                 Loss: 0.37532, Train_Acc:87.34%
Epoch [148/300], Step [370/391],                 Loss: 0.37546, Train_Acc:87.31%
Epoch [148/300], Step [380/391],                 Loss: 0.37493, Train_Acc:87.31%
Epoch [148/300], Step [390/391],                 Loss: 0.37383, Train_Acc:87.34%
Accuary on test images:77.22%
Epoch [149/300], Step [10/391],                 Loss: 0.37664, Train_Acc:86.88%
Epoch [149/300], Step [20/391],                 Loss: 0.37635, Train_Acc:87.15%
Epoch [149/300], Step [30/391],                 Loss: 0.37353, Train_Acc:87.53%
Epoch [149/300], Step [40/391],                 Loss: 0.37601, Train_Acc:87.58%
Epoch [149/300], Step [50/391],                 Loss: 0.37585, Train_Acc:87.47%
Epoch [149/300], Step [60/391],                 Loss: 0.38092, Train_Acc:87.27%
Epoch [149/300], Step [70/391],                 Loss: 0.38251, Train_Acc:87.14%
Epoch [149/300], Step [80/391],                 Loss: 0.38151, Train_Acc:87.26%
Epoch [149/300], Step [90/391],                 Loss: 0.38204, Train_Acc:87.12%
Epoch [149/300], Step [100/391],                 Loss: 0.38161, Train_Acc:87.14%
Epoch [149/300], Step [110/391],                 Loss: 0.38108, Train_Acc:87.24%
Epoch [149/300], Step [120/391],                 Loss: 0.38152, Train_Acc:87.25%
Epoch [149/300], Step [130/391],                 Loss: 0.38021, Train_Acc:87.29%
Epoch [149/300], Step [140/391],                 Loss: 0.37943, Train_Acc:87.25%
Epoch [149/300], Step [150/391],                 Loss: 0.37965, Train_Acc:87.21%
Epoch [149/300], Step [160/391],                 Loss: 0.37852, Train_Acc:87.29%
Epoch [149/300], Step [170/391],                 Loss: 0.37790, Train_Acc:87.32%
Epoch [149/300], Step [180/391],                 Loss: 0.37684, Train_Acc:87.33%
Epoch [149/300], Step [190/391],                 Loss: 0.37682, Train_Acc:87.36%
Epoch [149/300], Step [200/391],                 Loss: 0.37931, Train_Acc:87.27%
Epoch [149/300], Step [210/391],                 Loss: 0.37978, Train_Acc:87.24%
Epoch [149/300], Step [220/391],                 Loss: 0.38089, Train_Acc:87.18%
Epoch [149/300], Step [230/391],                 Loss: 0.37972, Train_Acc:87.21%
Epoch [149/300], Step [240/391],                 Loss: 0.37883, Train_Acc:87.26%
Epoch [149/300], Step [250/391],                 Loss: 0.37926, Train_Acc:87.23%
Epoch [149/300], Step [260/391],                 Loss: 0.38126, Train_Acc:87.21%
Epoch [149/300], Step [270/391],                 Loss: 0.38270, Train_Acc:87.15%
Epoch [149/300], Step [280/391],                 Loss: 0.38252, Train_Acc:87.16%
Epoch [149/300], Step [290/391],                 Loss: 0.38263, Train_Acc:87.14%
Epoch [149/300], Step [300/391],                 Loss: 0.38103, Train_Acc:87.18%
Epoch [149/300], Step [310/391],                 Loss: 0.38013, Train_Acc:87.18%
Epoch [149/300], Step [320/391],                 Loss: 0.38005, Train_Acc:87.19%
Epoch [149/300], Step [330/391],                 Loss: 0.37892, Train_Acc:87.24%
Epoch [149/300], Step [340/391],                 Loss: 0.37952, Train_Acc:87.22%
Epoch [149/300], Step [350/391],                 Loss: 0.37936, Train_Acc:87.23%
Epoch [149/300], Step [360/391],                 Loss: 0.37895, Train_Acc:87.24%
Epoch [149/300], Step [370/391],                 Loss: 0.37898, Train_Acc:87.21%
Epoch [149/300], Step [380/391],                 Loss: 0.37852, Train_Acc:87.22%
Epoch [149/300], Step [390/391],                 Loss: 0.37748, Train_Acc:87.24%
Accuary on test images:79.60%
Epoch [150/300], Step [10/391],                 Loss: 0.38416, Train_Acc:86.64%
Epoch [150/300], Step [20/391],                 Loss: 0.37503, Train_Acc:87.54%
Epoch [150/300], Step [30/391],                 Loss: 0.36802, Train_Acc:87.63%
Epoch [150/300], Step [40/391],                 Loss: 0.37211, Train_Acc:87.48%
Epoch [150/300], Step [50/391],                 Loss: 0.37373, Train_Acc:87.31%
Epoch [150/300], Step [60/391],                 Loss: 0.37689, Train_Acc:87.36%
Epoch [150/300], Step [70/391],                 Loss: 0.37826, Train_Acc:87.49%
Epoch [150/300], Step [80/391],                 Loss: 0.37919, Train_Acc:87.37%
Epoch [150/300], Step [90/391],                 Loss: 0.38192, Train_Acc:87.27%
Epoch [150/300], Step [100/391],                 Loss: 0.38024, Train_Acc:87.30%
Epoch [150/300], Step [110/391],                 Loss: 0.37837, Train_Acc:87.40%
Epoch [150/300], Step [120/391],                 Loss: 0.37611, Train_Acc:87.43%
Epoch [150/300], Step [130/391],                 Loss: 0.37588, Train_Acc:87.45%
Epoch [150/300], Step [140/391],                 Loss: 0.37727, Train_Acc:87.35%
Epoch [150/300], Step [150/391],                 Loss: 0.38040, Train_Acc:87.23%
Epoch [150/300], Step [160/391],                 Loss: 0.37946, Train_Acc:87.25%
Epoch [150/300], Step [170/391],                 Loss: 0.37763, Train_Acc:87.31%
Epoch [150/300], Step [180/391],                 Loss: 0.37609, Train_Acc:87.36%
Epoch [150/300], Step [190/391],                 Loss: 0.37590, Train_Acc:87.32%
Epoch [150/300], Step [200/391],                 Loss: 0.37737, Train_Acc:87.28%
Epoch [150/300], Step [210/391],                 Loss: 0.37809, Train_Acc:87.24%
Epoch [150/300], Step [220/391],                 Loss: 0.37760, Train_Acc:87.29%
Epoch [150/300], Step [230/391],                 Loss: 0.37578, Train_Acc:87.35%
Epoch [150/300], Step [240/391],                 Loss: 0.37477, Train_Acc:87.37%
Epoch [150/300], Step [250/391],                 Loss: 0.37488, Train_Acc:87.32%
Epoch [150/300], Step [260/391],                 Loss: 0.37756, Train_Acc:87.24%
Epoch [150/300], Step [270/391],                 Loss: 0.37973, Train_Acc:87.13%
Epoch [150/300], Step [280/391],                 Loss: 0.37968, Train_Acc:87.14%
Epoch [150/300], Step [290/391],                 Loss: 0.37983, Train_Acc:87.12%
Epoch [150/300], Step [300/391],                 Loss: 0.37922, Train_Acc:87.11%
Epoch [150/300], Step [310/391],                 Loss: 0.37776, Train_Acc:87.17%
Epoch [150/300], Step [320/391],                 Loss: 0.37803, Train_Acc:87.17%
Epoch [150/300], Step [330/391],                 Loss: 0.37779, Train_Acc:87.17%
Epoch [150/300], Step [340/391],                 Loss: 0.37783, Train_Acc:87.15%
Epoch [150/300], Step [350/391],                 Loss: 0.37756, Train_Acc:87.18%
Epoch [150/300], Step [360/391],                 Loss: 0.37712, Train_Acc:87.19%
Epoch [150/300], Step [370/391],                 Loss: 0.37776, Train_Acc:87.16%
Epoch [150/300], Step [380/391],                 Loss: 0.37802, Train_Acc:87.18%
Epoch [150/300], Step [390/391],                 Loss: 0.37744, Train_Acc:87.17%
Accuary on test images:74.14%
Epoch [151/300], Step [10/391],                 Loss: 0.37520, Train_Acc:88.12%
Epoch [151/300], Step [20/391],                 Loss: 0.34189, Train_Acc:88.59%
Epoch [151/300], Step [30/391],                 Loss: 0.32262, Train_Acc:89.19%
Epoch [151/300], Step [40/391],                 Loss: 0.31064, Train_Acc:89.71%
Epoch [151/300], Step [50/391],                 Loss: 0.29764, Train_Acc:90.08%
Epoch [151/300], Step [60/391],                 Loss: 0.29133, Train_Acc:90.16%
Epoch [151/300], Step [70/391],                 Loss: 0.28434, Train_Acc:90.48%
Epoch [151/300], Step [80/391],                 Loss: 0.28116, Train_Acc:90.62%
Epoch [151/300], Step [90/391],                 Loss: 0.27855, Train_Acc:90.65%
Epoch [151/300], Step [100/391],                 Loss: 0.27068, Train_Acc:90.97%
Epoch [151/300], Step [110/391],                 Loss: 0.26552, Train_Acc:91.19%
Epoch [151/300], Step [120/391],                 Loss: 0.26235, Train_Acc:91.37%
Epoch [151/300], Step [130/391],                 Loss: 0.26016, Train_Acc:91.41%
Epoch [151/300], Step [140/391],                 Loss: 0.25573, Train_Acc:91.53%
Epoch [151/300], Step [150/391],                 Loss: 0.25314, Train_Acc:91.62%
Epoch [151/300], Step [160/391],                 Loss: 0.24817, Train_Acc:91.80%
Epoch [151/300], Step [170/391],                 Loss: 0.24546, Train_Acc:91.83%
Epoch [151/300], Step [180/391],                 Loss: 0.24255, Train_Acc:91.88%
Epoch [151/300], Step [190/391],                 Loss: 0.23924, Train_Acc:91.98%
Epoch [151/300], Step [200/391],                 Loss: 0.23673, Train_Acc:92.07%
Epoch [151/300], Step [210/391],                 Loss: 0.23411, Train_Acc:92.18%
Epoch [151/300], Step [220/391],                 Loss: 0.23151, Train_Acc:92.30%
Epoch [151/300], Step [230/391],                 Loss: 0.22854, Train_Acc:92.42%
Epoch [151/300], Step [240/391],                 Loss: 0.22606, Train_Acc:92.52%
Epoch [151/300], Step [250/391],                 Loss: 0.22359, Train_Acc:92.60%
Epoch [151/300], Step [260/391],                 Loss: 0.22198, Train_Acc:92.66%
Epoch [151/300], Step [270/391],                 Loss: 0.21995, Train_Acc:92.72%
Epoch [151/300], Step [280/391],                 Loss: 0.21685, Train_Acc:92.81%
Epoch [151/300], Step [290/391],                 Loss: 0.21414, Train_Acc:92.91%
Epoch [151/300], Step [300/391],                 Loss: 0.21181, Train_Acc:93.00%
Epoch [151/300], Step [310/391],                 Loss: 0.20962, Train_Acc:93.08%
Epoch [151/300], Step [320/391],                 Loss: 0.20731, Train_Acc:93.16%
Epoch [151/300], Step [330/391],                 Loss: 0.20458, Train_Acc:93.26%
Epoch [151/300], Step [340/391],                 Loss: 0.20187, Train_Acc:93.37%
Epoch [151/300], Step [350/391],                 Loss: 0.19902, Train_Acc:93.48%
Epoch [151/300], Step [360/391],                 Loss: 0.19655, Train_Acc:93.55%
Epoch [151/300], Step [370/391],                 Loss: 0.19418, Train_Acc:93.63%
Epoch [151/300], Step [380/391],                 Loss: 0.19116, Train_Acc:93.74%
Epoch [151/300], Step [390/391],                 Loss: 0.18861, Train_Acc:93.83%
Accuary on test images:90.38%
Epoch [152/300], Step [10/391],                 Loss: 0.15595, Train_Acc:94.92%
Epoch [152/300], Step [20/391],                 Loss: 0.14971, Train_Acc:95.27%
Epoch [152/300], Step [30/391],                 Loss: 0.14292, Train_Acc:95.65%
Epoch [152/300], Step [40/391],                 Loss: 0.14370, Train_Acc:95.43%
Epoch [152/300], Step [50/391],                 Loss: 0.14538, Train_Acc:95.42%
Epoch [152/300], Step [60/391],                 Loss: 0.14376, Train_Acc:95.48%
Epoch [152/300], Step [70/391],                 Loss: 0.14388, Train_Acc:95.50%
Epoch [152/300], Step [80/391],                 Loss: 0.14358, Train_Acc:95.52%
Epoch [152/300], Step [90/391],                 Loss: 0.14378, Train_Acc:95.51%
Epoch [152/300], Step [100/391],                 Loss: 0.14020, Train_Acc:95.66%
Epoch [152/300], Step [110/391],                 Loss: 0.13946, Train_Acc:95.67%
Epoch [152/300], Step [120/391],                 Loss: 0.13953, Train_Acc:95.65%
Epoch [152/300], Step [130/391],                 Loss: 0.13996, Train_Acc:95.60%
Epoch [152/300], Step [140/391],                 Loss: 0.13911, Train_Acc:95.59%
Epoch [152/300], Step [150/391],                 Loss: 0.13867, Train_Acc:95.64%
Epoch [152/300], Step [160/391],                 Loss: 0.13789, Train_Acc:95.67%
Epoch [152/300], Step [170/391],                 Loss: 0.13675, Train_Acc:95.72%
Epoch [152/300], Step [180/391],                 Loss: 0.13565, Train_Acc:95.76%
Epoch [152/300], Step [190/391],                 Loss: 0.13540, Train_Acc:95.76%
Epoch [152/300], Step [200/391],                 Loss: 0.13433, Train_Acc:95.82%
Epoch [152/300], Step [210/391],                 Loss: 0.13324, Train_Acc:95.86%
Epoch [152/300], Step [220/391],                 Loss: 0.13230, Train_Acc:95.88%
Epoch [152/300], Step [230/391],                 Loss: 0.13052, Train_Acc:95.93%
Epoch [152/300], Step [240/391],                 Loss: 0.12942, Train_Acc:95.98%
Epoch [152/300], Step [250/391],                 Loss: 0.12833, Train_Acc:96.02%
Epoch [152/300], Step [260/391],                 Loss: 0.12815, Train_Acc:96.02%
Epoch [152/300], Step [270/391],                 Loss: 0.12792, Train_Acc:96.04%
Epoch [152/300], Step [280/391],                 Loss: 0.12692, Train_Acc:96.07%
Epoch [152/300], Step [290/391],                 Loss: 0.12574, Train_Acc:96.11%
Epoch [152/300], Step [300/391],                 Loss: 0.12429, Train_Acc:96.16%
Epoch [152/300], Step [310/391],                 Loss: 0.12306, Train_Acc:96.20%
Epoch [152/300], Step [320/391],                 Loss: 0.12183, Train_Acc:96.26%
Epoch [152/300], Step [330/391],                 Loss: 0.12073, Train_Acc:96.30%
Epoch [152/300], Step [340/391],                 Loss: 0.11968, Train_Acc:96.34%
Epoch [152/300], Step [350/391],                 Loss: 0.11834, Train_Acc:96.40%
Epoch [152/300], Step [360/391],                 Loss: 0.11705, Train_Acc:96.44%
Epoch [152/300], Step [370/391],                 Loss: 0.11602, Train_Acc:96.47%
Epoch [152/300], Step [380/391],                 Loss: 0.11440, Train_Acc:96.52%
Epoch [152/300], Step [390/391],                 Loss: 0.11319, Train_Acc:96.57%
Accuary on test images:90.94%
Epoch [153/300], Step [10/391],                 Loss: 0.10494, Train_Acc:97.27%
Epoch [153/300], Step [20/391],                 Loss: 0.10064, Train_Acc:97.27%
Epoch [153/300], Step [30/391],                 Loss: 0.09492, Train_Acc:97.29%
Epoch [153/300], Step [40/391],                 Loss: 0.09777, Train_Acc:97.21%
Epoch [153/300], Step [50/391],                 Loss: 0.09569, Train_Acc:97.28%
Epoch [153/300], Step [60/391],                 Loss: 0.09710, Train_Acc:97.23%
Epoch [153/300], Step [70/391],                 Loss: 0.09806, Train_Acc:97.21%
Epoch [153/300], Step [80/391],                 Loss: 0.09958, Train_Acc:97.22%
Epoch [153/300], Step [90/391],                 Loss: 0.09872, Train_Acc:97.20%
Epoch [153/300], Step [100/391],                 Loss: 0.09536, Train_Acc:97.31%
Epoch [153/300], Step [110/391],                 Loss: 0.09504, Train_Acc:97.29%
Epoch [153/300], Step [120/391],                 Loss: 0.09471, Train_Acc:97.32%
Epoch [153/300], Step [130/391],                 Loss: 0.09531, Train_Acc:97.30%
Epoch [153/300], Step [140/391],                 Loss: 0.09451, Train_Acc:97.30%
Epoch [153/300], Step [150/391],                 Loss: 0.09438, Train_Acc:97.29%
Epoch [153/300], Step [160/391],                 Loss: 0.09370, Train_Acc:97.32%
Epoch [153/300], Step [170/391],                 Loss: 0.09312, Train_Acc:97.34%
Epoch [153/300], Step [180/391],                 Loss: 0.09260, Train_Acc:97.34%
Epoch [153/300], Step [190/391],                 Loss: 0.09177, Train_Acc:97.34%
Epoch [153/300], Step [200/391],                 Loss: 0.09200, Train_Acc:97.35%
Epoch [153/300], Step [210/391],                 Loss: 0.09154, Train_Acc:97.37%
Epoch [153/300], Step [220/391],                 Loss: 0.09072, Train_Acc:97.41%
Epoch [153/300], Step [230/391],                 Loss: 0.08978, Train_Acc:97.46%
Epoch [153/300], Step [240/391],                 Loss: 0.08942, Train_Acc:97.48%
Epoch [153/300], Step [250/391],                 Loss: 0.08857, Train_Acc:97.51%
Epoch [153/300], Step [260/391],                 Loss: 0.08837, Train_Acc:97.53%
Epoch [153/300], Step [270/391],                 Loss: 0.08791, Train_Acc:97.54%
Epoch [153/300], Step [280/391],                 Loss: 0.08675, Train_Acc:97.58%
Epoch [153/300], Step [290/391],                 Loss: 0.08577, Train_Acc:97.61%
Epoch [153/300], Step [300/391],                 Loss: 0.08457, Train_Acc:97.66%
Epoch [153/300], Step [310/391],                 Loss: 0.08404, Train_Acc:97.67%
Epoch [153/300], Step [320/391],                 Loss: 0.08321, Train_Acc:97.71%
Epoch [153/300], Step [330/391],                 Loss: 0.08225, Train_Acc:97.74%
Epoch [153/300], Step [340/391],                 Loss: 0.08150, Train_Acc:97.74%
Epoch [153/300], Step [350/391],                 Loss: 0.08076, Train_Acc:97.76%
Epoch [153/300], Step [360/391],                 Loss: 0.07969, Train_Acc:97.80%
Epoch [153/300], Step [370/391],                 Loss: 0.07922, Train_Acc:97.81%
Epoch [153/300], Step [380/391],                 Loss: 0.07809, Train_Acc:97.85%
Epoch [153/300], Step [390/391],                 Loss: 0.07735, Train_Acc:97.88%
Accuary on test images:90.84%
Epoch [154/300], Step [10/391],                 Loss: 0.07865, Train_Acc:97.50%
Epoch [154/300], Step [20/391],                 Loss: 0.07351, Train_Acc:97.85%
Epoch [154/300], Step [30/391],                 Loss: 0.06647, Train_Acc:98.10%
Epoch [154/300], Step [40/391],                 Loss: 0.06841, Train_Acc:98.12%
Epoch [154/300], Step [50/391],                 Loss: 0.06710, Train_Acc:98.17%
Epoch [154/300], Step [60/391],                 Loss: 0.06597, Train_Acc:98.14%
Epoch [154/300], Step [70/391],                 Loss: 0.06572, Train_Acc:98.19%
Epoch [154/300], Step [80/391],                 Loss: 0.06637, Train_Acc:98.17%
Epoch [154/300], Step [90/391],                 Loss: 0.06492, Train_Acc:98.26%
Epoch [154/300], Step [100/391],                 Loss: 0.06406, Train_Acc:98.28%
Epoch [154/300], Step [110/391],                 Loss: 0.06301, Train_Acc:98.32%
Epoch [154/300], Step [120/391],                 Loss: 0.06240, Train_Acc:98.37%
Epoch [154/300], Step [130/391],                 Loss: 0.06285, Train_Acc:98.35%
Epoch [154/300], Step [140/391],                 Loss: 0.06269, Train_Acc:98.35%
Epoch [154/300], Step [150/391],                 Loss: 0.06158, Train_Acc:98.39%
Epoch [154/300], Step [160/391],                 Loss: 0.06036, Train_Acc:98.42%
Epoch [154/300], Step [170/391],                 Loss: 0.06057, Train_Acc:98.41%
Epoch [154/300], Step [180/391],                 Loss: 0.05994, Train_Acc:98.43%
Epoch [154/300], Step [190/391],                 Loss: 0.05951, Train_Acc:98.44%
Epoch [154/300], Step [200/391],                 Loss: 0.06013, Train_Acc:98.44%
Epoch [154/300], Step [210/391],                 Loss: 0.05966, Train_Acc:98.44%
Epoch [154/300], Step [220/391],                 Loss: 0.05935, Train_Acc:98.46%
Epoch [154/300], Step [230/391],                 Loss: 0.05879, Train_Acc:98.46%
Epoch [154/300], Step [240/391],                 Loss: 0.05828, Train_Acc:98.48%
Epoch [154/300], Step [250/391],                 Loss: 0.05769, Train_Acc:98.49%
Epoch [154/300], Step [260/391],                 Loss: 0.05744, Train_Acc:98.49%
Epoch [154/300], Step [270/391],                 Loss: 0.05730, Train_Acc:98.50%
Epoch [154/300], Step [280/391],                 Loss: 0.05666, Train_Acc:98.52%
Epoch [154/300], Step [290/391],                 Loss: 0.05597, Train_Acc:98.55%
Epoch [154/300], Step [300/391],                 Loss: 0.05532, Train_Acc:98.58%
Epoch [154/300], Step [310/391],                 Loss: 0.05481, Train_Acc:98.60%
Epoch [154/300], Step [320/391],                 Loss: 0.05440, Train_Acc:98.61%
Epoch [154/300], Step [330/391],                 Loss: 0.05383, Train_Acc:98.62%
Epoch [154/300], Step [340/391],                 Loss: 0.05329, Train_Acc:98.64%
Epoch [154/300], Step [350/391],                 Loss: 0.05283, Train_Acc:98.66%
Epoch [154/300], Step [360/391],                 Loss: 0.05231, Train_Acc:98.67%
Epoch [154/300], Step [370/391],                 Loss: 0.05183, Train_Acc:98.68%
Epoch [154/300], Step [380/391],                 Loss: 0.05118, Train_Acc:98.70%
Epoch [154/300], Step [390/391],                 Loss: 0.05075, Train_Acc:98.72%
Accuary on test images:90.68%
Epoch [155/300], Step [10/391],                 Loss: 0.04181, Train_Acc:99.14%
Epoch [155/300], Step [20/391],                 Loss: 0.03850, Train_Acc:99.26%
Epoch [155/300], Step [30/391],                 Loss: 0.03753, Train_Acc:99.27%
Epoch [155/300], Step [40/391],                 Loss: 0.04018, Train_Acc:99.22%
Epoch [155/300], Step [50/391],                 Loss: 0.04068, Train_Acc:99.17%
Epoch [155/300], Step [60/391],                 Loss: 0.04136, Train_Acc:99.17%
Epoch [155/300], Step [70/391],                 Loss: 0.04114, Train_Acc:99.14%
Epoch [155/300], Step [80/391],                 Loss: 0.04210, Train_Acc:99.09%
Epoch [155/300], Step [90/391],                 Loss: 0.04292, Train_Acc:99.08%
Epoch [155/300], Step [100/391],                 Loss: 0.04177, Train_Acc:99.09%
Epoch [155/300], Step [110/391],                 Loss: 0.04146, Train_Acc:99.08%
Epoch [155/300], Step [120/391],                 Loss: 0.04098, Train_Acc:99.08%
Epoch [155/300], Step [130/391],                 Loss: 0.04112, Train_Acc:99.09%
Epoch [155/300], Step [140/391],                 Loss: 0.04067, Train_Acc:99.11%
Epoch [155/300], Step [150/391],                 Loss: 0.04023, Train_Acc:99.11%
Epoch [155/300], Step [160/391],                 Loss: 0.03948, Train_Acc:99.14%
Epoch [155/300], Step [170/391],                 Loss: 0.03894, Train_Acc:99.15%
Epoch [155/300], Step [180/391],                 Loss: 0.03859, Train_Acc:99.17%
Epoch [155/300], Step [190/391],                 Loss: 0.03834, Train_Acc:99.18%
Epoch [155/300], Step [200/391],                 Loss: 0.03837, Train_Acc:99.19%
Epoch [155/300], Step [210/391],                 Loss: 0.03782, Train_Acc:99.21%
Epoch [155/300], Step [220/391],                 Loss: 0.03767, Train_Acc:99.22%
Epoch [155/300], Step [230/391],                 Loss: 0.03744, Train_Acc:99.22%
Epoch [155/300], Step [240/391],                 Loss: 0.03732, Train_Acc:99.21%
Epoch [155/300], Step [250/391],                 Loss: 0.03677, Train_Acc:99.24%
Epoch [155/300], Step [260/391],                 Loss: 0.03687, Train_Acc:99.22%
Epoch [155/300], Step [270/391],                 Loss: 0.03655, Train_Acc:99.22%
Epoch [155/300], Step [280/391],                 Loss: 0.03634, Train_Acc:99.24%
Epoch [155/300], Step [290/391],                 Loss: 0.03594, Train_Acc:99.25%
Epoch [155/300], Step [300/391],                 Loss: 0.03571, Train_Acc:99.26%
Epoch [155/300], Step [310/391],                 Loss: 0.03551, Train_Acc:99.26%
Epoch [155/300], Step [320/391],                 Loss: 0.03516, Train_Acc:99.27%
Epoch [155/300], Step [330/391],                 Loss: 0.03473, Train_Acc:99.28%
Epoch [155/300], Step [340/391],                 Loss: 0.03448, Train_Acc:99.29%
Epoch [155/300], Step [350/391],                 Loss: 0.03402, Train_Acc:99.30%
Epoch [155/300], Step [360/391],                 Loss: 0.03364, Train_Acc:99.30%
Epoch [155/300], Step [370/391],                 Loss: 0.03336, Train_Acc:99.31%
Epoch [155/300], Step [380/391],                 Loss: 0.03309, Train_Acc:99.32%
Epoch [155/300], Step [390/391],                 Loss: 0.03276, Train_Acc:99.32%
Accuary on test images:90.94%
Epoch [156/300], Step [10/391],                 Loss: 0.02901, Train_Acc:99.45%
Epoch [156/300], Step [20/391],                 Loss: 0.02697, Train_Acc:99.41%
Epoch [156/300], Step [30/391],                 Loss: 0.02569, Train_Acc:99.48%
Epoch [156/300], Step [40/391],                 Loss: 0.02634, Train_Acc:99.47%
Epoch [156/300], Step [50/391],                 Loss: 0.02624, Train_Acc:99.50%
Epoch [156/300], Step [60/391],                 Loss: 0.02636, Train_Acc:99.49%
Epoch [156/300], Step [70/391],                 Loss: 0.02731, Train_Acc:99.50%
Epoch [156/300], Step [80/391],                 Loss: 0.02742, Train_Acc:99.49%
Epoch [156/300], Step [90/391],                 Loss: 0.02773, Train_Acc:99.45%
Epoch [156/300], Step [100/391],                 Loss: 0.02655, Train_Acc:99.49%
Epoch [156/300], Step [110/391],                 Loss: 0.02726, Train_Acc:99.47%
Epoch [156/300], Step [120/391],                 Loss: 0.02660, Train_Acc:99.49%
Epoch [156/300], Step [130/391],                 Loss: 0.02691, Train_Acc:99.49%
Epoch [156/300], Step [140/391],                 Loss: 0.02655, Train_Acc:99.50%
Epoch [156/300], Step [150/391],                 Loss: 0.02640, Train_Acc:99.50%
Epoch [156/300], Step [160/391],                 Loss: 0.02603, Train_Acc:99.52%
Epoch [156/300], Step [170/391],                 Loss: 0.02641, Train_Acc:99.50%
Epoch [156/300], Step [180/391],                 Loss: 0.02608, Train_Acc:99.52%
Epoch [156/300], Step [190/391],                 Loss: 0.02591, Train_Acc:99.52%
Epoch [156/300], Step [200/391],                 Loss: 0.02591, Train_Acc:99.53%
Epoch [156/300], Step [210/391],                 Loss: 0.02562, Train_Acc:99.53%
Epoch [156/300], Step [220/391],                 Loss: 0.02530, Train_Acc:99.54%
Epoch [156/300], Step [230/391],                 Loss: 0.02511, Train_Acc:99.54%
Epoch [156/300], Step [240/391],                 Loss: 0.02491, Train_Acc:99.54%
Epoch [156/300], Step [250/391],                 Loss: 0.02460, Train_Acc:99.55%
Epoch [156/300], Step [260/391],                 Loss: 0.02453, Train_Acc:99.55%
Epoch [156/300], Step [270/391],                 Loss: 0.02439, Train_Acc:99.56%
Epoch [156/300], Step [280/391],                 Loss: 0.02422, Train_Acc:99.56%
Epoch [156/300], Step [290/391],                 Loss: 0.02419, Train_Acc:99.56%
Epoch [156/300], Step [300/391],                 Loss: 0.02403, Train_Acc:99.57%
Epoch [156/300], Step [310/391],                 Loss: 0.02383, Train_Acc:99.57%
Epoch [156/300], Step [320/391],                 Loss: 0.02369, Train_Acc:99.58%
Epoch [156/300], Step [330/391],                 Loss: 0.02383, Train_Acc:99.57%
Epoch [156/300], Step [340/391],                 Loss: 0.02366, Train_Acc:99.57%
Epoch [156/300], Step [350/391],                 Loss: 0.02325, Train_Acc:99.58%
Epoch [156/300], Step [360/391],                 Loss: 0.02305, Train_Acc:99.59%
Epoch [156/300], Step [370/391],                 Loss: 0.02293, Train_Acc:99.59%
Epoch [156/300], Step [380/391],                 Loss: 0.02273, Train_Acc:99.59%
Epoch [156/300], Step [390/391],                 Loss: 0.02250, Train_Acc:99.60%
Accuary on test images:90.80%
Epoch [157/300], Step [10/391],                 Loss: 0.01588, Train_Acc:99.84%
Epoch [157/300], Step [20/391],                 Loss: 0.01706, Train_Acc:99.88%
Epoch [157/300], Step [30/391],                 Loss: 0.01910, Train_Acc:99.77%
Epoch [157/300], Step [40/391],                 Loss: 0.02109, Train_Acc:99.71%
Epoch [157/300], Step [50/391],                 Loss: 0.02097, Train_Acc:99.70%
Epoch [157/300], Step [60/391],                 Loss: 0.02158, Train_Acc:99.67%
Epoch [157/300], Step [70/391],                 Loss: 0.02089, Train_Acc:99.68%
Epoch [157/300], Step [80/391],                 Loss: 0.02059, Train_Acc:99.69%
Epoch [157/300], Step [90/391],                 Loss: 0.01997, Train_Acc:99.71%
Epoch [157/300], Step [100/391],                 Loss: 0.01923, Train_Acc:99.73%
Epoch [157/300], Step [110/391],                 Loss: 0.01891, Train_Acc:99.74%
Epoch [157/300], Step [120/391],                 Loss: 0.01880, Train_Acc:99.75%
Epoch [157/300], Step [130/391],                 Loss: 0.01843, Train_Acc:99.75%
Epoch [157/300], Step [140/391],                 Loss: 0.01813, Train_Acc:99.77%
Epoch [157/300], Step [150/391],                 Loss: 0.01782, Train_Acc:99.78%
Epoch [157/300], Step [160/391],                 Loss: 0.01774, Train_Acc:99.78%
Epoch [157/300], Step [170/391],                 Loss: 0.01764, Train_Acc:99.77%
Epoch [157/300], Step [180/391],                 Loss: 0.01768, Train_Acc:99.76%
Epoch [157/300], Step [190/391],                 Loss: 0.01740, Train_Acc:99.76%
Epoch [157/300], Step [200/391],                 Loss: 0.01731, Train_Acc:99.77%
Epoch [157/300], Step [210/391],                 Loss: 0.01756, Train_Acc:99.76%
Epoch [157/300], Step [220/391],                 Loss: 0.01740, Train_Acc:99.76%
Epoch [157/300], Step [230/391],                 Loss: 0.01721, Train_Acc:99.76%
Epoch [157/300], Step [240/391],                 Loss: 0.01703, Train_Acc:99.77%
Epoch [157/300], Step [250/391],                 Loss: 0.01682, Train_Acc:99.77%
Epoch [157/300], Step [260/391],                 Loss: 0.01667, Train_Acc:99.78%
Epoch [157/300], Step [270/391],                 Loss: 0.01664, Train_Acc:99.78%
Epoch [157/300], Step [280/391],                 Loss: 0.01649, Train_Acc:99.79%
Epoch [157/300], Step [290/391],                 Loss: 0.01640, Train_Acc:99.79%
Epoch [157/300], Step [300/391],                 Loss: 0.01623, Train_Acc:99.79%
Epoch [157/300], Step [310/391],                 Loss: 0.01617, Train_Acc:99.79%
Epoch [157/300], Step [320/391],                 Loss: 0.01608, Train_Acc:99.79%
Epoch [157/300], Step [330/391],                 Loss: 0.01611, Train_Acc:99.78%
Epoch [157/300], Step [340/391],                 Loss: 0.01591, Train_Acc:99.79%
Epoch [157/300], Step [350/391],                 Loss: 0.01579, Train_Acc:99.79%
Epoch [157/300], Step [360/391],                 Loss: 0.01558, Train_Acc:99.79%
Epoch [157/300], Step [370/391],                 Loss: 0.01543, Train_Acc:99.79%
Epoch [157/300], Step [380/391],                 Loss: 0.01524, Train_Acc:99.80%
Epoch [157/300], Step [390/391],                 Loss: 0.01517, Train_Acc:99.80%
Accuary on test images:91.20%
Epoch [158/300], Step [10/391],                 Loss: 0.01411, Train_Acc:99.92%
Epoch [158/300], Step [20/391],                 Loss: 0.01316, Train_Acc:99.92%
Epoch [158/300], Step [30/391],                 Loss: 0.01395, Train_Acc:99.87%
Epoch [158/300], Step [40/391],                 Loss: 0.01399, Train_Acc:99.84%
Epoch [158/300], Step [50/391],                 Loss: 0.01335, Train_Acc:99.84%
Epoch [158/300], Step [60/391],                 Loss: 0.01286, Train_Acc:99.84%
Epoch [158/300], Step [70/391],                 Loss: 0.01327, Train_Acc:99.82%
Epoch [158/300], Step [80/391],                 Loss: 0.01354, Train_Acc:99.81%
Epoch [158/300], Step [90/391],                 Loss: 0.01332, Train_Acc:99.81%
Epoch [158/300], Step [100/391],                 Loss: 0.01285, Train_Acc:99.83%
Epoch [158/300], Step [110/391],                 Loss: 0.01256, Train_Acc:99.84%
Epoch [158/300], Step [120/391],                 Loss: 0.01226, Train_Acc:99.85%
Epoch [158/300], Step [130/391],                 Loss: 0.01302, Train_Acc:99.84%
Epoch [158/300], Step [140/391],                 Loss: 0.01283, Train_Acc:99.85%
Epoch [158/300], Step [150/391],                 Loss: 0.01276, Train_Acc:99.84%
Epoch [158/300], Step [160/391],                 Loss: 0.01277, Train_Acc:99.83%
Epoch [158/300], Step [170/391],                 Loss: 0.01288, Train_Acc:99.82%
Epoch [158/300], Step [180/391],                 Loss: 0.01267, Train_Acc:99.83%
Epoch [158/300], Step [190/391],                 Loss: 0.01259, Train_Acc:99.83%
Epoch [158/300], Step [200/391],                 Loss: 0.01262, Train_Acc:99.83%
Epoch [158/300], Step [210/391],                 Loss: 0.01256, Train_Acc:99.83%
Epoch [158/300], Step [220/391],                 Loss: 0.01242, Train_Acc:99.84%
Epoch [158/300], Step [230/391],                 Loss: 0.01232, Train_Acc:99.84%
Epoch [158/300], Step [240/391],                 Loss: 0.01213, Train_Acc:99.85%
Epoch [158/300], Step [250/391],                 Loss: 0.01206, Train_Acc:99.85%
Epoch [158/300], Step [260/391],                 Loss: 0.01194, Train_Acc:99.86%
Epoch [158/300], Step [270/391],                 Loss: 0.01181, Train_Acc:99.86%
Epoch [158/300], Step [280/391],                 Loss: 0.01167, Train_Acc:99.86%
Epoch [158/300], Step [290/391],                 Loss: 0.01159, Train_Acc:99.86%
Epoch [158/300], Step [300/391],                 Loss: 0.01164, Train_Acc:99.86%
Epoch [158/300], Step [310/391],                 Loss: 0.01149, Train_Acc:99.86%
Epoch [158/300], Step [320/391],                 Loss: 0.01138, Train_Acc:99.87%
Epoch [158/300], Step [330/391],                 Loss: 0.01126, Train_Acc:99.87%
Epoch [158/300], Step [340/391],                 Loss: 0.01115, Train_Acc:99.87%
Epoch [158/300], Step [350/391],                 Loss: 0.01109, Train_Acc:99.88%
Epoch [158/300], Step [360/391],                 Loss: 0.01095, Train_Acc:99.88%
Epoch [158/300], Step [370/391],                 Loss: 0.01092, Train_Acc:99.88%
Epoch [158/300], Step [380/391],                 Loss: 0.01079, Train_Acc:99.88%
Epoch [158/300], Step [390/391],                 Loss: 0.01069, Train_Acc:99.88%
Accuary on test images:91.34%
Epoch [159/300], Step [10/391],                 Loss: 0.00765, Train_Acc:100.00%
Epoch [159/300], Step [20/391],                 Loss: 0.00932, Train_Acc:99.92%
Epoch [159/300], Step [30/391],                 Loss: 0.00860, Train_Acc:99.95%
Epoch [159/300], Step [40/391],                 Loss: 0.00861, Train_Acc:99.94%
Epoch [159/300], Step [50/391],                 Loss: 0.00844, Train_Acc:99.94%
Epoch [159/300], Step [60/391],                 Loss: 0.00839, Train_Acc:99.93%
Epoch [159/300], Step [70/391],                 Loss: 0.00852, Train_Acc:99.93%
Epoch [159/300], Step [80/391],                 Loss: 0.00860, Train_Acc:99.93%
Epoch [159/300], Step [90/391],                 Loss: 0.00850, Train_Acc:99.93%
Epoch [159/300], Step [100/391],                 Loss: 0.00852, Train_Acc:99.92%
Epoch [159/300], Step [110/391],                 Loss: 0.00836, Train_Acc:99.93%
Epoch [159/300], Step [120/391],                 Loss: 0.00828, Train_Acc:99.93%
Epoch [159/300], Step [130/391],                 Loss: 0.00860, Train_Acc:99.92%
Epoch [159/300], Step [140/391],                 Loss: 0.00848, Train_Acc:99.93%
Epoch [159/300], Step [150/391],                 Loss: 0.00854, Train_Acc:99.93%
Epoch [159/300], Step [160/391],                 Loss: 0.00855, Train_Acc:99.93%
Epoch [159/300], Step [170/391],                 Loss: 0.00856, Train_Acc:99.93%
Epoch [159/300], Step [180/391],                 Loss: 0.00845, Train_Acc:99.93%
Epoch [159/300], Step [190/391],                 Loss: 0.00840, Train_Acc:99.93%
Epoch [159/300], Step [200/391],                 Loss: 0.00843, Train_Acc:99.93%
Epoch [159/300], Step [210/391],                 Loss: 0.00838, Train_Acc:99.93%
Epoch [159/300], Step [220/391],                 Loss: 0.00824, Train_Acc:99.93%
Epoch [159/300], Step [230/391],                 Loss: 0.00821, Train_Acc:99.93%
Epoch [159/300], Step [240/391],                 Loss: 0.00810, Train_Acc:99.93%
Epoch [159/300], Step [250/391],                 Loss: 0.00811, Train_Acc:99.93%
Epoch [159/300], Step [260/391],                 Loss: 0.00808, Train_Acc:99.94%
Epoch [159/300], Step [270/391],                 Loss: 0.00802, Train_Acc:99.94%
Epoch [159/300], Step [280/391],                 Loss: 0.00793, Train_Acc:99.94%
Epoch [159/300], Step [290/391],                 Loss: 0.00789, Train_Acc:99.94%
Epoch [159/300], Step [300/391],                 Loss: 0.00790, Train_Acc:99.94%
Epoch [159/300], Step [310/391],                 Loss: 0.00804, Train_Acc:99.93%
Epoch [159/300], Step [320/391],                 Loss: 0.00799, Train_Acc:99.93%
Epoch [159/300], Step [330/391],                 Loss: 0.00792, Train_Acc:99.93%
Epoch [159/300], Step [340/391],                 Loss: 0.00785, Train_Acc:99.94%
Epoch [159/300], Step [350/391],                 Loss: 0.00782, Train_Acc:99.94%
Epoch [159/300], Step [360/391],                 Loss: 0.00775, Train_Acc:99.94%
Epoch [159/300], Step [370/391],                 Loss: 0.00773, Train_Acc:99.94%
Epoch [159/300], Step [380/391],                 Loss: 0.00766, Train_Acc:99.94%
Epoch [159/300], Step [390/391],                 Loss: 0.00765, Train_Acc:99.94%
Accuary on test images:91.50%
Epoch [160/300], Step [10/391],                 Loss: 0.00698, Train_Acc:100.00%
Epoch [160/300], Step [20/391],                 Loss: 0.00654, Train_Acc:100.00%
Epoch [160/300], Step [30/391],                 Loss: 0.00630, Train_Acc:100.00%
Epoch [160/300], Step [40/391],                 Loss: 0.00629, Train_Acc:100.00%
Epoch [160/300], Step [50/391],                 Loss: 0.00646, Train_Acc:99.98%
Epoch [160/300], Step [60/391],                 Loss: 0.00695, Train_Acc:99.97%
Epoch [160/300], Step [70/391],                 Loss: 0.00690, Train_Acc:99.98%
Epoch [160/300], Step [80/391],                 Loss: 0.00680, Train_Acc:99.98%
Epoch [160/300], Step [90/391],                 Loss: 0.00669, Train_Acc:99.98%
Epoch [160/300], Step [100/391],                 Loss: 0.00657, Train_Acc:99.98%
Epoch [160/300], Step [110/391],                 Loss: 0.00667, Train_Acc:99.98%
Epoch [160/300], Step [120/391],                 Loss: 0.00701, Train_Acc:99.97%
Epoch [160/300], Step [130/391],                 Loss: 0.00690, Train_Acc:99.98%
Epoch [160/300], Step [140/391],                 Loss: 0.00682, Train_Acc:99.98%
Epoch [160/300], Step [150/391],                 Loss: 0.00687, Train_Acc:99.97%
Epoch [160/300], Step [160/391],                 Loss: 0.00679, Train_Acc:99.98%
Epoch [160/300], Step [170/391],                 Loss: 0.00677, Train_Acc:99.97%
Epoch [160/300], Step [180/391],                 Loss: 0.00673, Train_Acc:99.97%
Epoch [160/300], Step [190/391],                 Loss: 0.00676, Train_Acc:99.97%
Epoch [160/300], Step [200/391],                 Loss: 0.00676, Train_Acc:99.97%
Epoch [160/300], Step [210/391],                 Loss: 0.00669, Train_Acc:99.97%
Epoch [160/300], Step [220/391],                 Loss: 0.00671, Train_Acc:99.97%
Epoch [160/300], Step [230/391],                 Loss: 0.00663, Train_Acc:99.97%
Epoch [160/300], Step [240/391],                 Loss: 0.00658, Train_Acc:99.97%
Epoch [160/300], Step [250/391],                 Loss: 0.00651, Train_Acc:99.97%
Epoch [160/300], Step [260/391],                 Loss: 0.00651, Train_Acc:99.97%
Epoch [160/300], Step [270/391],                 Loss: 0.00648, Train_Acc:99.97%
Epoch [160/300], Step [280/391],                 Loss: 0.00642, Train_Acc:99.97%
Epoch [160/300], Step [290/391],                 Loss: 0.00637, Train_Acc:99.97%
Epoch [160/300], Step [300/391],                 Loss: 0.00645, Train_Acc:99.97%
Epoch [160/300], Step [310/391],                 Loss: 0.00641, Train_Acc:99.97%
Epoch [160/300], Step [320/391],                 Loss: 0.00640, Train_Acc:99.97%
Epoch [160/300], Step [330/391],                 Loss: 0.00651, Train_Acc:99.97%
Epoch [160/300], Step [340/391],                 Loss: 0.00648, Train_Acc:99.97%
Epoch [160/300], Step [350/391],                 Loss: 0.00643, Train_Acc:99.97%
Epoch [160/300], Step [360/391],                 Loss: 0.00637, Train_Acc:99.97%
Epoch [160/300], Step [370/391],                 Loss: 0.00631, Train_Acc:99.97%
Epoch [160/300], Step [380/391],                 Loss: 0.00625, Train_Acc:99.97%
Epoch [160/300], Step [390/391],                 Loss: 0.00622, Train_Acc:99.97%
Accuary on test images:91.34%
Epoch [161/300], Step [10/391],                 Loss: 0.00509, Train_Acc:100.00%
Epoch [161/300], Step [20/391],                 Loss: 0.00527, Train_Acc:100.00%
Epoch [161/300], Step [30/391],                 Loss: 0.00558, Train_Acc:99.97%
Epoch [161/300], Step [40/391],                 Loss: 0.00552, Train_Acc:99.98%
Epoch [161/300], Step [50/391],                 Loss: 0.00551, Train_Acc:99.98%
Epoch [161/300], Step [60/391],                 Loss: 0.00537, Train_Acc:99.99%
Epoch [161/300], Step [70/391],                 Loss: 0.00541, Train_Acc:99.99%
Epoch [161/300], Step [80/391],                 Loss: 0.00542, Train_Acc:99.99%
Epoch [161/300], Step [90/391],                 Loss: 0.00537, Train_Acc:99.99%
Epoch [161/300], Step [100/391],                 Loss: 0.00533, Train_Acc:99.99%
Epoch [161/300], Step [110/391],                 Loss: 0.00529, Train_Acc:99.99%
Epoch [161/300], Step [120/391],                 Loss: 0.00523, Train_Acc:99.99%
Epoch [161/300], Step [130/391],                 Loss: 0.00527, Train_Acc:99.99%
Epoch [161/300], Step [140/391],                 Loss: 0.00526, Train_Acc:99.99%
Epoch [161/300], Step [150/391],                 Loss: 0.00528, Train_Acc:99.99%
Epoch [161/300], Step [160/391],                 Loss: 0.00522, Train_Acc:100.00%
Epoch [161/300], Step [170/391],                 Loss: 0.00521, Train_Acc:100.00%
Epoch [161/300], Step [180/391],                 Loss: 0.00518, Train_Acc:100.00%
Epoch [161/300], Step [190/391],                 Loss: 0.00515, Train_Acc:100.00%
Epoch [161/300], Step [200/391],                 Loss: 0.00513, Train_Acc:100.00%
Epoch [161/300], Step [210/391],                 Loss: 0.00511, Train_Acc:100.00%
Epoch [161/300], Step [220/391],                 Loss: 0.00509, Train_Acc:100.00%
Epoch [161/300], Step [230/391],                 Loss: 0.00509, Train_Acc:100.00%
Epoch [161/300], Step [240/391],                 Loss: 0.00506, Train_Acc:100.00%
Epoch [161/300], Step [250/391],                 Loss: 0.00503, Train_Acc:100.00%
Epoch [161/300], Step [260/391],                 Loss: 0.00513, Train_Acc:99.99%
Epoch [161/300], Step [270/391],                 Loss: 0.00512, Train_Acc:99.99%
Epoch [161/300], Step [280/391],                 Loss: 0.00507, Train_Acc:99.99%
Epoch [161/300], Step [290/391],                 Loss: 0.00504, Train_Acc:99.99%
Epoch [161/300], Step [300/391],                 Loss: 0.00503, Train_Acc:99.99%
Epoch [161/300], Step [310/391],                 Loss: 0.00502, Train_Acc:99.99%
Epoch [161/300], Step [320/391],                 Loss: 0.00501, Train_Acc:100.00%
Epoch [161/300], Step [330/391],                 Loss: 0.00500, Train_Acc:100.00%
Epoch [161/300], Step [340/391],                 Loss: 0.00497, Train_Acc:100.00%
Epoch [161/300], Step [350/391],                 Loss: 0.00496, Train_Acc:100.00%
Epoch [161/300], Step [360/391],                 Loss: 0.00493, Train_Acc:100.00%
Epoch [161/300], Step [370/391],                 Loss: 0.00491, Train_Acc:100.00%
Epoch [161/300], Step [380/391],                 Loss: 0.00489, Train_Acc:100.00%
Epoch [161/300], Step [390/391],                 Loss: 0.00488, Train_Acc:100.00%
Accuary on test images:91.42%
Epoch [162/300], Step [10/391],                 Loss: 0.00519, Train_Acc:100.00%
Epoch [162/300], Step [20/391],                 Loss: 0.00494, Train_Acc:100.00%
Epoch [162/300], Step [30/391],                 Loss: 0.00477, Train_Acc:100.00%
Epoch [162/300], Step [40/391],                 Loss: 0.00474, Train_Acc:100.00%
Epoch [162/300], Step [50/391],                 Loss: 0.00469, Train_Acc:100.00%
Epoch [162/300], Step [60/391],                 Loss: 0.00471, Train_Acc:100.00%
Epoch [162/300], Step [70/391],                 Loss: 0.00472, Train_Acc:100.00%
Epoch [162/300], Step [80/391],                 Loss: 0.00476, Train_Acc:99.99%
Epoch [162/300], Step [90/391],                 Loss: 0.00469, Train_Acc:99.99%
Epoch [162/300], Step [100/391],                 Loss: 0.00463, Train_Acc:99.99%
Epoch [162/300], Step [110/391],                 Loss: 0.00463, Train_Acc:99.99%
Epoch [162/300], Step [120/391],                 Loss: 0.00465, Train_Acc:99.99%
Epoch [162/300], Step [130/391],                 Loss: 0.00468, Train_Acc:99.99%
Epoch [162/300], Step [140/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [162/300], Step [150/391],                 Loss: 0.00469, Train_Acc:99.99%
Epoch [162/300], Step [160/391],                 Loss: 0.00468, Train_Acc:100.00%
Epoch [162/300], Step [170/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [180/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [190/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [200/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [162/300], Step [210/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [220/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [230/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [240/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [250/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [162/300], Step [260/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [162/300], Step [270/391],                 Loss: 0.00477, Train_Acc:99.99%
Epoch [162/300], Step [280/391],                 Loss: 0.00474, Train_Acc:99.99%
Epoch [162/300], Step [290/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [162/300], Step [300/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [162/300], Step [310/391],                 Loss: 0.00466, Train_Acc:99.99%
Epoch [162/300], Step [320/391],                 Loss: 0.00465, Train_Acc:99.99%
Epoch [162/300], Step [330/391],                 Loss: 0.00463, Train_Acc:99.99%
Epoch [162/300], Step [340/391],                 Loss: 0.00461, Train_Acc:99.99%
Epoch [162/300], Step [350/391],                 Loss: 0.00460, Train_Acc:99.99%
Epoch [162/300], Step [360/391],                 Loss: 0.00458, Train_Acc:99.99%
Epoch [162/300], Step [370/391],                 Loss: 0.00456, Train_Acc:99.99%
Epoch [162/300], Step [380/391],                 Loss: 0.00453, Train_Acc:99.99%
Epoch [162/300], Step [390/391],                 Loss: 0.00453, Train_Acc:99.99%
Accuary on test images:91.32%
Epoch [163/300], Step [10/391],                 Loss: 0.00432, Train_Acc:100.00%
Epoch [163/300], Step [20/391],                 Loss: 0.00458, Train_Acc:100.00%
Epoch [163/300], Step [30/391],                 Loss: 0.00439, Train_Acc:100.00%
Epoch [163/300], Step [40/391],                 Loss: 0.00435, Train_Acc:100.00%
Epoch [163/300], Step [50/391],                 Loss: 0.00466, Train_Acc:99.98%
Epoch [163/300], Step [60/391],                 Loss: 0.00461, Train_Acc:99.99%
Epoch [163/300], Step [70/391],                 Loss: 0.00466, Train_Acc:99.99%
Epoch [163/300], Step [80/391],                 Loss: 0.00462, Train_Acc:99.99%
Epoch [163/300], Step [90/391],                 Loss: 0.00457, Train_Acc:99.99%
Epoch [163/300], Step [100/391],                 Loss: 0.00455, Train_Acc:99.99%
Epoch [163/300], Step [110/391],                 Loss: 0.00454, Train_Acc:99.99%
Epoch [163/300], Step [120/391],                 Loss: 0.00453, Train_Acc:99.99%
Epoch [163/300], Step [130/391],                 Loss: 0.00452, Train_Acc:99.99%
Epoch [163/300], Step [140/391],                 Loss: 0.00451, Train_Acc:99.99%
Epoch [163/300], Step [150/391],                 Loss: 0.00455, Train_Acc:99.99%
Epoch [163/300], Step [160/391],                 Loss: 0.00451, Train_Acc:100.00%
Epoch [163/300], Step [170/391],                 Loss: 0.00450, Train_Acc:100.00%
Epoch [163/300], Step [180/391],                 Loss: 0.00451, Train_Acc:100.00%
Epoch [163/300], Step [190/391],                 Loss: 0.00448, Train_Acc:100.00%
Epoch [163/300], Step [200/391],                 Loss: 0.00448, Train_Acc:100.00%
Epoch [163/300], Step [210/391],                 Loss: 0.00447, Train_Acc:100.00%
Epoch [163/300], Step [220/391],                 Loss: 0.00446, Train_Acc:100.00%
Epoch [163/300], Step [230/391],                 Loss: 0.00444, Train_Acc:100.00%
Epoch [163/300], Step [240/391],                 Loss: 0.00446, Train_Acc:99.99%
Epoch [163/300], Step [250/391],                 Loss: 0.00445, Train_Acc:99.99%
Epoch [163/300], Step [260/391],                 Loss: 0.00446, Train_Acc:99.99%
Epoch [163/300], Step [270/391],                 Loss: 0.00446, Train_Acc:99.99%
Epoch [163/300], Step [280/391],                 Loss: 0.00445, Train_Acc:99.99%
Epoch [163/300], Step [290/391],                 Loss: 0.00443, Train_Acc:99.99%
Epoch [163/300], Step [300/391],                 Loss: 0.00443, Train_Acc:99.99%
Epoch [163/300], Step [310/391],                 Loss: 0.00441, Train_Acc:99.99%
Epoch [163/300], Step [320/391],                 Loss: 0.00442, Train_Acc:100.00%
Epoch [163/300], Step [330/391],                 Loss: 0.00441, Train_Acc:100.00%
Epoch [163/300], Step [340/391],                 Loss: 0.00440, Train_Acc:100.00%
Epoch [163/300], Step [350/391],                 Loss: 0.00440, Train_Acc:100.00%
Epoch [163/300], Step [360/391],                 Loss: 0.00438, Train_Acc:100.00%
Epoch [163/300], Step [370/391],                 Loss: 0.00436, Train_Acc:100.00%
Epoch [163/300], Step [380/391],                 Loss: 0.00434, Train_Acc:100.00%
Epoch [163/300], Step [390/391],                 Loss: 0.00433, Train_Acc:100.00%
Accuary on test images:91.48%
Epoch [164/300], Step [10/391],                 Loss: 0.00446, Train_Acc:100.00%
Epoch [164/300], Step [20/391],                 Loss: 0.00447, Train_Acc:100.00%
Epoch [164/300], Step [30/391],                 Loss: 0.00432, Train_Acc:100.00%
Epoch [164/300], Step [40/391],                 Loss: 0.00428, Train_Acc:100.00%
Epoch [164/300], Step [50/391],                 Loss: 0.00423, Train_Acc:100.00%
Epoch [164/300], Step [60/391],                 Loss: 0.00427, Train_Acc:100.00%
Epoch [164/300], Step [70/391],                 Loss: 0.00427, Train_Acc:100.00%
Epoch [164/300], Step [80/391],                 Loss: 0.00425, Train_Acc:100.00%
Epoch [164/300], Step [90/391],                 Loss: 0.00421, Train_Acc:100.00%
Epoch [164/300], Step [100/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [110/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [120/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [130/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [140/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [150/391],                 Loss: 0.00418, Train_Acc:100.00%
Epoch [164/300], Step [160/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [164/300], Step [170/391],                 Loss: 0.00415, Train_Acc:100.00%
Epoch [164/300], Step [180/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [190/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [200/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [210/391],                 Loss: 0.00418, Train_Acc:100.00%
Epoch [164/300], Step [220/391],                 Loss: 0.00418, Train_Acc:100.00%
Epoch [164/300], Step [230/391],                 Loss: 0.00419, Train_Acc:100.00%
Epoch [164/300], Step [240/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [250/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [260/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [270/391],                 Loss: 0.00418, Train_Acc:100.00%
Epoch [164/300], Step [280/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [164/300], Step [290/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [164/300], Step [300/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [164/300], Step [310/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [320/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [330/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [164/300], Step [340/391],                 Loss: 0.00415, Train_Acc:100.00%
Epoch [164/300], Step [350/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [360/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [370/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [164/300], Step [380/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [164/300], Step [390/391],                 Loss: 0.00411, Train_Acc:100.00%
Accuary on test images:91.46%
Epoch [165/300], Step [10/391],                 Loss: 0.00451, Train_Acc:100.00%
Epoch [165/300], Step [20/391],                 Loss: 0.00432, Train_Acc:100.00%
Epoch [165/300], Step [30/391],                 Loss: 0.00420, Train_Acc:100.00%
Epoch [165/300], Step [40/391],                 Loss: 0.00431, Train_Acc:100.00%
Epoch [165/300], Step [50/391],                 Loss: 0.00424, Train_Acc:100.00%
Epoch [165/300], Step [60/391],                 Loss: 0.00421, Train_Acc:100.00%
Epoch [165/300], Step [70/391],                 Loss: 0.00419, Train_Acc:100.00%
Epoch [165/300], Step [80/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [165/300], Step [90/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [165/300], Step [100/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [165/300], Step [110/391],                 Loss: 0.00410, Train_Acc:100.00%
Epoch [165/300], Step [120/391],                 Loss: 0.00410, Train_Acc:100.00%
Epoch [165/300], Step [130/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [165/300], Step [140/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [165/300], Step [150/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [165/300], Step [160/391],                 Loss: 0.00410, Train_Acc:100.00%
Epoch [165/300], Step [170/391],                 Loss: 0.00408, Train_Acc:100.00%
Epoch [165/300], Step [180/391],                 Loss: 0.00407, Train_Acc:100.00%
Epoch [165/300], Step [190/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [165/300], Step [200/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [165/300], Step [210/391],                 Loss: 0.00405, Train_Acc:100.00%
Epoch [165/300], Step [220/391],                 Loss: 0.00405, Train_Acc:100.00%
Epoch [165/300], Step [230/391],                 Loss: 0.00405, Train_Acc:100.00%
Epoch [165/300], Step [240/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [165/300], Step [250/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [165/300], Step [260/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [165/300], Step [270/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [165/300], Step [280/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [165/300], Step [290/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [165/300], Step [300/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [165/300], Step [310/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [165/300], Step [320/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [165/300], Step [330/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [165/300], Step [340/391],                 Loss: 0.00398, Train_Acc:100.00%
Epoch [165/300], Step [350/391],                 Loss: 0.00398, Train_Acc:100.00%
Epoch [165/300], Step [360/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [165/300], Step [370/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [165/300], Step [380/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [165/300], Step [390/391],                 Loss: 0.00395, Train_Acc:100.00%
Accuary on test images:91.54%
Epoch [166/300], Step [10/391],                 Loss: 0.00437, Train_Acc:100.00%
Epoch [166/300], Step [20/391],                 Loss: 0.00429, Train_Acc:100.00%
Epoch [166/300], Step [30/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [166/300], Step [40/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [166/300], Step [50/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [166/300], Step [60/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [166/300], Step [70/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [166/300], Step [80/391],                 Loss: 0.00410, Train_Acc:100.00%
Epoch [166/300], Step [90/391],                 Loss: 0.00408, Train_Acc:100.00%
Epoch [166/300], Step [100/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [166/300], Step [110/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [166/300], Step [120/391],                 Loss: 0.00405, Train_Acc:100.00%
Epoch [166/300], Step [130/391],                 Loss: 0.00406, Train_Acc:100.00%
Epoch [166/300], Step [140/391],                 Loss: 0.00405, Train_Acc:100.00%
Epoch [166/300], Step [150/391],                 Loss: 0.00406, Train_Acc:100.00%
Epoch [166/300], Step [160/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [166/300], Step [170/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [166/300], Step [180/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [166/300], Step [190/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [166/300], Step [200/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [166/300], Step [210/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [166/300], Step [220/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [166/300], Step [230/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [166/300], Step [240/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [166/300], Step [250/391],                 Loss: 0.00398, Train_Acc:100.00%
Epoch [166/300], Step [260/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [166/300], Step [270/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [166/300], Step [280/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [166/300], Step [290/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [166/300], Step [300/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [166/300], Step [310/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [166/300], Step [320/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [166/300], Step [330/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [166/300], Step [340/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [166/300], Step [350/391],                 Loss: 0.00392, Train_Acc:100.00%
Epoch [166/300], Step [360/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [166/300], Step [370/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [166/300], Step [380/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [166/300], Step [390/391],                 Loss: 0.00388, Train_Acc:100.00%
Accuary on test images:91.54%
Epoch [167/300], Step [10/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [167/300], Step [20/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [167/300], Step [30/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [167/300], Step [40/391],                 Loss: 0.00386, Train_Acc:100.00%
Epoch [167/300], Step [50/391],                 Loss: 0.00383, Train_Acc:100.00%
Epoch [167/300], Step [60/391],                 Loss: 0.00387, Train_Acc:100.00%
Epoch [167/300], Step [70/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [167/300], Step [80/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [167/300], Step [90/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [167/300], Step [100/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [167/300], Step [110/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [167/300], Step [120/391],                 Loss: 0.00394, Train_Acc:99.99%
Epoch [167/300], Step [130/391],                 Loss: 0.00394, Train_Acc:99.99%
Epoch [167/300], Step [140/391],                 Loss: 0.00395, Train_Acc:99.99%
Epoch [167/300], Step [150/391],                 Loss: 0.00398, Train_Acc:99.99%
Epoch [167/300], Step [160/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [167/300], Step [170/391],                 Loss: 0.00398, Train_Acc:100.00%
Epoch [167/300], Step [180/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [167/300], Step [190/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [167/300], Step [200/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [167/300], Step [210/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [167/300], Step [220/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [167/300], Step [230/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [167/300], Step [240/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [167/300], Step [250/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [167/300], Step [260/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [167/300], Step [270/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [167/300], Step [280/391],                 Loss: 0.00392, Train_Acc:100.00%
Epoch [167/300], Step [290/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [167/300], Step [300/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [167/300], Step [310/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [167/300], Step [320/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [167/300], Step [330/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [167/300], Step [340/391],                 Loss: 0.00387, Train_Acc:100.00%
Epoch [167/300], Step [350/391],                 Loss: 0.00386, Train_Acc:100.00%
Epoch [167/300], Step [360/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [167/300], Step [370/391],                 Loss: 0.00386, Train_Acc:100.00%
Epoch [167/300], Step [380/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [167/300], Step [390/391],                 Loss: 0.00385, Train_Acc:100.00%
Accuary on test images:91.50%
Epoch [168/300], Step [10/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [168/300], Step [20/391],                 Loss: 0.00405, Train_Acc:100.00%
Epoch [168/300], Step [30/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [168/300], Step [40/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [168/300], Step [50/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [168/300], Step [60/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [168/300], Step [70/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [168/300], Step [80/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [168/300], Step [90/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [168/300], Step [100/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [168/300], Step [110/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [168/300], Step [120/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [168/300], Step [130/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [168/300], Step [140/391],                 Loss: 0.00392, Train_Acc:100.00%
Epoch [168/300], Step [150/391],                 Loss: 0.00392, Train_Acc:100.00%
Epoch [168/300], Step [160/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [168/300], Step [170/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [168/300], Step [180/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [168/300], Step [190/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [168/300], Step [200/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [168/300], Step [210/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [168/300], Step [220/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [168/300], Step [230/391],                 Loss: 0.00398, Train_Acc:100.00%
Epoch [168/300], Step [240/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [168/300], Step [250/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [168/300], Step [260/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [168/300], Step [270/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [168/300], Step [280/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [168/300], Step [290/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [168/300], Step [300/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [168/300], Step [310/391],                 Loss: 0.00392, Train_Acc:100.00%
Epoch [168/300], Step [320/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [168/300], Step [330/391],                 Loss: 0.00392, Train_Acc:100.00%
Epoch [168/300], Step [340/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [168/300], Step [350/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [168/300], Step [360/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [168/300], Step [370/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [168/300], Step [380/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [168/300], Step [390/391],                 Loss: 0.00388, Train_Acc:100.00%
Accuary on test images:91.54%
Epoch [169/300], Step [10/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [169/300], Step [20/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [169/300], Step [30/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [169/300], Step [40/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [169/300], Step [50/391],                 Loss: 0.00406, Train_Acc:99.98%
Epoch [169/300], Step [60/391],                 Loss: 0.00407, Train_Acc:99.99%
Epoch [169/300], Step [70/391],                 Loss: 0.00408, Train_Acc:99.99%
Epoch [169/300], Step [80/391],                 Loss: 0.00404, Train_Acc:99.99%
Epoch [169/300], Step [90/391],                 Loss: 0.00402, Train_Acc:99.99%
Epoch [169/300], Step [100/391],                 Loss: 0.00398, Train_Acc:99.99%
Epoch [169/300], Step [110/391],                 Loss: 0.00398, Train_Acc:99.99%
Epoch [169/300], Step [120/391],                 Loss: 0.00399, Train_Acc:99.99%
Epoch [169/300], Step [130/391],                 Loss: 0.00398, Train_Acc:99.99%
Epoch [169/300], Step [140/391],                 Loss: 0.00399, Train_Acc:99.99%
Epoch [169/300], Step [150/391],                 Loss: 0.00399, Train_Acc:99.99%
Epoch [169/300], Step [160/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [169/300], Step [170/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [169/300], Step [180/391],                 Loss: 0.00396, Train_Acc:100.00%
Epoch [169/300], Step [190/391],                 Loss: 0.00395, Train_Acc:100.00%
Epoch [169/300], Step [200/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [169/300], Step [210/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [169/300], Step [220/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [169/300], Step [230/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [169/300], Step [240/391],                 Loss: 0.00391, Train_Acc:100.00%
Epoch [169/300], Step [250/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [169/300], Step [260/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [169/300], Step [270/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [169/300], Step [280/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [169/300], Step [290/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [169/300], Step [300/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [169/300], Step [310/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [169/300], Step [320/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [169/300], Step [330/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [169/300], Step [340/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [169/300], Step [350/391],                 Loss: 0.00387, Train_Acc:100.00%
Epoch [169/300], Step [360/391],                 Loss: 0.00386, Train_Acc:100.00%
Epoch [169/300], Step [370/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [169/300], Step [380/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [169/300], Step [390/391],                 Loss: 0.00385, Train_Acc:100.00%
Accuary on test images:91.44%
Epoch [170/300], Step [10/391],                 Loss: 0.00410, Train_Acc:100.00%
Epoch [170/300], Step [20/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [170/300], Step [30/391],                 Loss: 0.00394, Train_Acc:100.00%
Epoch [170/300], Step [40/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [170/300], Step [50/391],                 Loss: 0.00384, Train_Acc:100.00%
Epoch [170/300], Step [60/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [170/300], Step [70/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [170/300], Step [80/391],                 Loss: 0.00386, Train_Acc:100.00%
Epoch [170/300], Step [90/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [170/300], Step [100/391],                 Loss: 0.00382, Train_Acc:100.00%
Epoch [170/300], Step [110/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [170/300], Step [120/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [170/300], Step [130/391],                 Loss: 0.00380, Train_Acc:100.00%
Epoch [170/300], Step [140/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [170/300], Step [150/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [170/300], Step [160/391],                 Loss: 0.00379, Train_Acc:100.00%
Epoch [170/300], Step [170/391],                 Loss: 0.00379, Train_Acc:100.00%
Epoch [170/300], Step [180/391],                 Loss: 0.00379, Train_Acc:100.00%
Epoch [170/300], Step [190/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [200/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [210/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [220/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [230/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [240/391],                 Loss: 0.00377, Train_Acc:100.00%
Epoch [170/300], Step [250/391],                 Loss: 0.00377, Train_Acc:100.00%
Epoch [170/300], Step [260/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [270/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [170/300], Step [280/391],                 Loss: 0.00377, Train_Acc:100.00%
Epoch [170/300], Step [290/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [170/300], Step [300/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [170/300], Step [310/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [170/300], Step [320/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [170/300], Step [330/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [170/300], Step [340/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [170/300], Step [350/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [170/300], Step [360/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [170/300], Step [370/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [170/300], Step [380/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [170/300], Step [390/391],                 Loss: 0.00371, Train_Acc:100.00%
Accuary on test images:91.58%
Epoch [171/300], Step [10/391],                 Loss: 0.00408, Train_Acc:100.00%
Epoch [171/300], Step [20/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [171/300], Step [30/391],                 Loss: 0.00387, Train_Acc:100.00%
Epoch [171/300], Step [40/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [171/300], Step [50/391],                 Loss: 0.00380, Train_Acc:100.00%
Epoch [171/300], Step [60/391],                 Loss: 0.00380, Train_Acc:100.00%
Epoch [171/300], Step [70/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [171/300], Step [80/391],                 Loss: 0.00379, Train_Acc:100.00%
Epoch [171/300], Step [90/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [171/300], Step [100/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [171/300], Step [110/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [171/300], Step [120/391],                 Loss: 0.00375, Train_Acc:100.00%
Epoch [171/300], Step [130/391],                 Loss: 0.00375, Train_Acc:100.00%
Epoch [171/300], Step [140/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [171/300], Step [150/391],                 Loss: 0.00377, Train_Acc:100.00%
Epoch [171/300], Step [160/391],                 Loss: 0.00375, Train_Acc:100.00%
Epoch [171/300], Step [170/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [171/300], Step [180/391],                 Loss: 0.00375, Train_Acc:100.00%
Epoch [171/300], Step [190/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [171/300], Step [200/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [171/300], Step [210/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [171/300], Step [220/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [171/300], Step [230/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [171/300], Step [240/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [171/300], Step [250/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [171/300], Step [260/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [171/300], Step [270/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [171/300], Step [280/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [171/300], Step [290/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [171/300], Step [300/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [171/300], Step [310/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [171/300], Step [320/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [171/300], Step [330/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [171/300], Step [340/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [171/300], Step [350/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [171/300], Step [360/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [171/300], Step [370/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [171/300], Step [380/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [171/300], Step [390/391],                 Loss: 0.00366, Train_Acc:100.00%
Accuary on test images:91.58%
Epoch [172/300], Step [10/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [172/300], Step [20/391],                 Loss: 0.00393, Train_Acc:100.00%
Epoch [172/300], Step [30/391],                 Loss: 0.00385, Train_Acc:100.00%
Epoch [172/300], Step [40/391],                 Loss: 0.00383, Train_Acc:100.00%
Epoch [172/300], Step [50/391],                 Loss: 0.00380, Train_Acc:100.00%
Epoch [172/300], Step [60/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [172/300], Step [70/391],                 Loss: 0.00381, Train_Acc:100.00%
Epoch [172/300], Step [80/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [172/300], Step [90/391],                 Loss: 0.00375, Train_Acc:100.00%
Epoch [172/300], Step [100/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [172/300], Step [110/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [172/300], Step [120/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [172/300], Step [130/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [172/300], Step [140/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [172/300], Step [150/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [172/300], Step [160/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [172/300], Step [170/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [172/300], Step [180/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [172/300], Step [190/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [172/300], Step [200/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [172/300], Step [210/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [172/300], Step [220/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [172/300], Step [230/391],                 Loss: 0.00371, Train_Acc:100.00%
Epoch [172/300], Step [240/391],                 Loss: 0.00370, Train_Acc:100.00%
Epoch [172/300], Step [250/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [172/300], Step [260/391],                 Loss: 0.00370, Train_Acc:100.00%
Epoch [172/300], Step [270/391],                 Loss: 0.00370, Train_Acc:100.00%
Epoch [172/300], Step [280/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [172/300], Step [290/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [172/300], Step [300/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [172/300], Step [310/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [172/300], Step [320/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [172/300], Step [330/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [172/300], Step [340/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [172/300], Step [350/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [172/300], Step [360/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [172/300], Step [370/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [172/300], Step [380/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [172/300], Step [390/391],                 Loss: 0.00365, Train_Acc:100.00%
Accuary on test images:91.54%
Epoch [173/300], Step [10/391],                 Loss: 0.00390, Train_Acc:100.00%
Epoch [173/300], Step [20/391],                 Loss: 0.00389, Train_Acc:100.00%
Epoch [173/300], Step [30/391],                 Loss: 0.00380, Train_Acc:100.00%
Epoch [173/300], Step [40/391],                 Loss: 0.00378, Train_Acc:100.00%
Epoch [173/300], Step [50/391],                 Loss: 0.00373, Train_Acc:100.00%
Epoch [173/300], Step [60/391],                 Loss: 0.00377, Train_Acc:100.00%
Epoch [173/300], Step [70/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [173/300], Step [80/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [173/300], Step [90/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [173/300], Step [100/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [173/300], Step [110/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [173/300], Step [120/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [173/300], Step [130/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [173/300], Step [140/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [173/300], Step [150/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [173/300], Step [160/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [173/300], Step [170/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [173/300], Step [180/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [173/300], Step [190/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [173/300], Step [200/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [173/300], Step [210/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [173/300], Step [220/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [173/300], Step [230/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [173/300], Step [240/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [173/300], Step [250/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [173/300], Step [260/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [173/300], Step [270/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [173/300], Step [280/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [173/300], Step [290/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [173/300], Step [300/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [173/300], Step [310/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [173/300], Step [320/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [173/300], Step [330/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [173/300], Step [340/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [173/300], Step [350/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [173/300], Step [360/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [173/300], Step [370/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [173/300], Step [380/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [173/300], Step [390/391],                 Loss: 0.00360, Train_Acc:100.00%
Accuary on test images:91.64%
Epoch [174/300], Step [10/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [174/300], Step [20/391],                 Loss: 0.00376, Train_Acc:100.00%
Epoch [174/300], Step [30/391],                 Loss: 0.00367, Train_Acc:100.00%
Epoch [174/300], Step [40/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [174/300], Step [50/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [174/300], Step [60/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [174/300], Step [70/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [174/300], Step [80/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [174/300], Step [90/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [174/300], Step [100/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [174/300], Step [110/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [120/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [130/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [140/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [174/300], Step [150/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [174/300], Step [160/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [170/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [180/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [190/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [200/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [210/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [220/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [230/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [174/300], Step [240/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [250/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [174/300], Step [260/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [270/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [174/300], Step [280/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [174/300], Step [290/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [300/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [174/300], Step [310/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [320/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [330/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [340/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [350/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [360/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [174/300], Step [370/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [174/300], Step [380/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [174/300], Step [390/391],                 Loss: 0.00358, Train_Acc:100.00%
Accuary on test images:91.66%
Epoch [175/300], Step [10/391],                 Loss: 0.00379, Train_Acc:100.00%
Epoch [175/300], Step [20/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [175/300], Step [30/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [175/300], Step [40/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [175/300], Step [50/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [175/300], Step [60/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [175/300], Step [70/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [175/300], Step [80/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [175/300], Step [90/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [175/300], Step [100/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [175/300], Step [110/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [175/300], Step [120/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [175/300], Step [130/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [175/300], Step [140/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [175/300], Step [150/391],                 Loss: 0.00365, Train_Acc:100.00%
Epoch [175/300], Step [160/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [175/300], Step [170/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [175/300], Step [180/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [175/300], Step [190/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [175/300], Step [200/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [175/300], Step [210/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [175/300], Step [220/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [175/300], Step [230/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [175/300], Step [240/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [175/300], Step [250/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [175/300], Step [260/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [175/300], Step [270/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [175/300], Step [280/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [175/300], Step [290/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [175/300], Step [300/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [175/300], Step [310/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [175/300], Step [320/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [175/300], Step [330/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [175/300], Step [340/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [175/300], Step [350/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [175/300], Step [360/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [175/300], Step [370/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [175/300], Step [380/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [175/300], Step [390/391],                 Loss: 0.00356, Train_Acc:100.00%
Accuary on test images:91.54%
Epoch [176/300], Step [10/391],                 Loss: 0.00384, Train_Acc:100.00%
Epoch [176/300], Step [20/391],                 Loss: 0.00375, Train_Acc:100.00%
Epoch [176/300], Step [30/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [176/300], Step [40/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [176/300], Step [50/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [176/300], Step [60/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [176/300], Step [70/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [176/300], Step [80/391],                 Loss: 0.00363, Train_Acc:100.00%
Epoch [176/300], Step [90/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [176/300], Step [100/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [176/300], Step [110/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [176/300], Step [120/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [176/300], Step [130/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [176/300], Step [140/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [176/300], Step [150/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [176/300], Step [160/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [176/300], Step [170/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [176/300], Step [180/391],                 Loss: 0.00359, Train_Acc:100.00%
Epoch [176/300], Step [190/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [200/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [210/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [220/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [230/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [240/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [176/300], Step [250/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [176/300], Step [260/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [270/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [176/300], Step [280/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [176/300], Step [290/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [176/300], Step [300/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [176/300], Step [310/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [176/300], Step [320/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [176/300], Step [330/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [176/300], Step [340/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [176/300], Step [350/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [176/300], Step [360/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [176/300], Step [370/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [176/300], Step [380/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [176/300], Step [390/391],                 Loss: 0.00353, Train_Acc:100.00%
Accuary on test images:91.52%
Epoch [177/300], Step [10/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [177/300], Step [20/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [177/300], Step [30/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [177/300], Step [40/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [177/300], Step [50/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [177/300], Step [60/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [177/300], Step [70/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [177/300], Step [80/391],                 Loss: 0.00356, Train_Acc:100.00%
Epoch [177/300], Step [90/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [177/300], Step [100/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [110/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [177/300], Step [120/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [130/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [140/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [177/300], Step [150/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [177/300], Step [160/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [177/300], Step [170/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [177/300], Step [180/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [190/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [177/300], Step [200/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [177/300], Step [210/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [220/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [230/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [177/300], Step [240/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [177/300], Step [250/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [177/300], Step [260/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [177/300], Step [270/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [177/300], Step [280/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [177/300], Step [290/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [177/300], Step [300/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [177/300], Step [310/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [177/300], Step [320/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [177/300], Step [330/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [177/300], Step [340/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [177/300], Step [350/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [177/300], Step [360/391],                 Loss: 0.00348, Train_Acc:100.00%
Epoch [177/300], Step [370/391],                 Loss: 0.00348, Train_Acc:100.00%
Epoch [177/300], Step [380/391],                 Loss: 0.00348, Train_Acc:100.00%
Epoch [177/300], Step [390/391],                 Loss: 0.00348, Train_Acc:100.00%
Accuary on test images:91.52%
Epoch [178/300], Step [10/391],                 Loss: 0.00370, Train_Acc:100.00%
Epoch [178/300], Step [20/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [178/300], Step [30/391],                 Loss: 0.00360, Train_Acc:100.00%
Epoch [178/300], Step [40/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [178/300], Step [50/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [178/300], Step [60/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [178/300], Step [70/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [178/300], Step [80/391],                 Loss: 0.00358, Train_Acc:100.00%
Epoch [178/300], Step [90/391],                 Loss: 0.00355, Train_Acc:100.00%
Epoch [178/300], Step [100/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [110/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [178/300], Step [120/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [130/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [140/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [150/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [178/300], Step [160/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [178/300], Step [170/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [178/300], Step [180/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [190/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [178/300], Step [200/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [178/300], Step [210/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [220/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [178/300], Step [230/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [178/300], Step [240/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [250/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [178/300], Step [260/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [270/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [178/300], Step [280/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [178/300], Step [290/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [178/300], Step [300/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [178/300], Step [310/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [178/300], Step [320/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [178/300], Step [330/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [178/300], Step [340/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [178/300], Step [350/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [178/300], Step [360/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [178/300], Step [370/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [178/300], Step [380/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [178/300], Step [390/391],                 Loss: 0.00349, Train_Acc:100.00%
Accuary on test images:91.56%
Epoch [179/300], Step [10/391],                 Loss: 0.00362, Train_Acc:100.00%
Epoch [179/300], Step [20/391],                 Loss: 0.00361, Train_Acc:100.00%
Epoch [179/300], Step [30/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [179/300], Step [40/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [179/300], Step [50/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [179/300], Step [60/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [179/300], Step [70/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [179/300], Step [80/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [179/300], Step [90/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [179/300], Step [100/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [179/300], Step [110/391],                 Loss: 0.00348, Train_Acc:100.00%
Epoch [179/300], Step [120/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [179/300], Step [130/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [179/300], Step [140/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [179/300], Step [150/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [179/300], Step [160/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [179/300], Step [170/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [179/300], Step [180/391],                 Loss: 0.00348, Train_Acc:100.00%
Epoch [179/300], Step [190/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [200/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [210/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [220/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [230/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [240/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [250/391],                 Loss: 0.00346, Train_Acc:100.00%
Epoch [179/300], Step [260/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [270/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [179/300], Step [280/391],                 Loss: 0.00346, Train_Acc:100.00%
Epoch [179/300], Step [290/391],                 Loss: 0.00346, Train_Acc:100.00%
Epoch [179/300], Step [300/391],                 Loss: 0.00346, Train_Acc:100.00%
Epoch [179/300], Step [310/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [320/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [330/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [340/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [350/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [360/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [370/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [179/300], Step [380/391],                 Loss: 0.00344, Train_Acc:100.00%
Epoch [179/300], Step [390/391],                 Loss: 0.00344, Train_Acc:100.00%
Accuary on test images:91.84%
Epoch [180/300], Step [10/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [180/300], Step [20/391],                 Loss: 0.00357, Train_Acc:100.00%
Epoch [180/300], Step [30/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [180/300], Step [40/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [180/300], Step [50/391],                 Loss: 0.00346, Train_Acc:100.00%
Epoch [180/300], Step [60/391],                 Loss: 0.00350, Train_Acc:100.00%
Epoch [180/300], Step [70/391],                 Loss: 0.00349, Train_Acc:100.00%
Epoch [180/300], Step [80/391],                 Loss: 0.00348, Train_Acc:100.00%
Epoch [180/300], Step [90/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [180/300], Step [100/391],                 Loss: 0.00344, Train_Acc:100.00%
Epoch [180/300], Step [110/391],                 Loss: 0.00344, Train_Acc:100.00%
Epoch [180/300], Step [120/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [180/300], Step [130/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [180/300], Step [140/391],                 Loss: 0.00346, Train_Acc:100.00%
Epoch [180/300], Step [150/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [180/300], Step [160/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [180/300], Step [170/391],                 Loss: 0.00366, Train_Acc:100.00%
Epoch [180/300], Step [180/391],                 Loss: 0.00445, Train_Acc:99.98%
Epoch [180/300], Step [190/391],                 Loss: 0.00673, Train_Acc:99.92%
Epoch [180/300], Step [200/391],                 Loss: 0.01625, Train_Acc:99.63%
Epoch [180/300], Step [210/391],                 Loss: 0.02964, Train_Acc:99.16%
Epoch [180/300], Step [220/391],                 Loss: 0.04362, Train_Acc:98.71%
Epoch [180/300], Step [230/391],                 Loss: 0.05566, Train_Acc:98.33%
Epoch [180/300], Step [240/391],                 Loss: 0.06431, Train_Acc:98.03%
Epoch [180/300], Step [250/391],                 Loss: 0.07383, Train_Acc:97.72%
Epoch [180/300], Step [260/391],                 Loss: 0.08200, Train_Acc:97.45%
Epoch [180/300], Step [270/391],                 Loss: 0.09050, Train_Acc:97.18%
Epoch [180/300], Step [280/391],                 Loss: 0.09780, Train_Acc:96.92%
Epoch [180/300], Step [290/391],                 Loss: 0.10388, Train_Acc:96.72%
Epoch [180/300], Step [300/391],                 Loss: 0.10865, Train_Acc:96.55%
Epoch [180/300], Step [310/391],                 Loss: 0.11259, Train_Acc:96.42%
Epoch [180/300], Step [320/391],                 Loss: 0.11708, Train_Acc:96.26%
Epoch [180/300], Step [330/391],                 Loss: 0.12066, Train_Acc:96.13%
Epoch [180/300], Step [340/391],                 Loss: 0.12434, Train_Acc:96.00%
Epoch [180/300], Step [350/391],                 Loss: 0.12731, Train_Acc:95.91%
Epoch [180/300], Step [360/391],                 Loss: 0.13050, Train_Acc:95.81%
Epoch [180/300], Step [370/391],                 Loss: 0.13394, Train_Acc:95.67%
Epoch [180/300], Step [380/391],                 Loss: 0.13606, Train_Acc:95.60%
Epoch [180/300], Step [390/391],                 Loss: 0.13865, Train_Acc:95.50%
Accuary on test images:80.24%
Epoch [181/300], Step [10/391],                 Loss: 0.29442, Train_Acc:89.53%
Epoch [181/300], Step [20/391],                 Loss: 0.28844, Train_Acc:90.08%
Epoch [181/300], Step [30/391],                 Loss: 0.28613, Train_Acc:90.49%
Epoch [181/300], Step [40/391],                 Loss: 0.28724, Train_Acc:90.53%
Epoch [181/300], Step [50/391],                 Loss: 0.29095, Train_Acc:90.36%
Epoch [181/300], Step [60/391],                 Loss: 0.29771, Train_Acc:90.12%
Epoch [181/300], Step [70/391],                 Loss: 0.29544, Train_Acc:90.30%
Epoch [181/300], Step [80/391],                 Loss: 0.29495, Train_Acc:90.39%
Epoch [181/300], Step [90/391],                 Loss: 0.29399, Train_Acc:90.30%
Epoch [181/300], Step [100/391],                 Loss: 0.28847, Train_Acc:90.47%
Epoch [181/300], Step [110/391],                 Loss: 0.28554, Train_Acc:90.53%
Epoch [181/300], Step [120/391],                 Loss: 0.28544, Train_Acc:90.49%
Epoch [181/300], Step [130/391],                 Loss: 0.28359, Train_Acc:90.61%
Epoch [181/300], Step [140/391],                 Loss: 0.28071, Train_Acc:90.70%
Epoch [181/300], Step [150/391],                 Loss: 0.27964, Train_Acc:90.76%
Epoch [181/300], Step [160/391],                 Loss: 0.27763, Train_Acc:90.79%
Epoch [181/300], Step [170/391],                 Loss: 0.27705, Train_Acc:90.82%
Epoch [181/300], Step [180/391],                 Loss: 0.27571, Train_Acc:90.86%
Epoch [181/300], Step [190/391],                 Loss: 0.27320, Train_Acc:90.93%
Epoch [181/300], Step [200/391],                 Loss: 0.27149, Train_Acc:90.98%
Epoch [181/300], Step [210/391],                 Loss: 0.26887, Train_Acc:91.11%
Epoch [181/300], Step [220/391],                 Loss: 0.26699, Train_Acc:91.19%
Epoch [181/300], Step [230/391],                 Loss: 0.26362, Train_Acc:91.28%
Epoch [181/300], Step [240/391],                 Loss: 0.26051, Train_Acc:91.35%
Epoch [181/300], Step [250/391],                 Loss: 0.25796, Train_Acc:91.41%
Epoch [181/300], Step [260/391],                 Loss: 0.25661, Train_Acc:91.47%
Epoch [181/300], Step [270/391],                 Loss: 0.25486, Train_Acc:91.53%
Epoch [181/300], Step [280/391],                 Loss: 0.25189, Train_Acc:91.62%
Epoch [181/300], Step [290/391],                 Loss: 0.24952, Train_Acc:91.72%
Epoch [181/300], Step [300/391],                 Loss: 0.24654, Train_Acc:91.83%
Epoch [181/300], Step [310/391],                 Loss: 0.24365, Train_Acc:91.92%
Epoch [181/300], Step [320/391],                 Loss: 0.24029, Train_Acc:92.05%
Epoch [181/300], Step [330/391],                 Loss: 0.23784, Train_Acc:92.13%
Epoch [181/300], Step [340/391],                 Loss: 0.23594, Train_Acc:92.20%
Epoch [181/300], Step [350/391],                 Loss: 0.23393, Train_Acc:92.27%
Epoch [181/300], Step [360/391],                 Loss: 0.23209, Train_Acc:92.33%
Epoch [181/300], Step [370/391],                 Loss: 0.23066, Train_Acc:92.38%
Epoch [181/300], Step [380/391],                 Loss: 0.22837, Train_Acc:92.45%
Epoch [181/300], Step [390/391],                 Loss: 0.22657, Train_Acc:92.50%
Accuary on test images:85.86%
Epoch [182/300], Step [10/391],                 Loss: 0.17716, Train_Acc:94.38%
Epoch [182/300], Step [20/391],                 Loss: 0.17469, Train_Acc:94.30%
Epoch [182/300], Step [30/391],                 Loss: 0.17099, Train_Acc:94.53%
Epoch [182/300], Step [40/391],                 Loss: 0.17520, Train_Acc:94.28%
Epoch [182/300], Step [50/391],                 Loss: 0.17459, Train_Acc:94.27%
Epoch [182/300], Step [60/391],                 Loss: 0.17519, Train_Acc:94.19%
Epoch [182/300], Step [70/391],                 Loss: 0.17778, Train_Acc:94.03%
Epoch [182/300], Step [80/391],                 Loss: 0.18401, Train_Acc:93.89%
Epoch [182/300], Step [90/391],                 Loss: 0.18978, Train_Acc:93.68%
Epoch [182/300], Step [100/391],                 Loss: 0.18642, Train_Acc:93.74%
Epoch [182/300], Step [110/391],                 Loss: 0.18786, Train_Acc:93.74%
Epoch [182/300], Step [120/391],                 Loss: 0.18757, Train_Acc:93.80%
Epoch [182/300], Step [130/391],                 Loss: 0.18834, Train_Acc:93.78%
Epoch [182/300], Step [140/391],                 Loss: 0.18860, Train_Acc:93.74%
Epoch [182/300], Step [150/391],                 Loss: 0.18979, Train_Acc:93.68%
Epoch [182/300], Step [160/391],                 Loss: 0.18864, Train_Acc:93.72%
Epoch [182/300], Step [170/391],                 Loss: 0.18770, Train_Acc:93.78%
Epoch [182/300], Step [180/391],                 Loss: 0.18801, Train_Acc:93.76%
Epoch [182/300], Step [190/391],                 Loss: 0.18706, Train_Acc:93.78%
Epoch [182/300], Step [200/391],                 Loss: 0.18637, Train_Acc:93.82%
Epoch [182/300], Step [210/391],                 Loss: 0.18397, Train_Acc:93.91%
Epoch [182/300], Step [220/391],                 Loss: 0.18203, Train_Acc:94.00%
Epoch [182/300], Step [230/391],                 Loss: 0.18007, Train_Acc:94.10%
Epoch [182/300], Step [240/391],                 Loss: 0.17875, Train_Acc:94.11%
Epoch [182/300], Step [250/391],                 Loss: 0.17658, Train_Acc:94.18%
Epoch [182/300], Step [260/391],                 Loss: 0.17515, Train_Acc:94.24%
Epoch [182/300], Step [270/391],                 Loss: 0.17468, Train_Acc:94.25%
Epoch [182/300], Step [280/391],                 Loss: 0.17315, Train_Acc:94.32%
Epoch [182/300], Step [290/391],                 Loss: 0.17170, Train_Acc:94.36%
Epoch [182/300], Step [300/391],                 Loss: 0.16997, Train_Acc:94.43%
Epoch [182/300], Step [310/391],                 Loss: 0.16932, Train_Acc:94.47%
Epoch [182/300], Step [320/391],                 Loss: 0.16721, Train_Acc:94.54%
Epoch [182/300], Step [330/391],                 Loss: 0.16591, Train_Acc:94.58%
Epoch [182/300], Step [340/391],                 Loss: 0.16376, Train_Acc:94.64%
Epoch [182/300], Step [350/391],                 Loss: 0.16207, Train_Acc:94.70%
Epoch [182/300], Step [360/391],                 Loss: 0.16063, Train_Acc:94.75%
Epoch [182/300], Step [370/391],                 Loss: 0.15949, Train_Acc:94.78%
Epoch [182/300], Step [380/391],                 Loss: 0.15873, Train_Acc:94.81%
Epoch [182/300], Step [390/391],                 Loss: 0.15767, Train_Acc:94.85%
Accuary on test images:87.80%
Epoch [183/300], Step [10/391],                 Loss: 0.13531, Train_Acc:95.31%
Epoch [183/300], Step [20/391],                 Loss: 0.14147, Train_Acc:95.31%
Epoch [183/300], Step [30/391],                 Loss: 0.13736, Train_Acc:95.34%
Epoch [183/300], Step [40/391],                 Loss: 0.14284, Train_Acc:95.23%
Epoch [183/300], Step [50/391],                 Loss: 0.13984, Train_Acc:95.30%
Epoch [183/300], Step [60/391],                 Loss: 0.14118, Train_Acc:95.26%
Epoch [183/300], Step [70/391],                 Loss: 0.14121, Train_Acc:95.33%
Epoch [183/300], Step [80/391],                 Loss: 0.14099, Train_Acc:95.38%
Epoch [183/300], Step [90/391],                 Loss: 0.14025, Train_Acc:95.36%
Epoch [183/300], Step [100/391],                 Loss: 0.13888, Train_Acc:95.42%
Epoch [183/300], Step [110/391],                 Loss: 0.13998, Train_Acc:95.34%
Epoch [183/300], Step [120/391],                 Loss: 0.13995, Train_Acc:95.36%
Epoch [183/300], Step [130/391],                 Loss: 0.13992, Train_Acc:95.38%
Epoch [183/300], Step [140/391],                 Loss: 0.13923, Train_Acc:95.45%
Epoch [183/300], Step [150/391],                 Loss: 0.13976, Train_Acc:95.40%
Epoch [183/300], Step [160/391],                 Loss: 0.13993, Train_Acc:95.42%
Epoch [183/300], Step [170/391],                 Loss: 0.13976, Train_Acc:95.42%
Epoch [183/300], Step [180/391],                 Loss: 0.13912, Train_Acc:95.45%
Epoch [183/300], Step [190/391],                 Loss: 0.13867, Train_Acc:95.41%
Epoch [183/300], Step [200/391],                 Loss: 0.13850, Train_Acc:95.42%
Epoch [183/300], Step [210/391],                 Loss: 0.13760, Train_Acc:95.47%
Epoch [183/300], Step [220/391],                 Loss: 0.13631, Train_Acc:95.51%
Epoch [183/300], Step [230/391],                 Loss: 0.13470, Train_Acc:95.56%
Epoch [183/300], Step [240/391],                 Loss: 0.13416, Train_Acc:95.59%
Epoch [183/300], Step [250/391],                 Loss: 0.13332, Train_Acc:95.62%
Epoch [183/300], Step [260/391],                 Loss: 0.13264, Train_Acc:95.63%
Epoch [183/300], Step [270/391],                 Loss: 0.13200, Train_Acc:95.66%
Epoch [183/300], Step [280/391],                 Loss: 0.13137, Train_Acc:95.66%
Epoch [183/300], Step [290/391],                 Loss: 0.13020, Train_Acc:95.69%
Epoch [183/300], Step [300/391],                 Loss: 0.12929, Train_Acc:95.73%
Epoch [183/300], Step [310/391],                 Loss: 0.12876, Train_Acc:95.77%
Epoch [183/300], Step [320/391],                 Loss: 0.12803, Train_Acc:95.78%
Epoch [183/300], Step [330/391],                 Loss: 0.12671, Train_Acc:95.83%
Epoch [183/300], Step [340/391],                 Loss: 0.12565, Train_Acc:95.87%
Epoch [183/300], Step [350/391],                 Loss: 0.12456, Train_Acc:95.91%
Epoch [183/300], Step [360/391],                 Loss: 0.12351, Train_Acc:95.95%
Epoch [183/300], Step [370/391],                 Loss: 0.12394, Train_Acc:95.94%
Epoch [183/300], Step [380/391],                 Loss: 0.12321, Train_Acc:95.96%
Epoch [183/300], Step [390/391],                 Loss: 0.12257, Train_Acc:95.99%
Accuary on test images:87.38%
Epoch [184/300], Step [10/391],                 Loss: 0.11045, Train_Acc:95.70%
Epoch [184/300], Step [20/391],                 Loss: 0.11680, Train_Acc:95.90%
Epoch [184/300], Step [30/391],                 Loss: 0.11922, Train_Acc:95.81%
Epoch [184/300], Step [40/391],                 Loss: 0.11961, Train_Acc:95.92%
Epoch [184/300], Step [50/391],                 Loss: 0.11838, Train_Acc:95.94%
Epoch [184/300], Step [60/391],                 Loss: 0.11532, Train_Acc:96.00%
Epoch [184/300], Step [70/391],                 Loss: 0.11494, Train_Acc:96.05%
Epoch [184/300], Step [80/391],                 Loss: 0.11371, Train_Acc:96.08%
Epoch [184/300], Step [90/391],                 Loss: 0.11348, Train_Acc:96.13%
Epoch [184/300], Step [100/391],                 Loss: 0.11206, Train_Acc:96.20%
Epoch [184/300], Step [110/391],                 Loss: 0.11102, Train_Acc:96.24%
Epoch [184/300], Step [120/391],                 Loss: 0.11146, Train_Acc:96.23%
Epoch [184/300], Step [130/391],                 Loss: 0.11024, Train_Acc:96.29%
Epoch [184/300], Step [140/391],                 Loss: 0.10967, Train_Acc:96.31%
Epoch [184/300], Step [150/391],                 Loss: 0.11037, Train_Acc:96.29%
Epoch [184/300], Step [160/391],                 Loss: 0.10956, Train_Acc:96.34%
Epoch [184/300], Step [170/391],                 Loss: 0.11023, Train_Acc:96.29%
Epoch [184/300], Step [180/391],                 Loss: 0.10940, Train_Acc:96.34%
Epoch [184/300], Step [190/391],                 Loss: 0.10924, Train_Acc:96.35%
Epoch [184/300], Step [200/391],                 Loss: 0.10898, Train_Acc:96.37%
Epoch [184/300], Step [210/391],                 Loss: 0.10812, Train_Acc:96.42%
Epoch [184/300], Step [220/391],                 Loss: 0.10743, Train_Acc:96.45%
Epoch [184/300], Step [230/391],                 Loss: 0.10735, Train_Acc:96.47%
Epoch [184/300], Step [240/391],                 Loss: 0.10692, Train_Acc:96.49%
Epoch [184/300], Step [250/391],                 Loss: 0.10603, Train_Acc:96.53%
Epoch [184/300], Step [260/391],                 Loss: 0.10615, Train_Acc:96.52%
Epoch [184/300], Step [270/391],                 Loss: 0.10617, Train_Acc:96.52%
Epoch [184/300], Step [280/391],                 Loss: 0.10576, Train_Acc:96.54%
Epoch [184/300], Step [290/391],                 Loss: 0.10514, Train_Acc:96.56%
Epoch [184/300], Step [300/391],                 Loss: 0.10466, Train_Acc:96.58%
Epoch [184/300], Step [310/391],                 Loss: 0.10383, Train_Acc:96.62%
Epoch [184/300], Step [320/391],                 Loss: 0.10261, Train_Acc:96.67%
Epoch [184/300], Step [330/391],                 Loss: 0.10189, Train_Acc:96.70%
Epoch [184/300], Step [340/391],                 Loss: 0.10122, Train_Acc:96.73%
Epoch [184/300], Step [350/391],                 Loss: 0.10070, Train_Acc:96.74%
Epoch [184/300], Step [360/391],                 Loss: 0.09994, Train_Acc:96.78%
Epoch [184/300], Step [370/391],                 Loss: 0.09971, Train_Acc:96.79%
Epoch [184/300], Step [380/391],                 Loss: 0.09903, Train_Acc:96.82%
Epoch [184/300], Step [390/391],                 Loss: 0.09852, Train_Acc:96.84%
Accuary on test images:87.16%
Epoch [185/300], Step [10/391],                 Loss: 0.08360, Train_Acc:97.58%
Epoch [185/300], Step [20/391],                 Loss: 0.07576, Train_Acc:97.66%
Epoch [185/300], Step [30/391],                 Loss: 0.07069, Train_Acc:97.89%
Epoch [185/300], Step [40/391],                 Loss: 0.07670, Train_Acc:97.73%
Epoch [185/300], Step [50/391],                 Loss: 0.07527, Train_Acc:97.75%
Epoch [185/300], Step [60/391],                 Loss: 0.07736, Train_Acc:97.63%
Epoch [185/300], Step [70/391],                 Loss: 0.07898, Train_Acc:97.53%
Epoch [185/300], Step [80/391],                 Loss: 0.08331, Train_Acc:97.36%
Epoch [185/300], Step [90/391],                 Loss: 0.08457, Train_Acc:97.31%
Epoch [185/300], Step [100/391],                 Loss: 0.08487, Train_Acc:97.26%
Epoch [185/300], Step [110/391],                 Loss: 0.08578, Train_Acc:97.28%
Epoch [185/300], Step [120/391],                 Loss: 0.08698, Train_Acc:97.22%
Epoch [185/300], Step [130/391],                 Loss: 0.08641, Train_Acc:97.24%
Epoch [185/300], Step [140/391],                 Loss: 0.08692, Train_Acc:97.20%
Epoch [185/300], Step [150/391],                 Loss: 0.08717, Train_Acc:97.17%
Epoch [185/300], Step [160/391],                 Loss: 0.08754, Train_Acc:97.15%
Epoch [185/300], Step [170/391],                 Loss: 0.08850, Train_Acc:97.12%
Epoch [185/300], Step [180/391],                 Loss: 0.08835, Train_Acc:97.15%
Epoch [185/300], Step [190/391],                 Loss: 0.08787, Train_Acc:97.16%
Epoch [185/300], Step [200/391],                 Loss: 0.08799, Train_Acc:97.15%
Epoch [185/300], Step [210/391],                 Loss: 0.08781, Train_Acc:97.18%
Epoch [185/300], Step [220/391],                 Loss: 0.08758, Train_Acc:97.19%
Epoch [185/300], Step [230/391],                 Loss: 0.08676, Train_Acc:97.22%
Epoch [185/300], Step [240/391],                 Loss: 0.08628, Train_Acc:97.24%
Epoch [185/300], Step [250/391],                 Loss: 0.08562, Train_Acc:97.27%
Epoch [185/300], Step [260/391],                 Loss: 0.08539, Train_Acc:97.28%
Epoch [185/300], Step [270/391],                 Loss: 0.08521, Train_Acc:97.27%
Epoch [185/300], Step [280/391],                 Loss: 0.08532, Train_Acc:97.26%
Epoch [185/300], Step [290/391],                 Loss: 0.08494, Train_Acc:97.27%
Epoch [185/300], Step [300/391],                 Loss: 0.08493, Train_Acc:97.29%
Epoch [185/300], Step [310/391],                 Loss: 0.08404, Train_Acc:97.32%
Epoch [185/300], Step [320/391],                 Loss: 0.08412, Train_Acc:97.33%
Epoch [185/300], Step [330/391],                 Loss: 0.08388, Train_Acc:97.34%
Epoch [185/300], Step [340/391],                 Loss: 0.08341, Train_Acc:97.35%
Epoch [185/300], Step [350/391],                 Loss: 0.08376, Train_Acc:97.35%
Epoch [185/300], Step [360/391],                 Loss: 0.08378, Train_Acc:97.35%
Epoch [185/300], Step [370/391],                 Loss: 0.08342, Train_Acc:97.35%
Epoch [185/300], Step [380/391],                 Loss: 0.08333, Train_Acc:97.34%
Epoch [185/300], Step [390/391],                 Loss: 0.08310, Train_Acc:97.35%
Accuary on test images:88.14%
Epoch [186/300], Step [10/391],                 Loss: 0.08589, Train_Acc:97.27%
Epoch [186/300], Step [20/391],                 Loss: 0.07560, Train_Acc:97.42%
Epoch [186/300], Step [30/391],                 Loss: 0.07886, Train_Acc:97.47%
Epoch [186/300], Step [40/391],                 Loss: 0.07624, Train_Acc:97.54%
Epoch [186/300], Step [50/391],                 Loss: 0.07429, Train_Acc:97.59%
Epoch [186/300], Step [60/391],                 Loss: 0.07388, Train_Acc:97.63%
Epoch [186/300], Step [70/391],                 Loss: 0.07443, Train_Acc:97.63%
Epoch [186/300], Step [80/391],                 Loss: 0.07437, Train_Acc:97.60%
Epoch [186/300], Step [90/391],                 Loss: 0.07500, Train_Acc:97.56%
Epoch [186/300], Step [100/391],                 Loss: 0.07382, Train_Acc:97.59%
Epoch [186/300], Step [110/391],                 Loss: 0.07305, Train_Acc:97.62%
Epoch [186/300], Step [120/391],                 Loss: 0.07379, Train_Acc:97.58%
Epoch [186/300], Step [130/391],                 Loss: 0.07374, Train_Acc:97.61%
Epoch [186/300], Step [140/391],                 Loss: 0.07392, Train_Acc:97.63%
Epoch [186/300], Step [150/391],                 Loss: 0.07379, Train_Acc:97.65%
Epoch [186/300], Step [160/391],                 Loss: 0.07324, Train_Acc:97.67%
Epoch [186/300], Step [170/391],                 Loss: 0.07370, Train_Acc:97.65%
Epoch [186/300], Step [180/391],                 Loss: 0.07369, Train_Acc:97.63%
Epoch [186/300], Step [190/391],                 Loss: 0.07354, Train_Acc:97.67%
Epoch [186/300], Step [200/391],                 Loss: 0.07407, Train_Acc:97.64%
Epoch [186/300], Step [210/391],                 Loss: 0.07419, Train_Acc:97.62%
Epoch [186/300], Step [220/391],                 Loss: 0.07400, Train_Acc:97.63%
Epoch [186/300], Step [230/391],                 Loss: 0.07321, Train_Acc:97.65%
Epoch [186/300], Step [240/391],                 Loss: 0.07276, Train_Acc:97.66%
Epoch [186/300], Step [250/391],                 Loss: 0.07221, Train_Acc:97.68%
Epoch [186/300], Step [260/391],                 Loss: 0.07185, Train_Acc:97.68%
Epoch [186/300], Step [270/391],                 Loss: 0.07187, Train_Acc:97.67%
Epoch [186/300], Step [280/391],                 Loss: 0.07160, Train_Acc:97.69%
Epoch [186/300], Step [290/391],                 Loss: 0.07112, Train_Acc:97.71%
Epoch [186/300], Step [300/391],                 Loss: 0.07087, Train_Acc:97.73%
Epoch [186/300], Step [310/391],                 Loss: 0.07023, Train_Acc:97.75%
Epoch [186/300], Step [320/391],                 Loss: 0.07093, Train_Acc:97.73%
Epoch [186/300], Step [330/391],                 Loss: 0.07051, Train_Acc:97.75%
Epoch [186/300], Step [340/391],                 Loss: 0.06999, Train_Acc:97.78%
Epoch [186/300], Step [350/391],                 Loss: 0.06905, Train_Acc:97.81%
Epoch [186/300], Step [360/391],                 Loss: 0.06871, Train_Acc:97.83%
Epoch [186/300], Step [370/391],                 Loss: 0.06848, Train_Acc:97.84%
Epoch [186/300], Step [380/391],                 Loss: 0.06837, Train_Acc:97.84%
Epoch [186/300], Step [390/391],                 Loss: 0.06826, Train_Acc:97.85%
Accuary on test images:89.08%
Epoch [187/300], Step [10/391],                 Loss: 0.07473, Train_Acc:97.73%
Epoch [187/300], Step [20/391],                 Loss: 0.06855, Train_Acc:97.85%
Epoch [187/300], Step [30/391],                 Loss: 0.06944, Train_Acc:97.81%
Epoch [187/300], Step [40/391],                 Loss: 0.07213, Train_Acc:97.70%
Epoch [187/300], Step [50/391],                 Loss: 0.06889, Train_Acc:97.77%
Epoch [187/300], Step [60/391],                 Loss: 0.06839, Train_Acc:97.81%
Epoch [187/300], Step [70/391],                 Loss: 0.06740, Train_Acc:97.87%
Epoch [187/300], Step [80/391],                 Loss: 0.06866, Train_Acc:97.79%
Epoch [187/300], Step [90/391],                 Loss: 0.06895, Train_Acc:97.80%
Epoch [187/300], Step [100/391],                 Loss: 0.06885, Train_Acc:97.79%
Epoch [187/300], Step [110/391],                 Loss: 0.06795, Train_Acc:97.83%
Epoch [187/300], Step [120/391],                 Loss: 0.06811, Train_Acc:97.83%
Epoch [187/300], Step [130/391],                 Loss: 0.06839, Train_Acc:97.84%
Epoch [187/300], Step [140/391],                 Loss: 0.06822, Train_Acc:97.83%
Epoch [187/300], Step [150/391],                 Loss: 0.06822, Train_Acc:97.84%
Epoch [187/300], Step [160/391],                 Loss: 0.06772, Train_Acc:97.84%
Epoch [187/300], Step [170/391],                 Loss: 0.06845, Train_Acc:97.84%
Epoch [187/300], Step [180/391],                 Loss: 0.06873, Train_Acc:97.82%
Epoch [187/300], Step [190/391],                 Loss: 0.06835, Train_Acc:97.84%
Epoch [187/300], Step [200/391],                 Loss: 0.06771, Train_Acc:97.85%
Epoch [187/300], Step [210/391],                 Loss: 0.06682, Train_Acc:97.88%
Epoch [187/300], Step [220/391],                 Loss: 0.06602, Train_Acc:97.93%
Epoch [187/300], Step [230/391],                 Loss: 0.06551, Train_Acc:97.96%
Epoch [187/300], Step [240/391],                 Loss: 0.06461, Train_Acc:97.97%
Epoch [187/300], Step [250/391],                 Loss: 0.06489, Train_Acc:97.97%
Epoch [187/300], Step [260/391],                 Loss: 0.06462, Train_Acc:97.96%
Epoch [187/300], Step [270/391],                 Loss: 0.06451, Train_Acc:97.96%
Epoch [187/300], Step [280/391],                 Loss: 0.06427, Train_Acc:97.97%
Epoch [187/300], Step [290/391],                 Loss: 0.06408, Train_Acc:97.99%
Epoch [187/300], Step [300/391],                 Loss: 0.06367, Train_Acc:98.01%
Epoch [187/300], Step [310/391],                 Loss: 0.06354, Train_Acc:98.01%
Epoch [187/300], Step [320/391],                 Loss: 0.06371, Train_Acc:97.99%
Epoch [187/300], Step [330/391],                 Loss: 0.06371, Train_Acc:97.99%
Epoch [187/300], Step [340/391],                 Loss: 0.06322, Train_Acc:98.02%
Epoch [187/300], Step [350/391],                 Loss: 0.06277, Train_Acc:98.04%
Epoch [187/300], Step [360/391],                 Loss: 0.06241, Train_Acc:98.04%
Epoch [187/300], Step [370/391],                 Loss: 0.06221, Train_Acc:98.05%
Epoch [187/300], Step [380/391],                 Loss: 0.06182, Train_Acc:98.07%
Epoch [187/300], Step [390/391],                 Loss: 0.06193, Train_Acc:98.06%
Accuary on test images:87.94%
Epoch [188/300], Step [10/391],                 Loss: 0.05037, Train_Acc:98.44%
Epoch [188/300], Step [20/391],                 Loss: 0.05551, Train_Acc:98.32%
Epoch [188/300], Step [30/391],                 Loss: 0.05443, Train_Acc:98.33%
Epoch [188/300], Step [40/391],                 Loss: 0.05439, Train_Acc:98.32%
Epoch [188/300], Step [50/391],                 Loss: 0.05409, Train_Acc:98.41%
Epoch [188/300], Step [60/391],                 Loss: 0.05412, Train_Acc:98.33%
Epoch [188/300], Step [70/391],                 Loss: 0.05505, Train_Acc:98.25%
Epoch [188/300], Step [80/391],                 Loss: 0.05598, Train_Acc:98.24%
Epoch [188/300], Step [90/391],                 Loss: 0.05616, Train_Acc:98.28%
Epoch [188/300], Step [100/391],                 Loss: 0.05474, Train_Acc:98.34%
Epoch [188/300], Step [110/391],                 Loss: 0.05419, Train_Acc:98.32%
Epoch [188/300], Step [120/391],                 Loss: 0.05445, Train_Acc:98.33%
Epoch [188/300], Step [130/391],                 Loss: 0.05486, Train_Acc:98.31%
Epoch [188/300], Step [140/391],                 Loss: 0.05501, Train_Acc:98.29%
Epoch [188/300], Step [150/391],                 Loss: 0.05550, Train_Acc:98.29%
Epoch [188/300], Step [160/391],                 Loss: 0.05581, Train_Acc:98.27%
Epoch [188/300], Step [170/391],                 Loss: 0.05573, Train_Acc:98.26%
Epoch [188/300], Step [180/391],                 Loss: 0.05550, Train_Acc:98.26%
Epoch [188/300], Step [190/391],                 Loss: 0.05516, Train_Acc:98.28%
Epoch [188/300], Step [200/391],                 Loss: 0.05496, Train_Acc:98.29%
Epoch [188/300], Step [210/391],                 Loss: 0.05466, Train_Acc:98.29%
Epoch [188/300], Step [220/391],                 Loss: 0.05454, Train_Acc:98.31%
Epoch [188/300], Step [230/391],                 Loss: 0.05426, Train_Acc:98.33%
Epoch [188/300], Step [240/391],                 Loss: 0.05376, Train_Acc:98.34%
Epoch [188/300], Step [250/391],                 Loss: 0.05341, Train_Acc:98.36%
Epoch [188/300], Step [260/391],                 Loss: 0.05376, Train_Acc:98.35%
Epoch [188/300], Step [270/391],                 Loss: 0.05406, Train_Acc:98.34%
Epoch [188/300], Step [280/391],                 Loss: 0.05413, Train_Acc:98.33%
Epoch [188/300], Step [290/391],                 Loss: 0.05410, Train_Acc:98.33%
Epoch [188/300], Step [300/391],                 Loss: 0.05430, Train_Acc:98.31%
Epoch [188/300], Step [310/391],                 Loss: 0.05422, Train_Acc:98.31%
Epoch [188/300], Step [320/391],                 Loss: 0.05429, Train_Acc:98.30%
Epoch [188/300], Step [330/391],                 Loss: 0.05420, Train_Acc:98.30%
Epoch [188/300], Step [340/391],                 Loss: 0.05428, Train_Acc:98.30%
Epoch [188/300], Step [350/391],                 Loss: 0.05445, Train_Acc:98.29%
Epoch [188/300], Step [360/391],                 Loss: 0.05438, Train_Acc:98.29%
Epoch [188/300], Step [370/391],                 Loss: 0.05430, Train_Acc:98.29%
Epoch [188/300], Step [380/391],                 Loss: 0.05387, Train_Acc:98.32%
Epoch [188/300], Step [390/391],                 Loss: 0.05369, Train_Acc:98.33%
Accuary on test images:88.00%
Epoch [189/300], Step [10/391],                 Loss: 0.05227, Train_Acc:98.59%
Epoch [189/300], Step [20/391],                 Loss: 0.05204, Train_Acc:98.40%
Epoch [189/300], Step [30/391],                 Loss: 0.04954, Train_Acc:98.54%
Epoch [189/300], Step [40/391],                 Loss: 0.04849, Train_Acc:98.55%
Epoch [189/300], Step [50/391],                 Loss: 0.05028, Train_Acc:98.41%
Epoch [189/300], Step [60/391],                 Loss: 0.05152, Train_Acc:98.36%
Epoch [189/300], Step [70/391],                 Loss: 0.05226, Train_Acc:98.38%
Epoch [189/300], Step [80/391],                 Loss: 0.05212, Train_Acc:98.39%
Epoch [189/300], Step [90/391],                 Loss: 0.05200, Train_Acc:98.36%
Epoch [189/300], Step [100/391],                 Loss: 0.05171, Train_Acc:98.38%
Epoch [189/300], Step [110/391],                 Loss: 0.05269, Train_Acc:98.32%
Epoch [189/300], Step [120/391],                 Loss: 0.05228, Train_Acc:98.35%
Epoch [189/300], Step [130/391],                 Loss: 0.05239, Train_Acc:98.34%
Epoch [189/300], Step [140/391],                 Loss: 0.05333, Train_Acc:98.31%
Epoch [189/300], Step [150/391],                 Loss: 0.05482, Train_Acc:98.26%
Epoch [189/300], Step [160/391],                 Loss: 0.05467, Train_Acc:98.26%
Epoch [189/300], Step [170/391],                 Loss: 0.05507, Train_Acc:98.25%
Epoch [189/300], Step [180/391],                 Loss: 0.05553, Train_Acc:98.24%
Epoch [189/300], Step [190/391],                 Loss: 0.05583, Train_Acc:98.24%
Epoch [189/300], Step [200/391],                 Loss: 0.05582, Train_Acc:98.26%
Epoch [189/300], Step [210/391],                 Loss: 0.05565, Train_Acc:98.26%
Epoch [189/300], Step [220/391],                 Loss: 0.05515, Train_Acc:98.28%
Epoch [189/300], Step [230/391],                 Loss: 0.05509, Train_Acc:98.29%
Epoch [189/300], Step [240/391],                 Loss: 0.05504, Train_Acc:98.28%
Epoch [189/300], Step [250/391],                 Loss: 0.05503, Train_Acc:98.28%
Epoch [189/300], Step [260/391],                 Loss: 0.05506, Train_Acc:98.30%
Epoch [189/300], Step [270/391],                 Loss: 0.05543, Train_Acc:98.28%
Epoch [189/300], Step [280/391],                 Loss: 0.05515, Train_Acc:98.29%
Epoch [189/300], Step [290/391],                 Loss: 0.05499, Train_Acc:98.30%
Epoch [189/300], Step [300/391],                 Loss: 0.05501, Train_Acc:98.30%
Epoch [189/300], Step [310/391],                 Loss: 0.05550, Train_Acc:98.29%
Epoch [189/300], Step [320/391],                 Loss: 0.05583, Train_Acc:98.28%
Epoch [189/300], Step [330/391],                 Loss: 0.05563, Train_Acc:98.29%
Epoch [189/300], Step [340/391],                 Loss: 0.05551, Train_Acc:98.29%
Epoch [189/300], Step [350/391],                 Loss: 0.05547, Train_Acc:98.29%
Epoch [189/300], Step [360/391],                 Loss: 0.05516, Train_Acc:98.30%
Epoch [189/300], Step [370/391],                 Loss: 0.05492, Train_Acc:98.31%
Epoch [189/300], Step [380/391],                 Loss: 0.05458, Train_Acc:98.32%
Epoch [189/300], Step [390/391],                 Loss: 0.05449, Train_Acc:98.32%
Accuary on test images:88.84%
Epoch [190/300], Step [10/391],                 Loss: 0.03742, Train_Acc:98.98%
Epoch [190/300], Step [20/391],                 Loss: 0.04539, Train_Acc:98.63%
Epoch [190/300], Step [30/391],                 Loss: 0.04788, Train_Acc:98.52%
Epoch [190/300], Step [40/391],                 Loss: 0.04813, Train_Acc:98.50%
Epoch [190/300], Step [50/391],                 Loss: 0.04675, Train_Acc:98.55%
Epoch [190/300], Step [60/391],                 Loss: 0.04716, Train_Acc:98.58%
Epoch [190/300], Step [70/391],                 Loss: 0.04667, Train_Acc:98.59%
Epoch [190/300], Step [80/391],                 Loss: 0.04854, Train_Acc:98.52%
Epoch [190/300], Step [90/391],                 Loss: 0.04970, Train_Acc:98.45%
Epoch [190/300], Step [100/391],                 Loss: 0.05038, Train_Acc:98.44%
Epoch [190/300], Step [110/391],                 Loss: 0.05021, Train_Acc:98.45%
Epoch [190/300], Step [120/391],                 Loss: 0.05069, Train_Acc:98.45%
Epoch [190/300], Step [130/391],                 Loss: 0.05127, Train_Acc:98.44%
Epoch [190/300], Step [140/391],                 Loss: 0.05124, Train_Acc:98.45%
Epoch [190/300], Step [150/391],                 Loss: 0.05053, Train_Acc:98.47%
Epoch [190/300], Step [160/391],                 Loss: 0.05054, Train_Acc:98.47%
Epoch [190/300], Step [170/391],                 Loss: 0.04981, Train_Acc:98.50%
Epoch [190/300], Step [180/391],                 Loss: 0.04969, Train_Acc:98.50%
Epoch [190/300], Step [190/391],                 Loss: 0.04915, Train_Acc:98.51%
Epoch [190/300], Step [200/391],                 Loss: 0.04883, Train_Acc:98.54%
Epoch [190/300], Step [210/391],                 Loss: 0.04853, Train_Acc:98.55%
Epoch [190/300], Step [220/391],                 Loss: 0.04779, Train_Acc:98.59%
Epoch [190/300], Step [230/391],                 Loss: 0.04738, Train_Acc:98.59%
Epoch [190/300], Step [240/391],                 Loss: 0.04683, Train_Acc:98.60%
Epoch [190/300], Step [250/391],                 Loss: 0.04675, Train_Acc:98.59%
Epoch [190/300], Step [260/391],                 Loss: 0.04753, Train_Acc:98.56%
Epoch [190/300], Step [270/391],                 Loss: 0.04747, Train_Acc:98.56%
Epoch [190/300], Step [280/391],                 Loss: 0.04747, Train_Acc:98.56%
Epoch [190/300], Step [290/391],                 Loss: 0.04725, Train_Acc:98.57%
Epoch [190/300], Step [300/391],                 Loss: 0.04722, Train_Acc:98.57%
Epoch [190/300], Step [310/391],                 Loss: 0.04687, Train_Acc:98.59%
Epoch [190/300], Step [320/391],                 Loss: 0.04686, Train_Acc:98.60%
Epoch [190/300], Step [330/391],                 Loss: 0.04667, Train_Acc:98.61%
Epoch [190/300], Step [340/391],                 Loss: 0.04667, Train_Acc:98.61%
Epoch [190/300], Step [350/391],                 Loss: 0.04654, Train_Acc:98.60%
Epoch [190/300], Step [360/391],                 Loss: 0.04666, Train_Acc:98.60%
Epoch [190/300], Step [370/391],                 Loss: 0.04658, Train_Acc:98.60%
Epoch [190/300], Step [380/391],                 Loss: 0.04621, Train_Acc:98.62%
Epoch [190/300], Step [390/391],                 Loss: 0.04610, Train_Acc:98.62%
Accuary on test images:89.08%
Epoch [191/300], Step [10/391],                 Loss: 0.04553, Train_Acc:98.52%
Epoch [191/300], Step [20/391],                 Loss: 0.04021, Train_Acc:98.87%
Epoch [191/300], Step [30/391],                 Loss: 0.03672, Train_Acc:98.98%
Epoch [191/300], Step [40/391],                 Loss: 0.03889, Train_Acc:98.83%
Epoch [191/300], Step [50/391],                 Loss: 0.03964, Train_Acc:98.77%
Epoch [191/300], Step [60/391],                 Loss: 0.04097, Train_Acc:98.67%
Epoch [191/300], Step [70/391],                 Loss: 0.04229, Train_Acc:98.62%
Epoch [191/300], Step [80/391],                 Loss: 0.04331, Train_Acc:98.59%
Epoch [191/300], Step [90/391],                 Loss: 0.04418, Train_Acc:98.60%
Epoch [191/300], Step [100/391],                 Loss: 0.04551, Train_Acc:98.55%
Epoch [191/300], Step [110/391],                 Loss: 0.04534, Train_Acc:98.59%
Epoch [191/300], Step [120/391],                 Loss: 0.04563, Train_Acc:98.55%
Epoch [191/300], Step [130/391],                 Loss: 0.04640, Train_Acc:98.50%
Epoch [191/300], Step [140/391],                 Loss: 0.04581, Train_Acc:98.54%
Epoch [191/300], Step [150/391],                 Loss: 0.04674, Train_Acc:98.53%
Epoch [191/300], Step [160/391],                 Loss: 0.04717, Train_Acc:98.52%
Epoch [191/300], Step [170/391],                 Loss: 0.04812, Train_Acc:98.50%
Epoch [191/300], Step [180/391],                 Loss: 0.04922, Train_Acc:98.47%
Epoch [191/300], Step [190/391],                 Loss: 0.04899, Train_Acc:98.49%
Epoch [191/300], Step [200/391],                 Loss: 0.04900, Train_Acc:98.51%
Epoch [191/300], Step [210/391],                 Loss: 0.04882, Train_Acc:98.50%
Epoch [191/300], Step [220/391],                 Loss: 0.04877, Train_Acc:98.51%
Epoch [191/300], Step [230/391],                 Loss: 0.04831, Train_Acc:98.53%
Epoch [191/300], Step [240/391],                 Loss: 0.04793, Train_Acc:98.54%
Epoch [191/300], Step [250/391],                 Loss: 0.04785, Train_Acc:98.54%
Epoch [191/300], Step [260/391],                 Loss: 0.04783, Train_Acc:98.55%
Epoch [191/300], Step [270/391],                 Loss: 0.04800, Train_Acc:98.53%
Epoch [191/300], Step [280/391],                 Loss: 0.04783, Train_Acc:98.53%
Epoch [191/300], Step [290/391],                 Loss: 0.04778, Train_Acc:98.54%
Epoch [191/300], Step [300/391],                 Loss: 0.04767, Train_Acc:98.53%
Epoch [191/300], Step [310/391],                 Loss: 0.04737, Train_Acc:98.54%
Epoch [191/300], Step [320/391],                 Loss: 0.04715, Train_Acc:98.54%
Epoch [191/300], Step [330/391],                 Loss: 0.04686, Train_Acc:98.57%
Epoch [191/300], Step [340/391],                 Loss: 0.04664, Train_Acc:98.58%
Epoch [191/300], Step [350/391],                 Loss: 0.04626, Train_Acc:98.60%
Epoch [191/300], Step [360/391],                 Loss: 0.04608, Train_Acc:98.59%
Epoch [191/300], Step [370/391],                 Loss: 0.04577, Train_Acc:98.60%
Epoch [191/300], Step [380/391],                 Loss: 0.04546, Train_Acc:98.60%
Epoch [191/300], Step [390/391],                 Loss: 0.04565, Train_Acc:98.60%
Accuary on test images:87.38%
Epoch [192/300], Step [10/391],                 Loss: 0.04630, Train_Acc:98.44%
Epoch [192/300], Step [20/391],                 Loss: 0.04990, Train_Acc:98.44%
Epoch [192/300], Step [30/391],                 Loss: 0.04708, Train_Acc:98.57%
Epoch [192/300], Step [40/391],                 Loss: 0.04686, Train_Acc:98.57%
Epoch [192/300], Step [50/391],                 Loss: 0.04877, Train_Acc:98.50%
Epoch [192/300], Step [60/391],                 Loss: 0.05013, Train_Acc:98.48%
Epoch [192/300], Step [70/391],                 Loss: 0.04805, Train_Acc:98.59%
Epoch [192/300], Step [80/391],                 Loss: 0.04812, Train_Acc:98.59%
Epoch [192/300], Step [90/391],                 Loss: 0.04799, Train_Acc:98.54%
Epoch [192/300], Step [100/391],                 Loss: 0.04721, Train_Acc:98.59%
Epoch [192/300], Step [110/391],                 Loss: 0.04722, Train_Acc:98.58%
Epoch [192/300], Step [120/391],                 Loss: 0.04788, Train_Acc:98.55%
Epoch [192/300], Step [130/391],                 Loss: 0.04788, Train_Acc:98.53%
Epoch [192/300], Step [140/391],                 Loss: 0.04762, Train_Acc:98.54%
Epoch [192/300], Step [150/391],                 Loss: 0.04719, Train_Acc:98.56%
Epoch [192/300], Step [160/391],                 Loss: 0.04721, Train_Acc:98.54%
Epoch [192/300], Step [170/391],                 Loss: 0.04683, Train_Acc:98.56%
Epoch [192/300], Step [180/391],                 Loss: 0.04606, Train_Acc:98.59%
Epoch [192/300], Step [190/391],                 Loss: 0.04549, Train_Acc:98.60%
Epoch [192/300], Step [200/391],                 Loss: 0.04505, Train_Acc:98.62%
Epoch [192/300], Step [210/391],                 Loss: 0.04507, Train_Acc:98.63%
Epoch [192/300], Step [220/391],                 Loss: 0.04468, Train_Acc:98.64%
Epoch [192/300], Step [230/391],                 Loss: 0.04418, Train_Acc:98.67%
Epoch [192/300], Step [240/391],                 Loss: 0.04386, Train_Acc:98.67%
Epoch [192/300], Step [250/391],                 Loss: 0.04355, Train_Acc:98.68%
Epoch [192/300], Step [260/391],                 Loss: 0.04377, Train_Acc:98.68%
Epoch [192/300], Step [270/391],                 Loss: 0.04366, Train_Acc:98.69%
Epoch [192/300], Step [280/391],                 Loss: 0.04345, Train_Acc:98.68%
Epoch [192/300], Step [290/391],                 Loss: 0.04329, Train_Acc:98.69%
Epoch [192/300], Step [300/391],                 Loss: 0.04325, Train_Acc:98.70%
Epoch [192/300], Step [310/391],                 Loss: 0.04334, Train_Acc:98.70%
Epoch [192/300], Step [320/391],                 Loss: 0.04312, Train_Acc:98.70%
Epoch [192/300], Step [330/391],                 Loss: 0.04311, Train_Acc:98.70%
Epoch [192/300], Step [340/391],                 Loss: 0.04324, Train_Acc:98.70%
Epoch [192/300], Step [350/391],                 Loss: 0.04312, Train_Acc:98.69%
Epoch [192/300], Step [360/391],                 Loss: 0.04331, Train_Acc:98.69%
Epoch [192/300], Step [370/391],                 Loss: 0.04321, Train_Acc:98.70%
Epoch [192/300], Step [380/391],                 Loss: 0.04288, Train_Acc:98.71%
Epoch [192/300], Step [390/391],                 Loss: 0.04291, Train_Acc:98.71%
Accuary on test images:87.56%
Epoch [193/300], Step [10/391],                 Loss: 0.04454, Train_Acc:98.75%
Epoch [193/300], Step [20/391],                 Loss: 0.04035, Train_Acc:98.59%
Epoch [193/300], Step [30/391],                 Loss: 0.03893, Train_Acc:98.75%
Epoch [193/300], Step [40/391],                 Loss: 0.03849, Train_Acc:98.71%
Epoch [193/300], Step [50/391],                 Loss: 0.03752, Train_Acc:98.77%
Epoch [193/300], Step [60/391],                 Loss: 0.03973, Train_Acc:98.76%
Epoch [193/300], Step [70/391],                 Loss: 0.04140, Train_Acc:98.69%
Epoch [193/300], Step [80/391],                 Loss: 0.04205, Train_Acc:98.71%
Epoch [193/300], Step [90/391],                 Loss: 0.04238, Train_Acc:98.70%
Epoch [193/300], Step [100/391],                 Loss: 0.04151, Train_Acc:98.72%
Epoch [193/300], Step [110/391],                 Loss: 0.04176, Train_Acc:98.70%
Epoch [193/300], Step [120/391],                 Loss: 0.04121, Train_Acc:98.72%
Epoch [193/300], Step [130/391],                 Loss: 0.04090, Train_Acc:98.76%
Epoch [193/300], Step [140/391],                 Loss: 0.04083, Train_Acc:98.73%
Epoch [193/300], Step [150/391],                 Loss: 0.04053, Train_Acc:98.74%
Epoch [193/300], Step [160/391],                 Loss: 0.04077, Train_Acc:98.72%
Epoch [193/300], Step [170/391],                 Loss: 0.04066, Train_Acc:98.71%
Epoch [193/300], Step [180/391],                 Loss: 0.04066, Train_Acc:98.72%
Epoch [193/300], Step [190/391],                 Loss: 0.04061, Train_Acc:98.72%
Epoch [193/300], Step [200/391],                 Loss: 0.04071, Train_Acc:98.71%
Epoch [193/300], Step [210/391],                 Loss: 0.04063, Train_Acc:98.72%
Epoch [193/300], Step [220/391],                 Loss: 0.04042, Train_Acc:98.74%
Epoch [193/300], Step [230/391],                 Loss: 0.04028, Train_Acc:98.75%
Epoch [193/300], Step [240/391],                 Loss: 0.03991, Train_Acc:98.77%
Epoch [193/300], Step [250/391],                 Loss: 0.04010, Train_Acc:98.78%
Epoch [193/300], Step [260/391],                 Loss: 0.04005, Train_Acc:98.78%
Epoch [193/300], Step [270/391],                 Loss: 0.04018, Train_Acc:98.76%
Epoch [193/300], Step [280/391],                 Loss: 0.04080, Train_Acc:98.72%
Epoch [193/300], Step [290/391],                 Loss: 0.04093, Train_Acc:98.73%
Epoch [193/300], Step [300/391],                 Loss: 0.04144, Train_Acc:98.72%
Epoch [193/300], Step [310/391],                 Loss: 0.04185, Train_Acc:98.69%
Epoch [193/300], Step [320/391],                 Loss: 0.04212, Train_Acc:98.68%
Epoch [193/300], Step [330/391],                 Loss: 0.04222, Train_Acc:98.67%
Epoch [193/300], Step [340/391],                 Loss: 0.04237, Train_Acc:98.68%
Epoch [193/300], Step [350/391],                 Loss: 0.04224, Train_Acc:98.69%
Epoch [193/300], Step [360/391],                 Loss: 0.04215, Train_Acc:98.69%
Epoch [193/300], Step [370/391],                 Loss: 0.04212, Train_Acc:98.68%
Epoch [193/300], Step [380/391],                 Loss: 0.04234, Train_Acc:98.68%
Epoch [193/300], Step [390/391],                 Loss: 0.04236, Train_Acc:98.68%
Accuary on test images:87.90%
Epoch [194/300], Step [10/391],                 Loss: 0.04755, Train_Acc:98.59%
Epoch [194/300], Step [20/391],                 Loss: 0.04454, Train_Acc:98.71%
Epoch [194/300], Step [30/391],                 Loss: 0.04346, Train_Acc:98.78%
Epoch [194/300], Step [40/391],                 Loss: 0.04200, Train_Acc:98.89%
Epoch [194/300], Step [50/391],                 Loss: 0.03976, Train_Acc:98.94%
Epoch [194/300], Step [60/391],                 Loss: 0.04109, Train_Acc:98.89%
Epoch [194/300], Step [70/391],                 Loss: 0.04047, Train_Acc:98.88%
Epoch [194/300], Step [80/391],                 Loss: 0.03994, Train_Acc:98.90%
Epoch [194/300], Step [90/391],                 Loss: 0.04034, Train_Acc:98.86%
Epoch [194/300], Step [100/391],                 Loss: 0.03969, Train_Acc:98.85%
Epoch [194/300], Step [110/391],                 Loss: 0.03987, Train_Acc:98.85%
Epoch [194/300], Step [120/391],                 Loss: 0.04062, Train_Acc:98.82%
Epoch [194/300], Step [130/391],                 Loss: 0.04028, Train_Acc:98.82%
Epoch [194/300], Step [140/391],                 Loss: 0.04038, Train_Acc:98.80%
Epoch [194/300], Step [150/391],                 Loss: 0.04034, Train_Acc:98.81%
Epoch [194/300], Step [160/391],                 Loss: 0.03956, Train_Acc:98.84%
Epoch [194/300], Step [170/391],                 Loss: 0.03912, Train_Acc:98.86%
Epoch [194/300], Step [180/391],                 Loss: 0.03852, Train_Acc:98.87%
Epoch [194/300], Step [190/391],                 Loss: 0.03843, Train_Acc:98.87%
Epoch [194/300], Step [200/391],                 Loss: 0.03768, Train_Acc:98.90%
Epoch [194/300], Step [210/391],                 Loss: 0.03768, Train_Acc:98.90%
Epoch [194/300], Step [220/391],                 Loss: 0.03773, Train_Acc:98.89%
Epoch [194/300], Step [230/391],                 Loss: 0.03739, Train_Acc:98.91%
Epoch [194/300], Step [240/391],                 Loss: 0.03735, Train_Acc:98.91%
Epoch [194/300], Step [250/391],                 Loss: 0.03744, Train_Acc:98.89%
Epoch [194/300], Step [260/391],                 Loss: 0.03770, Train_Acc:98.87%
Epoch [194/300], Step [270/391],                 Loss: 0.03763, Train_Acc:98.88%
Epoch [194/300], Step [280/391],                 Loss: 0.03743, Train_Acc:98.89%
Epoch [194/300], Step [290/391],                 Loss: 0.03750, Train_Acc:98.89%
Epoch [194/300], Step [300/391],                 Loss: 0.03749, Train_Acc:98.89%
Epoch [194/300], Step [310/391],                 Loss: 0.03743, Train_Acc:98.90%
Epoch [194/300], Step [320/391],                 Loss: 0.03751, Train_Acc:98.89%
Epoch [194/300], Step [330/391],                 Loss: 0.03741, Train_Acc:98.88%
Epoch [194/300], Step [340/391],                 Loss: 0.03724, Train_Acc:98.89%
Epoch [194/300], Step [350/391],                 Loss: 0.03704, Train_Acc:98.90%
Epoch [194/300], Step [360/391],                 Loss: 0.03711, Train_Acc:98.89%
Epoch [194/300], Step [370/391],                 Loss: 0.03683, Train_Acc:98.90%
Epoch [194/300], Step [380/391],                 Loss: 0.03687, Train_Acc:98.90%
Epoch [194/300], Step [390/391],                 Loss: 0.03696, Train_Acc:98.89%
Accuary on test images:87.28%
Epoch [195/300], Step [10/391],                 Loss: 0.04058, Train_Acc:98.52%
Epoch [195/300], Step [20/391],                 Loss: 0.04115, Train_Acc:98.67%
Epoch [195/300], Step [30/391],                 Loss: 0.04066, Train_Acc:98.67%
Epoch [195/300], Step [40/391],                 Loss: 0.04183, Train_Acc:98.61%
Epoch [195/300], Step [50/391],                 Loss: 0.04144, Train_Acc:98.66%
Epoch [195/300], Step [60/391],                 Loss: 0.04162, Train_Acc:98.65%
Epoch [195/300], Step [70/391],                 Loss: 0.04279, Train_Acc:98.60%
Epoch [195/300], Step [80/391],                 Loss: 0.04340, Train_Acc:98.59%
Epoch [195/300], Step [90/391],                 Loss: 0.04480, Train_Acc:98.53%
Epoch [195/300], Step [100/391],                 Loss: 0.04417, Train_Acc:98.55%
Epoch [195/300], Step [110/391],                 Loss: 0.04548, Train_Acc:98.52%
Epoch [195/300], Step [120/391],                 Loss: 0.04587, Train_Acc:98.50%
Epoch [195/300], Step [130/391],                 Loss: 0.04639, Train_Acc:98.47%
Epoch [195/300], Step [140/391],                 Loss: 0.04604, Train_Acc:98.47%
Epoch [195/300], Step [150/391],                 Loss: 0.04644, Train_Acc:98.49%
Epoch [195/300], Step [160/391],                 Loss: 0.04741, Train_Acc:98.48%
Epoch [195/300], Step [170/391],                 Loss: 0.04833, Train_Acc:98.46%
Epoch [195/300], Step [180/391],                 Loss: 0.04817, Train_Acc:98.48%
Epoch [195/300], Step [190/391],                 Loss: 0.04837, Train_Acc:98.47%
Epoch [195/300], Step [200/391],                 Loss: 0.04829, Train_Acc:98.48%
Epoch [195/300], Step [210/391],                 Loss: 0.04913, Train_Acc:98.46%
Epoch [195/300], Step [220/391],                 Loss: 0.04925, Train_Acc:98.46%
Epoch [195/300], Step [230/391],                 Loss: 0.04893, Train_Acc:98.48%
Epoch [195/300], Step [240/391],                 Loss: 0.04851, Train_Acc:98.51%
Epoch [195/300], Step [250/391],                 Loss: 0.04869, Train_Acc:98.50%
Epoch [195/300], Step [260/391],                 Loss: 0.04843, Train_Acc:98.50%
Epoch [195/300], Step [270/391],                 Loss: 0.04850, Train_Acc:98.51%
Epoch [195/300], Step [280/391],                 Loss: 0.04839, Train_Acc:98.51%
Epoch [195/300], Step [290/391],                 Loss: 0.04809, Train_Acc:98.53%
Epoch [195/300], Step [300/391],                 Loss: 0.04771, Train_Acc:98.54%
Epoch [195/300], Step [310/391],                 Loss: 0.04732, Train_Acc:98.55%
Epoch [195/300], Step [320/391],                 Loss: 0.04714, Train_Acc:98.54%
Epoch [195/300], Step [330/391],                 Loss: 0.04693, Train_Acc:98.55%
Epoch [195/300], Step [340/391],                 Loss: 0.04670, Train_Acc:98.55%
Epoch [195/300], Step [350/391],                 Loss: 0.04647, Train_Acc:98.56%
Epoch [195/300], Step [360/391],                 Loss: 0.04627, Train_Acc:98.58%
Epoch [195/300], Step [370/391],                 Loss: 0.04634, Train_Acc:98.59%
Epoch [195/300], Step [380/391],                 Loss: 0.04597, Train_Acc:98.60%
Epoch [195/300], Step [390/391],                 Loss: 0.04596, Train_Acc:98.60%
Accuary on test images:88.08%
Epoch [196/300], Step [10/391],                 Loss: 0.03960, Train_Acc:98.75%
Epoch [196/300], Step [20/391],                 Loss: 0.03887, Train_Acc:98.87%
Epoch [196/300], Step [30/391],                 Loss: 0.03958, Train_Acc:98.80%
Epoch [196/300], Step [40/391],                 Loss: 0.04338, Train_Acc:98.65%
Epoch [196/300], Step [50/391],                 Loss: 0.04300, Train_Acc:98.72%
Epoch [196/300], Step [60/391],                 Loss: 0.04222, Train_Acc:98.78%
Epoch [196/300], Step [70/391],                 Loss: 0.04246, Train_Acc:98.73%
Epoch [196/300], Step [80/391],                 Loss: 0.04156, Train_Acc:98.78%
Epoch [196/300], Step [90/391],                 Loss: 0.04082, Train_Acc:98.82%
Epoch [196/300], Step [100/391],                 Loss: 0.04022, Train_Acc:98.84%
Epoch [196/300], Step [110/391],                 Loss: 0.03980, Train_Acc:98.84%
Epoch [196/300], Step [120/391],                 Loss: 0.03916, Train_Acc:98.85%
Epoch [196/300], Step [130/391],                 Loss: 0.03950, Train_Acc:98.83%
Epoch [196/300], Step [140/391],                 Loss: 0.03934, Train_Acc:98.84%
Epoch [196/300], Step [150/391],                 Loss: 0.03925, Train_Acc:98.85%
Epoch [196/300], Step [160/391],                 Loss: 0.03881, Train_Acc:98.88%
Epoch [196/300], Step [170/391],                 Loss: 0.03925, Train_Acc:98.87%
Epoch [196/300], Step [180/391],                 Loss: 0.03894, Train_Acc:98.88%
Epoch [196/300], Step [190/391],                 Loss: 0.03927, Train_Acc:98.85%
Epoch [196/300], Step [200/391],                 Loss: 0.03937, Train_Acc:98.84%
Epoch [196/300], Step [210/391],                 Loss: 0.03952, Train_Acc:98.82%
Epoch [196/300], Step [220/391],                 Loss: 0.03962, Train_Acc:98.81%
Epoch [196/300], Step [230/391],                 Loss: 0.03966, Train_Acc:98.80%
Epoch [196/300], Step [240/391],                 Loss: 0.04010, Train_Acc:98.79%
Epoch [196/300], Step [250/391],                 Loss: 0.04009, Train_Acc:98.79%
Epoch [196/300], Step [260/391],                 Loss: 0.04026, Train_Acc:98.78%
Epoch [196/300], Step [270/391],                 Loss: 0.04097, Train_Acc:98.76%
Epoch [196/300], Step [280/391],                 Loss: 0.04116, Train_Acc:98.74%
Epoch [196/300], Step [290/391],                 Loss: 0.04147, Train_Acc:98.74%
Epoch [196/300], Step [300/391],                 Loss: 0.04164, Train_Acc:98.73%
Epoch [196/300], Step [310/391],                 Loss: 0.04209, Train_Acc:98.73%
Epoch [196/300], Step [320/391],                 Loss: 0.04238, Train_Acc:98.72%
Epoch [196/300], Step [330/391],                 Loss: 0.04247, Train_Acc:98.71%
Epoch [196/300], Step [340/391],                 Loss: 0.04244, Train_Acc:98.72%
Epoch [196/300], Step [350/391],                 Loss: 0.04236, Train_Acc:98.72%
Epoch [196/300], Step [360/391],                 Loss: 0.04245, Train_Acc:98.71%
Epoch [196/300], Step [370/391],                 Loss: 0.04211, Train_Acc:98.72%
Epoch [196/300], Step [380/391],                 Loss: 0.04182, Train_Acc:98.73%
Epoch [196/300], Step [390/391],                 Loss: 0.04187, Train_Acc:98.72%
Accuary on test images:88.48%
Epoch [197/300], Step [10/391],                 Loss: 0.04311, Train_Acc:98.59%
Epoch [197/300], Step [20/391],                 Loss: 0.04434, Train_Acc:98.55%
Epoch [197/300], Step [30/391],                 Loss: 0.04104, Train_Acc:98.75%
Epoch [197/300], Step [40/391],                 Loss: 0.04260, Train_Acc:98.67%
Epoch [197/300], Step [50/391],                 Loss: 0.04447, Train_Acc:98.61%
Epoch [197/300], Step [60/391],                 Loss: 0.04342, Train_Acc:98.67%
Epoch [197/300], Step [70/391],                 Loss: 0.04304, Train_Acc:98.69%
Epoch [197/300], Step [80/391],                 Loss: 0.04611, Train_Acc:98.60%
Epoch [197/300], Step [90/391],                 Loss: 0.04618, Train_Acc:98.59%
Epoch [197/300], Step [100/391],                 Loss: 0.04615, Train_Acc:98.59%
Epoch [197/300], Step [110/391],                 Loss: 0.04606, Train_Acc:98.63%
Epoch [197/300], Step [120/391],                 Loss: 0.04573, Train_Acc:98.65%
Epoch [197/300], Step [130/391],                 Loss: 0.04608, Train_Acc:98.62%
Epoch [197/300], Step [140/391],                 Loss: 0.04556, Train_Acc:98.64%
Epoch [197/300], Step [150/391],                 Loss: 0.04573, Train_Acc:98.64%
Epoch [197/300], Step [160/391],                 Loss: 0.04519, Train_Acc:98.65%
Epoch [197/300], Step [170/391],                 Loss: 0.04487, Train_Acc:98.65%
Epoch [197/300], Step [180/391],                 Loss: 0.04585, Train_Acc:98.63%
Epoch [197/300], Step [190/391],                 Loss: 0.04537, Train_Acc:98.64%
Epoch [197/300], Step [200/391],                 Loss: 0.04505, Train_Acc:98.64%
Epoch [197/300], Step [210/391],                 Loss: 0.04489, Train_Acc:98.64%
Epoch [197/300], Step [220/391],                 Loss: 0.04472, Train_Acc:98.64%
Epoch [197/300], Step [230/391],                 Loss: 0.04427, Train_Acc:98.66%
Epoch [197/300], Step [240/391],                 Loss: 0.04384, Train_Acc:98.68%
Epoch [197/300], Step [250/391],                 Loss: 0.04390, Train_Acc:98.67%
Epoch [197/300], Step [260/391],                 Loss: 0.04382, Train_Acc:98.68%
Epoch [197/300], Step [270/391],                 Loss: 0.04362, Train_Acc:98.69%
Epoch [197/300], Step [280/391],                 Loss: 0.04369, Train_Acc:98.69%
Epoch [197/300], Step [290/391],                 Loss: 0.04374, Train_Acc:98.68%
Epoch [197/300], Step [300/391],                 Loss: 0.04442, Train_Acc:98.66%
Epoch [197/300], Step [310/391],                 Loss: 0.04446, Train_Acc:98.66%
Epoch [197/300], Step [320/391],                 Loss: 0.04497, Train_Acc:98.65%
Epoch [197/300], Step [330/391],                 Loss: 0.04476, Train_Acc:98.65%
Epoch [197/300], Step [340/391],                 Loss: 0.04516, Train_Acc:98.63%
Epoch [197/300], Step [350/391],                 Loss: 0.04535, Train_Acc:98.62%
Epoch [197/300], Step [360/391],                 Loss: 0.04541, Train_Acc:98.62%
Epoch [197/300], Step [370/391],                 Loss: 0.04572, Train_Acc:98.60%
Epoch [197/300], Step [380/391],                 Loss: 0.04575, Train_Acc:98.60%
Epoch [197/300], Step [390/391],                 Loss: 0.04575, Train_Acc:98.59%
Accuary on test images:88.80%
Epoch [198/300], Step [10/391],                 Loss: 0.03980, Train_Acc:98.83%
Epoch [198/300], Step [20/391],                 Loss: 0.04854, Train_Acc:98.36%
Epoch [198/300], Step [30/391],                 Loss: 0.04784, Train_Acc:98.44%
Epoch [198/300], Step [40/391],                 Loss: 0.04761, Train_Acc:98.44%
Epoch [198/300], Step [50/391],                 Loss: 0.04599, Train_Acc:98.56%
Epoch [198/300], Step [60/391],                 Loss: 0.04506, Train_Acc:98.66%
Epoch [198/300], Step [70/391],                 Loss: 0.04445, Train_Acc:98.64%
Epoch [198/300], Step [80/391],                 Loss: 0.04354, Train_Acc:98.67%
Epoch [198/300], Step [90/391],                 Loss: 0.04472, Train_Acc:98.65%
Epoch [198/300], Step [100/391],                 Loss: 0.04284, Train_Acc:98.72%
Epoch [198/300], Step [110/391],                 Loss: 0.04307, Train_Acc:98.71%
Epoch [198/300], Step [120/391],                 Loss: 0.04221, Train_Acc:98.72%
Epoch [198/300], Step [130/391],                 Loss: 0.04200, Train_Acc:98.73%
Epoch [198/300], Step [140/391],                 Loss: 0.04192, Train_Acc:98.74%
Epoch [198/300], Step [150/391],                 Loss: 0.04134, Train_Acc:98.77%
Epoch [198/300], Step [160/391],                 Loss: 0.04103, Train_Acc:98.77%
Epoch [198/300], Step [170/391],                 Loss: 0.04077, Train_Acc:98.77%
Epoch [198/300], Step [180/391],                 Loss: 0.04096, Train_Acc:98.78%
Epoch [198/300], Step [190/391],                 Loss: 0.04126, Train_Acc:98.77%
Epoch [198/300], Step [200/391],                 Loss: 0.04148, Train_Acc:98.76%
Epoch [198/300], Step [210/391],                 Loss: 0.04096, Train_Acc:98.78%
Epoch [198/300], Step [220/391],                 Loss: 0.04169, Train_Acc:98.75%
Epoch [198/300], Step [230/391],                 Loss: 0.04159, Train_Acc:98.76%
Epoch [198/300], Step [240/391],                 Loss: 0.04168, Train_Acc:98.75%
Epoch [198/300], Step [250/391],                 Loss: 0.04166, Train_Acc:98.74%
Epoch [198/300], Step [260/391],                 Loss: 0.04212, Train_Acc:98.73%
Epoch [198/300], Step [270/391],                 Loss: 0.04200, Train_Acc:98.74%
Epoch [198/300], Step [280/391],                 Loss: 0.04173, Train_Acc:98.76%
Epoch [198/300], Step [290/391],                 Loss: 0.04185, Train_Acc:98.76%
Epoch [198/300], Step [300/391],                 Loss: 0.04165, Train_Acc:98.77%
Epoch [198/300], Step [310/391],                 Loss: 0.04182, Train_Acc:98.77%
Epoch [198/300], Step [320/391],                 Loss: 0.04178, Train_Acc:98.77%
Epoch [198/300], Step [330/391],                 Loss: 0.04171, Train_Acc:98.78%
Epoch [198/300], Step [340/391],                 Loss: 0.04160, Train_Acc:98.78%
Epoch [198/300], Step [350/391],                 Loss: 0.04156, Train_Acc:98.79%
Epoch [198/300], Step [360/391],                 Loss: 0.04123, Train_Acc:98.79%
Epoch [198/300], Step [370/391],                 Loss: 0.04160, Train_Acc:98.77%
Epoch [198/300], Step [380/391],                 Loss: 0.04156, Train_Acc:98.76%
Epoch [198/300], Step [390/391],                 Loss: 0.04174, Train_Acc:98.76%
Accuary on test images:87.88%
Epoch [199/300], Step [10/391],                 Loss: 0.04068, Train_Acc:98.67%
Epoch [199/300], Step [20/391],                 Loss: 0.04257, Train_Acc:98.71%
Epoch [199/300], Step [30/391],                 Loss: 0.03963, Train_Acc:98.91%
Epoch [199/300], Step [40/391],                 Loss: 0.04108, Train_Acc:98.83%
Epoch [199/300], Step [50/391],                 Loss: 0.04276, Train_Acc:98.75%
Epoch [199/300], Step [60/391],                 Loss: 0.04182, Train_Acc:98.83%
Epoch [199/300], Step [70/391],                 Loss: 0.04049, Train_Acc:98.85%
Epoch [199/300], Step [80/391],                 Loss: 0.04281, Train_Acc:98.75%
Epoch [199/300], Step [90/391],                 Loss: 0.04346, Train_Acc:98.67%
Epoch [199/300], Step [100/391],                 Loss: 0.04281, Train_Acc:98.69%
Epoch [199/300], Step [110/391],                 Loss: 0.04394, Train_Acc:98.64%
Epoch [199/300], Step [120/391],                 Loss: 0.04452, Train_Acc:98.60%
Epoch [199/300], Step [130/391],                 Loss: 0.04456, Train_Acc:98.59%
Epoch [199/300], Step [140/391],                 Loss: 0.04448, Train_Acc:98.59%
Epoch [199/300], Step [150/391],                 Loss: 0.04512, Train_Acc:98.55%
Epoch [199/300], Step [160/391],                 Loss: 0.04559, Train_Acc:98.53%
Epoch [199/300], Step [170/391],                 Loss: 0.04558, Train_Acc:98.52%
Epoch [199/300], Step [180/391],                 Loss: 0.04507, Train_Acc:98.56%
Epoch [199/300], Step [190/391],                 Loss: 0.04488, Train_Acc:98.56%
Epoch [199/300], Step [200/391],                 Loss: 0.04454, Train_Acc:98.58%
Epoch [199/300], Step [210/391],                 Loss: 0.04426, Train_Acc:98.60%
Epoch [199/300], Step [220/391],                 Loss: 0.04413, Train_Acc:98.61%
Epoch [199/300], Step [230/391],                 Loss: 0.04409, Train_Acc:98.61%
Epoch [199/300], Step [240/391],                 Loss: 0.04391, Train_Acc:98.62%
Epoch [199/300], Step [250/391],                 Loss: 0.04378, Train_Acc:98.62%
Epoch [199/300], Step [260/391],                 Loss: 0.04393, Train_Acc:98.61%
Epoch [199/300], Step [270/391],                 Loss: 0.04369, Train_Acc:98.62%
Epoch [199/300], Step [280/391],                 Loss: 0.04387, Train_Acc:98.62%
Epoch [199/300], Step [290/391],                 Loss: 0.04369, Train_Acc:98.62%
Epoch [199/300], Step [300/391],                 Loss: 0.04377, Train_Acc:98.61%
Epoch [199/300], Step [310/391],                 Loss: 0.04369, Train_Acc:98.61%
Epoch [199/300], Step [320/391],                 Loss: 0.04368, Train_Acc:98.62%
Epoch [199/300], Step [330/391],                 Loss: 0.04352, Train_Acc:98.63%
Epoch [199/300], Step [340/391],                 Loss: 0.04333, Train_Acc:98.65%
Epoch [199/300], Step [350/391],                 Loss: 0.04308, Train_Acc:98.66%
Epoch [199/300], Step [360/391],                 Loss: 0.04277, Train_Acc:98.67%
Epoch [199/300], Step [370/391],                 Loss: 0.04284, Train_Acc:98.67%
Epoch [199/300], Step [380/391],                 Loss: 0.04255, Train_Acc:98.67%
Epoch [199/300], Step [390/391],                 Loss: 0.04237, Train_Acc:98.68%
Accuary on test images:88.46%
Epoch [200/300], Step [10/391],                 Loss: 0.03618, Train_Acc:99.14%
Epoch [200/300], Step [20/391],                 Loss: 0.03935, Train_Acc:99.02%
Epoch [200/300], Step [30/391],                 Loss: 0.03880, Train_Acc:98.96%
Epoch [200/300], Step [40/391],                 Loss: 0.03841, Train_Acc:98.89%
Epoch [200/300], Step [50/391],                 Loss: 0.04002, Train_Acc:98.84%
Epoch [200/300], Step [60/391],                 Loss: 0.04086, Train_Acc:98.83%
Epoch [200/300], Step [70/391],                 Loss: 0.04087, Train_Acc:98.82%
Epoch [200/300], Step [80/391],                 Loss: 0.04020, Train_Acc:98.82%
Epoch [200/300], Step [90/391],                 Loss: 0.04102, Train_Acc:98.78%
Epoch [200/300], Step [100/391],                 Loss: 0.04101, Train_Acc:98.77%
Epoch [200/300], Step [110/391],                 Loss: 0.03939, Train_Acc:98.86%
Epoch [200/300], Step [120/391],                 Loss: 0.03887, Train_Acc:98.87%
Epoch [200/300], Step [130/391],                 Loss: 0.03928, Train_Acc:98.86%
Epoch [200/300], Step [140/391],                 Loss: 0.03804, Train_Acc:98.91%
Epoch [200/300], Step [150/391],                 Loss: 0.03793, Train_Acc:98.91%
Epoch [200/300], Step [160/391],                 Loss: 0.03787, Train_Acc:98.91%
Epoch [200/300], Step [170/391],                 Loss: 0.03762, Train_Acc:98.91%
Epoch [200/300], Step [180/391],                 Loss: 0.03725, Train_Acc:98.91%
Epoch [200/300], Step [190/391],                 Loss: 0.03744, Train_Acc:98.90%
Epoch [200/300], Step [200/391],                 Loss: 0.03803, Train_Acc:98.88%
Epoch [200/300], Step [210/391],                 Loss: 0.03777, Train_Acc:98.88%
Epoch [200/300], Step [220/391],                 Loss: 0.03774, Train_Acc:98.88%
Epoch [200/300], Step [230/391],                 Loss: 0.03761, Train_Acc:98.88%
Epoch [200/300], Step [240/391],                 Loss: 0.03695, Train_Acc:98.90%
Epoch [200/300], Step [250/391],                 Loss: 0.03714, Train_Acc:98.90%
Epoch [200/300], Step [260/391],                 Loss: 0.03683, Train_Acc:98.91%
Epoch [200/300], Step [270/391],                 Loss: 0.03705, Train_Acc:98.90%
Epoch [200/300], Step [280/391],                 Loss: 0.03701, Train_Acc:98.91%
Epoch [200/300], Step [290/391],                 Loss: 0.03688, Train_Acc:98.91%
Epoch [200/300], Step [300/391],                 Loss: 0.03696, Train_Acc:98.90%
Epoch [200/300], Step [310/391],                 Loss: 0.03736, Train_Acc:98.88%
Epoch [200/300], Step [320/391],                 Loss: 0.03728, Train_Acc:98.89%
Epoch [200/300], Step [330/391],                 Loss: 0.03767, Train_Acc:98.87%
Epoch [200/300], Step [340/391],                 Loss: 0.03798, Train_Acc:98.86%
Epoch [200/300], Step [350/391],                 Loss: 0.03802, Train_Acc:98.87%
Epoch [200/300], Step [360/391],                 Loss: 0.03781, Train_Acc:98.88%
Epoch [200/300], Step [370/391],                 Loss: 0.03769, Train_Acc:98.89%
Epoch [200/300], Step [380/391],                 Loss: 0.03761, Train_Acc:98.89%
Epoch [200/300], Step [390/391],                 Loss: 0.03792, Train_Acc:98.88%
Accuary on test images:88.64%
Epoch [201/300], Step [10/391],                 Loss: 0.03188, Train_Acc:99.14%
Epoch [201/300], Step [20/391],                 Loss: 0.03107, Train_Acc:99.10%
Epoch [201/300], Step [30/391],                 Loss: 0.02865, Train_Acc:99.24%
Epoch [201/300], Step [40/391],                 Loss: 0.03178, Train_Acc:99.14%
Epoch [201/300], Step [50/391],                 Loss: 0.03089, Train_Acc:99.14%
Epoch [201/300], Step [60/391],                 Loss: 0.03157, Train_Acc:99.15%
Epoch [201/300], Step [70/391],                 Loss: 0.03054, Train_Acc:99.17%
Epoch [201/300], Step [80/391],                 Loss: 0.03070, Train_Acc:99.13%
Epoch [201/300], Step [90/391],                 Loss: 0.03064, Train_Acc:99.11%
Epoch [201/300], Step [100/391],                 Loss: 0.03010, Train_Acc:99.12%
Epoch [201/300], Step [110/391],                 Loss: 0.03075, Train_Acc:99.11%
Epoch [201/300], Step [120/391],                 Loss: 0.03056, Train_Acc:99.12%
Epoch [201/300], Step [130/391],                 Loss: 0.03109, Train_Acc:99.10%
Epoch [201/300], Step [140/391],                 Loss: 0.03135, Train_Acc:99.08%
Epoch [201/300], Step [150/391],                 Loss: 0.03136, Train_Acc:99.07%
Epoch [201/300], Step [160/391],                 Loss: 0.03189, Train_Acc:99.05%
Epoch [201/300], Step [170/391],                 Loss: 0.03232, Train_Acc:99.03%
Epoch [201/300], Step [180/391],                 Loss: 0.03263, Train_Acc:99.03%
Epoch [201/300], Step [190/391],                 Loss: 0.03288, Train_Acc:99.03%
Epoch [201/300], Step [200/391],                 Loss: 0.03399, Train_Acc:98.99%
Epoch [201/300], Step [210/391],                 Loss: 0.03397, Train_Acc:98.99%
Epoch [201/300], Step [220/391],                 Loss: 0.03436, Train_Acc:98.97%
Epoch [201/300], Step [230/391],                 Loss: 0.03434, Train_Acc:98.97%
Epoch [201/300], Step [240/391],                 Loss: 0.03475, Train_Acc:98.96%
Epoch [201/300], Step [250/391],                 Loss: 0.03471, Train_Acc:98.98%
Epoch [201/300], Step [260/391],                 Loss: 0.03475, Train_Acc:98.98%
Epoch [201/300], Step [270/391],                 Loss: 0.03453, Train_Acc:98.98%
Epoch [201/300], Step [280/391],                 Loss: 0.03434, Train_Acc:98.98%
Epoch [201/300], Step [290/391],                 Loss: 0.03430, Train_Acc:98.98%
Epoch [201/300], Step [300/391],                 Loss: 0.03425, Train_Acc:98.98%
Epoch [201/300], Step [310/391],                 Loss: 0.03431, Train_Acc:98.99%
Epoch [201/300], Step [320/391],                 Loss: 0.03430, Train_Acc:98.99%
Epoch [201/300], Step [330/391],                 Loss: 0.03422, Train_Acc:99.01%
Epoch [201/300], Step [340/391],                 Loss: 0.03421, Train_Acc:99.01%
Epoch [201/300], Step [350/391],                 Loss: 0.03418, Train_Acc:99.01%
Epoch [201/300], Step [360/391],                 Loss: 0.03404, Train_Acc:99.01%
Epoch [201/300], Step [370/391],                 Loss: 0.03384, Train_Acc:99.02%
Epoch [201/300], Step [380/391],                 Loss: 0.03365, Train_Acc:99.02%
Epoch [201/300], Step [390/391],                 Loss: 0.03353, Train_Acc:99.03%
Accuary on test images:88.46%
Epoch [202/300], Step [10/391],                 Loss: 0.03519, Train_Acc:98.75%
Epoch [202/300], Step [20/391],                 Loss: 0.03032, Train_Acc:99.10%
Epoch [202/300], Step [30/391],                 Loss: 0.03156, Train_Acc:98.96%
Epoch [202/300], Step [40/391],                 Loss: 0.03339, Train_Acc:98.93%
Epoch [202/300], Step [50/391],                 Loss: 0.03266, Train_Acc:99.03%
Epoch [202/300], Step [60/391],                 Loss: 0.03212, Train_Acc:99.02%
Epoch [202/300], Step [70/391],                 Loss: 0.03354, Train_Acc:99.00%
Epoch [202/300], Step [80/391],                 Loss: 0.03419, Train_Acc:98.97%
Epoch [202/300], Step [90/391],                 Loss: 0.03479, Train_Acc:98.95%
Epoch [202/300], Step [100/391],                 Loss: 0.03374, Train_Acc:98.99%
Epoch [202/300], Step [110/391],                 Loss: 0.03478, Train_Acc:98.95%
Epoch [202/300], Step [120/391],                 Loss: 0.03562, Train_Acc:98.95%
Epoch [202/300], Step [130/391],                 Loss: 0.03647, Train_Acc:98.91%
Epoch [202/300], Step [140/391],                 Loss: 0.03725, Train_Acc:98.88%
Epoch [202/300], Step [150/391],                 Loss: 0.03771, Train_Acc:98.89%
Epoch [202/300], Step [160/391],                 Loss: 0.03730, Train_Acc:98.91%
Epoch [202/300], Step [170/391],                 Loss: 0.03751, Train_Acc:98.89%
Epoch [202/300], Step [180/391],                 Loss: 0.03803, Train_Acc:98.87%
Epoch [202/300], Step [190/391],                 Loss: 0.03791, Train_Acc:98.87%
Epoch [202/300], Step [200/391],                 Loss: 0.03868, Train_Acc:98.83%
Epoch [202/300], Step [210/391],                 Loss: 0.03859, Train_Acc:98.83%
Epoch [202/300], Step [220/391],                 Loss: 0.03886, Train_Acc:98.82%
Epoch [202/300], Step [230/391],                 Loss: 0.03870, Train_Acc:98.83%
Epoch [202/300], Step [240/391],                 Loss: 0.03871, Train_Acc:98.82%
Epoch [202/300], Step [250/391],                 Loss: 0.03874, Train_Acc:98.82%
Epoch [202/300], Step [260/391],                 Loss: 0.03921, Train_Acc:98.81%
Epoch [202/300], Step [270/391],                 Loss: 0.03909, Train_Acc:98.80%
Epoch [202/300], Step [280/391],                 Loss: 0.03869, Train_Acc:98.82%
Epoch [202/300], Step [290/391],                 Loss: 0.03858, Train_Acc:98.84%
Epoch [202/300], Step [300/391],                 Loss: 0.03855, Train_Acc:98.85%
Epoch [202/300], Step [310/391],                 Loss: 0.03829, Train_Acc:98.86%
Epoch [202/300], Step [320/391],                 Loss: 0.03806, Train_Acc:98.87%
Epoch [202/300], Step [330/391],                 Loss: 0.03768, Train_Acc:98.89%
Epoch [202/300], Step [340/391],                 Loss: 0.03738, Train_Acc:98.89%
Epoch [202/300], Step [350/391],                 Loss: 0.03727, Train_Acc:98.90%
Epoch [202/300], Step [360/391],                 Loss: 0.03690, Train_Acc:98.91%
Epoch [202/300], Step [370/391],                 Loss: 0.03719, Train_Acc:98.91%
Epoch [202/300], Step [380/391],                 Loss: 0.03692, Train_Acc:98.92%
Epoch [202/300], Step [390/391],                 Loss: 0.03737, Train_Acc:98.91%
Accuary on test images:87.86%
Epoch [203/300], Step [10/391],                 Loss: 0.04301, Train_Acc:98.83%
Epoch [203/300], Step [20/391],                 Loss: 0.04562, Train_Acc:98.40%
Epoch [203/300], Step [30/391],                 Loss: 0.04391, Train_Acc:98.52%
Epoch [203/300], Step [40/391],                 Loss: 0.04612, Train_Acc:98.48%
Epoch [203/300], Step [50/391],                 Loss: 0.04551, Train_Acc:98.48%
Epoch [203/300], Step [60/391],                 Loss: 0.04569, Train_Acc:98.49%
Epoch [203/300], Step [70/391],                 Loss: 0.04744, Train_Acc:98.47%
Epoch [203/300], Step [80/391],                 Loss: 0.04618, Train_Acc:98.54%
Epoch [203/300], Step [90/391],                 Loss: 0.04593, Train_Acc:98.54%
Epoch [203/300], Step [100/391],                 Loss: 0.04556, Train_Acc:98.58%
Epoch [203/300], Step [110/391],                 Loss: 0.04535, Train_Acc:98.58%
Epoch [203/300], Step [120/391],                 Loss: 0.04558, Train_Acc:98.59%
Epoch [203/300], Step [130/391],                 Loss: 0.04529, Train_Acc:98.61%
Epoch [203/300], Step [140/391],                 Loss: 0.04463, Train_Acc:98.65%
Epoch [203/300], Step [150/391],                 Loss: 0.04472, Train_Acc:98.64%
Epoch [203/300], Step [160/391],                 Loss: 0.04467, Train_Acc:98.64%
Epoch [203/300], Step [170/391],                 Loss: 0.04442, Train_Acc:98.66%
Epoch [203/300], Step [180/391],                 Loss: 0.04383, Train_Acc:98.68%
Epoch [203/300], Step [190/391],                 Loss: 0.04319, Train_Acc:98.69%
Epoch [203/300], Step [200/391],                 Loss: 0.04296, Train_Acc:98.70%
Epoch [203/300], Step [210/391],                 Loss: 0.04265, Train_Acc:98.71%
Epoch [203/300], Step [220/391],                 Loss: 0.04286, Train_Acc:98.71%
Epoch [203/300], Step [230/391],                 Loss: 0.04321, Train_Acc:98.68%
Epoch [203/300], Step [240/391],                 Loss: 0.04308, Train_Acc:98.68%
Epoch [203/300], Step [250/391],                 Loss: 0.04343, Train_Acc:98.66%
Epoch [203/300], Step [260/391],                 Loss: 0.04339, Train_Acc:98.67%
Epoch [203/300], Step [270/391],                 Loss: 0.04365, Train_Acc:98.67%
Epoch [203/300], Step [280/391],                 Loss: 0.04373, Train_Acc:98.66%
Epoch [203/300], Step [290/391],                 Loss: 0.04384, Train_Acc:98.66%
Epoch [203/300], Step [300/391],                 Loss: 0.04360, Train_Acc:98.68%
Epoch [203/300], Step [310/391],                 Loss: 0.04325, Train_Acc:98.70%
Epoch [203/300], Step [320/391],                 Loss: 0.04303, Train_Acc:98.70%
Epoch [203/300], Step [330/391],                 Loss: 0.04283, Train_Acc:98.71%
Epoch [203/300], Step [340/391],                 Loss: 0.04274, Train_Acc:98.71%
Epoch [203/300], Step [350/391],                 Loss: 0.04283, Train_Acc:98.71%
Epoch [203/300], Step [360/391],                 Loss: 0.04283, Train_Acc:98.69%
Epoch [203/300], Step [370/391],                 Loss: 0.04265, Train_Acc:98.70%
Epoch [203/300], Step [380/391],                 Loss: 0.04247, Train_Acc:98.70%
Epoch [203/300], Step [390/391],                 Loss: 0.04250, Train_Acc:98.71%
Accuary on test images:87.18%
Epoch [204/300], Step [10/391],                 Loss: 0.03729, Train_Acc:98.91%
Epoch [204/300], Step [20/391],                 Loss: 0.03922, Train_Acc:98.79%
Epoch [204/300], Step [30/391],                 Loss: 0.03949, Train_Acc:98.78%
Epoch [204/300], Step [40/391],                 Loss: 0.04013, Train_Acc:98.75%
Epoch [204/300], Step [50/391],                 Loss: 0.03916, Train_Acc:98.81%
Epoch [204/300], Step [60/391],                 Loss: 0.03856, Train_Acc:98.83%
Epoch [204/300], Step [70/391],                 Loss: 0.03799, Train_Acc:98.88%
Epoch [204/300], Step [80/391],                 Loss: 0.03832, Train_Acc:98.85%
Epoch [204/300], Step [90/391],                 Loss: 0.03759, Train_Acc:98.90%
Epoch [204/300], Step [100/391],                 Loss: 0.03755, Train_Acc:98.91%
Epoch [204/300], Step [110/391],                 Loss: 0.03732, Train_Acc:98.91%
Epoch [204/300], Step [120/391],                 Loss: 0.03727, Train_Acc:98.91%
Epoch [204/300], Step [130/391],                 Loss: 0.03781, Train_Acc:98.86%
Epoch [204/300], Step [140/391],                 Loss: 0.03872, Train_Acc:98.84%
Epoch [204/300], Step [150/391],                 Loss: 0.03851, Train_Acc:98.86%
Epoch [204/300], Step [160/391],                 Loss: 0.03899, Train_Acc:98.84%
Epoch [204/300], Step [170/391],                 Loss: 0.03851, Train_Acc:98.86%
Epoch [204/300], Step [180/391],                 Loss: 0.03828, Train_Acc:98.86%
Epoch [204/300], Step [190/391],                 Loss: 0.03787, Train_Acc:98.87%
Epoch [204/300], Step [200/391],                 Loss: 0.03790, Train_Acc:98.88%
Epoch [204/300], Step [210/391],                 Loss: 0.03763, Train_Acc:98.88%
Epoch [204/300], Step [220/391],                 Loss: 0.03783, Train_Acc:98.87%
Epoch [204/300], Step [230/391],                 Loss: 0.03824, Train_Acc:98.87%
Epoch [204/300], Step [240/391],                 Loss: 0.03849, Train_Acc:98.84%
Epoch [204/300], Step [250/391],                 Loss: 0.03861, Train_Acc:98.83%
Epoch [204/300], Step [260/391],                 Loss: 0.03859, Train_Acc:98.84%
Epoch [204/300], Step [270/391],                 Loss: 0.03908, Train_Acc:98.83%
Epoch [204/300], Step [280/391],                 Loss: 0.03931, Train_Acc:98.82%
Epoch [204/300], Step [290/391],                 Loss: 0.03946, Train_Acc:98.81%
Epoch [204/300], Step [300/391],                 Loss: 0.03920, Train_Acc:98.82%
Epoch [204/300], Step [310/391],                 Loss: 0.03953, Train_Acc:98.81%
Epoch [204/300], Step [320/391],                 Loss: 0.04051, Train_Acc:98.78%
Epoch [204/300], Step [330/391],                 Loss: 0.04043, Train_Acc:98.78%
Epoch [204/300], Step [340/391],                 Loss: 0.04015, Train_Acc:98.80%
Epoch [204/300], Step [350/391],                 Loss: 0.04006, Train_Acc:98.79%
Epoch [204/300], Step [360/391],                 Loss: 0.04026, Train_Acc:98.78%
Epoch [204/300], Step [370/391],                 Loss: 0.04045, Train_Acc:98.76%
Epoch [204/300], Step [380/391],                 Loss: 0.04067, Train_Acc:98.75%
Epoch [204/300], Step [390/391],                 Loss: 0.04090, Train_Acc:98.74%
Accuary on test images:88.58%
Epoch [205/300], Step [10/391],                 Loss: 0.04100, Train_Acc:98.67%
Epoch [205/300], Step [20/391],                 Loss: 0.03738, Train_Acc:98.75%
Epoch [205/300], Step [30/391],                 Loss: 0.03531, Train_Acc:98.96%
Epoch [205/300], Step [40/391],                 Loss: 0.03765, Train_Acc:98.93%
Epoch [205/300], Step [50/391],                 Loss: 0.04028, Train_Acc:98.84%
Epoch [205/300], Step [60/391],                 Loss: 0.04165, Train_Acc:98.80%
Epoch [205/300], Step [70/391],                 Loss: 0.04301, Train_Acc:98.76%
Epoch [205/300], Step [80/391],                 Loss: 0.04469, Train_Acc:98.66%
Epoch [205/300], Step [90/391],                 Loss: 0.04504, Train_Acc:98.66%
Epoch [205/300], Step [100/391],                 Loss: 0.04531, Train_Acc:98.66%
Epoch [205/300], Step [110/391],                 Loss: 0.04528, Train_Acc:98.66%
Epoch [205/300], Step [120/391],                 Loss: 0.04423, Train_Acc:98.69%
Epoch [205/300], Step [130/391],                 Loss: 0.04359, Train_Acc:98.71%
Epoch [205/300], Step [140/391],                 Loss: 0.04349, Train_Acc:98.73%
Epoch [205/300], Step [150/391],                 Loss: 0.04326, Train_Acc:98.73%
Epoch [205/300], Step [160/391],                 Loss: 0.04264, Train_Acc:98.75%
Epoch [205/300], Step [170/391],                 Loss: 0.04295, Train_Acc:98.75%
Epoch [205/300], Step [180/391],                 Loss: 0.04295, Train_Acc:98.74%
Epoch [205/300], Step [190/391],                 Loss: 0.04360, Train_Acc:98.70%
Epoch [205/300], Step [200/391],                 Loss: 0.04402, Train_Acc:98.69%
Epoch [205/300], Step [210/391],                 Loss: 0.04398, Train_Acc:98.69%
Epoch [205/300], Step [220/391],                 Loss: 0.04371, Train_Acc:98.71%
Epoch [205/300], Step [230/391],                 Loss: 0.04364, Train_Acc:98.69%
Epoch [205/300], Step [240/391],                 Loss: 0.04342, Train_Acc:98.70%
Epoch [205/300], Step [250/391],                 Loss: 0.04286, Train_Acc:98.73%
Epoch [205/300], Step [260/391],                 Loss: 0.04324, Train_Acc:98.71%
Epoch [205/300], Step [270/391],                 Loss: 0.04341, Train_Acc:98.71%
Epoch [205/300], Step [280/391],                 Loss: 0.04300, Train_Acc:98.73%
Epoch [205/300], Step [290/391],                 Loss: 0.04292, Train_Acc:98.73%
Epoch [205/300], Step [300/391],                 Loss: 0.04326, Train_Acc:98.71%
Epoch [205/300], Step [310/391],                 Loss: 0.04294, Train_Acc:98.72%
Epoch [205/300], Step [320/391],                 Loss: 0.04309, Train_Acc:98.73%
Epoch [205/300], Step [330/391],                 Loss: 0.04304, Train_Acc:98.73%
Epoch [205/300], Step [340/391],                 Loss: 0.04320, Train_Acc:98.72%
Epoch [205/300], Step [350/391],                 Loss: 0.04322, Train_Acc:98.72%
Epoch [205/300], Step [360/391],                 Loss: 0.04363, Train_Acc:98.69%
Epoch [205/300], Step [370/391],                 Loss: 0.04377, Train_Acc:98.68%
Epoch [205/300], Step [380/391],                 Loss: 0.04358, Train_Acc:98.68%
Epoch [205/300], Step [390/391],                 Loss: 0.04361, Train_Acc:98.68%
Accuary on test images:88.04%
Epoch [206/300], Step [10/391],                 Loss: 0.05630, Train_Acc:97.97%
Epoch [206/300], Step [20/391],                 Loss: 0.04796, Train_Acc:98.44%
Epoch [206/300], Step [30/391],                 Loss: 0.04405, Train_Acc:98.57%
Epoch [206/300], Step [40/391],                 Loss: 0.04516, Train_Acc:98.42%
Epoch [206/300], Step [50/391],                 Loss: 0.04431, Train_Acc:98.50%
Epoch [206/300], Step [60/391],                 Loss: 0.04378, Train_Acc:98.53%
Epoch [206/300], Step [70/391],                 Loss: 0.04205, Train_Acc:98.63%
Epoch [206/300], Step [80/391],                 Loss: 0.04202, Train_Acc:98.65%
Epoch [206/300], Step [90/391],                 Loss: 0.04096, Train_Acc:98.68%
Epoch [206/300], Step [100/391],                 Loss: 0.04001, Train_Acc:98.73%
Epoch [206/300], Step [110/391],                 Loss: 0.03978, Train_Acc:98.76%
Epoch [206/300], Step [120/391],                 Loss: 0.03999, Train_Acc:98.76%
Epoch [206/300], Step [130/391],                 Loss: 0.04020, Train_Acc:98.76%
Epoch [206/300], Step [140/391],                 Loss: 0.03975, Train_Acc:98.76%
Epoch [206/300], Step [150/391],                 Loss: 0.03992, Train_Acc:98.77%
Epoch [206/300], Step [160/391],                 Loss: 0.03959, Train_Acc:98.79%
Epoch [206/300], Step [170/391],                 Loss: 0.03930, Train_Acc:98.79%
Epoch [206/300], Step [180/391],                 Loss: 0.03892, Train_Acc:98.80%
Epoch [206/300], Step [190/391],                 Loss: 0.03877, Train_Acc:98.80%
Epoch [206/300], Step [200/391],                 Loss: 0.03852, Train_Acc:98.83%
Epoch [206/300], Step [210/391],                 Loss: 0.03833, Train_Acc:98.83%
Epoch [206/300], Step [220/391],                 Loss: 0.03821, Train_Acc:98.82%
Epoch [206/300], Step [230/391],                 Loss: 0.03818, Train_Acc:98.82%
Epoch [206/300], Step [240/391],                 Loss: 0.03782, Train_Acc:98.84%
Epoch [206/300], Step [250/391],                 Loss: 0.03744, Train_Acc:98.86%
Epoch [206/300], Step [260/391],                 Loss: 0.03758, Train_Acc:98.86%
Epoch [206/300], Step [270/391],                 Loss: 0.03748, Train_Acc:98.86%
Epoch [206/300], Step [280/391],                 Loss: 0.03736, Train_Acc:98.86%
Epoch [206/300], Step [290/391],                 Loss: 0.03746, Train_Acc:98.87%
Epoch [206/300], Step [300/391],                 Loss: 0.03755, Train_Acc:98.88%
Epoch [206/300], Step [310/391],                 Loss: 0.03757, Train_Acc:98.87%
Epoch [206/300], Step [320/391],                 Loss: 0.03747, Train_Acc:98.87%
Epoch [206/300], Step [330/391],                 Loss: 0.03724, Train_Acc:98.89%
Epoch [206/300], Step [340/391],                 Loss: 0.03702, Train_Acc:98.90%
Epoch [206/300], Step [350/391],                 Loss: 0.03680, Train_Acc:98.91%
Epoch [206/300], Step [360/391],                 Loss: 0.03661, Train_Acc:98.91%
Epoch [206/300], Step [370/391],                 Loss: 0.03628, Train_Acc:98.92%
Epoch [206/300], Step [380/391],                 Loss: 0.03617, Train_Acc:98.93%
Epoch [206/300], Step [390/391],                 Loss: 0.03584, Train_Acc:98.94%
Accuary on test images:88.96%
Epoch [207/300], Step [10/391],                 Loss: 0.02263, Train_Acc:99.30%
Epoch [207/300], Step [20/391],                 Loss: 0.02522, Train_Acc:99.22%
Epoch [207/300], Step [30/391],                 Loss: 0.02836, Train_Acc:99.06%
Epoch [207/300], Step [40/391],                 Loss: 0.02791, Train_Acc:99.06%
Epoch [207/300], Step [50/391],                 Loss: 0.03157, Train_Acc:98.97%
Epoch [207/300], Step [60/391],                 Loss: 0.03101, Train_Acc:99.01%
Epoch [207/300], Step [70/391],                 Loss: 0.03047, Train_Acc:99.04%
Epoch [207/300], Step [80/391],                 Loss: 0.03069, Train_Acc:99.06%
Epoch [207/300], Step [90/391],                 Loss: 0.03003, Train_Acc:99.08%
Epoch [207/300], Step [100/391],                 Loss: 0.03023, Train_Acc:99.09%
Epoch [207/300], Step [110/391],                 Loss: 0.03083, Train_Acc:99.07%
Epoch [207/300], Step [120/391],                 Loss: 0.03250, Train_Acc:99.02%
Epoch [207/300], Step [130/391],                 Loss: 0.03403, Train_Acc:98.98%
Epoch [207/300], Step [140/391],                 Loss: 0.03444, Train_Acc:98.97%
Epoch [207/300], Step [150/391],                 Loss: 0.03537, Train_Acc:98.92%
Epoch [207/300], Step [160/391],                 Loss: 0.03570, Train_Acc:98.91%
Epoch [207/300], Step [170/391],                 Loss: 0.03595, Train_Acc:98.91%
Epoch [207/300], Step [180/391],                 Loss: 0.03659, Train_Acc:98.88%
Epoch [207/300], Step [190/391],                 Loss: 0.03694, Train_Acc:98.88%
Epoch [207/300], Step [200/391],                 Loss: 0.03699, Train_Acc:98.88%
Epoch [207/300], Step [210/391],                 Loss: 0.03718, Train_Acc:98.88%
Epoch [207/300], Step [220/391],                 Loss: 0.03774, Train_Acc:98.87%
Epoch [207/300], Step [230/391],                 Loss: 0.03789, Train_Acc:98.86%
Epoch [207/300], Step [240/391],                 Loss: 0.03798, Train_Acc:98.86%
Epoch [207/300], Step [250/391],                 Loss: 0.03866, Train_Acc:98.83%
Epoch [207/300], Step [260/391],                 Loss: 0.03899, Train_Acc:98.83%
Epoch [207/300], Step [270/391],                 Loss: 0.03909, Train_Acc:98.82%
Epoch [207/300], Step [280/391],                 Loss: 0.03900, Train_Acc:98.82%
Epoch [207/300], Step [290/391],                 Loss: 0.03907, Train_Acc:98.82%
Epoch [207/300], Step [300/391],                 Loss: 0.03923, Train_Acc:98.81%
Epoch [207/300], Step [310/391],                 Loss: 0.03914, Train_Acc:98.82%
Epoch [207/300], Step [320/391],                 Loss: 0.03889, Train_Acc:98.84%
Epoch [207/300], Step [330/391],                 Loss: 0.03887, Train_Acc:98.84%
Epoch [207/300], Step [340/391],                 Loss: 0.03871, Train_Acc:98.84%
Epoch [207/300], Step [350/391],                 Loss: 0.03847, Train_Acc:98.85%
Epoch [207/300], Step [360/391],                 Loss: 0.03831, Train_Acc:98.85%
Epoch [207/300], Step [370/391],                 Loss: 0.03807, Train_Acc:98.86%
Epoch [207/300], Step [380/391],                 Loss: 0.03756, Train_Acc:98.88%
Epoch [207/300], Step [390/391],                 Loss: 0.03742, Train_Acc:98.88%
Accuary on test images:89.26%
Epoch [208/300], Step [10/391],                 Loss: 0.03280, Train_Acc:99.14%
Epoch [208/300], Step [20/391],                 Loss: 0.03129, Train_Acc:99.22%
Epoch [208/300], Step [30/391],                 Loss: 0.03322, Train_Acc:99.11%
Epoch [208/300], Step [40/391],                 Loss: 0.03498, Train_Acc:99.02%
Epoch [208/300], Step [50/391],                 Loss: 0.03308, Train_Acc:99.08%
Epoch [208/300], Step [60/391],                 Loss: 0.03302, Train_Acc:99.10%
Epoch [208/300], Step [70/391],                 Loss: 0.03389, Train_Acc:99.03%
Epoch [208/300], Step [80/391],                 Loss: 0.03289, Train_Acc:99.08%
Epoch [208/300], Step [90/391],                 Loss: 0.03360, Train_Acc:99.05%
Epoch [208/300], Step [100/391],                 Loss: 0.03422, Train_Acc:99.04%
Epoch [208/300], Step [110/391],                 Loss: 0.03464, Train_Acc:99.03%
Epoch [208/300], Step [120/391],                 Loss: 0.03431, Train_Acc:99.05%
Epoch [208/300], Step [130/391],                 Loss: 0.03411, Train_Acc:99.03%
Epoch [208/300], Step [140/391],                 Loss: 0.03422, Train_Acc:99.02%
Epoch [208/300], Step [150/391],                 Loss: 0.03445, Train_Acc:99.01%
Epoch [208/300], Step [160/391],                 Loss: 0.03462, Train_Acc:98.99%
Epoch [208/300], Step [170/391],                 Loss: 0.03470, Train_Acc:98.99%
Epoch [208/300], Step [180/391],                 Loss: 0.03444, Train_Acc:99.00%
Epoch [208/300], Step [190/391],                 Loss: 0.03422, Train_Acc:99.00%
Epoch [208/300], Step [200/391],                 Loss: 0.03456, Train_Acc:99.00%
Epoch [208/300], Step [210/391],                 Loss: 0.03491, Train_Acc:98.98%
Epoch [208/300], Step [220/391],                 Loss: 0.03465, Train_Acc:98.99%
Epoch [208/300], Step [230/391],                 Loss: 0.03466, Train_Acc:99.01%
Epoch [208/300], Step [240/391],                 Loss: 0.03487, Train_Acc:99.01%
Epoch [208/300], Step [250/391],                 Loss: 0.03497, Train_Acc:99.00%
Epoch [208/300], Step [260/391],                 Loss: 0.03513, Train_Acc:98.99%
Epoch [208/300], Step [270/391],                 Loss: 0.03521, Train_Acc:98.99%
Epoch [208/300], Step [280/391],                 Loss: 0.03541, Train_Acc:98.98%
Epoch [208/300], Step [290/391],                 Loss: 0.03545, Train_Acc:98.98%
Epoch [208/300], Step [300/391],                 Loss: 0.03531, Train_Acc:98.99%
Epoch [208/300], Step [310/391],                 Loss: 0.03518, Train_Acc:98.99%
Epoch [208/300], Step [320/391],                 Loss: 0.03535, Train_Acc:98.98%
Epoch [208/300], Step [330/391],                 Loss: 0.03517, Train_Acc:98.98%
Epoch [208/300], Step [340/391],                 Loss: 0.03517, Train_Acc:98.97%
Epoch [208/300], Step [350/391],                 Loss: 0.03535, Train_Acc:98.97%
Epoch [208/300], Step [360/391],                 Loss: 0.03542, Train_Acc:98.96%
Epoch [208/300], Step [370/391],                 Loss: 0.03553, Train_Acc:98.96%
Epoch [208/300], Step [380/391],                 Loss: 0.03557, Train_Acc:98.95%
Epoch [208/300], Step [390/391],                 Loss: 0.03567, Train_Acc:98.95%
Accuary on test images:88.28%
Epoch [209/300], Step [10/391],                 Loss: 0.04260, Train_Acc:98.91%
Epoch [209/300], Step [20/391],                 Loss: 0.04291, Train_Acc:98.83%
Epoch [209/300], Step [30/391],                 Loss: 0.04343, Train_Acc:98.78%
Epoch [209/300], Step [40/391],                 Loss: 0.04022, Train_Acc:98.91%
Epoch [209/300], Step [50/391],                 Loss: 0.03857, Train_Acc:98.95%
Epoch [209/300], Step [60/391],                 Loss: 0.03746, Train_Acc:98.96%
Epoch [209/300], Step [70/391],                 Loss: 0.03751, Train_Acc:98.94%
Epoch [209/300], Step [80/391],                 Loss: 0.03756, Train_Acc:98.94%
Epoch [209/300], Step [90/391],                 Loss: 0.03699, Train_Acc:98.96%
Epoch [209/300], Step [100/391],                 Loss: 0.03660, Train_Acc:98.98%
Epoch [209/300], Step [110/391],                 Loss: 0.03751, Train_Acc:98.91%
Epoch [209/300], Step [120/391],                 Loss: 0.03819, Train_Acc:98.91%
Epoch [209/300], Step [130/391],                 Loss: 0.03876, Train_Acc:98.89%
Epoch [209/300], Step [140/391],                 Loss: 0.03806, Train_Acc:98.92%
Epoch [209/300], Step [150/391],                 Loss: 0.03848, Train_Acc:98.88%
Epoch [209/300], Step [160/391],                 Loss: 0.03857, Train_Acc:98.87%
Epoch [209/300], Step [170/391],                 Loss: 0.03873, Train_Acc:98.87%
Epoch [209/300], Step [180/391],                 Loss: 0.03897, Train_Acc:98.85%
Epoch [209/300], Step [190/391],                 Loss: 0.03881, Train_Acc:98.85%
Epoch [209/300], Step [200/391],                 Loss: 0.03869, Train_Acc:98.86%
Epoch [209/300], Step [210/391],                 Loss: 0.03883, Train_Acc:98.85%
Epoch [209/300], Step [220/391],                 Loss: 0.03887, Train_Acc:98.84%
Epoch [209/300], Step [230/391],                 Loss: 0.03866, Train_Acc:98.84%
Epoch [209/300], Step [240/391],                 Loss: 0.03891, Train_Acc:98.83%
Epoch [209/300], Step [250/391],                 Loss: 0.03929, Train_Acc:98.82%
Epoch [209/300], Step [260/391],                 Loss: 0.03939, Train_Acc:98.81%
Epoch [209/300], Step [270/391],                 Loss: 0.03942, Train_Acc:98.81%
Epoch [209/300], Step [280/391],                 Loss: 0.03973, Train_Acc:98.80%
Epoch [209/300], Step [290/391],                 Loss: 0.04017, Train_Acc:98.77%
Epoch [209/300], Step [300/391],                 Loss: 0.04020, Train_Acc:98.77%
Epoch [209/300], Step [310/391],                 Loss: 0.04025, Train_Acc:98.78%
Epoch [209/300], Step [320/391],                 Loss: 0.04035, Train_Acc:98.77%
Epoch [209/300], Step [330/391],                 Loss: 0.04020, Train_Acc:98.77%
Epoch [209/300], Step [340/391],                 Loss: 0.04018, Train_Acc:98.78%
Epoch [209/300], Step [350/391],                 Loss: 0.03996, Train_Acc:98.79%
Epoch [209/300], Step [360/391],                 Loss: 0.04021, Train_Acc:98.78%
Epoch [209/300], Step [370/391],                 Loss: 0.04033, Train_Acc:98.78%
Epoch [209/300], Step [380/391],                 Loss: 0.04026, Train_Acc:98.77%
Epoch [209/300], Step [390/391],                 Loss: 0.04029, Train_Acc:98.77%
Accuary on test images:87.22%
Epoch [210/300], Step [10/391],                 Loss: 0.04833, Train_Acc:98.12%
Epoch [210/300], Step [20/391],                 Loss: 0.05210, Train_Acc:98.12%
Epoch [210/300], Step [30/391],                 Loss: 0.05005, Train_Acc:98.31%
Epoch [210/300], Step [40/391],                 Loss: 0.05015, Train_Acc:98.36%
Epoch [210/300], Step [50/391],                 Loss: 0.04697, Train_Acc:98.53%
Epoch [210/300], Step [60/391],                 Loss: 0.04602, Train_Acc:98.58%
Epoch [210/300], Step [70/391],                 Loss: 0.04750, Train_Acc:98.54%
Epoch [210/300], Step [80/391],                 Loss: 0.04679, Train_Acc:98.56%
Epoch [210/300], Step [90/391],                 Loss: 0.04735, Train_Acc:98.56%
Epoch [210/300], Step [100/391],                 Loss: 0.04653, Train_Acc:98.60%
Epoch [210/300], Step [110/391],                 Loss: 0.04708, Train_Acc:98.59%
Epoch [210/300], Step [120/391],                 Loss: 0.04721, Train_Acc:98.57%
Epoch [210/300], Step [130/391],                 Loss: 0.04730, Train_Acc:98.54%
Epoch [210/300], Step [140/391],                 Loss: 0.04710, Train_Acc:98.53%
Epoch [210/300], Step [150/391],                 Loss: 0.04696, Train_Acc:98.55%
Epoch [210/300], Step [160/391],                 Loss: 0.04626, Train_Acc:98.57%
Epoch [210/300], Step [170/391],                 Loss: 0.04607, Train_Acc:98.57%
Epoch [210/300], Step [180/391],                 Loss: 0.04573, Train_Acc:98.59%
Epoch [210/300], Step [190/391],                 Loss: 0.04531, Train_Acc:98.61%
Epoch [210/300], Step [200/391],                 Loss: 0.04554, Train_Acc:98.59%
Epoch [210/300], Step [210/391],                 Loss: 0.04547, Train_Acc:98.60%
Epoch [210/300], Step [220/391],                 Loss: 0.04517, Train_Acc:98.63%
Epoch [210/300], Step [230/391],                 Loss: 0.04561, Train_Acc:98.60%
Epoch [210/300], Step [240/391],                 Loss: 0.04563, Train_Acc:98.63%
Epoch [210/300], Step [250/391],                 Loss: 0.04594, Train_Acc:98.62%
Epoch [210/300], Step [260/391],                 Loss: 0.04620, Train_Acc:98.61%
Epoch [210/300], Step [270/391],                 Loss: 0.04590, Train_Acc:98.63%
Epoch [210/300], Step [280/391],                 Loss: 0.04570, Train_Acc:98.64%
Epoch [210/300], Step [290/391],                 Loss: 0.04560, Train_Acc:98.64%
Epoch [210/300], Step [300/391],                 Loss: 0.04529, Train_Acc:98.65%
Epoch [210/300], Step [310/391],                 Loss: 0.04489, Train_Acc:98.66%
Epoch [210/300], Step [320/391],                 Loss: 0.04445, Train_Acc:98.68%
Epoch [210/300], Step [330/391],                 Loss: 0.04417, Train_Acc:98.68%
Epoch [210/300], Step [340/391],                 Loss: 0.04434, Train_Acc:98.67%
Epoch [210/300], Step [350/391],                 Loss: 0.04401, Train_Acc:98.68%
Epoch [210/300], Step [360/391],                 Loss: 0.04392, Train_Acc:98.68%
Epoch [210/300], Step [370/391],                 Loss: 0.04372, Train_Acc:98.69%
Epoch [210/300], Step [380/391],                 Loss: 0.04348, Train_Acc:98.70%
Epoch [210/300], Step [390/391],                 Loss: 0.04331, Train_Acc:98.71%
Accuary on test images:88.56%
Epoch [211/300], Step [10/391],                 Loss: 0.04547, Train_Acc:98.59%
Epoch [211/300], Step [20/391],                 Loss: 0.03898, Train_Acc:98.71%
Epoch [211/300], Step [30/391],                 Loss: 0.03539, Train_Acc:98.85%
Epoch [211/300], Step [40/391],                 Loss: 0.03374, Train_Acc:98.95%
Epoch [211/300], Step [50/391],                 Loss: 0.03265, Train_Acc:98.97%
Epoch [211/300], Step [60/391],                 Loss: 0.03317, Train_Acc:98.93%
Epoch [211/300], Step [70/391],                 Loss: 0.03320, Train_Acc:98.94%
Epoch [211/300], Step [80/391],                 Loss: 0.03288, Train_Acc:99.00%
Epoch [211/300], Step [90/391],                 Loss: 0.03264, Train_Acc:99.00%
Epoch [211/300], Step [100/391],                 Loss: 0.03289, Train_Acc:99.00%
Epoch [211/300], Step [110/391],                 Loss: 0.03277, Train_Acc:99.03%
Epoch [211/300], Step [120/391],                 Loss: 0.03233, Train_Acc:99.05%
Epoch [211/300], Step [130/391],                 Loss: 0.03387, Train_Acc:98.98%
Epoch [211/300], Step [140/391],                 Loss: 0.03378, Train_Acc:98.98%
Epoch [211/300], Step [150/391],                 Loss: 0.03349, Train_Acc:99.00%
Epoch [211/300], Step [160/391],                 Loss: 0.03351, Train_Acc:99.03%
Epoch [211/300], Step [170/391],                 Loss: 0.03329, Train_Acc:99.03%
Epoch [211/300], Step [180/391],                 Loss: 0.03361, Train_Acc:99.02%
Epoch [211/300], Step [190/391],                 Loss: 0.03350, Train_Acc:99.02%
Epoch [211/300], Step [200/391],                 Loss: 0.03299, Train_Acc:99.04%
Epoch [211/300], Step [210/391],                 Loss: 0.03291, Train_Acc:99.06%
Epoch [211/300], Step [220/391],                 Loss: 0.03310, Train_Acc:99.06%
Epoch [211/300], Step [230/391],                 Loss: 0.03286, Train_Acc:99.08%
Epoch [211/300], Step [240/391],                 Loss: 0.03293, Train_Acc:99.07%
Epoch [211/300], Step [250/391],                 Loss: 0.03268, Train_Acc:99.07%
Epoch [211/300], Step [260/391],                 Loss: 0.03312, Train_Acc:99.04%
Epoch [211/300], Step [270/391],                 Loss: 0.03322, Train_Acc:99.03%
Epoch [211/300], Step [280/391],                 Loss: 0.03343, Train_Acc:99.02%
Epoch [211/300], Step [290/391],                 Loss: 0.03391, Train_Acc:99.01%
Epoch [211/300], Step [300/391],                 Loss: 0.03394, Train_Acc:99.01%
Epoch [211/300], Step [310/391],                 Loss: 0.03377, Train_Acc:99.02%
Epoch [211/300], Step [320/391],                 Loss: 0.03388, Train_Acc:99.01%
Epoch [211/300], Step [330/391],                 Loss: 0.03409, Train_Acc:99.00%
Epoch [211/300], Step [340/391],                 Loss: 0.03428, Train_Acc:98.98%
Epoch [211/300], Step [350/391],                 Loss: 0.03464, Train_Acc:98.98%
Epoch [211/300], Step [360/391],                 Loss: 0.03472, Train_Acc:98.97%
Epoch [211/300], Step [370/391],                 Loss: 0.03490, Train_Acc:98.95%
Epoch [211/300], Step [380/391],                 Loss: 0.03500, Train_Acc:98.95%
Epoch [211/300], Step [390/391],                 Loss: 0.03499, Train_Acc:98.94%
Accuary on test images:87.56%
Epoch [212/300], Step [10/391],                 Loss: 0.04364, Train_Acc:98.67%
Epoch [212/300], Step [20/391],                 Loss: 0.04146, Train_Acc:98.79%
Epoch [212/300], Step [30/391],                 Loss: 0.03924, Train_Acc:98.85%
Epoch [212/300], Step [40/391],                 Loss: 0.03902, Train_Acc:98.83%
Epoch [212/300], Step [50/391],                 Loss: 0.03798, Train_Acc:98.88%
Epoch [212/300], Step [60/391],                 Loss: 0.03685, Train_Acc:98.92%
Epoch [212/300], Step [70/391],                 Loss: 0.03668, Train_Acc:98.96%
Epoch [212/300], Step [80/391],                 Loss: 0.03611, Train_Acc:98.98%
Epoch [212/300], Step [90/391],                 Loss: 0.03602, Train_Acc:99.00%
Epoch [212/300], Step [100/391],                 Loss: 0.03554, Train_Acc:99.01%
Epoch [212/300], Step [110/391],                 Loss: 0.03474, Train_Acc:99.03%
Epoch [212/300], Step [120/391],                 Loss: 0.03453, Train_Acc:99.04%
Epoch [212/300], Step [130/391],                 Loss: 0.03400, Train_Acc:99.05%
Epoch [212/300], Step [140/391],                 Loss: 0.03361, Train_Acc:99.06%
Epoch [212/300], Step [150/391],                 Loss: 0.03431, Train_Acc:99.03%
Epoch [212/300], Step [160/391],                 Loss: 0.03387, Train_Acc:99.02%
Epoch [212/300], Step [170/391],                 Loss: 0.03400, Train_Acc:99.01%
Epoch [212/300], Step [180/391],                 Loss: 0.03405, Train_Acc:99.02%
Epoch [212/300], Step [190/391],                 Loss: 0.03397, Train_Acc:99.02%
Epoch [212/300], Step [200/391],                 Loss: 0.03454, Train_Acc:99.01%
Epoch [212/300], Step [210/391],                 Loss: 0.03485, Train_Acc:99.00%
Epoch [212/300], Step [220/391],                 Loss: 0.03447, Train_Acc:99.02%
Epoch [212/300], Step [230/391],                 Loss: 0.03420, Train_Acc:99.04%
Epoch [212/300], Step [240/391],                 Loss: 0.03366, Train_Acc:99.06%
Epoch [212/300], Step [250/391],                 Loss: 0.03359, Train_Acc:99.06%
Epoch [212/300], Step [260/391],                 Loss: 0.03384, Train_Acc:99.06%
Epoch [212/300], Step [270/391],                 Loss: 0.03365, Train_Acc:99.07%
Epoch [212/300], Step [280/391],                 Loss: 0.03341, Train_Acc:99.08%
Epoch [212/300], Step [290/391],                 Loss: 0.03319, Train_Acc:99.08%
Epoch [212/300], Step [300/391],                 Loss: 0.03304, Train_Acc:99.08%
Epoch [212/300], Step [310/391],                 Loss: 0.03270, Train_Acc:99.09%
Epoch [212/300], Step [320/391],                 Loss: 0.03243, Train_Acc:99.10%
Epoch [212/300], Step [330/391],                 Loss: 0.03246, Train_Acc:99.11%
Epoch [212/300], Step [340/391],                 Loss: 0.03228, Train_Acc:99.11%
Epoch [212/300], Step [350/391],                 Loss: 0.03208, Train_Acc:99.11%
Epoch [212/300], Step [360/391],                 Loss: 0.03189, Train_Acc:99.11%
Epoch [212/300], Step [370/391],                 Loss: 0.03191, Train_Acc:99.12%
Epoch [212/300], Step [380/391],                 Loss: 0.03206, Train_Acc:99.11%
Epoch [212/300], Step [390/391],                 Loss: 0.03187, Train_Acc:99.12%
Accuary on test images:88.16%
Epoch [213/300], Step [10/391],                 Loss: 0.02888, Train_Acc:99.06%
Epoch [213/300], Step [20/391],                 Loss: 0.03449, Train_Acc:98.67%
Epoch [213/300], Step [30/391],                 Loss: 0.03596, Train_Acc:98.70%
Epoch [213/300], Step [40/391],                 Loss: 0.03962, Train_Acc:98.61%
Epoch [213/300], Step [50/391],                 Loss: 0.03970, Train_Acc:98.58%
Epoch [213/300], Step [60/391],                 Loss: 0.04043, Train_Acc:98.59%
Epoch [213/300], Step [70/391],                 Loss: 0.04078, Train_Acc:98.60%
Epoch [213/300], Step [80/391],                 Loss: 0.04098, Train_Acc:98.63%
Epoch [213/300], Step [90/391],                 Loss: 0.04145, Train_Acc:98.63%
Epoch [213/300], Step [100/391],                 Loss: 0.04070, Train_Acc:98.64%
Epoch [213/300], Step [110/391],                 Loss: 0.04045, Train_Acc:98.69%
Epoch [213/300], Step [120/391],                 Loss: 0.04005, Train_Acc:98.72%
Epoch [213/300], Step [130/391],                 Loss: 0.03908, Train_Acc:98.77%
Epoch [213/300], Step [140/391],                 Loss: 0.03936, Train_Acc:98.74%
Epoch [213/300], Step [150/391],                 Loss: 0.03979, Train_Acc:98.72%
Epoch [213/300], Step [160/391],                 Loss: 0.03975, Train_Acc:98.74%
Epoch [213/300], Step [170/391],                 Loss: 0.04034, Train_Acc:98.73%
Epoch [213/300], Step [180/391],                 Loss: 0.04080, Train_Acc:98.70%
Epoch [213/300], Step [190/391],                 Loss: 0.04115, Train_Acc:98.68%
Epoch [213/300], Step [200/391],                 Loss: 0.04156, Train_Acc:98.67%
Epoch [213/300], Step [210/391],                 Loss: 0.04145, Train_Acc:98.67%
Epoch [213/300], Step [220/391],                 Loss: 0.04160, Train_Acc:98.68%
Epoch [213/300], Step [230/391],                 Loss: 0.04173, Train_Acc:98.67%
Epoch [213/300], Step [240/391],                 Loss: 0.04156, Train_Acc:98.69%
Epoch [213/300], Step [250/391],                 Loss: 0.04152, Train_Acc:98.70%
Epoch [213/300], Step [260/391],                 Loss: 0.04138, Train_Acc:98.70%
Epoch [213/300], Step [270/391],                 Loss: 0.04142, Train_Acc:98.70%
Epoch [213/300], Step [280/391],                 Loss: 0.04107, Train_Acc:98.72%
Epoch [213/300], Step [290/391],                 Loss: 0.04103, Train_Acc:98.73%
Epoch [213/300], Step [300/391],                 Loss: 0.04129, Train_Acc:98.72%
Epoch [213/300], Step [310/391],                 Loss: 0.04119, Train_Acc:98.72%
Epoch [213/300], Step [320/391],                 Loss: 0.04139, Train_Acc:98.72%
Epoch [213/300], Step [330/391],                 Loss: 0.04147, Train_Acc:98.71%
Epoch [213/300], Step [340/391],                 Loss: 0.04124, Train_Acc:98.72%
Epoch [213/300], Step [350/391],                 Loss: 0.04099, Train_Acc:98.73%
Epoch [213/300], Step [360/391],                 Loss: 0.04085, Train_Acc:98.75%
Epoch [213/300], Step [370/391],                 Loss: 0.04089, Train_Acc:98.74%
Epoch [213/300], Step [380/391],                 Loss: 0.04081, Train_Acc:98.74%
Epoch [213/300], Step [390/391],                 Loss: 0.04050, Train_Acc:98.75%
Accuary on test images:86.20%
Epoch [214/300], Step [10/391],                 Loss: 0.04386, Train_Acc:98.44%
Epoch [214/300], Step [20/391],                 Loss: 0.04455, Train_Acc:98.59%
Epoch [214/300], Step [30/391],                 Loss: 0.04338, Train_Acc:98.75%
Epoch [214/300], Step [40/391],                 Loss: 0.04285, Train_Acc:98.83%
Epoch [214/300], Step [50/391],                 Loss: 0.04126, Train_Acc:98.88%
Epoch [214/300], Step [60/391],                 Loss: 0.04165, Train_Acc:98.84%
Epoch [214/300], Step [70/391],                 Loss: 0.04171, Train_Acc:98.82%
Epoch [214/300], Step [80/391],                 Loss: 0.04475, Train_Acc:98.74%
Epoch [214/300], Step [90/391],                 Loss: 0.04360, Train_Acc:98.78%
Epoch [214/300], Step [100/391],                 Loss: 0.04227, Train_Acc:98.82%
Epoch [214/300], Step [110/391],                 Loss: 0.04170, Train_Acc:98.83%
Epoch [214/300], Step [120/391],                 Loss: 0.04130, Train_Acc:98.85%
Epoch [214/300], Step [130/391],                 Loss: 0.04200, Train_Acc:98.82%
Epoch [214/300], Step [140/391],                 Loss: 0.04312, Train_Acc:98.78%
Epoch [214/300], Step [150/391],                 Loss: 0.04278, Train_Acc:98.81%
Epoch [214/300], Step [160/391],                 Loss: 0.04239, Train_Acc:98.81%
Epoch [214/300], Step [170/391],                 Loss: 0.04229, Train_Acc:98.81%
Epoch [214/300], Step [180/391],                 Loss: 0.04260, Train_Acc:98.82%
Epoch [214/300], Step [190/391],                 Loss: 0.04219, Train_Acc:98.82%
Epoch [214/300], Step [200/391],                 Loss: 0.04192, Train_Acc:98.84%
Epoch [214/300], Step [210/391],                 Loss: 0.04230, Train_Acc:98.82%
Epoch [214/300], Step [220/391],                 Loss: 0.04213, Train_Acc:98.82%
Epoch [214/300], Step [230/391],                 Loss: 0.04165, Train_Acc:98.83%
Epoch [214/300], Step [240/391],                 Loss: 0.04143, Train_Acc:98.84%
Epoch [214/300], Step [250/391],                 Loss: 0.04147, Train_Acc:98.84%
Epoch [214/300], Step [260/391],                 Loss: 0.04164, Train_Acc:98.84%
Epoch [214/300], Step [270/391],                 Loss: 0.04189, Train_Acc:98.85%
Epoch [214/300], Step [280/391],                 Loss: 0.04161, Train_Acc:98.85%
Epoch [214/300], Step [290/391],                 Loss: 0.04118, Train_Acc:98.87%
Epoch [214/300], Step [300/391],                 Loss: 0.04099, Train_Acc:98.88%
Epoch [214/300], Step [310/391],                 Loss: 0.04079, Train_Acc:98.88%
Epoch [214/300], Step [320/391],                 Loss: 0.04055, Train_Acc:98.89%
Epoch [214/300], Step [330/391],                 Loss: 0.04032, Train_Acc:98.90%
Epoch [214/300], Step [340/391],                 Loss: 0.04010, Train_Acc:98.91%
Epoch [214/300], Step [350/391],                 Loss: 0.03953, Train_Acc:98.93%
Epoch [214/300], Step [360/391],                 Loss: 0.03951, Train_Acc:98.92%
Epoch [214/300], Step [370/391],                 Loss: 0.03947, Train_Acc:98.92%
Epoch [214/300], Step [380/391],                 Loss: 0.03930, Train_Acc:98.92%
Epoch [214/300], Step [390/391],                 Loss: 0.03959, Train_Acc:98.91%
Accuary on test images:87.64%
Epoch [215/300], Step [10/391],                 Loss: 0.03813, Train_Acc:98.67%
Epoch [215/300], Step [20/391],                 Loss: 0.03463, Train_Acc:98.91%
Epoch [215/300], Step [30/391],                 Loss: 0.03322, Train_Acc:98.93%
Epoch [215/300], Step [40/391],                 Loss: 0.03177, Train_Acc:98.98%
Epoch [215/300], Step [50/391],                 Loss: 0.03560, Train_Acc:98.81%
Epoch [215/300], Step [60/391],                 Loss: 0.03648, Train_Acc:98.78%
Epoch [215/300], Step [70/391],                 Loss: 0.03701, Train_Acc:98.79%
Epoch [215/300], Step [80/391],                 Loss: 0.03829, Train_Acc:98.73%
Epoch [215/300], Step [90/391],                 Loss: 0.03915, Train_Acc:98.71%
Epoch [215/300], Step [100/391],                 Loss: 0.03899, Train_Acc:98.73%
Epoch [215/300], Step [110/391],                 Loss: 0.03904, Train_Acc:98.76%
Epoch [215/300], Step [120/391],                 Loss: 0.03924, Train_Acc:98.74%
Epoch [215/300], Step [130/391],                 Loss: 0.03875, Train_Acc:98.77%
Epoch [215/300], Step [140/391],                 Loss: 0.03843, Train_Acc:98.76%
Epoch [215/300], Step [150/391],                 Loss: 0.03813, Train_Acc:98.78%
Epoch [215/300], Step [160/391],                 Loss: 0.03827, Train_Acc:98.76%
Epoch [215/300], Step [170/391],                 Loss: 0.03805, Train_Acc:98.77%
Epoch [215/300], Step [180/391],                 Loss: 0.03864, Train_Acc:98.75%
Epoch [215/300], Step [190/391],                 Loss: 0.03892, Train_Acc:98.74%
Epoch [215/300], Step [200/391],                 Loss: 0.03920, Train_Acc:98.73%
Epoch [215/300], Step [210/391],                 Loss: 0.03896, Train_Acc:98.74%
Epoch [215/300], Step [220/391],                 Loss: 0.03851, Train_Acc:98.76%
Epoch [215/300], Step [230/391],                 Loss: 0.03849, Train_Acc:98.77%
Epoch [215/300], Step [240/391],                 Loss: 0.03832, Train_Acc:98.79%
Epoch [215/300], Step [250/391],                 Loss: 0.03828, Train_Acc:98.80%
Epoch [215/300], Step [260/391],                 Loss: 0.03844, Train_Acc:98.79%
Epoch [215/300], Step [270/391],                 Loss: 0.03878, Train_Acc:98.78%
Epoch [215/300], Step [280/391],                 Loss: 0.03863, Train_Acc:98.78%
Epoch [215/300], Step [290/391],                 Loss: 0.03872, Train_Acc:98.78%
Epoch [215/300], Step [300/391],                 Loss: 0.03883, Train_Acc:98.78%
Epoch [215/300], Step [310/391],                 Loss: 0.03912, Train_Acc:98.77%
Epoch [215/300], Step [320/391],                 Loss: 0.03912, Train_Acc:98.77%
Epoch [215/300], Step [330/391],                 Loss: 0.03907, Train_Acc:98.77%
Epoch [215/300], Step [340/391],                 Loss: 0.03900, Train_Acc:98.77%
Epoch [215/300], Step [350/391],                 Loss: 0.03911, Train_Acc:98.77%
Epoch [215/300], Step [360/391],                 Loss: 0.03890, Train_Acc:98.78%
Epoch [215/300], Step [370/391],                 Loss: 0.03932, Train_Acc:98.77%
Epoch [215/300], Step [380/391],                 Loss: 0.03892, Train_Acc:98.78%
Epoch [215/300], Step [390/391],                 Loss: 0.03904, Train_Acc:98.79%
Accuary on test images:87.12%
Epoch [216/300], Step [10/391],                 Loss: 0.03411, Train_Acc:99.14%
Epoch [216/300], Step [20/391],                 Loss: 0.03112, Train_Acc:99.30%
Epoch [216/300], Step [30/391],                 Loss: 0.03170, Train_Acc:99.19%
Epoch [216/300], Step [40/391],                 Loss: 0.03469, Train_Acc:99.10%
Epoch [216/300], Step [50/391],                 Loss: 0.03166, Train_Acc:99.22%
Epoch [216/300], Step [60/391],                 Loss: 0.03157, Train_Acc:99.22%
Epoch [216/300], Step [70/391],                 Loss: 0.03073, Train_Acc:99.21%
Epoch [216/300], Step [80/391],                 Loss: 0.02979, Train_Acc:99.26%
Epoch [216/300], Step [90/391],                 Loss: 0.02967, Train_Acc:99.25%
Epoch [216/300], Step [100/391],                 Loss: 0.02887, Train_Acc:99.27%
Epoch [216/300], Step [110/391],                 Loss: 0.02871, Train_Acc:99.28%
Epoch [216/300], Step [120/391],                 Loss: 0.02834, Train_Acc:99.28%
Epoch [216/300], Step [130/391],                 Loss: 0.02829, Train_Acc:99.28%
Epoch [216/300], Step [140/391],                 Loss: 0.02825, Train_Acc:99.28%
Epoch [216/300], Step [150/391],                 Loss: 0.02840, Train_Acc:99.28%
Epoch [216/300], Step [160/391],                 Loss: 0.02840, Train_Acc:99.27%
Epoch [216/300], Step [170/391],                 Loss: 0.02881, Train_Acc:99.26%
Epoch [216/300], Step [180/391],                 Loss: 0.02961, Train_Acc:99.21%
Epoch [216/300], Step [190/391],                 Loss: 0.02991, Train_Acc:99.20%
Epoch [216/300], Step [200/391],                 Loss: 0.03063, Train_Acc:99.18%
Epoch [216/300], Step [210/391],                 Loss: 0.03123, Train_Acc:99.17%
Epoch [216/300], Step [220/391],                 Loss: 0.03195, Train_Acc:99.14%
Epoch [216/300], Step [230/391],                 Loss: 0.03205, Train_Acc:99.14%
Epoch [216/300], Step [240/391],                 Loss: 0.03246, Train_Acc:99.14%
Epoch [216/300], Step [250/391],                 Loss: 0.03230, Train_Acc:99.14%
Epoch [216/300], Step [260/391],                 Loss: 0.03228, Train_Acc:99.14%
Epoch [216/300], Step [270/391],                 Loss: 0.03278, Train_Acc:99.12%
Epoch [216/300], Step [280/391],                 Loss: 0.03290, Train_Acc:99.12%
Epoch [216/300], Step [290/391],                 Loss: 0.03330, Train_Acc:99.11%
Epoch [216/300], Step [300/391],                 Loss: 0.03300, Train_Acc:99.10%
Epoch [216/300], Step [310/391],                 Loss: 0.03297, Train_Acc:99.11%
Epoch [216/300], Step [320/391],                 Loss: 0.03269, Train_Acc:99.12%
Epoch [216/300], Step [330/391],                 Loss: 0.03267, Train_Acc:99.12%
Epoch [216/300], Step [340/391],                 Loss: 0.03246, Train_Acc:99.13%
Epoch [216/300], Step [350/391],                 Loss: 0.03259, Train_Acc:99.12%
Epoch [216/300], Step [360/391],                 Loss: 0.03270, Train_Acc:99.12%
Epoch [216/300], Step [370/391],                 Loss: 0.03325, Train_Acc:99.10%
Epoch [216/300], Step [380/391],                 Loss: 0.03360, Train_Acc:99.09%
Epoch [216/300], Step [390/391],                 Loss: 0.03389, Train_Acc:99.07%
Accuary on test images:88.10%
Epoch [217/300], Step [10/391],                 Loss: 0.04156, Train_Acc:98.75%
Epoch [217/300], Step [20/391],                 Loss: 0.03906, Train_Acc:98.95%
Epoch [217/300], Step [30/391],                 Loss: 0.03743, Train_Acc:99.01%
Epoch [217/300], Step [40/391],                 Loss: 0.03664, Train_Acc:98.98%
Epoch [217/300], Step [50/391],                 Loss: 0.03831, Train_Acc:98.88%
Epoch [217/300], Step [60/391],                 Loss: 0.03945, Train_Acc:98.87%
Epoch [217/300], Step [70/391],                 Loss: 0.04027, Train_Acc:98.82%
Epoch [217/300], Step [80/391],                 Loss: 0.04172, Train_Acc:98.73%
Epoch [217/300], Step [90/391],                 Loss: 0.04202, Train_Acc:98.72%
Epoch [217/300], Step [100/391],                 Loss: 0.04196, Train_Acc:98.72%
Epoch [217/300], Step [110/391],                 Loss: 0.04133, Train_Acc:98.77%
Epoch [217/300], Step [120/391],                 Loss: 0.04134, Train_Acc:98.75%
Epoch [217/300], Step [130/391],                 Loss: 0.04058, Train_Acc:98.78%
Epoch [217/300], Step [140/391],                 Loss: 0.03981, Train_Acc:98.81%
Epoch [217/300], Step [150/391],                 Loss: 0.03951, Train_Acc:98.81%
Epoch [217/300], Step [160/391],                 Loss: 0.03917, Train_Acc:98.81%
Epoch [217/300], Step [170/391],                 Loss: 0.03874, Train_Acc:98.83%
Epoch [217/300], Step [180/391],                 Loss: 0.03837, Train_Acc:98.82%
Epoch [217/300], Step [190/391],                 Loss: 0.03871, Train_Acc:98.81%
Epoch [217/300], Step [200/391],                 Loss: 0.03896, Train_Acc:98.78%
Epoch [217/300], Step [210/391],                 Loss: 0.03928, Train_Acc:98.76%
Epoch [217/300], Step [220/391],                 Loss: 0.03910, Train_Acc:98.77%
Epoch [217/300], Step [230/391],                 Loss: 0.03909, Train_Acc:98.77%
Epoch [217/300], Step [240/391],                 Loss: 0.03897, Train_Acc:98.77%
Epoch [217/300], Step [250/391],                 Loss: 0.03892, Train_Acc:98.78%
Epoch [217/300], Step [260/391],                 Loss: 0.03891, Train_Acc:98.79%
Epoch [217/300], Step [270/391],                 Loss: 0.03947, Train_Acc:98.78%
Epoch [217/300], Step [280/391],                 Loss: 0.03902, Train_Acc:98.79%
Epoch [217/300], Step [290/391],                 Loss: 0.03902, Train_Acc:98.79%
Epoch [217/300], Step [300/391],                 Loss: 0.03887, Train_Acc:98.80%
Epoch [217/300], Step [310/391],                 Loss: 0.03873, Train_Acc:98.81%
Epoch [217/300], Step [320/391],                 Loss: 0.03863, Train_Acc:98.82%
Epoch [217/300], Step [330/391],                 Loss: 0.03848, Train_Acc:98.82%
Epoch [217/300], Step [340/391],                 Loss: 0.03834, Train_Acc:98.83%
Epoch [217/300], Step [350/391],                 Loss: 0.03802, Train_Acc:98.84%
Epoch [217/300], Step [360/391],                 Loss: 0.03766, Train_Acc:98.85%
Epoch [217/300], Step [370/391],                 Loss: 0.03760, Train_Acc:98.85%
Epoch [217/300], Step [380/391],                 Loss: 0.03727, Train_Acc:98.86%
Epoch [217/300], Step [390/391],                 Loss: 0.03712, Train_Acc:98.86%
Accuary on test images:88.78%
Epoch [218/300], Step [10/391],                 Loss: 0.02789, Train_Acc:99.45%
Epoch [218/300], Step [20/391],                 Loss: 0.03250, Train_Acc:99.26%
Epoch [218/300], Step [30/391],                 Loss: 0.03459, Train_Acc:99.06%
Epoch [218/300], Step [40/391],                 Loss: 0.03260, Train_Acc:99.12%
Epoch [218/300], Step [50/391],                 Loss: 0.03188, Train_Acc:99.16%
Epoch [218/300], Step [60/391],                 Loss: 0.03356, Train_Acc:99.06%
Epoch [218/300], Step [70/391],                 Loss: 0.03420, Train_Acc:99.03%
Epoch [218/300], Step [80/391],                 Loss: 0.03537, Train_Acc:99.03%
Epoch [218/300], Step [90/391],                 Loss: 0.03570, Train_Acc:99.03%
Epoch [218/300], Step [100/391],                 Loss: 0.03607, Train_Acc:98.98%
Epoch [218/300], Step [110/391],                 Loss: 0.03655, Train_Acc:98.95%
Epoch [218/300], Step [120/391],                 Loss: 0.03786, Train_Acc:98.89%
Epoch [218/300], Step [130/391],                 Loss: 0.03879, Train_Acc:98.87%
Epoch [218/300], Step [140/391],                 Loss: 0.04011, Train_Acc:98.83%
Epoch [218/300], Step [150/391],                 Loss: 0.04034, Train_Acc:98.83%
Epoch [218/300], Step [160/391],                 Loss: 0.04080, Train_Acc:98.80%
Epoch [218/300], Step [170/391],                 Loss: 0.04157, Train_Acc:98.75%
Epoch [218/300], Step [180/391],                 Loss: 0.04178, Train_Acc:98.75%
Epoch [218/300], Step [190/391],                 Loss: 0.04157, Train_Acc:98.75%
Epoch [218/300], Step [200/391],                 Loss: 0.04186, Train_Acc:98.75%
Epoch [218/300], Step [210/391],                 Loss: 0.04215, Train_Acc:98.75%
Epoch [218/300], Step [220/391],                 Loss: 0.04269, Train_Acc:98.72%
Epoch [218/300], Step [230/391],                 Loss: 0.04295, Train_Acc:98.72%
Epoch [218/300], Step [240/391],                 Loss: 0.04317, Train_Acc:98.71%
Epoch [218/300], Step [250/391],                 Loss: 0.04361, Train_Acc:98.69%
Epoch [218/300], Step [260/391],                 Loss: 0.04369, Train_Acc:98.69%
Epoch [218/300], Step [270/391],                 Loss: 0.04350, Train_Acc:98.70%
Epoch [218/300], Step [280/391],                 Loss: 0.04325, Train_Acc:98.71%
Epoch [218/300], Step [290/391],                 Loss: 0.04293, Train_Acc:98.73%
Epoch [218/300], Step [300/391],                 Loss: 0.04306, Train_Acc:98.72%
Epoch [218/300], Step [310/391],                 Loss: 0.04307, Train_Acc:98.72%
Epoch [218/300], Step [320/391],                 Loss: 0.04296, Train_Acc:98.72%
Epoch [218/300], Step [330/391],                 Loss: 0.04313, Train_Acc:98.71%
Epoch [218/300], Step [340/391],                 Loss: 0.04335, Train_Acc:98.69%
Epoch [218/300], Step [350/391],                 Loss: 0.04352, Train_Acc:98.69%
Epoch [218/300], Step [360/391],                 Loss: 0.04339, Train_Acc:98.69%
Epoch [218/300], Step [370/391],                 Loss: 0.04332, Train_Acc:98.69%
Epoch [218/300], Step [380/391],                 Loss: 0.04361, Train_Acc:98.68%
Epoch [218/300], Step [390/391],                 Loss: 0.04357, Train_Acc:98.68%
Accuary on test images:87.24%
Epoch [219/300], Step [10/391],                 Loss: 0.04848, Train_Acc:98.44%
Epoch [219/300], Step [20/391],                 Loss: 0.04324, Train_Acc:98.52%
Epoch [219/300], Step [30/391],                 Loss: 0.04379, Train_Acc:98.67%
Epoch [219/300], Step [40/391],                 Loss: 0.04597, Train_Acc:98.55%
Epoch [219/300], Step [50/391],                 Loss: 0.04679, Train_Acc:98.56%
Epoch [219/300], Step [60/391],                 Loss: 0.04857, Train_Acc:98.45%
Epoch [219/300], Step [70/391],                 Loss: 0.04852, Train_Acc:98.43%
Epoch [219/300], Step [80/391],                 Loss: 0.04880, Train_Acc:98.44%
Epoch [219/300], Step [90/391],                 Loss: 0.04834, Train_Acc:98.45%
Epoch [219/300], Step [100/391],                 Loss: 0.04718, Train_Acc:98.46%
Epoch [219/300], Step [110/391],                 Loss: 0.04789, Train_Acc:98.47%
Epoch [219/300], Step [120/391],                 Loss: 0.04758, Train_Acc:98.51%
Epoch [219/300], Step [130/391],                 Loss: 0.04800, Train_Acc:98.51%
Epoch [219/300], Step [140/391],                 Loss: 0.04753, Train_Acc:98.55%
Epoch [219/300], Step [150/391],                 Loss: 0.04662, Train_Acc:98.58%
Epoch [219/300], Step [160/391],                 Loss: 0.04614, Train_Acc:98.60%
Epoch [219/300], Step [170/391],                 Loss: 0.04506, Train_Acc:98.63%
Epoch [219/300], Step [180/391],                 Loss: 0.04469, Train_Acc:98.65%
Epoch [219/300], Step [190/391],                 Loss: 0.04399, Train_Acc:98.68%
Epoch [219/300], Step [200/391],                 Loss: 0.04403, Train_Acc:98.69%
Epoch [219/300], Step [210/391],                 Loss: 0.04405, Train_Acc:98.69%
Epoch [219/300], Step [220/391],                 Loss: 0.04327, Train_Acc:98.72%
Epoch [219/300], Step [230/391],                 Loss: 0.04250, Train_Acc:98.75%
Epoch [219/300], Step [240/391],                 Loss: 0.04246, Train_Acc:98.75%
Epoch [219/300], Step [250/391],                 Loss: 0.04183, Train_Acc:98.77%
Epoch [219/300], Step [260/391],                 Loss: 0.04109, Train_Acc:98.80%
Epoch [219/300], Step [270/391],                 Loss: 0.04084, Train_Acc:98.80%
Epoch [219/300], Step [280/391],                 Loss: 0.04064, Train_Acc:98.80%
Epoch [219/300], Step [290/391],                 Loss: 0.04030, Train_Acc:98.81%
Epoch [219/300], Step [300/391],                 Loss: 0.04026, Train_Acc:98.81%
Epoch [219/300], Step [310/391],                 Loss: 0.04017, Train_Acc:98.81%
Epoch [219/300], Step [320/391],                 Loss: 0.03999, Train_Acc:98.83%
Epoch [219/300], Step [330/391],                 Loss: 0.04032, Train_Acc:98.82%
Epoch [219/300], Step [340/391],                 Loss: 0.04071, Train_Acc:98.81%
Epoch [219/300], Step [350/391],                 Loss: 0.04102, Train_Acc:98.79%
Epoch [219/300], Step [360/391],                 Loss: 0.04095, Train_Acc:98.79%
Epoch [219/300], Step [370/391],                 Loss: 0.04120, Train_Acc:98.79%
Epoch [219/300], Step [380/391],                 Loss: 0.04096, Train_Acc:98.81%
Epoch [219/300], Step [390/391],                 Loss: 0.04089, Train_Acc:98.81%
Accuary on test images:87.94%
Epoch [220/300], Step [10/391],                 Loss: 0.04277, Train_Acc:99.06%
Epoch [220/300], Step [20/391],                 Loss: 0.04286, Train_Acc:98.83%
Epoch [220/300], Step [30/391],                 Loss: 0.03860, Train_Acc:99.04%
Epoch [220/300], Step [40/391],                 Loss: 0.03694, Train_Acc:99.12%
Epoch [220/300], Step [50/391],                 Loss: 0.03628, Train_Acc:99.12%
Epoch [220/300], Step [60/391],                 Loss: 0.03634, Train_Acc:99.08%
Epoch [220/300], Step [70/391],                 Loss: 0.03559, Train_Acc:99.07%
Epoch [220/300], Step [80/391],                 Loss: 0.03484, Train_Acc:99.08%
Epoch [220/300], Step [90/391],                 Loss: 0.03603, Train_Acc:99.03%
Epoch [220/300], Step [100/391],                 Loss: 0.03520, Train_Acc:99.05%
Epoch [220/300], Step [110/391],                 Loss: 0.03446, Train_Acc:99.08%
Epoch [220/300], Step [120/391],                 Loss: 0.03422, Train_Acc:99.08%
Epoch [220/300], Step [130/391],                 Loss: 0.03422, Train_Acc:99.07%
Epoch [220/300], Step [140/391],                 Loss: 0.03463, Train_Acc:99.03%
Epoch [220/300], Step [150/391],                 Loss: 0.03481, Train_Acc:99.03%
Epoch [220/300], Step [160/391],                 Loss: 0.03424, Train_Acc:99.04%
Epoch [220/300], Step [170/391],                 Loss: 0.03406, Train_Acc:99.05%
Epoch [220/300], Step [180/391],                 Loss: 0.03354, Train_Acc:99.06%
Epoch [220/300], Step [190/391],                 Loss: 0.03347, Train_Acc:99.06%
Epoch [220/300], Step [200/391],                 Loss: 0.03371, Train_Acc:99.04%
Epoch [220/300], Step [210/391],                 Loss: 0.03407, Train_Acc:99.02%
Epoch [220/300], Step [220/391],                 Loss: 0.03391, Train_Acc:99.03%
Epoch [220/300], Step [230/391],                 Loss: 0.03399, Train_Acc:99.04%
Epoch [220/300], Step [240/391],                 Loss: 0.03390, Train_Acc:99.03%
Epoch [220/300], Step [250/391],                 Loss: 0.03451, Train_Acc:99.02%
Epoch [220/300], Step [260/391],                 Loss: 0.03471, Train_Acc:99.01%
Epoch [220/300], Step [270/391],                 Loss: 0.03465, Train_Acc:99.00%
Epoch [220/300], Step [280/391],                 Loss: 0.03466, Train_Acc:99.01%
Epoch [220/300], Step [290/391],                 Loss: 0.03455, Train_Acc:99.01%
Epoch [220/300], Step [300/391],                 Loss: 0.03448, Train_Acc:99.00%
Epoch [220/300], Step [310/391],                 Loss: 0.03438, Train_Acc:99.00%
Epoch [220/300], Step [320/391],                 Loss: 0.03423, Train_Acc:99.01%
Epoch [220/300], Step [330/391],                 Loss: 0.03412, Train_Acc:99.02%
Epoch [220/300], Step [340/391],                 Loss: 0.03392, Train_Acc:99.01%
Epoch [220/300], Step [350/391],                 Loss: 0.03377, Train_Acc:99.02%
Epoch [220/300], Step [360/391],                 Loss: 0.03407, Train_Acc:99.01%
Epoch [220/300], Step [370/391],                 Loss: 0.03385, Train_Acc:99.02%
Epoch [220/300], Step [380/391],                 Loss: 0.03376, Train_Acc:99.02%
Epoch [220/300], Step [390/391],                 Loss: 0.03416, Train_Acc:99.00%
Accuary on test images:88.56%
Epoch [221/300], Step [10/391],                 Loss: 0.03808, Train_Acc:98.83%
Epoch [221/300], Step [20/391],                 Loss: 0.03687, Train_Acc:98.75%
Epoch [221/300], Step [30/391],                 Loss: 0.03453, Train_Acc:98.93%
Epoch [221/300], Step [40/391],                 Loss: 0.03630, Train_Acc:98.89%
Epoch [221/300], Step [50/391],                 Loss: 0.03587, Train_Acc:98.91%
Epoch [221/300], Step [60/391],                 Loss: 0.03409, Train_Acc:98.98%
Epoch [221/300], Step [70/391],                 Loss: 0.03397, Train_Acc:99.01%
Epoch [221/300], Step [80/391],                 Loss: 0.03462, Train_Acc:99.01%
Epoch [221/300], Step [90/391],                 Loss: 0.03482, Train_Acc:99.03%
Epoch [221/300], Step [100/391],                 Loss: 0.03572, Train_Acc:98.98%
Epoch [221/300], Step [110/391],                 Loss: 0.03703, Train_Acc:98.93%
Epoch [221/300], Step [120/391],                 Loss: 0.03720, Train_Acc:98.95%
Epoch [221/300], Step [130/391],                 Loss: 0.03754, Train_Acc:98.95%
Epoch [221/300], Step [140/391],                 Loss: 0.03718, Train_Acc:98.96%
Epoch [221/300], Step [150/391],                 Loss: 0.03731, Train_Acc:98.95%
Epoch [221/300], Step [160/391],                 Loss: 0.03694, Train_Acc:98.95%
Epoch [221/300], Step [170/391],                 Loss: 0.03695, Train_Acc:98.96%
Epoch [221/300], Step [180/391],                 Loss: 0.03670, Train_Acc:98.95%
Epoch [221/300], Step [190/391],                 Loss: 0.03632, Train_Acc:98.97%
Epoch [221/300], Step [200/391],                 Loss: 0.03602, Train_Acc:98.98%
Epoch [221/300], Step [210/391],                 Loss: 0.03600, Train_Acc:98.98%
Epoch [221/300], Step [220/391],                 Loss: 0.03551, Train_Acc:99.00%
Epoch [221/300], Step [230/391],                 Loss: 0.03568, Train_Acc:98.99%
Epoch [221/300], Step [240/391],                 Loss: 0.03541, Train_Acc:99.00%
Epoch [221/300], Step [250/391],                 Loss: 0.03531, Train_Acc:99.00%
Epoch [221/300], Step [260/391],                 Loss: 0.03489, Train_Acc:99.02%
Epoch [221/300], Step [270/391],                 Loss: 0.03500, Train_Acc:99.01%
Epoch [221/300], Step [280/391],                 Loss: 0.03506, Train_Acc:99.00%
Epoch [221/300], Step [290/391],                 Loss: 0.03515, Train_Acc:99.01%
Epoch [221/300], Step [300/391],                 Loss: 0.03493, Train_Acc:99.02%
Epoch [221/300], Step [310/391],                 Loss: 0.03489, Train_Acc:99.02%
Epoch [221/300], Step [320/391],                 Loss: 0.03476, Train_Acc:99.03%
Epoch [221/300], Step [330/391],                 Loss: 0.03482, Train_Acc:99.02%
Epoch [221/300], Step [340/391],                 Loss: 0.03471, Train_Acc:99.02%
Epoch [221/300], Step [350/391],                 Loss: 0.03474, Train_Acc:99.02%
Epoch [221/300], Step [360/391],                 Loss: 0.03493, Train_Acc:99.01%
Epoch [221/300], Step [370/391],                 Loss: 0.03515, Train_Acc:99.00%
Epoch [221/300], Step [380/391],                 Loss: 0.03496, Train_Acc:99.00%
Epoch [221/300], Step [390/391],                 Loss: 0.03498, Train_Acc:98.99%
Accuary on test images:87.70%
Epoch [222/300], Step [10/391],                 Loss: 0.03134, Train_Acc:99.06%
Epoch [222/300], Step [20/391],                 Loss: 0.03194, Train_Acc:99.14%
Epoch [222/300], Step [30/391],                 Loss: 0.03309, Train_Acc:99.06%
Epoch [222/300], Step [40/391],                 Loss: 0.03188, Train_Acc:99.08%
Epoch [222/300], Step [50/391],                 Loss: 0.03022, Train_Acc:99.16%
Epoch [222/300], Step [60/391],                 Loss: 0.02902, Train_Acc:99.22%
Epoch [222/300], Step [70/391],                 Loss: 0.02908, Train_Acc:99.20%
Epoch [222/300], Step [80/391],                 Loss: 0.02871, Train_Acc:99.20%
Epoch [222/300], Step [90/391],                 Loss: 0.02811, Train_Acc:99.21%
Epoch [222/300], Step [100/391],                 Loss: 0.02823, Train_Acc:99.19%
Epoch [222/300], Step [110/391],                 Loss: 0.02828, Train_Acc:99.20%
Epoch [222/300], Step [120/391],                 Loss: 0.02797, Train_Acc:99.21%
Epoch [222/300], Step [130/391],                 Loss: 0.02807, Train_Acc:99.22%
Epoch [222/300], Step [140/391],                 Loss: 0.02741, Train_Acc:99.25%
Epoch [222/300], Step [150/391],                 Loss: 0.02765, Train_Acc:99.24%
Epoch [222/300], Step [160/391],                 Loss: 0.02764, Train_Acc:99.24%
Epoch [222/300], Step [170/391],                 Loss: 0.02777, Train_Acc:99.22%
Epoch [222/300], Step [180/391],                 Loss: 0.02764, Train_Acc:99.23%
Epoch [222/300], Step [190/391],                 Loss: 0.02746, Train_Acc:99.25%
Epoch [222/300], Step [200/391],                 Loss: 0.02726, Train_Acc:99.25%
Epoch [222/300], Step [210/391],                 Loss: 0.02743, Train_Acc:99.23%
Epoch [222/300], Step [220/391],                 Loss: 0.02763, Train_Acc:99.23%
Epoch [222/300], Step [230/391],                 Loss: 0.02798, Train_Acc:99.22%
Epoch [222/300], Step [240/391],                 Loss: 0.02806, Train_Acc:99.21%
Epoch [222/300], Step [250/391],                 Loss: 0.02785, Train_Acc:99.22%
Epoch [222/300], Step [260/391],                 Loss: 0.02863, Train_Acc:99.18%
Epoch [222/300], Step [270/391],                 Loss: 0.02889, Train_Acc:99.17%
Epoch [222/300], Step [280/391],                 Loss: 0.02919, Train_Acc:99.16%
Epoch [222/300], Step [290/391],                 Loss: 0.02941, Train_Acc:99.15%
Epoch [222/300], Step [300/391],                 Loss: 0.02936, Train_Acc:99.16%
Epoch [222/300], Step [310/391],                 Loss: 0.02933, Train_Acc:99.16%
Epoch [222/300], Step [320/391],                 Loss: 0.02930, Train_Acc:99.17%
Epoch [222/300], Step [330/391],                 Loss: 0.02920, Train_Acc:99.17%
Epoch [222/300], Step [340/391],                 Loss: 0.02934, Train_Acc:99.17%
Epoch [222/300], Step [350/391],                 Loss: 0.02962, Train_Acc:99.16%
Epoch [222/300], Step [360/391],                 Loss: 0.02986, Train_Acc:99.16%
Epoch [222/300], Step [370/391],                 Loss: 0.02982, Train_Acc:99.16%
Epoch [222/300], Step [380/391],                 Loss: 0.02962, Train_Acc:99.17%
Epoch [222/300], Step [390/391],                 Loss: 0.02981, Train_Acc:99.17%
Accuary on test images:88.56%
Epoch [223/300], Step [10/391],                 Loss: 0.02933, Train_Acc:99.38%
Epoch [223/300], Step [20/391],                 Loss: 0.02909, Train_Acc:99.30%
Epoch [223/300], Step [30/391],                 Loss: 0.02870, Train_Acc:99.27%
Epoch [223/300], Step [40/391],                 Loss: 0.03054, Train_Acc:99.14%
Epoch [223/300], Step [50/391],                 Loss: 0.03174, Train_Acc:99.16%
Epoch [223/300], Step [60/391],                 Loss: 0.03192, Train_Acc:99.14%
Epoch [223/300], Step [70/391],                 Loss: 0.03192, Train_Acc:99.12%
Epoch [223/300], Step [80/391],                 Loss: 0.03256, Train_Acc:99.08%
Epoch [223/300], Step [90/391],                 Loss: 0.03350, Train_Acc:99.07%
Epoch [223/300], Step [100/391],                 Loss: 0.03263, Train_Acc:99.10%
Epoch [223/300], Step [110/391],                 Loss: 0.03299, Train_Acc:99.09%
Epoch [223/300], Step [120/391],                 Loss: 0.03272, Train_Acc:99.12%
Epoch [223/300], Step [130/391],                 Loss: 0.03285, Train_Acc:99.12%
Epoch [223/300], Step [140/391],                 Loss: 0.03300, Train_Acc:99.11%
Epoch [223/300], Step [150/391],                 Loss: 0.03322, Train_Acc:99.09%
Epoch [223/300], Step [160/391],                 Loss: 0.03356, Train_Acc:99.08%
Epoch [223/300], Step [170/391],                 Loss: 0.03333, Train_Acc:99.07%
Epoch [223/300], Step [180/391],                 Loss: 0.03306, Train_Acc:99.09%
Epoch [223/300], Step [190/391],                 Loss: 0.03330, Train_Acc:99.07%
Epoch [223/300], Step [200/391],                 Loss: 0.03320, Train_Acc:99.09%
Epoch [223/300], Step [210/391],                 Loss: 0.03275, Train_Acc:99.11%
Epoch [223/300], Step [220/391],                 Loss: 0.03239, Train_Acc:99.11%
Epoch [223/300], Step [230/391],                 Loss: 0.03232, Train_Acc:99.11%
Epoch [223/300], Step [240/391],                 Loss: 0.03217, Train_Acc:99.12%
Epoch [223/300], Step [250/391],                 Loss: 0.03216, Train_Acc:99.12%
Epoch [223/300], Step [260/391],                 Loss: 0.03220, Train_Acc:99.11%
Epoch [223/300], Step [270/391],                 Loss: 0.03258, Train_Acc:99.10%
Epoch [223/300], Step [280/391],                 Loss: 0.03235, Train_Acc:99.10%
Epoch [223/300], Step [290/391],                 Loss: 0.03193, Train_Acc:99.11%
Epoch [223/300], Step [300/391],                 Loss: 0.03168, Train_Acc:99.11%
Epoch [223/300], Step [310/391],                 Loss: 0.03168, Train_Acc:99.11%
Epoch [223/300], Step [320/391],                 Loss: 0.03164, Train_Acc:99.11%
Epoch [223/300], Step [330/391],                 Loss: 0.03167, Train_Acc:99.10%
Epoch [223/300], Step [340/391],                 Loss: 0.03166, Train_Acc:99.10%
Epoch [223/300], Step [350/391],                 Loss: 0.03182, Train_Acc:99.09%
Epoch [223/300], Step [360/391],                 Loss: 0.03200, Train_Acc:99.06%
Epoch [223/300], Step [370/391],                 Loss: 0.03216, Train_Acc:99.06%
Epoch [223/300], Step [380/391],                 Loss: 0.03235, Train_Acc:99.05%
Epoch [223/300], Step [390/391],                 Loss: 0.03232, Train_Acc:99.05%
Accuary on test images:87.54%
Epoch [224/300], Step [10/391],                 Loss: 0.02978, Train_Acc:99.06%
Epoch [224/300], Step [20/391],                 Loss: 0.03120, Train_Acc:99.02%
Epoch [224/300], Step [30/391],                 Loss: 0.02827, Train_Acc:99.14%
Epoch [224/300], Step [40/391],                 Loss: 0.02838, Train_Acc:99.22%
Epoch [224/300], Step [50/391],                 Loss: 0.02758, Train_Acc:99.28%
Epoch [224/300], Step [60/391],                 Loss: 0.02904, Train_Acc:99.27%
Epoch [224/300], Step [70/391],                 Loss: 0.02938, Train_Acc:99.24%
Epoch [224/300], Step [80/391],                 Loss: 0.03033, Train_Acc:99.23%
Epoch [224/300], Step [90/391],                 Loss: 0.03085, Train_Acc:99.19%
Epoch [224/300], Step [100/391],                 Loss: 0.03154, Train_Acc:99.14%
Epoch [224/300], Step [110/391],                 Loss: 0.03239, Train_Acc:99.09%
Epoch [224/300], Step [120/391],                 Loss: 0.03215, Train_Acc:99.11%
Epoch [224/300], Step [130/391],                 Loss: 0.03216, Train_Acc:99.10%
Epoch [224/300], Step [140/391],                 Loss: 0.03261, Train_Acc:99.07%
Epoch [224/300], Step [150/391],                 Loss: 0.03303, Train_Acc:99.08%
Epoch [224/300], Step [160/391],                 Loss: 0.03273, Train_Acc:99.08%
Epoch [224/300], Step [170/391],                 Loss: 0.03368, Train_Acc:99.03%
Epoch [224/300], Step [180/391],                 Loss: 0.03460, Train_Acc:99.00%
Epoch [224/300], Step [190/391],                 Loss: 0.03551, Train_Acc:98.96%
Epoch [224/300], Step [200/391],                 Loss: 0.03623, Train_Acc:98.95%
Epoch [224/300], Step [210/391],                 Loss: 0.03662, Train_Acc:98.93%
Epoch [224/300], Step [220/391],                 Loss: 0.03671, Train_Acc:98.93%
Epoch [224/300], Step [230/391],                 Loss: 0.03649, Train_Acc:98.93%
Epoch [224/300], Step [240/391],                 Loss: 0.03674, Train_Acc:98.92%
Epoch [224/300], Step [250/391],                 Loss: 0.03689, Train_Acc:98.91%
Epoch [224/300], Step [260/391],                 Loss: 0.03723, Train_Acc:98.90%
Epoch [224/300], Step [270/391],                 Loss: 0.03767, Train_Acc:98.88%
Epoch [224/300], Step [280/391],                 Loss: 0.03785, Train_Acc:98.88%
Epoch [224/300], Step [290/391],                 Loss: 0.03860, Train_Acc:98.86%
Epoch [224/300], Step [300/391],                 Loss: 0.03876, Train_Acc:98.86%
Epoch [224/300], Step [310/391],                 Loss: 0.03925, Train_Acc:98.85%
Epoch [224/300], Step [320/391],                 Loss: 0.03932, Train_Acc:98.86%
Epoch [224/300], Step [330/391],                 Loss: 0.03911, Train_Acc:98.87%
Epoch [224/300], Step [340/391],                 Loss: 0.03941, Train_Acc:98.85%
Epoch [224/300], Step [350/391],                 Loss: 0.03944, Train_Acc:98.84%
Epoch [224/300], Step [360/391],                 Loss: 0.03971, Train_Acc:98.83%
Epoch [224/300], Step [370/391],                 Loss: 0.04008, Train_Acc:98.82%
Epoch [224/300], Step [380/391],                 Loss: 0.04046, Train_Acc:98.80%
Epoch [224/300], Step [390/391],                 Loss: 0.04045, Train_Acc:98.79%
Accuary on test images:87.92%
Epoch [225/300], Step [10/391],                 Loss: 0.05210, Train_Acc:98.28%
Epoch [225/300], Step [20/391],                 Loss: 0.04947, Train_Acc:98.44%
Epoch [225/300], Step [30/391],                 Loss: 0.04594, Train_Acc:98.59%
Epoch [225/300], Step [40/391],                 Loss: 0.04479, Train_Acc:98.71%
Epoch [225/300], Step [50/391],                 Loss: 0.04484, Train_Acc:98.69%
Epoch [225/300], Step [60/391],                 Loss: 0.04609, Train_Acc:98.67%
Epoch [225/300], Step [70/391],                 Loss: 0.04603, Train_Acc:98.67%
Epoch [225/300], Step [80/391],                 Loss: 0.04414, Train_Acc:98.76%
Epoch [225/300], Step [90/391],                 Loss: 0.04355, Train_Acc:98.78%
Epoch [225/300], Step [100/391],                 Loss: 0.04340, Train_Acc:98.80%
Epoch [225/300], Step [110/391],                 Loss: 0.04273, Train_Acc:98.82%
Epoch [225/300], Step [120/391],                 Loss: 0.04335, Train_Acc:98.80%
Epoch [225/300], Step [130/391],                 Loss: 0.04290, Train_Acc:98.81%
Epoch [225/300], Step [140/391],                 Loss: 0.04230, Train_Acc:98.81%
Epoch [225/300], Step [150/391],                 Loss: 0.04232, Train_Acc:98.79%
Epoch [225/300], Step [160/391],                 Loss: 0.04228, Train_Acc:98.78%
Epoch [225/300], Step [170/391],                 Loss: 0.04199, Train_Acc:98.78%
Epoch [225/300], Step [180/391],                 Loss: 0.04218, Train_Acc:98.76%
Epoch [225/300], Step [190/391],                 Loss: 0.04246, Train_Acc:98.75%
Epoch [225/300], Step [200/391],                 Loss: 0.04239, Train_Acc:98.75%
Epoch [225/300], Step [210/391],                 Loss: 0.04255, Train_Acc:98.76%
Epoch [225/300], Step [220/391],                 Loss: 0.04237, Train_Acc:98.77%
Epoch [225/300], Step [230/391],                 Loss: 0.04196, Train_Acc:98.78%
Epoch [225/300], Step [240/391],                 Loss: 0.04204, Train_Acc:98.78%
Epoch [225/300], Step [250/391],                 Loss: 0.04233, Train_Acc:98.78%
Epoch [225/300], Step [260/391],                 Loss: 0.04250, Train_Acc:98.78%
Epoch [225/300], Step [270/391],                 Loss: 0.04227, Train_Acc:98.79%
Epoch [225/300], Step [280/391],                 Loss: 0.04199, Train_Acc:98.80%
Epoch [225/300], Step [290/391],                 Loss: 0.04240, Train_Acc:98.79%
Epoch [225/300], Step [300/391],                 Loss: 0.04232, Train_Acc:98.80%
Epoch [225/300], Step [310/391],                 Loss: 0.04220, Train_Acc:98.81%
Epoch [225/300], Step [320/391],                 Loss: 0.04217, Train_Acc:98.82%
Epoch [225/300], Step [330/391],                 Loss: 0.04179, Train_Acc:98.83%
Epoch [225/300], Step [340/391],                 Loss: 0.04158, Train_Acc:98.83%
Epoch [225/300], Step [350/391],                 Loss: 0.04127, Train_Acc:98.83%
Epoch [225/300], Step [360/391],                 Loss: 0.04096, Train_Acc:98.84%
Epoch [225/300], Step [370/391],                 Loss: 0.04086, Train_Acc:98.84%
Epoch [225/300], Step [380/391],                 Loss: 0.04051, Train_Acc:98.85%
Epoch [225/300], Step [390/391],                 Loss: 0.04034, Train_Acc:98.86%
Accuary on test images:87.54%
Epoch [226/300], Step [10/391],                 Loss: 0.04301, Train_Acc:98.36%
Epoch [226/300], Step [20/391],                 Loss: 0.04003, Train_Acc:98.55%
Epoch [226/300], Step [30/391],                 Loss: 0.03407, Train_Acc:98.83%
Epoch [226/300], Step [40/391],                 Loss: 0.03642, Train_Acc:98.87%
Epoch [226/300], Step [50/391],                 Loss: 0.03462, Train_Acc:98.92%
Epoch [226/300], Step [60/391],                 Loss: 0.03281, Train_Acc:99.04%
Epoch [226/300], Step [70/391],                 Loss: 0.03191, Train_Acc:99.07%
Epoch [226/300], Step [80/391],                 Loss: 0.03078, Train_Acc:99.12%
Epoch [226/300], Step [90/391],                 Loss: 0.03034, Train_Acc:99.11%
Epoch [226/300], Step [100/391],                 Loss: 0.02930, Train_Acc:99.13%
Epoch [226/300], Step [110/391],                 Loss: 0.02879, Train_Acc:99.14%
Epoch [226/300], Step [120/391],                 Loss: 0.02839, Train_Acc:99.15%
Epoch [226/300], Step [130/391],                 Loss: 0.02801, Train_Acc:99.16%
Epoch [226/300], Step [140/391],                 Loss: 0.02720, Train_Acc:99.20%
Epoch [226/300], Step [150/391],                 Loss: 0.02699, Train_Acc:99.20%
Epoch [226/300], Step [160/391],                 Loss: 0.02656, Train_Acc:99.21%
Epoch [226/300], Step [170/391],                 Loss: 0.02597, Train_Acc:99.24%
Epoch [226/300], Step [180/391],                 Loss: 0.02510, Train_Acc:99.27%
Epoch [226/300], Step [190/391],                 Loss: 0.02464, Train_Acc:99.28%
Epoch [226/300], Step [200/391],                 Loss: 0.02428, Train_Acc:99.30%
Epoch [226/300], Step [210/391],                 Loss: 0.02381, Train_Acc:99.31%
Epoch [226/300], Step [220/391],                 Loss: 0.02356, Train_Acc:99.32%
Epoch [226/300], Step [230/391],                 Loss: 0.02310, Train_Acc:99.34%
Epoch [226/300], Step [240/391],                 Loss: 0.02281, Train_Acc:99.35%
Epoch [226/300], Step [250/391],                 Loss: 0.02251, Train_Acc:99.37%
Epoch [226/300], Step [260/391],                 Loss: 0.02234, Train_Acc:99.38%
Epoch [226/300], Step [270/391],                 Loss: 0.02193, Train_Acc:99.39%
Epoch [226/300], Step [280/391],                 Loss: 0.02142, Train_Acc:99.41%
Epoch [226/300], Step [290/391],                 Loss: 0.02105, Train_Acc:99.43%
Epoch [226/300], Step [300/391],                 Loss: 0.02065, Train_Acc:99.44%
Epoch [226/300], Step [310/391],                 Loss: 0.02034, Train_Acc:99.45%
Epoch [226/300], Step [320/391],                 Loss: 0.02001, Train_Acc:99.46%
Epoch [226/300], Step [330/391],                 Loss: 0.01969, Train_Acc:99.47%
Epoch [226/300], Step [340/391],                 Loss: 0.01927, Train_Acc:99.49%
Epoch [226/300], Step [350/391],                 Loss: 0.01904, Train_Acc:99.50%
Epoch [226/300], Step [360/391],                 Loss: 0.01872, Train_Acc:99.51%
Epoch [226/300], Step [370/391],                 Loss: 0.01841, Train_Acc:99.52%
Epoch [226/300], Step [380/391],                 Loss: 0.01815, Train_Acc:99.52%
Epoch [226/300], Step [390/391],                 Loss: 0.01788, Train_Acc:99.53%
Accuary on test images:90.60%
Epoch [227/300], Step [10/391],                 Loss: 0.01186, Train_Acc:99.77%
Epoch [227/300], Step [20/391],                 Loss: 0.00949, Train_Acc:99.84%
Epoch [227/300], Step [30/391],                 Loss: 0.00853, Train_Acc:99.87%
Epoch [227/300], Step [40/391],                 Loss: 0.00822, Train_Acc:99.90%
Epoch [227/300], Step [50/391],                 Loss: 0.00789, Train_Acc:99.91%
Epoch [227/300], Step [60/391],                 Loss: 0.00802, Train_Acc:99.91%
Epoch [227/300], Step [70/391],                 Loss: 0.00849, Train_Acc:99.88%
Epoch [227/300], Step [80/391],                 Loss: 0.00856, Train_Acc:99.88%
Epoch [227/300], Step [90/391],                 Loss: 0.00859, Train_Acc:99.87%
Epoch [227/300], Step [100/391],                 Loss: 0.00843, Train_Acc:99.88%
Epoch [227/300], Step [110/391],                 Loss: 0.00829, Train_Acc:99.88%
Epoch [227/300], Step [120/391],                 Loss: 0.00837, Train_Acc:99.88%
Epoch [227/300], Step [130/391],                 Loss: 0.00869, Train_Acc:99.85%
Epoch [227/300], Step [140/391],                 Loss: 0.00889, Train_Acc:99.85%
Epoch [227/300], Step [150/391],                 Loss: 0.00886, Train_Acc:99.84%
Epoch [227/300], Step [160/391],                 Loss: 0.00888, Train_Acc:99.84%
Epoch [227/300], Step [170/391],                 Loss: 0.00876, Train_Acc:99.85%
Epoch [227/300], Step [180/391],                 Loss: 0.00855, Train_Acc:99.86%
Epoch [227/300], Step [190/391],                 Loss: 0.00849, Train_Acc:99.86%
Epoch [227/300], Step [200/391],                 Loss: 0.00850, Train_Acc:99.86%
Epoch [227/300], Step [210/391],                 Loss: 0.00840, Train_Acc:99.86%
Epoch [227/300], Step [220/391],                 Loss: 0.00853, Train_Acc:99.85%
Epoch [227/300], Step [230/391],                 Loss: 0.00849, Train_Acc:99.85%
Epoch [227/300], Step [240/391],                 Loss: 0.00838, Train_Acc:99.86%
Epoch [227/300], Step [250/391],                 Loss: 0.00836, Train_Acc:99.86%
Epoch [227/300], Step [260/391],                 Loss: 0.00836, Train_Acc:99.86%
Epoch [227/300], Step [270/391],                 Loss: 0.00838, Train_Acc:99.86%
Epoch [227/300], Step [280/391],                 Loss: 0.00833, Train_Acc:99.86%
Epoch [227/300], Step [290/391],                 Loss: 0.00831, Train_Acc:99.86%
Epoch [227/300], Step [300/391],                 Loss: 0.00826, Train_Acc:99.86%
Epoch [227/300], Step [310/391],                 Loss: 0.00816, Train_Acc:99.86%
Epoch [227/300], Step [320/391],                 Loss: 0.00806, Train_Acc:99.87%
Epoch [227/300], Step [330/391],                 Loss: 0.00795, Train_Acc:99.87%
Epoch [227/300], Step [340/391],                 Loss: 0.00787, Train_Acc:99.88%
Epoch [227/300], Step [350/391],                 Loss: 0.00779, Train_Acc:99.88%
Epoch [227/300], Step [360/391],                 Loss: 0.00769, Train_Acc:99.88%
Epoch [227/300], Step [370/391],                 Loss: 0.00759, Train_Acc:99.89%
Epoch [227/300], Step [380/391],                 Loss: 0.00753, Train_Acc:99.89%
Epoch [227/300], Step [390/391],                 Loss: 0.00745, Train_Acc:99.89%
Accuary on test images:90.78%
Epoch [228/300], Step [10/391],                 Loss: 0.00515, Train_Acc:100.00%
Epoch [228/300], Step [20/391],                 Loss: 0.00502, Train_Acc:100.00%
Epoch [228/300], Step [30/391],                 Loss: 0.00508, Train_Acc:99.97%
Epoch [228/300], Step [40/391],                 Loss: 0.00548, Train_Acc:99.94%
Epoch [228/300], Step [50/391],                 Loss: 0.00540, Train_Acc:99.95%
Epoch [228/300], Step [60/391],                 Loss: 0.00574, Train_Acc:99.95%
Epoch [228/300], Step [70/391],                 Loss: 0.00573, Train_Acc:99.96%
Epoch [228/300], Step [80/391],                 Loss: 0.00559, Train_Acc:99.96%
Epoch [228/300], Step [90/391],                 Loss: 0.00550, Train_Acc:99.97%
Epoch [228/300], Step [100/391],                 Loss: 0.00545, Train_Acc:99.96%
Epoch [228/300], Step [110/391],                 Loss: 0.00539, Train_Acc:99.96%
Epoch [228/300], Step [120/391],                 Loss: 0.00548, Train_Acc:99.96%
Epoch [228/300], Step [130/391],                 Loss: 0.00547, Train_Acc:99.96%
Epoch [228/300], Step [140/391],                 Loss: 0.00562, Train_Acc:99.95%
Epoch [228/300], Step [150/391],                 Loss: 0.00555, Train_Acc:99.95%
Epoch [228/300], Step [160/391],                 Loss: 0.00546, Train_Acc:99.96%
Epoch [228/300], Step [170/391],                 Loss: 0.00546, Train_Acc:99.95%
Epoch [228/300], Step [180/391],                 Loss: 0.00538, Train_Acc:99.96%
Epoch [228/300], Step [190/391],                 Loss: 0.00540, Train_Acc:99.95%
Epoch [228/300], Step [200/391],                 Loss: 0.00544, Train_Acc:99.95%
Epoch [228/300], Step [210/391],                 Loss: 0.00538, Train_Acc:99.96%
Epoch [228/300], Step [220/391],                 Loss: 0.00534, Train_Acc:99.96%
Epoch [228/300], Step [230/391],                 Loss: 0.00525, Train_Acc:99.96%
Epoch [228/300], Step [240/391],                 Loss: 0.00519, Train_Acc:99.96%
Epoch [228/300], Step [250/391],                 Loss: 0.00516, Train_Acc:99.96%
Epoch [228/300], Step [260/391],                 Loss: 0.00513, Train_Acc:99.96%
Epoch [228/300], Step [270/391],                 Loss: 0.00520, Train_Acc:99.96%
Epoch [228/300], Step [280/391],                 Loss: 0.00519, Train_Acc:99.96%
Epoch [228/300], Step [290/391],                 Loss: 0.00514, Train_Acc:99.96%
Epoch [228/300], Step [300/391],                 Loss: 0.00514, Train_Acc:99.96%
Epoch [228/300], Step [310/391],                 Loss: 0.00511, Train_Acc:99.96%
Epoch [228/300], Step [320/391],                 Loss: 0.00505, Train_Acc:99.96%
Epoch [228/300], Step [330/391],                 Loss: 0.00502, Train_Acc:99.96%
Epoch [228/300], Step [340/391],                 Loss: 0.00500, Train_Acc:99.96%
Epoch [228/300], Step [350/391],                 Loss: 0.00495, Train_Acc:99.96%
Epoch [228/300], Step [360/391],                 Loss: 0.00491, Train_Acc:99.97%
Epoch [228/300], Step [370/391],                 Loss: 0.00488, Train_Acc:99.97%
Epoch [228/300], Step [380/391],                 Loss: 0.00482, Train_Acc:99.97%
Epoch [228/300], Step [390/391],                 Loss: 0.00479, Train_Acc:99.97%
Accuary on test images:91.04%
Epoch [229/300], Step [10/391],                 Loss: 0.00469, Train_Acc:100.00%
Epoch [229/300], Step [20/391],                 Loss: 0.00479, Train_Acc:99.96%
Epoch [229/300], Step [30/391],                 Loss: 0.00476, Train_Acc:99.95%
Epoch [229/300], Step [40/391],                 Loss: 0.00458, Train_Acc:99.96%
Epoch [229/300], Step [50/391],                 Loss: 0.00473, Train_Acc:99.97%
Epoch [229/300], Step [60/391],                 Loss: 0.00470, Train_Acc:99.97%
Epoch [229/300], Step [70/391],                 Loss: 0.00466, Train_Acc:99.98%
Epoch [229/300], Step [80/391],                 Loss: 0.00454, Train_Acc:99.98%
Epoch [229/300], Step [90/391],                 Loss: 0.00445, Train_Acc:99.98%
Epoch [229/300], Step [100/391],                 Loss: 0.00445, Train_Acc:99.98%
Epoch [229/300], Step [110/391],                 Loss: 0.00438, Train_Acc:99.98%
Epoch [229/300], Step [120/391],                 Loss: 0.00440, Train_Acc:99.98%
Epoch [229/300], Step [130/391],                 Loss: 0.00439, Train_Acc:99.98%
Epoch [229/300], Step [140/391],                 Loss: 0.00453, Train_Acc:99.98%
Epoch [229/300], Step [150/391],                 Loss: 0.00450, Train_Acc:99.98%
Epoch [229/300], Step [160/391],                 Loss: 0.00453, Train_Acc:99.98%
Epoch [229/300], Step [170/391],                 Loss: 0.00450, Train_Acc:99.98%
Epoch [229/300], Step [180/391],                 Loss: 0.00444, Train_Acc:99.98%
Epoch [229/300], Step [190/391],                 Loss: 0.00440, Train_Acc:99.98%
Epoch [229/300], Step [200/391],                 Loss: 0.00436, Train_Acc:99.98%
Epoch [229/300], Step [210/391],                 Loss: 0.00432, Train_Acc:99.98%
Epoch [229/300], Step [220/391],                 Loss: 0.00428, Train_Acc:99.98%
Epoch [229/300], Step [230/391],                 Loss: 0.00433, Train_Acc:99.98%
Epoch [229/300], Step [240/391],                 Loss: 0.00429, Train_Acc:99.98%
Epoch [229/300], Step [250/391],                 Loss: 0.00426, Train_Acc:99.98%
Epoch [229/300], Step [260/391],                 Loss: 0.00424, Train_Acc:99.98%
Epoch [229/300], Step [270/391],                 Loss: 0.00421, Train_Acc:99.98%
Epoch [229/300], Step [280/391],                 Loss: 0.00423, Train_Acc:99.98%
Epoch [229/300], Step [290/391],                 Loss: 0.00420, Train_Acc:99.98%
Epoch [229/300], Step [300/391],                 Loss: 0.00418, Train_Acc:99.98%
Epoch [229/300], Step [310/391],                 Loss: 0.00424, Train_Acc:99.98%
Epoch [229/300], Step [320/391],                 Loss: 0.00422, Train_Acc:99.98%
Epoch [229/300], Step [330/391],                 Loss: 0.00419, Train_Acc:99.98%
Epoch [229/300], Step [340/391],                 Loss: 0.00419, Train_Acc:99.98%
Epoch [229/300], Step [350/391],                 Loss: 0.00417, Train_Acc:99.98%
Epoch [229/300], Step [360/391],                 Loss: 0.00416, Train_Acc:99.98%
Epoch [229/300], Step [370/391],                 Loss: 0.00413, Train_Acc:99.98%
Epoch [229/300], Step [380/391],                 Loss: 0.00409, Train_Acc:99.98%
Epoch [229/300], Step [390/391],                 Loss: 0.00406, Train_Acc:99.98%
Accuary on test images:91.12%
Epoch [230/300], Step [10/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [230/300], Step [20/391],                 Loss: 0.00423, Train_Acc:100.00%
Epoch [230/300], Step [30/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [230/300], Step [40/391],                 Loss: 0.00388, Train_Acc:100.00%
Epoch [230/300], Step [50/391],                 Loss: 0.00377, Train_Acc:100.00%
Epoch [230/300], Step [60/391],                 Loss: 0.00374, Train_Acc:100.00%
Epoch [230/300], Step [70/391],                 Loss: 0.00372, Train_Acc:100.00%
Epoch [230/300], Step [80/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [230/300], Step [90/391],                 Loss: 0.00368, Train_Acc:100.00%
Epoch [230/300], Step [100/391],                 Loss: 0.00369, Train_Acc:100.00%
Epoch [230/300], Step [110/391],                 Loss: 0.00384, Train_Acc:99.99%
Epoch [230/300], Step [120/391],                 Loss: 0.00384, Train_Acc:99.99%
Epoch [230/300], Step [130/391],                 Loss: 0.00383, Train_Acc:99.99%
Epoch [230/300], Step [140/391],                 Loss: 0.00383, Train_Acc:99.99%
Epoch [230/300], Step [150/391],                 Loss: 0.00381, Train_Acc:99.99%
Epoch [230/300], Step [160/391],                 Loss: 0.00377, Train_Acc:99.99%
Epoch [230/300], Step [170/391],                 Loss: 0.00376, Train_Acc:99.99%
Epoch [230/300], Step [180/391],                 Loss: 0.00373, Train_Acc:99.99%
Epoch [230/300], Step [190/391],                 Loss: 0.00370, Train_Acc:99.99%
Epoch [230/300], Step [200/391],                 Loss: 0.00368, Train_Acc:99.99%
Epoch [230/300], Step [210/391],                 Loss: 0.00365, Train_Acc:99.99%
Epoch [230/300], Step [220/391],                 Loss: 0.00362, Train_Acc:99.99%
Epoch [230/300], Step [230/391],                 Loss: 0.00359, Train_Acc:99.99%
Epoch [230/300], Step [240/391],                 Loss: 0.00356, Train_Acc:99.99%
Epoch [230/300], Step [250/391],                 Loss: 0.00355, Train_Acc:99.99%
Epoch [230/300], Step [260/391],                 Loss: 0.00354, Train_Acc:99.99%
Epoch [230/300], Step [270/391],                 Loss: 0.00353, Train_Acc:99.99%
Epoch [230/300], Step [280/391],                 Loss: 0.00353, Train_Acc:99.99%
Epoch [230/300], Step [290/391],                 Loss: 0.00351, Train_Acc:99.99%
Epoch [230/300], Step [300/391],                 Loss: 0.00350, Train_Acc:99.99%
Epoch [230/300], Step [310/391],                 Loss: 0.00348, Train_Acc:99.99%
Epoch [230/300], Step [320/391],                 Loss: 0.00347, Train_Acc:100.00%
Epoch [230/300], Step [330/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [230/300], Step [340/391],                 Loss: 0.00343, Train_Acc:100.00%
Epoch [230/300], Step [350/391],                 Loss: 0.00342, Train_Acc:100.00%
Epoch [230/300], Step [360/391],                 Loss: 0.00341, Train_Acc:100.00%
Epoch [230/300], Step [370/391],                 Loss: 0.00339, Train_Acc:100.00%
Epoch [230/300], Step [380/391],                 Loss: 0.00337, Train_Acc:100.00%
Epoch [230/300], Step [390/391],                 Loss: 0.00336, Train_Acc:100.00%
Accuary on test images:91.10%
Epoch [231/300], Step [10/391],                 Loss: 0.00384, Train_Acc:100.00%
Epoch [231/300], Step [20/391],                 Loss: 0.00379, Train_Acc:100.00%
Epoch [231/300], Step [30/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [231/300], Step [40/391],                 Loss: 0.00364, Train_Acc:100.00%
Epoch [231/300], Step [50/391],                 Loss: 0.00353, Train_Acc:100.00%
Epoch [231/300], Step [60/391],                 Loss: 0.00352, Train_Acc:100.00%
Epoch [231/300], Step [70/391],                 Loss: 0.00354, Train_Acc:100.00%
Epoch [231/300], Step [80/391],                 Loss: 0.00351, Train_Acc:100.00%
Epoch [231/300], Step [90/391],                 Loss: 0.00345, Train_Acc:100.00%
Epoch [231/300], Step [100/391],                 Loss: 0.00339, Train_Acc:100.00%
Epoch [231/300], Step [110/391],                 Loss: 0.00337, Train_Acc:100.00%
Epoch [231/300], Step [120/391],                 Loss: 0.00338, Train_Acc:100.00%
Epoch [231/300], Step [130/391],                 Loss: 0.00336, Train_Acc:100.00%
Epoch [231/300], Step [140/391],                 Loss: 0.00334, Train_Acc:100.00%
Epoch [231/300], Step [150/391],                 Loss: 0.00333, Train_Acc:100.00%
Epoch [231/300], Step [160/391],                 Loss: 0.00332, Train_Acc:100.00%
Epoch [231/300], Step [170/391],                 Loss: 0.00331, Train_Acc:100.00%
Epoch [231/300], Step [180/391],                 Loss: 0.00328, Train_Acc:100.00%
Epoch [231/300], Step [190/391],                 Loss: 0.00329, Train_Acc:100.00%
Epoch [231/300], Step [200/391],                 Loss: 0.00329, Train_Acc:100.00%
Epoch [231/300], Step [210/391],                 Loss: 0.00327, Train_Acc:100.00%
Epoch [231/300], Step [220/391],                 Loss: 0.00326, Train_Acc:100.00%
Epoch [231/300], Step [230/391],                 Loss: 0.00325, Train_Acc:100.00%
Epoch [231/300], Step [240/391],                 Loss: 0.00323, Train_Acc:100.00%
Epoch [231/300], Step [250/391],                 Loss: 0.00323, Train_Acc:100.00%
Epoch [231/300], Step [260/391],                 Loss: 0.00322, Train_Acc:100.00%
Epoch [231/300], Step [270/391],                 Loss: 0.00321, Train_Acc:100.00%
Epoch [231/300], Step [280/391],                 Loss: 0.00320, Train_Acc:100.00%
Epoch [231/300], Step [290/391],                 Loss: 0.00318, Train_Acc:100.00%
Epoch [231/300], Step [300/391],                 Loss: 0.00317, Train_Acc:100.00%
Epoch [231/300], Step [310/391],                 Loss: 0.00316, Train_Acc:100.00%
Epoch [231/300], Step [320/391],                 Loss: 0.00314, Train_Acc:100.00%
Epoch [231/300], Step [330/391],                 Loss: 0.00313, Train_Acc:100.00%
Epoch [231/300], Step [340/391],                 Loss: 0.00312, Train_Acc:100.00%
Epoch [231/300], Step [350/391],                 Loss: 0.00311, Train_Acc:100.00%
Epoch [231/300], Step [360/391],                 Loss: 0.00309, Train_Acc:100.00%
Epoch [231/300], Step [370/391],                 Loss: 0.00308, Train_Acc:100.00%
Epoch [231/300], Step [380/391],                 Loss: 0.00307, Train_Acc:100.00%
Epoch [231/300], Step [390/391],                 Loss: 0.00305, Train_Acc:100.00%
Accuary on test images:91.06%
Epoch [232/300], Step [10/391],                 Loss: 0.00332, Train_Acc:100.00%
Epoch [232/300], Step [20/391],                 Loss: 0.00316, Train_Acc:100.00%
Epoch [232/300], Step [30/391],                 Loss: 0.00305, Train_Acc:100.00%
Epoch [232/300], Step [40/391],                 Loss: 0.00318, Train_Acc:100.00%
Epoch [232/300], Step [50/391],                 Loss: 0.00313, Train_Acc:100.00%
Epoch [232/300], Step [60/391],                 Loss: 0.00312, Train_Acc:100.00%
Epoch [232/300], Step [70/391],                 Loss: 0.00311, Train_Acc:100.00%
Epoch [232/300], Step [80/391],                 Loss: 0.00311, Train_Acc:100.00%
Epoch [232/300], Step [90/391],                 Loss: 0.00319, Train_Acc:99.99%
Epoch [232/300], Step [100/391],                 Loss: 0.00320, Train_Acc:99.99%
Epoch [232/300], Step [110/391],                 Loss: 0.00319, Train_Acc:99.99%
Epoch [232/300], Step [120/391],                 Loss: 0.00320, Train_Acc:99.99%
Epoch [232/300], Step [130/391],                 Loss: 0.00319, Train_Acc:99.99%
Epoch [232/300], Step [140/391],                 Loss: 0.00321, Train_Acc:99.99%
Epoch [232/300], Step [150/391],                 Loss: 0.00320, Train_Acc:99.99%
Epoch [232/300], Step [160/391],                 Loss: 0.00318, Train_Acc:100.00%
Epoch [232/300], Step [170/391],                 Loss: 0.00317, Train_Acc:100.00%
Epoch [232/300], Step [180/391],                 Loss: 0.00314, Train_Acc:100.00%
Epoch [232/300], Step [190/391],                 Loss: 0.00314, Train_Acc:100.00%
Epoch [232/300], Step [200/391],                 Loss: 0.00313, Train_Acc:100.00%
Epoch [232/300], Step [210/391],                 Loss: 0.00312, Train_Acc:100.00%
Epoch [232/300], Step [220/391],                 Loss: 0.00311, Train_Acc:100.00%
Epoch [232/300], Step [230/391],                 Loss: 0.00309, Train_Acc:100.00%
Epoch [232/300], Step [240/391],                 Loss: 0.00308, Train_Acc:100.00%
Epoch [232/300], Step [250/391],                 Loss: 0.00307, Train_Acc:100.00%
Epoch [232/300], Step [260/391],                 Loss: 0.00307, Train_Acc:100.00%
Epoch [232/300], Step [270/391],                 Loss: 0.00306, Train_Acc:100.00%
Epoch [232/300], Step [280/391],                 Loss: 0.00305, Train_Acc:100.00%
Epoch [232/300], Step [290/391],                 Loss: 0.00304, Train_Acc:100.00%
Epoch [232/300], Step [300/391],                 Loss: 0.00302, Train_Acc:100.00%
Epoch [232/300], Step [310/391],                 Loss: 0.00302, Train_Acc:100.00%
Epoch [232/300], Step [320/391],                 Loss: 0.00301, Train_Acc:100.00%
Epoch [232/300], Step [330/391],                 Loss: 0.00300, Train_Acc:100.00%
Epoch [232/300], Step [340/391],                 Loss: 0.00299, Train_Acc:100.00%
Epoch [232/300], Step [350/391],                 Loss: 0.00298, Train_Acc:100.00%
Epoch [232/300], Step [360/391],                 Loss: 0.00297, Train_Acc:100.00%
Epoch [232/300], Step [370/391],                 Loss: 0.00296, Train_Acc:100.00%
Epoch [232/300], Step [380/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [232/300], Step [390/391],                 Loss: 0.00293, Train_Acc:100.00%
Accuary on test images:90.98%
Epoch [233/300], Step [10/391],                 Loss: 0.00314, Train_Acc:100.00%
Epoch [233/300], Step [20/391],                 Loss: 0.00306, Train_Acc:100.00%
Epoch [233/300], Step [30/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [233/300], Step [40/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [233/300], Step [50/391],                 Loss: 0.00296, Train_Acc:100.00%
Epoch [233/300], Step [60/391],                 Loss: 0.00302, Train_Acc:100.00%
Epoch [233/300], Step [70/391],                 Loss: 0.00302, Train_Acc:100.00%
Epoch [233/300], Step [80/391],                 Loss: 0.00300, Train_Acc:100.00%
Epoch [233/300], Step [90/391],                 Loss: 0.00299, Train_Acc:100.00%
Epoch [233/300], Step [100/391],                 Loss: 0.00300, Train_Acc:100.00%
Epoch [233/300], Step [110/391],                 Loss: 0.00299, Train_Acc:100.00%
Epoch [233/300], Step [120/391],                 Loss: 0.00300, Train_Acc:100.00%
Epoch [233/300], Step [130/391],                 Loss: 0.00301, Train_Acc:100.00%
Epoch [233/300], Step [140/391],                 Loss: 0.00301, Train_Acc:100.00%
Epoch [233/300], Step [150/391],                 Loss: 0.00300, Train_Acc:100.00%
Epoch [233/300], Step [160/391],                 Loss: 0.00299, Train_Acc:100.00%
Epoch [233/300], Step [170/391],                 Loss: 0.00298, Train_Acc:100.00%
Epoch [233/300], Step [180/391],                 Loss: 0.00298, Train_Acc:100.00%
Epoch [233/300], Step [190/391],                 Loss: 0.00299, Train_Acc:100.00%
Epoch [233/300], Step [200/391],                 Loss: 0.00299, Train_Acc:100.00%
Epoch [233/300], Step [210/391],                 Loss: 0.00298, Train_Acc:100.00%
Epoch [233/300], Step [220/391],                 Loss: 0.00297, Train_Acc:100.00%
Epoch [233/300], Step [230/391],                 Loss: 0.00297, Train_Acc:100.00%
Epoch [233/300], Step [240/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [233/300], Step [250/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [233/300], Step [260/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [233/300], Step [270/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [233/300], Step [280/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [233/300], Step [290/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [233/300], Step [300/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [233/300], Step [310/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [233/300], Step [320/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [233/300], Step [330/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [233/300], Step [340/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [233/300], Step [350/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [233/300], Step [360/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [233/300], Step [370/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [233/300], Step [380/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [233/300], Step [390/391],                 Loss: 0.00282, Train_Acc:100.00%
Accuary on test images:91.06%
Epoch [234/300], Step [10/391],                 Loss: 0.00314, Train_Acc:100.00%
Epoch [234/300], Step [20/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [234/300], Step [30/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [234/300], Step [40/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [234/300], Step [50/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [234/300], Step [60/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [234/300], Step [70/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [234/300], Step [80/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [234/300], Step [90/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [234/300], Step [100/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [234/300], Step [110/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [234/300], Step [120/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [234/300], Step [130/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [234/300], Step [140/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [234/300], Step [150/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [234/300], Step [160/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [234/300], Step [170/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [234/300], Step [180/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [234/300], Step [190/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [234/300], Step [200/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [234/300], Step [210/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [234/300], Step [220/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [234/300], Step [230/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [234/300], Step [240/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [234/300], Step [250/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [234/300], Step [260/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [234/300], Step [270/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [234/300], Step [280/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [234/300], Step [290/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [234/300], Step [300/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [234/300], Step [310/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [234/300], Step [320/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [234/300], Step [330/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [234/300], Step [340/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [234/300], Step [350/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [234/300], Step [360/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [234/300], Step [370/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [234/300], Step [380/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [234/300], Step [390/391],                 Loss: 0.00271, Train_Acc:100.00%
Accuary on test images:91.08%
Epoch [235/300], Step [10/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [235/300], Step [20/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [235/300], Step [30/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [235/300], Step [40/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [235/300], Step [50/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [235/300], Step [60/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [235/300], Step [70/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [235/300], Step [80/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [235/300], Step [90/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [235/300], Step [100/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [235/300], Step [110/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [235/300], Step [120/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [235/300], Step [130/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [235/300], Step [140/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [235/300], Step [150/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [235/300], Step [160/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [235/300], Step [170/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [235/300], Step [180/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [235/300], Step [190/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [235/300], Step [200/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [235/300], Step [210/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [235/300], Step [220/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [235/300], Step [230/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [235/300], Step [240/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [235/300], Step [250/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [235/300], Step [260/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [235/300], Step [270/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [235/300], Step [280/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [235/300], Step [290/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [235/300], Step [300/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [235/300], Step [310/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [235/300], Step [320/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [235/300], Step [330/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [235/300], Step [340/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [235/300], Step [350/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [235/300], Step [360/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [235/300], Step [370/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [235/300], Step [380/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [235/300], Step [390/391],                 Loss: 0.00264, Train_Acc:100.00%
Accuary on test images:91.08%
Epoch [236/300], Step [10/391],                 Loss: 0.00304, Train_Acc:100.00%
Epoch [236/300], Step [20/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [236/300], Step [30/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [236/300], Step [40/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [236/300], Step [50/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [236/300], Step [60/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [236/300], Step [70/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [236/300], Step [80/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [236/300], Step [90/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [236/300], Step [100/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [236/300], Step [110/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [236/300], Step [120/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [236/300], Step [130/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [236/300], Step [140/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [236/300], Step [150/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [236/300], Step [160/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [236/300], Step [170/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [236/300], Step [180/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [236/300], Step [190/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [236/300], Step [200/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [236/300], Step [210/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [236/300], Step [220/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [236/300], Step [230/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [236/300], Step [240/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [236/300], Step [250/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [236/300], Step [260/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [236/300], Step [270/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [236/300], Step [280/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [236/300], Step [290/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [236/300], Step [300/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [236/300], Step [310/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [236/300], Step [320/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [236/300], Step [330/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [236/300], Step [340/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [236/300], Step [350/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [236/300], Step [360/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [236/300], Step [370/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [236/300], Step [380/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [236/300], Step [390/391],                 Loss: 0.00264, Train_Acc:100.00%
Accuary on test images:91.10%
Epoch [237/300], Step [10/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [237/300], Step [20/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [237/300], Step [30/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [237/300], Step [40/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [237/300], Step [50/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [237/300], Step [60/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [237/300], Step [70/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [237/300], Step [80/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [237/300], Step [90/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [237/300], Step [100/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [237/300], Step [110/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [237/300], Step [120/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [237/300], Step [130/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [237/300], Step [140/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [237/300], Step [150/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [237/300], Step [160/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [237/300], Step [170/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [237/300], Step [180/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [237/300], Step [190/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [237/300], Step [200/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [237/300], Step [210/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [237/300], Step [220/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [237/300], Step [230/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [237/300], Step [240/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [237/300], Step [250/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [237/300], Step [260/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [237/300], Step [270/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [237/300], Step [280/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [237/300], Step [290/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [237/300], Step [300/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [237/300], Step [310/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [237/300], Step [320/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [237/300], Step [330/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [237/300], Step [340/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [237/300], Step [350/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [237/300], Step [360/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [237/300], Step [370/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [237/300], Step [380/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [237/300], Step [390/391],                 Loss: 0.00260, Train_Acc:100.00%
Accuary on test images:91.04%
Epoch [238/300], Step [10/391],                 Loss: 0.00301, Train_Acc:100.00%
Epoch [238/300], Step [20/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [238/300], Step [30/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [238/300], Step [40/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [238/300], Step [50/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [238/300], Step [60/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [238/300], Step [70/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [238/300], Step [80/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [238/300], Step [90/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [238/300], Step [100/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [238/300], Step [110/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [238/300], Step [120/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [238/300], Step [130/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [238/300], Step [140/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [238/300], Step [150/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [238/300], Step [160/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [238/300], Step [170/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [238/300], Step [180/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [238/300], Step [190/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [238/300], Step [200/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [238/300], Step [210/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [238/300], Step [220/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [238/300], Step [230/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [238/300], Step [240/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [238/300], Step [250/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [238/300], Step [260/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [238/300], Step [270/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [238/300], Step [280/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [238/300], Step [290/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [238/300], Step [300/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [238/300], Step [310/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [238/300], Step [320/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [238/300], Step [330/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [238/300], Step [340/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [238/300], Step [350/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [238/300], Step [360/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [238/300], Step [370/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [238/300], Step [380/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [238/300], Step [390/391],                 Loss: 0.00258, Train_Acc:100.00%
Accuary on test images:91.00%
Epoch [239/300], Step [10/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [239/300], Step [20/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [239/300], Step [30/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [40/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [239/300], Step [50/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [239/300], Step [60/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [239/300], Step [70/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [239/300], Step [80/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [239/300], Step [90/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [239/300], Step [100/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [239/300], Step [110/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [120/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [239/300], Step [130/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [140/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [239/300], Step [150/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [239/300], Step [160/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [170/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [180/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [190/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [239/300], Step [200/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [239/300], Step [210/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [220/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [239/300], Step [230/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [239/300], Step [240/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [239/300], Step [250/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [239/300], Step [260/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [239/300], Step [270/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [239/300], Step [280/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [239/300], Step [290/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [239/300], Step [300/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [239/300], Step [310/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [239/300], Step [320/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [239/300], Step [330/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [239/300], Step [340/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [239/300], Step [350/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [239/300], Step [360/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [239/300], Step [370/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [239/300], Step [380/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [239/300], Step [390/391],                 Loss: 0.00256, Train_Acc:100.00%
Accuary on test images:91.10%
Epoch [240/300], Step [10/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [240/300], Step [20/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [240/300], Step [30/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [240/300], Step [40/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [240/300], Step [50/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [240/300], Step [60/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [240/300], Step [70/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [240/300], Step [80/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [240/300], Step [90/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [240/300], Step [100/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [240/300], Step [110/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [240/300], Step [120/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [240/300], Step [130/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [240/300], Step [140/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [240/300], Step [150/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [240/300], Step [160/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [240/300], Step [170/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [240/300], Step [180/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [240/300], Step [190/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [240/300], Step [200/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [240/300], Step [210/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [240/300], Step [220/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [240/300], Step [230/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [240/300], Step [240/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [240/300], Step [250/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [240/300], Step [260/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [240/300], Step [270/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [240/300], Step [280/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [240/300], Step [290/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [240/300], Step [300/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [240/300], Step [310/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [240/300], Step [320/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [240/300], Step [330/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [240/300], Step [340/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [240/300], Step [350/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [240/300], Step [360/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [240/300], Step [370/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [240/300], Step [380/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [240/300], Step [390/391],                 Loss: 0.00255, Train_Acc:100.00%
Accuary on test images:91.02%
Epoch [241/300], Step [10/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [241/300], Step [20/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [241/300], Step [30/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [241/300], Step [40/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [241/300], Step [50/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [241/300], Step [60/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [241/300], Step [70/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [241/300], Step [80/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [241/300], Step [90/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [241/300], Step [100/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [241/300], Step [110/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [241/300], Step [120/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [241/300], Step [130/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [241/300], Step [140/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [241/300], Step [150/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [241/300], Step [160/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [241/300], Step [170/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [241/300], Step [180/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [241/300], Step [190/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [241/300], Step [200/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [241/300], Step [210/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [241/300], Step [220/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [241/300], Step [230/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [241/300], Step [240/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [250/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [260/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [270/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [280/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [290/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [300/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [241/300], Step [310/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [241/300], Step [320/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [241/300], Step [330/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [241/300], Step [340/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [241/300], Step [350/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [241/300], Step [360/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [241/300], Step [370/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [241/300], Step [380/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [241/300], Step [390/391],                 Loss: 0.00256, Train_Acc:100.00%
Accuary on test images:91.14%
Epoch [242/300], Step [10/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [242/300], Step [20/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [242/300], Step [30/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [242/300], Step [40/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [242/300], Step [50/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [242/300], Step [60/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [242/300], Step [70/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [242/300], Step [80/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [242/300], Step [90/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [242/300], Step [100/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [242/300], Step [110/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [242/300], Step [120/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [242/300], Step [130/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [242/300], Step [140/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [242/300], Step [150/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [242/300], Step [160/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [242/300], Step [170/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [242/300], Step [180/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [242/300], Step [190/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [242/300], Step [200/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [242/300], Step [210/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [242/300], Step [220/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [242/300], Step [230/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [242/300], Step [240/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [242/300], Step [250/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [242/300], Step [260/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [242/300], Step [270/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [242/300], Step [280/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [242/300], Step [290/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [242/300], Step [300/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [242/300], Step [310/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [242/300], Step [320/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [242/300], Step [330/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [242/300], Step [340/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [242/300], Step [350/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [242/300], Step [360/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [242/300], Step [370/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [242/300], Step [380/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [242/300], Step [390/391],                 Loss: 0.00255, Train_Acc:100.00%
Accuary on test images:91.16%
Epoch [243/300], Step [10/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [243/300], Step [20/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [243/300], Step [30/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [243/300], Step [40/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [243/300], Step [50/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [243/300], Step [60/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [243/300], Step [70/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [243/300], Step [80/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [243/300], Step [90/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [243/300], Step [100/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [243/300], Step [110/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [243/300], Step [120/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [243/300], Step [130/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [243/300], Step [140/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [243/300], Step [150/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [243/300], Step [160/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [243/300], Step [170/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [243/300], Step [180/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [243/300], Step [190/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [243/300], Step [200/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [243/300], Step [210/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [243/300], Step [220/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [243/300], Step [230/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [243/300], Step [240/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [243/300], Step [250/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [243/300], Step [260/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [243/300], Step [270/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [243/300], Step [280/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [243/300], Step [290/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [243/300], Step [300/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [243/300], Step [310/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [243/300], Step [320/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [243/300], Step [330/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [243/300], Step [340/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [243/300], Step [350/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [243/300], Step [360/391],                 Loss: 0.00255, Train_Acc:100.00%
Epoch [243/300], Step [370/391],                 Loss: 0.00255, Train_Acc:100.00%
Epoch [243/300], Step [380/391],                 Loss: 0.00254, Train_Acc:100.00%
Epoch [243/300], Step [390/391],                 Loss: 0.00254, Train_Acc:100.00%
Accuary on test images:91.18%
Epoch [244/300], Step [10/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [244/300], Step [20/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [244/300], Step [30/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [244/300], Step [40/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [244/300], Step [50/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [60/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [244/300], Step [70/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [244/300], Step [80/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [244/300], Step [90/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [244/300], Step [100/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [110/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [244/300], Step [120/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [244/300], Step [130/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [244/300], Step [140/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [244/300], Step [150/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [244/300], Step [160/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [244/300], Step [170/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [244/300], Step [180/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [190/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [200/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [210/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [220/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [244/300], Step [230/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [244/300], Step [240/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [244/300], Step [250/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [244/300], Step [260/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [244/300], Step [270/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [244/300], Step [280/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [244/300], Step [290/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [244/300], Step [300/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [244/300], Step [310/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [244/300], Step [320/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [244/300], Step [330/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [244/300], Step [340/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [244/300], Step [350/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [244/300], Step [360/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [244/300], Step [370/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [244/300], Step [380/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [244/300], Step [390/391],                 Loss: 0.00255, Train_Acc:100.00%
Accuary on test images:91.18%
Epoch [245/300], Step [10/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [245/300], Step [20/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [245/300], Step [30/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [245/300], Step [40/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [245/300], Step [50/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [245/300], Step [60/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [245/300], Step [70/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [245/300], Step [80/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [245/300], Step [90/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [245/300], Step [100/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [245/300], Step [110/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [245/300], Step [120/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [245/300], Step [130/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [245/300], Step [140/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [245/300], Step [150/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [245/300], Step [160/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [245/300], Step [170/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [245/300], Step [180/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [245/300], Step [190/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [245/300], Step [200/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [245/300], Step [210/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [245/300], Step [220/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [245/300], Step [230/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [245/300], Step [240/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [245/300], Step [250/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [245/300], Step [260/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [245/300], Step [270/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [245/300], Step [280/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [245/300], Step [290/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [245/300], Step [300/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [245/300], Step [310/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [245/300], Step [320/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [245/300], Step [330/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [245/300], Step [340/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [245/300], Step [350/391],                 Loss: 0.00256, Train_Acc:100.00%
Epoch [245/300], Step [360/391],                 Loss: 0.00255, Train_Acc:100.00%
Epoch [245/300], Step [370/391],                 Loss: 0.00255, Train_Acc:100.00%
Epoch [245/300], Step [380/391],                 Loss: 0.00255, Train_Acc:100.00%
Epoch [245/300], Step [390/391],                 Loss: 0.00254, Train_Acc:100.00%
Accuary on test images:91.20%
Epoch [246/300], Step [10/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [246/300], Step [20/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [246/300], Step [30/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [246/300], Step [40/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [246/300], Step [50/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [246/300], Step [60/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [246/300], Step [70/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [246/300], Step [80/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [246/300], Step [90/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [246/300], Step [100/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [246/300], Step [110/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [246/300], Step [120/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [246/300], Step [130/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [246/300], Step [140/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [246/300], Step [150/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [246/300], Step [160/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [246/300], Step [170/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [246/300], Step [180/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [246/300], Step [190/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [246/300], Step [200/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [246/300], Step [210/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [246/300], Step [220/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [246/300], Step [230/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [246/300], Step [240/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [246/300], Step [250/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [246/300], Step [260/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [246/300], Step [270/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [246/300], Step [280/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [246/300], Step [290/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [246/300], Step [300/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [246/300], Step [310/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [246/300], Step [320/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [246/300], Step [330/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [246/300], Step [340/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [246/300], Step [350/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [246/300], Step [360/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [246/300], Step [370/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [246/300], Step [380/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [246/300], Step [390/391],                 Loss: 0.00258, Train_Acc:100.00%
Accuary on test images:91.26%
Epoch [247/300], Step [10/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [247/300], Step [20/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [247/300], Step [30/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [40/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [247/300], Step [50/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [60/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [247/300], Step [70/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [247/300], Step [80/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [247/300], Step [90/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [247/300], Step [100/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [110/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [120/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [247/300], Step [130/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [247/300], Step [140/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [247/300], Step [150/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [247/300], Step [160/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [247/300], Step [170/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [247/300], Step [180/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [190/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [200/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [210/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [220/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [247/300], Step [230/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [247/300], Step [240/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [250/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [260/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [270/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [280/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [247/300], Step [290/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [247/300], Step [300/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [247/300], Step [310/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [247/300], Step [320/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [247/300], Step [330/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [247/300], Step [340/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [247/300], Step [350/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [247/300], Step [360/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [247/300], Step [370/391],                 Loss: 0.00258, Train_Acc:100.00%
Epoch [247/300], Step [380/391],                 Loss: 0.00257, Train_Acc:100.00%
Epoch [247/300], Step [390/391],                 Loss: 0.00257, Train_Acc:100.00%
Accuary on test images:91.26%
Epoch [248/300], Step [10/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [248/300], Step [20/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [248/300], Step [30/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [248/300], Step [40/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [248/300], Step [50/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [248/300], Step [60/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [248/300], Step [70/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [248/300], Step [80/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [248/300], Step [90/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [248/300], Step [100/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [248/300], Step [110/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [248/300], Step [120/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [248/300], Step [130/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [248/300], Step [140/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [248/300], Step [150/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [248/300], Step [160/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [248/300], Step [170/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [248/300], Step [180/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [248/300], Step [190/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [248/300], Step [200/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [248/300], Step [210/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [248/300], Step [220/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [248/300], Step [230/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [248/300], Step [240/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [248/300], Step [250/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [248/300], Step [260/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [248/300], Step [270/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [248/300], Step [280/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [248/300], Step [290/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [248/300], Step [300/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [248/300], Step [310/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [248/300], Step [320/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [248/300], Step [330/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [248/300], Step [340/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [248/300], Step [350/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [248/300], Step [360/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [248/300], Step [370/391],                 Loss: 0.00260, Train_Acc:100.00%
Epoch [248/300], Step [380/391],                 Loss: 0.00259, Train_Acc:100.00%
Epoch [248/300], Step [390/391],                 Loss: 0.00259, Train_Acc:100.00%
Accuary on test images:91.34%
Epoch [249/300], Step [10/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [249/300], Step [20/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [249/300], Step [30/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [249/300], Step [40/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [249/300], Step [50/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [249/300], Step [60/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [249/300], Step [70/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [249/300], Step [80/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [249/300], Step [90/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [249/300], Step [100/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [249/300], Step [110/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [249/300], Step [120/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [249/300], Step [130/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [249/300], Step [140/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [249/300], Step [150/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [249/300], Step [160/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [249/300], Step [170/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [249/300], Step [180/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [249/300], Step [190/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [249/300], Step [200/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [249/300], Step [210/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [249/300], Step [220/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [249/300], Step [230/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [249/300], Step [240/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [249/300], Step [250/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [249/300], Step [260/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [249/300], Step [270/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [249/300], Step [280/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [249/300], Step [290/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [249/300], Step [300/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [249/300], Step [310/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [249/300], Step [320/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [249/300], Step [330/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [249/300], Step [340/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [249/300], Step [350/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [249/300], Step [360/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [249/300], Step [370/391],                 Loss: 0.00262, Train_Acc:100.00%
Epoch [249/300], Step [380/391],                 Loss: 0.00261, Train_Acc:100.00%
Epoch [249/300], Step [390/391],                 Loss: 0.00261, Train_Acc:100.00%
Accuary on test images:91.26%
Epoch [250/300], Step [10/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [250/300], Step [20/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [250/300], Step [30/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [250/300], Step [40/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [250/300], Step [50/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [250/300], Step [60/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [250/300], Step [70/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [250/300], Step [80/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [250/300], Step [90/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [250/300], Step [100/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [250/300], Step [110/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [250/300], Step [120/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [250/300], Step [130/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [250/300], Step [140/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [250/300], Step [150/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [250/300], Step [160/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [250/300], Step [170/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [250/300], Step [180/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [250/300], Step [190/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [250/300], Step [200/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [250/300], Step [210/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [250/300], Step [220/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [250/300], Step [230/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [250/300], Step [240/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [250/300], Step [250/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [250/300], Step [260/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [250/300], Step [270/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [250/300], Step [280/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [250/300], Step [290/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [250/300], Step [300/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [250/300], Step [310/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [250/300], Step [320/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [250/300], Step [330/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [250/300], Step [340/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [250/300], Step [350/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [250/300], Step [360/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [250/300], Step [370/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [250/300], Step [380/391],                 Loss: 0.00263, Train_Acc:100.00%
Epoch [250/300], Step [390/391],                 Loss: 0.00263, Train_Acc:100.00%
Accuary on test images:91.28%
Epoch [251/300], Step [10/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [251/300], Step [20/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [251/300], Step [30/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [251/300], Step [40/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [251/300], Step [50/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [251/300], Step [60/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [70/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [80/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [251/300], Step [90/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [100/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [251/300], Step [110/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [251/300], Step [120/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [130/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [251/300], Step [140/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [150/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [160/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [170/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [251/300], Step [180/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [251/300], Step [190/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [251/300], Step [200/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [251/300], Step [210/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [251/300], Step [220/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [251/300], Step [230/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [251/300], Step [240/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [251/300], Step [250/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [251/300], Step [260/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [251/300], Step [270/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [251/300], Step [280/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [251/300], Step [290/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [251/300], Step [300/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [251/300], Step [310/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [251/300], Step [320/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [251/300], Step [330/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [251/300], Step [340/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [251/300], Step [350/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [251/300], Step [360/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [251/300], Step [370/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [251/300], Step [380/391],                 Loss: 0.00264, Train_Acc:100.00%
Epoch [251/300], Step [390/391],                 Loss: 0.00264, Train_Acc:100.00%
Accuary on test images:91.26%
Epoch [252/300], Step [10/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [252/300], Step [20/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [252/300], Step [30/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [252/300], Step [40/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [252/300], Step [50/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [252/300], Step [60/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [252/300], Step [70/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [252/300], Step [80/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [252/300], Step [90/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [252/300], Step [100/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [252/300], Step [110/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [252/300], Step [120/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [252/300], Step [130/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [252/300], Step [140/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [252/300], Step [150/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [252/300], Step [160/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [252/300], Step [170/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [252/300], Step [180/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [252/300], Step [190/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [252/300], Step [200/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [252/300], Step [210/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [252/300], Step [220/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [252/300], Step [230/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [252/300], Step [240/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [252/300], Step [250/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [252/300], Step [260/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [252/300], Step [270/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [252/300], Step [280/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [252/300], Step [290/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [252/300], Step [300/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [252/300], Step [310/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [252/300], Step [320/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [252/300], Step [330/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [252/300], Step [340/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [252/300], Step [350/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [252/300], Step [360/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [252/300], Step [370/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [252/300], Step [380/391],                 Loss: 0.00265, Train_Acc:100.00%
Epoch [252/300], Step [390/391],                 Loss: 0.00265, Train_Acc:100.00%
Accuary on test images:91.32%
Epoch [253/300], Step [10/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [253/300], Step [20/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [253/300], Step [30/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [253/300], Step [40/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [253/300], Step [50/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [253/300], Step [60/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [253/300], Step [70/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [253/300], Step [80/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [253/300], Step [90/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [253/300], Step [100/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [253/300], Step [110/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [253/300], Step [120/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [253/300], Step [130/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [253/300], Step [140/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [253/300], Step [150/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [253/300], Step [160/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [253/300], Step [170/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [253/300], Step [180/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [253/300], Step [190/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [253/300], Step [200/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [253/300], Step [210/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [253/300], Step [220/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [253/300], Step [230/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [253/300], Step [240/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [253/300], Step [250/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [253/300], Step [260/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [253/300], Step [270/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [253/300], Step [280/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [253/300], Step [290/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [253/300], Step [300/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [253/300], Step [310/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [253/300], Step [320/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [253/300], Step [330/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [253/300], Step [340/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [253/300], Step [350/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [253/300], Step [360/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [253/300], Step [370/391],                 Loss: 0.00267, Train_Acc:100.00%
Epoch [253/300], Step [380/391],                 Loss: 0.00266, Train_Acc:100.00%
Epoch [253/300], Step [390/391],                 Loss: 0.00266, Train_Acc:100.00%
Accuary on test images:91.22%
Epoch [254/300], Step [10/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [254/300], Step [20/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [254/300], Step [30/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [254/300], Step [40/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [254/300], Step [50/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [254/300], Step [60/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [254/300], Step [70/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [254/300], Step [80/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [254/300], Step [90/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [254/300], Step [100/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [254/300], Step [110/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [254/300], Step [120/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [254/300], Step [130/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [254/300], Step [140/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [254/300], Step [150/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [254/300], Step [160/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [254/300], Step [170/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [254/300], Step [180/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [254/300], Step [190/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [254/300], Step [200/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [254/300], Step [210/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [254/300], Step [220/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [254/300], Step [230/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [254/300], Step [240/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [254/300], Step [250/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [254/300], Step [260/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [254/300], Step [270/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [254/300], Step [280/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [254/300], Step [290/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [254/300], Step [300/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [254/300], Step [310/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [254/300], Step [320/391],                 Loss: 0.00271, Train_Acc:100.00%
Epoch [254/300], Step [330/391],                 Loss: 0.00270, Train_Acc:100.00%
Epoch [254/300], Step [340/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [254/300], Step [350/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [254/300], Step [360/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [254/300], Step [370/391],                 Loss: 0.00269, Train_Acc:100.00%
Epoch [254/300], Step [380/391],                 Loss: 0.00268, Train_Acc:100.00%
Epoch [254/300], Step [390/391],                 Loss: 0.00268, Train_Acc:100.00%
Accuary on test images:91.28%
Epoch [255/300], Step [10/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [255/300], Step [20/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [255/300], Step [30/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [255/300], Step [40/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [255/300], Step [50/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [255/300], Step [60/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [255/300], Step [70/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [255/300], Step [80/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [255/300], Step [90/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [255/300], Step [100/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [255/300], Step [110/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [255/300], Step [120/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [255/300], Step [130/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [255/300], Step [140/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [255/300], Step [150/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [255/300], Step [160/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [255/300], Step [170/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [255/300], Step [180/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [255/300], Step [190/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [255/300], Step [200/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [255/300], Step [210/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [255/300], Step [220/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [255/300], Step [230/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [255/300], Step [240/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [255/300], Step [250/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [255/300], Step [260/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [255/300], Step [270/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [255/300], Step [280/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [255/300], Step [290/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [255/300], Step [300/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [255/300], Step [310/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [255/300], Step [320/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [255/300], Step [330/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [255/300], Step [340/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [255/300], Step [350/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [255/300], Step [360/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [255/300], Step [370/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [255/300], Step [380/391],                 Loss: 0.00272, Train_Acc:100.00%
Epoch [255/300], Step [390/391],                 Loss: 0.00272, Train_Acc:100.00%
Accuary on test images:91.32%
Epoch [256/300], Step [10/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [256/300], Step [20/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [256/300], Step [30/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [256/300], Step [40/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [256/300], Step [50/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [256/300], Step [60/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [256/300], Step [70/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [256/300], Step [80/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [256/300], Step [90/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [256/300], Step [100/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [256/300], Step [110/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [256/300], Step [120/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [256/300], Step [130/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [256/300], Step [140/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [256/300], Step [150/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [256/300], Step [160/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [256/300], Step [170/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [256/300], Step [180/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [256/300], Step [190/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [256/300], Step [200/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [256/300], Step [210/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [256/300], Step [220/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [256/300], Step [230/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [256/300], Step [240/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [256/300], Step [250/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [256/300], Step [260/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [256/300], Step [270/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [256/300], Step [280/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [256/300], Step [290/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [256/300], Step [300/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [256/300], Step [310/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [256/300], Step [320/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [256/300], Step [330/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [256/300], Step [340/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [256/300], Step [350/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [256/300], Step [360/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [256/300], Step [370/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [256/300], Step [380/391],                 Loss: 0.00273, Train_Acc:100.00%
Epoch [256/300], Step [390/391],                 Loss: 0.00272, Train_Acc:100.00%
Accuary on test images:91.34%
Epoch [257/300], Step [10/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [257/300], Step [20/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [257/300], Step [30/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [257/300], Step [40/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [50/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [257/300], Step [60/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [257/300], Step [70/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [257/300], Step [80/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [257/300], Step [90/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [257/300], Step [100/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [110/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [120/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [257/300], Step [130/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [257/300], Step [140/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [257/300], Step [150/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [257/300], Step [160/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [257/300], Step [170/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [257/300], Step [180/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [190/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [200/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [210/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [220/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [257/300], Step [230/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [257/300], Step [240/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [257/300], Step [250/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [257/300], Step [260/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [257/300], Step [270/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [257/300], Step [280/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [257/300], Step [290/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [257/300], Step [300/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [257/300], Step [310/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [257/300], Step [320/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [257/300], Step [330/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [257/300], Step [340/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [257/300], Step [350/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [257/300], Step [360/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [257/300], Step [370/391],                 Loss: 0.00275, Train_Acc:100.00%
Epoch [257/300], Step [380/391],                 Loss: 0.00274, Train_Acc:100.00%
Epoch [257/300], Step [390/391],                 Loss: 0.00274, Train_Acc:100.00%
Accuary on test images:91.30%
Epoch [258/300], Step [10/391],                 Loss: 0.00307, Train_Acc:100.00%
Epoch [258/300], Step [20/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [258/300], Step [30/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [258/300], Step [40/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [258/300], Step [50/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [258/300], Step [60/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [258/300], Step [70/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [258/300], Step [80/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [258/300], Step [90/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [258/300], Step [100/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [258/300], Step [110/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [258/300], Step [120/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [258/300], Step [130/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [258/300], Step [140/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [258/300], Step [150/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [258/300], Step [160/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [258/300], Step [170/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [258/300], Step [180/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [258/300], Step [190/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [258/300], Step [200/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [258/300], Step [210/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [258/300], Step [220/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [258/300], Step [230/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [258/300], Step [240/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [258/300], Step [250/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [258/300], Step [260/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [258/300], Step [270/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [258/300], Step [280/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [258/300], Step [290/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [258/300], Step [300/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [258/300], Step [310/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [258/300], Step [320/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [258/300], Step [330/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [258/300], Step [340/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [258/300], Step [350/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [258/300], Step [360/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [258/300], Step [370/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [258/300], Step [380/391],                 Loss: 0.00276, Train_Acc:100.00%
Epoch [258/300], Step [390/391],                 Loss: 0.00275, Train_Acc:100.00%
Accuary on test images:91.32%
Epoch [259/300], Step [10/391],                 Loss: 0.00304, Train_Acc:100.00%
Epoch [259/300], Step [20/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [259/300], Step [30/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [259/300], Step [40/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [259/300], Step [50/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [259/300], Step [60/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [259/300], Step [70/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [259/300], Step [80/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [259/300], Step [90/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [259/300], Step [100/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [259/300], Step [110/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [259/300], Step [120/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [259/300], Step [130/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [259/300], Step [140/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [259/300], Step [150/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [259/300], Step [160/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [259/300], Step [170/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [259/300], Step [180/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [259/300], Step [190/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [259/300], Step [200/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [259/300], Step [210/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [259/300], Step [220/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [259/300], Step [230/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [259/300], Step [240/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [259/300], Step [250/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [259/300], Step [260/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [259/300], Step [270/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [259/300], Step [280/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [259/300], Step [290/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [259/300], Step [300/391],                 Loss: 0.00280, Train_Acc:100.00%
Epoch [259/300], Step [310/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [259/300], Step [320/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [259/300], Step [330/391],                 Loss: 0.00279, Train_Acc:100.00%
Epoch [259/300], Step [340/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [259/300], Step [350/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [259/300], Step [360/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [259/300], Step [370/391],                 Loss: 0.00278, Train_Acc:100.00%
Epoch [259/300], Step [380/391],                 Loss: 0.00277, Train_Acc:100.00%
Epoch [259/300], Step [390/391],                 Loss: 0.00277, Train_Acc:100.00%
Accuary on test images:91.40%
Epoch [260/300], Step [10/391],                 Loss: 0.00306, Train_Acc:100.00%
Epoch [260/300], Step [20/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [260/300], Step [30/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [260/300], Step [40/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [260/300], Step [50/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [260/300], Step [60/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [260/300], Step [70/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [260/300], Step [80/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [260/300], Step [90/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [260/300], Step [100/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [260/300], Step [110/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [260/300], Step [120/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [260/300], Step [130/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [260/300], Step [140/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [260/300], Step [150/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [260/300], Step [160/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [260/300], Step [170/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [260/300], Step [180/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [260/300], Step [190/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [260/300], Step [200/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [260/300], Step [210/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [260/300], Step [220/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [260/300], Step [230/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [260/300], Step [240/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [260/300], Step [250/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [260/300], Step [260/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [260/300], Step [270/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [260/300], Step [280/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [260/300], Step [290/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [260/300], Step [300/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [260/300], Step [310/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [260/300], Step [320/391],                 Loss: 0.00283, Train_Acc:100.00%
Epoch [260/300], Step [330/391],                 Loss: 0.00282, Train_Acc:100.00%
Epoch [260/300], Step [340/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [260/300], Step [350/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [260/300], Step [360/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [260/300], Step [370/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [260/300], Step [380/391],                 Loss: 0.00281, Train_Acc:100.00%
Epoch [260/300], Step [390/391],                 Loss: 0.00280, Train_Acc:100.00%
Accuary on test images:91.42%
Epoch [261/300], Step [10/391],                 Loss: 0.00303, Train_Acc:100.00%
Epoch [261/300], Step [20/391],                 Loss: 0.00296, Train_Acc:100.00%
Epoch [261/300], Step [30/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [261/300], Step [40/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [261/300], Step [50/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [261/300], Step [60/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [261/300], Step [70/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [261/300], Step [80/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [261/300], Step [90/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [261/300], Step [100/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [261/300], Step [110/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [261/300], Step [120/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [261/300], Step [130/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [261/300], Step [140/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [261/300], Step [150/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [261/300], Step [160/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [261/300], Step [170/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [261/300], Step [180/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [261/300], Step [190/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [261/300], Step [200/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [261/300], Step [210/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [261/300], Step [220/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [261/300], Step [230/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [261/300], Step [240/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [261/300], Step [250/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [261/300], Step [260/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [261/300], Step [270/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [261/300], Step [280/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [261/300], Step [290/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [261/300], Step [300/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [261/300], Step [310/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [261/300], Step [320/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [261/300], Step [330/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [261/300], Step [340/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [261/300], Step [350/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [261/300], Step [360/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [261/300], Step [370/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [261/300], Step [380/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [261/300], Step [390/391],                 Loss: 0.00284, Train_Acc:100.00%
Accuary on test images:91.38%
Epoch [262/300], Step [10/391],                 Loss: 0.00307, Train_Acc:100.00%
Epoch [262/300], Step [20/391],                 Loss: 0.00298, Train_Acc:100.00%
Epoch [262/300], Step [30/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [262/300], Step [40/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [262/300], Step [50/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [262/300], Step [60/391],                 Loss: 0.00296, Train_Acc:100.00%
Epoch [262/300], Step [70/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [262/300], Step [80/391],                 Loss: 0.00295, Train_Acc:100.00%
Epoch [262/300], Step [90/391],                 Loss: 0.00294, Train_Acc:100.00%
Epoch [262/300], Step [100/391],                 Loss: 0.00290, Train_Acc:100.00%
Epoch [262/300], Step [110/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [262/300], Step [120/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [262/300], Step [130/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [262/300], Step [140/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [262/300], Step [150/391],                 Loss: 0.00292, Train_Acc:100.00%
Epoch [262/300], Step [160/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [262/300], Step [170/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [262/300], Step [180/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [262/300], Step [190/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [262/300], Step [200/391],                 Loss: 0.00289, Train_Acc:100.00%
Epoch [262/300], Step [210/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [262/300], Step [220/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [262/300], Step [230/391],                 Loss: 0.00288, Train_Acc:100.00%
Epoch [262/300], Step [240/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [262/300], Step [250/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [262/300], Step [260/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [262/300], Step [270/391],                 Loss: 0.00287, Train_Acc:100.00%
Epoch [262/300], Step [280/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [262/300], Step [290/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [262/300], Step [300/391],                 Loss: 0.00286, Train_Acc:100.00%
Epoch [262/300], Step [310/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [262/300], Step [320/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [262/300], Step [330/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [262/300], Step [340/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [262/300], Step [350/391],                 Loss: 0.00285, Train_Acc:100.00%
Epoch [262/300], Step [360/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [262/300], Step [370/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [262/300], Step [380/391],                 Loss: 0.00284, Train_Acc:100.00%
Epoch [262/300], Step [390/391],                 Loss: 0.00283, Train_Acc:100.00%
Accuary on test images:91.38%
Epoch [263/300], Step [10/391],                 Loss: 0.00312, Train_Acc:100.00%
Epoch [263/300], Step [20/391],                 Loss: 0.00298, Train_Acc:100.00%
Epoch [263/300], Step [30/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [263/300], Step [40/391],                 Loss: 0.00293, Train_Acc:100.00%
Epoch [263/300], Step [50/391],                 Loss: 0.00291, Train_Acc:100.00%
Epoch [263/300], Step [60/391],                 Loss: 0.00294, Train_Acc:100.00%