nohup: ignoring input
Files already downloaded and verified
Epoch [1/300], Step [10/391],                 Loss: 2.84644, Train_Acc:11.09%
Epoch [1/300], Step [20/391],                 Loss: 2.68973, Train_Acc:12.85%
Epoch [1/300], Step [30/391],                 Loss: 2.56069, Train_Acc:15.26%
Epoch [1/300], Step [40/391],                 Loss: 2.44925, Train_Acc:17.03%
Epoch [1/300], Step [50/391],                 Loss: 2.36391, Train_Acc:18.64%
Epoch [1/300], Step [60/391],                 Loss: 2.30104, Train_Acc:19.96%
Epoch [1/300], Step [70/391],                 Loss: 2.25608, Train_Acc:20.98%
Epoch [1/300], Step [80/391],                 Loss: 2.22054, Train_Acc:21.80%
Epoch [1/300], Step [90/391],                 Loss: 2.18905, Train_Acc:22.53%
Epoch [1/300], Step [100/391],                 Loss: 2.16060, Train_Acc:23.06%
Epoch [1/300], Step [110/391],                 Loss: 2.13627, Train_Acc:23.73%
Epoch [1/300], Step [120/391],                 Loss: 2.11945, Train_Acc:23.98%
Epoch [1/300], Step [130/391],                 Loss: 2.10410, Train_Acc:24.33%
Epoch [1/300], Step [140/391],                 Loss: 2.08446, Train_Acc:24.80%
Epoch [1/300], Step [150/391],                 Loss: 2.06685, Train_Acc:25.23%
Epoch [1/300], Step [160/391],                 Loss: 2.05119, Train_Acc:25.72%
Epoch [1/300], Step [170/391],                 Loss: 2.03621, Train_Acc:26.22%
Epoch [1/300], Step [180/391],                 Loss: 2.02366, Train_Acc:26.47%
Epoch [1/300], Step [190/391],                 Loss: 2.00939, Train_Acc:26.90%
Epoch [1/300], Step [200/391],                 Loss: 1.99545, Train_Acc:27.42%
Epoch [1/300], Step [210/391],                 Loss: 1.98241, Train_Acc:27.76%
Epoch [1/300], Step [220/391],                 Loss: 1.97203, Train_Acc:28.10%
Epoch [1/300], Step [230/391],                 Loss: 1.95888, Train_Acc:28.49%
Epoch [1/300], Step [240/391],                 Loss: 1.94601, Train_Acc:28.88%
Epoch [1/300], Step [250/391],                 Loss: 1.93355, Train_Acc:29.30%
Epoch [1/300], Step [260/391],                 Loss: 1.92477, Train_Acc:29.55%
Epoch [1/300], Step [270/391],                 Loss: 1.91523, Train_Acc:29.82%
Epoch [1/300], Step [280/391],                 Loss: 1.90512, Train_Acc:30.17%
Epoch [1/300], Step [290/391],                 Loss: 1.89580, Train_Acc:30.52%
Epoch [1/300], Step [300/391],                 Loss: 1.88904, Train_Acc:30.73%
Epoch [1/300], Step [310/391],                 Loss: 1.87828, Train_Acc:31.13%
Epoch [1/300], Step [320/391],                 Loss: 1.86857, Train_Acc:31.51%
Epoch [1/300], Step [330/391],                 Loss: 1.85919, Train_Acc:31.81%
Epoch [1/300], Step [340/391],                 Loss: 1.85088, Train_Acc:32.07%
Epoch [1/300], Step [350/391],                 Loss: 1.84366, Train_Acc:32.35%
Epoch [1/300], Step [360/391],                 Loss: 1.83409, Train_Acc:32.68%
Epoch [1/300], Step [370/391],                 Loss: 1.82582, Train_Acc:32.88%
Epoch [1/300], Step [380/391],                 Loss: 1.81893, Train_Acc:33.10%
Epoch [1/300], Step [390/391],                 Loss: 1.80954, Train_Acc:33.45%
Accuary on test images:35.98%
Epoch [2/300], Step [10/391],                 Loss: 1.43906, Train_Acc:46.41%
Epoch [2/300], Step [20/391],                 Loss: 1.45041, Train_Acc:46.13%
Epoch [2/300], Step [30/391],                 Loss: 1.46071, Train_Acc:46.30%
Epoch [2/300], Step [40/391],                 Loss: 1.46058, Train_Acc:46.07%
Epoch [2/300], Step [50/391],                 Loss: 1.45653, Train_Acc:46.05%
Epoch [2/300], Step [60/391],                 Loss: 1.44424, Train_Acc:46.55%
Epoch [2/300], Step [70/391],                 Loss: 1.43752, Train_Acc:47.02%
Epoch [2/300], Step [80/391],                 Loss: 1.43841, Train_Acc:47.21%
Epoch [2/300], Step [90/391],                 Loss: 1.43794, Train_Acc:47.38%
Epoch [2/300], Step [100/391],                 Loss: 1.43582, Train_Acc:47.47%
Epoch [2/300], Step [110/391],                 Loss: 1.43370, Train_Acc:47.52%
Epoch [2/300], Step [120/391],                 Loss: 1.43304, Train_Acc:47.46%
Epoch [2/300], Step [130/391],                 Loss: 1.43371, Train_Acc:47.51%
Epoch [2/300], Step [140/391],                 Loss: 1.42722, Train_Acc:47.77%
Epoch [2/300], Step [150/391],                 Loss: 1.42217, Train_Acc:47.99%
Epoch [2/300], Step [160/391],                 Loss: 1.41662, Train_Acc:48.06%
Epoch [2/300], Step [170/391],                 Loss: 1.41189, Train_Acc:48.20%
Epoch [2/300], Step [180/391],                 Loss: 1.40928, Train_Acc:48.26%
Epoch [2/300], Step [190/391],                 Loss: 1.40207, Train_Acc:48.61%
Epoch [2/300], Step [200/391],                 Loss: 1.39386, Train_Acc:48.97%
Epoch [2/300], Step [210/391],                 Loss: 1.38749, Train_Acc:49.20%
Epoch [2/300], Step [220/391],                 Loss: 1.38475, Train_Acc:49.29%
Epoch [2/300], Step [230/391],                 Loss: 1.37893, Train_Acc:49.46%
Epoch [2/300], Step [240/391],                 Loss: 1.37277, Train_Acc:49.63%
Epoch [2/300], Step [250/391],                 Loss: 1.36737, Train_Acc:49.87%
Epoch [2/300], Step [260/391],                 Loss: 1.36444, Train_Acc:50.00%
Epoch [2/300], Step [270/391],                 Loss: 1.36242, Train_Acc:50.02%
Epoch [2/300], Step [280/391],                 Loss: 1.35821, Train_Acc:50.19%
Epoch [2/300], Step [290/391],                 Loss: 1.35334, Train_Acc:50.42%
Epoch [2/300], Step [300/391],                 Loss: 1.35028, Train_Acc:50.49%
Epoch [2/300], Step [310/391],                 Loss: 1.34393, Train_Acc:50.74%
Epoch [2/300], Step [320/391],                 Loss: 1.34059, Train_Acc:50.93%
Epoch [2/300], Step [330/391],                 Loss: 1.33644, Train_Acc:51.11%
Epoch [2/300], Step [340/391],                 Loss: 1.33104, Train_Acc:51.34%
Epoch [2/300], Step [350/391],                 Loss: 1.32856, Train_Acc:51.45%
Epoch [2/300], Step [360/391],                 Loss: 1.32317, Train_Acc:51.66%
Epoch [2/300], Step [370/391],                 Loss: 1.31864, Train_Acc:51.83%
Epoch [2/300], Step [380/391],                 Loss: 1.31516, Train_Acc:52.01%
Epoch [2/300], Step [390/391],                 Loss: 1.31014, Train_Acc:52.21%
Accuary on test images:57.92%
Epoch [3/300], Step [10/391],                 Loss: 1.08961, Train_Acc:61.64%
Epoch [3/300], Step [20/391],                 Loss: 1.09615, Train_Acc:61.33%
Epoch [3/300], Step [30/391],                 Loss: 1.09296, Train_Acc:61.38%
Epoch [3/300], Step [40/391],                 Loss: 1.09656, Train_Acc:61.21%
Epoch [3/300], Step [50/391],                 Loss: 1.11200, Train_Acc:60.28%
Epoch [3/300], Step [60/391],                 Loss: 1.11306, Train_Acc:60.20%
Epoch [3/300], Step [70/391],                 Loss: 1.10743, Train_Acc:60.23%
Epoch [3/300], Step [80/391],                 Loss: 1.10393, Train_Acc:60.29%
Epoch [3/300], Step [90/391],                 Loss: 1.10875, Train_Acc:60.04%
Epoch [3/300], Step [100/391],                 Loss: 1.11014, Train_Acc:60.05%
Epoch [3/300], Step [110/391],                 Loss: 1.10614, Train_Acc:60.28%
Epoch [3/300], Step [120/391],                 Loss: 1.10102, Train_Acc:60.38%
Epoch [3/300], Step [130/391],                 Loss: 1.10120, Train_Acc:60.44%
Epoch [3/300], Step [140/391],                 Loss: 1.09618, Train_Acc:60.66%
Epoch [3/300], Step [150/391],                 Loss: 1.09452, Train_Acc:60.78%
Epoch [3/300], Step [160/391],                 Loss: 1.09583, Train_Acc:60.73%
Epoch [3/300], Step [170/391],                 Loss: 1.09310, Train_Acc:60.85%
Epoch [3/300], Step [180/391],                 Loss: 1.09570, Train_Acc:60.82%
Epoch [3/300], Step [190/391],                 Loss: 1.09345, Train_Acc:60.92%
Epoch [3/300], Step [200/391],                 Loss: 1.08954, Train_Acc:61.08%
Epoch [3/300], Step [210/391],                 Loss: 1.08657, Train_Acc:61.21%
Epoch [3/300], Step [220/391],                 Loss: 1.08536, Train_Acc:61.25%
Epoch [3/300], Step [230/391],                 Loss: 1.08319, Train_Acc:61.31%
Epoch [3/300], Step [240/391],                 Loss: 1.07832, Train_Acc:61.49%
Epoch [3/300], Step [250/391],                 Loss: 1.07493, Train_Acc:61.64%
Epoch [3/300], Step [260/391],                 Loss: 1.07368, Train_Acc:61.65%
Epoch [3/300], Step [270/391],                 Loss: 1.07250, Train_Acc:61.66%
Epoch [3/300], Step [280/391],                 Loss: 1.06870, Train_Acc:61.83%
Epoch [3/300], Step [290/391],                 Loss: 1.06533, Train_Acc:61.95%
Epoch [3/300], Step [300/391],                 Loss: 1.06380, Train_Acc:61.95%
Epoch [3/300], Step [310/391],                 Loss: 1.05949, Train_Acc:62.13%
Epoch [3/300], Step [320/391],                 Loss: 1.05830, Train_Acc:62.20%
Epoch [3/300], Step [330/391],                 Loss: 1.05586, Train_Acc:62.28%
Epoch [3/300], Step [340/391],                 Loss: 1.05177, Train_Acc:62.43%
Epoch [3/300], Step [350/391],                 Loss: 1.04973, Train_Acc:62.53%
Epoch [3/300], Step [360/391],                 Loss: 1.04600, Train_Acc:62.67%
Epoch [3/300], Step [370/391],                 Loss: 1.04326, Train_Acc:62.78%
Epoch [3/300], Step [380/391],                 Loss: 1.04146, Train_Acc:62.87%
Epoch [3/300], Step [390/391],                 Loss: 1.03772, Train_Acc:63.00%
Accuary on test images:57.20%
Epoch [4/300], Step [10/391],                 Loss: 0.89974, Train_Acc:68.52%
Epoch [4/300], Step [20/391],                 Loss: 0.92110, Train_Acc:67.85%
Epoch [4/300], Step [30/391],                 Loss: 0.90753, Train_Acc:68.05%
Epoch [4/300], Step [40/391],                 Loss: 0.90964, Train_Acc:67.93%
Epoch [4/300], Step [50/391],                 Loss: 0.91643, Train_Acc:67.77%
Epoch [4/300], Step [60/391],                 Loss: 0.91626, Train_Acc:67.80%
Epoch [4/300], Step [70/391],                 Loss: 0.91541, Train_Acc:67.91%
Epoch [4/300], Step [80/391],                 Loss: 0.91428, Train_Acc:67.97%
Epoch [4/300], Step [90/391],                 Loss: 0.91479, Train_Acc:67.70%
Epoch [4/300], Step [100/391],                 Loss: 0.91498, Train_Acc:67.77%
Epoch [4/300], Step [110/391],                 Loss: 0.91508, Train_Acc:67.76%
Epoch [4/300], Step [120/391],                 Loss: 0.91341, Train_Acc:67.70%
Epoch [4/300], Step [130/391],                 Loss: 0.91661, Train_Acc:67.54%
Epoch [4/300], Step [140/391],                 Loss: 0.91323, Train_Acc:67.63%
Epoch [4/300], Step [150/391],                 Loss: 0.91128, Train_Acc:67.83%
Epoch [4/300], Step [160/391],                 Loss: 0.91023, Train_Acc:67.91%
Epoch [4/300], Step [170/391],                 Loss: 0.90765, Train_Acc:68.01%
Epoch [4/300], Step [180/391],                 Loss: 0.90649, Train_Acc:68.03%
Epoch [4/300], Step [190/391],                 Loss: 0.90329, Train_Acc:68.16%
Epoch [4/300], Step [200/391],                 Loss: 0.89976, Train_Acc:68.29%
Epoch [4/300], Step [210/391],                 Loss: 0.89599, Train_Acc:68.52%
Epoch [4/300], Step [220/391],                 Loss: 0.89533, Train_Acc:68.55%
Epoch [4/300], Step [230/391],                 Loss: 0.89471, Train_Acc:68.55%
Epoch [4/300], Step [240/391],                 Loss: 0.89011, Train_Acc:68.75%
Epoch [4/300], Step [250/391],                 Loss: 0.88888, Train_Acc:68.79%
Epoch [4/300], Step [260/391],                 Loss: 0.88970, Train_Acc:68.73%
Epoch [4/300], Step [270/391],                 Loss: 0.88986, Train_Acc:68.69%
Epoch [4/300], Step [280/391],                 Loss: 0.88783, Train_Acc:68.76%
Epoch [4/300], Step [290/391],                 Loss: 0.88602, Train_Acc:68.82%
Epoch [4/300], Step [300/391],                 Loss: 0.88562, Train_Acc:68.81%
Epoch [4/300], Step [310/391],                 Loss: 0.88231, Train_Acc:68.94%
Epoch [4/300], Step [320/391],                 Loss: 0.88229, Train_Acc:68.96%
Epoch [4/300], Step [330/391],                 Loss: 0.88070, Train_Acc:69.03%
Epoch [4/300], Step [340/391],                 Loss: 0.87807, Train_Acc:69.11%
Epoch [4/300], Step [350/391],                 Loss: 0.87756, Train_Acc:69.11%
Epoch [4/300], Step [360/391],                 Loss: 0.87519, Train_Acc:69.23%
Epoch [4/300], Step [370/391],                 Loss: 0.87340, Train_Acc:69.28%
Epoch [4/300], Step [380/391],                 Loss: 0.87148, Train_Acc:69.34%
Epoch [4/300], Step [390/391],                 Loss: 0.86805, Train_Acc:69.46%
Accuary on test images:53.16%
Epoch [5/300], Step [10/391],                 Loss: 0.77947, Train_Acc:71.48%
Epoch [5/300], Step [20/391],                 Loss: 0.80504, Train_Acc:71.68%
Epoch [5/300], Step [30/391],                 Loss: 0.79569, Train_Acc:71.67%
Epoch [5/300], Step [40/391],                 Loss: 0.79020, Train_Acc:72.21%
Epoch [5/300], Step [50/391],                 Loss: 0.79032, Train_Acc:71.98%
Epoch [5/300], Step [60/391],                 Loss: 0.79456, Train_Acc:71.77%
Epoch [5/300], Step [70/391],                 Loss: 0.79081, Train_Acc:71.84%
Epoch [5/300], Step [80/391],                 Loss: 0.78692, Train_Acc:72.19%
Epoch [5/300], Step [90/391],                 Loss: 0.78950, Train_Acc:71.94%
Epoch [5/300], Step [100/391],                 Loss: 0.78726, Train_Acc:72.07%
Epoch [5/300], Step [110/391],                 Loss: 0.78911, Train_Acc:72.04%
Epoch [5/300], Step [120/391],                 Loss: 0.78566, Train_Acc:72.29%
Epoch [5/300], Step [130/391],                 Loss: 0.78315, Train_Acc:72.42%
Epoch [5/300], Step [140/391],                 Loss: 0.77794, Train_Acc:72.51%
Epoch [5/300], Step [150/391],                 Loss: 0.77650, Train_Acc:72.63%
Epoch [5/300], Step [160/391],                 Loss: 0.77615, Train_Acc:72.66%
Epoch [5/300], Step [170/391],                 Loss: 0.77585, Train_Acc:72.73%
Epoch [5/300], Step [180/391],                 Loss: 0.77708, Train_Acc:72.68%
Epoch [5/300], Step [190/391],                 Loss: 0.77515, Train_Acc:72.74%
Epoch [5/300], Step [200/391],                 Loss: 0.77355, Train_Acc:72.80%
Epoch [5/300], Step [210/391],                 Loss: 0.77048, Train_Acc:72.95%
Epoch [5/300], Step [220/391],                 Loss: 0.76941, Train_Acc:72.91%
Epoch [5/300], Step [230/391],                 Loss: 0.76740, Train_Acc:72.99%
Epoch [5/300], Step [240/391],                 Loss: 0.76387, Train_Acc:73.11%
Epoch [5/300], Step [250/391],                 Loss: 0.76200, Train_Acc:73.21%
Epoch [5/300], Step [260/391],                 Loss: 0.76142, Train_Acc:73.31%
Epoch [5/300], Step [270/391],                 Loss: 0.76203, Train_Acc:73.26%
Epoch [5/300], Step [280/391],                 Loss: 0.76045, Train_Acc:73.35%
Epoch [5/300], Step [290/391],                 Loss: 0.75975, Train_Acc:73.41%
Epoch [5/300], Step [300/391],                 Loss: 0.75855, Train_Acc:73.40%
Epoch [5/300], Step [310/391],                 Loss: 0.75520, Train_Acc:73.50%
Epoch [5/300], Step [320/391],                 Loss: 0.75583, Train_Acc:73.46%
Epoch [5/300], Step [330/391],                 Loss: 0.75427, Train_Acc:73.54%
Epoch [5/300], Step [340/391],                 Loss: 0.75273, Train_Acc:73.61%
Epoch [5/300], Step [350/391],                 Loss: 0.75177, Train_Acc:73.67%
Epoch [5/300], Step [360/391],                 Loss: 0.74965, Train_Acc:73.73%
Epoch [5/300], Step [370/391],                 Loss: 0.74775, Train_Acc:73.80%
Epoch [5/300], Step [380/391],                 Loss: 0.74720, Train_Acc:73.82%
Epoch [5/300], Step [390/391],                 Loss: 0.74432, Train_Acc:73.90%
Accuary on test images:59.34%
Epoch [6/300], Step [10/391],                 Loss: 0.67037, Train_Acc:77.50%
Epoch [6/300], Step [20/391],                 Loss: 0.68416, Train_Acc:77.46%
Epoch [6/300], Step [30/391],                 Loss: 0.68476, Train_Acc:76.93%
Epoch [6/300], Step [40/391],                 Loss: 0.68821, Train_Acc:76.82%
Epoch [6/300], Step [50/391],                 Loss: 0.69192, Train_Acc:76.25%
Epoch [6/300], Step [60/391],                 Loss: 0.69308, Train_Acc:76.05%
Epoch [6/300], Step [70/391],                 Loss: 0.69535, Train_Acc:75.95%
Epoch [6/300], Step [80/391],                 Loss: 0.69440, Train_Acc:76.01%
Epoch [6/300], Step [90/391],                 Loss: 0.69350, Train_Acc:75.90%
Epoch [6/300], Step [100/391],                 Loss: 0.69513, Train_Acc:75.86%
Epoch [6/300], Step [110/391],                 Loss: 0.69535, Train_Acc:75.83%
Epoch [6/300], Step [120/391],                 Loss: 0.69427, Train_Acc:75.77%
Epoch [6/300], Step [130/391],                 Loss: 0.69171, Train_Acc:75.92%
Epoch [6/300], Step [140/391],                 Loss: 0.68694, Train_Acc:76.08%
Epoch [6/300], Step [150/391],                 Loss: 0.68810, Train_Acc:76.08%
Epoch [6/300], Step [160/391],                 Loss: 0.68939, Train_Acc:76.04%
Epoch [6/300], Step [170/391],                 Loss: 0.68985, Train_Acc:76.08%
Epoch [6/300], Step [180/391],                 Loss: 0.68990, Train_Acc:76.08%
Epoch [6/300], Step [190/391],                 Loss: 0.68906, Train_Acc:76.09%
Epoch [6/300], Step [200/391],                 Loss: 0.68778, Train_Acc:76.17%
Epoch [6/300], Step [210/391],                 Loss: 0.68491, Train_Acc:76.30%
Epoch [6/300], Step [220/391],                 Loss: 0.68332, Train_Acc:76.35%
Epoch [6/300], Step [230/391],                 Loss: 0.68072, Train_Acc:76.41%
Epoch [6/300], Step [240/391],                 Loss: 0.67742, Train_Acc:76.48%
Epoch [6/300], Step [250/391],                 Loss: 0.67743, Train_Acc:76.48%
Epoch [6/300], Step [260/391],                 Loss: 0.67876, Train_Acc:76.43%
Epoch [6/300], Step [270/391],                 Loss: 0.67920, Train_Acc:76.37%
Epoch [6/300], Step [280/391],                 Loss: 0.67793, Train_Acc:76.40%
Epoch [6/300], Step [290/391],                 Loss: 0.67738, Train_Acc:76.43%
Epoch [6/300], Step [300/391],                 Loss: 0.67769, Train_Acc:76.39%
Epoch [6/300], Step [310/391],                 Loss: 0.67686, Train_Acc:76.43%
Epoch [6/300], Step [320/391],                 Loss: 0.67731, Train_Acc:76.43%
Epoch [6/300], Step [330/391],                 Loss: 0.67548, Train_Acc:76.52%
Epoch [6/300], Step [340/391],                 Loss: 0.67470, Train_Acc:76.51%
Epoch [6/300], Step [350/391],                 Loss: 0.67438, Train_Acc:76.50%
Epoch [6/300], Step [360/391],                 Loss: 0.67309, Train_Acc:76.54%
Epoch [6/300], Step [370/391],                 Loss: 0.67220, Train_Acc:76.58%
Epoch [6/300], Step [380/391],                 Loss: 0.67184, Train_Acc:76.59%
Epoch [6/300], Step [390/391],                 Loss: 0.66967, Train_Acc:76.68%
Accuary on test images:52.32%
Epoch [7/300], Step [10/391],                 Loss: 0.66724, Train_Acc:77.42%
Epoch [7/300], Step [20/391],                 Loss: 0.65115, Train_Acc:78.12%
Epoch [7/300], Step [30/391],                 Loss: 0.63773, Train_Acc:78.33%
Epoch [7/300], Step [40/391],                 Loss: 0.63840, Train_Acc:78.20%
Epoch [7/300], Step [50/391],                 Loss: 0.64336, Train_Acc:77.73%
Epoch [7/300], Step [60/391],                 Loss: 0.64562, Train_Acc:77.38%
Epoch [7/300], Step [70/391],                 Loss: 0.64742, Train_Acc:77.30%
Epoch [7/300], Step [80/391],                 Loss: 0.64138, Train_Acc:77.54%
Epoch [7/300], Step [90/391],                 Loss: 0.63964, Train_Acc:77.62%
Epoch [7/300], Step [100/391],                 Loss: 0.63862, Train_Acc:77.73%
Epoch [7/300], Step [110/391],                 Loss: 0.64033, Train_Acc:77.65%
Epoch [7/300], Step [120/391],                 Loss: 0.64116, Train_Acc:77.57%
Epoch [7/300], Step [130/391],                 Loss: 0.64250, Train_Acc:77.59%
Epoch [7/300], Step [140/391],                 Loss: 0.63950, Train_Acc:77.68%
Epoch [7/300], Step [150/391],                 Loss: 0.63812, Train_Acc:77.77%
Epoch [7/300], Step [160/391],                 Loss: 0.63742, Train_Acc:77.83%
Epoch [7/300], Step [170/391],                 Loss: 0.63677, Train_Acc:77.86%
Epoch [7/300], Step [180/391],                 Loss: 0.63720, Train_Acc:77.85%
Epoch [7/300], Step [190/391],                 Loss: 0.63619, Train_Acc:77.90%
Epoch [7/300], Step [200/391],                 Loss: 0.63499, Train_Acc:77.99%
Epoch [7/300], Step [210/391],                 Loss: 0.63355, Train_Acc:78.04%
Epoch [7/300], Step [220/391],                 Loss: 0.63295, Train_Acc:78.08%
Epoch [7/300], Step [230/391],                 Loss: 0.63054, Train_Acc:78.19%
Epoch [7/300], Step [240/391],                 Loss: 0.62852, Train_Acc:78.27%
Epoch [7/300], Step [250/391],                 Loss: 0.62771, Train_Acc:78.30%
Epoch [7/300], Step [260/391],                 Loss: 0.62938, Train_Acc:78.23%
Epoch [7/300], Step [270/391],                 Loss: 0.63083, Train_Acc:78.17%
Epoch [7/300], Step [280/391],                 Loss: 0.63083, Train_Acc:78.16%
Epoch [7/300], Step [290/391],                 Loss: 0.63018, Train_Acc:78.18%
Epoch [7/300], Step [300/391],                 Loss: 0.62987, Train_Acc:78.18%
Epoch [7/300], Step [310/391],                 Loss: 0.62826, Train_Acc:78.21%
Epoch [7/300], Step [320/391],                 Loss: 0.62818, Train_Acc:78.18%
Epoch [7/300], Step [330/391],                 Loss: 0.62664, Train_Acc:78.25%
Epoch [7/300], Step [340/391],                 Loss: 0.62603, Train_Acc:78.27%
Epoch [7/300], Step [350/391],                 Loss: 0.62616, Train_Acc:78.29%
Epoch [7/300], Step [360/391],                 Loss: 0.62538, Train_Acc:78.34%
Epoch [7/300], Step [370/391],                 Loss: 0.62438, Train_Acc:78.39%
Epoch [7/300], Step [380/391],                 Loss: 0.62422, Train_Acc:78.42%
Epoch [7/300], Step [390/391],                 Loss: 0.62233, Train_Acc:78.49%
Accuary on test images:70.56%
Epoch [8/300], Step [10/391],                 Loss: 0.57308, Train_Acc:80.08%
Epoch [8/300], Step [20/391],                 Loss: 0.56582, Train_Acc:80.35%
Epoch [8/300], Step [30/391],                 Loss: 0.56288, Train_Acc:80.44%
Epoch [8/300], Step [40/391],                 Loss: 0.56760, Train_Acc:80.21%
Epoch [8/300], Step [50/391],                 Loss: 0.57593, Train_Acc:79.89%
Epoch [8/300], Step [60/391],                 Loss: 0.58550, Train_Acc:79.77%
Epoch [8/300], Step [70/391],                 Loss: 0.58973, Train_Acc:79.70%
Epoch [8/300], Step [80/391],                 Loss: 0.59042, Train_Acc:79.60%
Epoch [8/300], Step [90/391],                 Loss: 0.59050, Train_Acc:79.56%
Epoch [8/300], Step [100/391],                 Loss: 0.58809, Train_Acc:79.59%
Epoch [8/300], Step [110/391],                 Loss: 0.58867, Train_Acc:79.51%
Epoch [8/300], Step [120/391],                 Loss: 0.58961, Train_Acc:79.47%
Epoch [8/300], Step [130/391],                 Loss: 0.59062, Train_Acc:79.38%
Epoch [8/300], Step [140/391],                 Loss: 0.58625, Train_Acc:79.54%
Epoch [8/300], Step [150/391],                 Loss: 0.58700, Train_Acc:79.58%
Epoch [8/300], Step [160/391],                 Loss: 0.58543, Train_Acc:79.64%
Epoch [8/300], Step [170/391],                 Loss: 0.58516, Train_Acc:79.64%
Epoch [8/300], Step [180/391],                 Loss: 0.58561, Train_Acc:79.68%
Epoch [8/300], Step [190/391],                 Loss: 0.58426, Train_Acc:79.71%
Epoch [8/300], Step [200/391],                 Loss: 0.58306, Train_Acc:79.76%
Epoch [8/300], Step [210/391],                 Loss: 0.58141, Train_Acc:79.85%
Epoch [8/300], Step [220/391],                 Loss: 0.58209, Train_Acc:79.83%
Epoch [8/300], Step [230/391],                 Loss: 0.58176, Train_Acc:79.82%
Epoch [8/300], Step [240/391],                 Loss: 0.57965, Train_Acc:79.89%
Epoch [8/300], Step [250/391],                 Loss: 0.57897, Train_Acc:79.87%
Epoch [8/300], Step [260/391],                 Loss: 0.57954, Train_Acc:79.91%
Epoch [8/300], Step [270/391],                 Loss: 0.58049, Train_Acc:79.88%
Epoch [8/300], Step [280/391],                 Loss: 0.58136, Train_Acc:79.82%
Epoch [8/300], Step [290/391],                 Loss: 0.58235, Train_Acc:79.82%
Epoch [8/300], Step [300/391],                 Loss: 0.58332, Train_Acc:79.73%
Epoch [8/300], Step [310/391],                 Loss: 0.58163, Train_Acc:79.79%
Epoch [8/300], Step [320/391],                 Loss: 0.58273, Train_Acc:79.79%
Epoch [8/300], Step [330/391],                 Loss: 0.58217, Train_Acc:79.80%
Epoch [8/300], Step [340/391],                 Loss: 0.58048, Train_Acc:79.85%
Epoch [8/300], Step [350/391],                 Loss: 0.57796, Train_Acc:79.92%
Epoch [8/300], Step [360/391],                 Loss: 0.57704, Train_Acc:79.95%
Epoch [8/300], Step [370/391],                 Loss: 0.57582, Train_Acc:80.01%
Epoch [8/300], Step [380/391],                 Loss: 0.57671, Train_Acc:80.00%
Epoch [8/300], Step [390/391],                 Loss: 0.57609, Train_Acc:80.03%
Accuary on test images:62.32%
Epoch [9/300], Step [10/391],                 Loss: 0.55966, Train_Acc:80.16%
Epoch [9/300], Step [20/391],                 Loss: 0.53889, Train_Acc:81.17%
Epoch [9/300], Step [30/391],                 Loss: 0.53588, Train_Acc:81.64%
Epoch [9/300], Step [40/391],                 Loss: 0.54629, Train_Acc:81.25%
Epoch [9/300], Step [50/391],                 Loss: 0.55070, Train_Acc:80.88%
Epoch [9/300], Step [60/391],                 Loss: 0.55510, Train_Acc:80.65%
Epoch [9/300], Step [70/391],                 Loss: 0.56066, Train_Acc:80.46%
Epoch [9/300], Step [80/391],                 Loss: 0.56526, Train_Acc:80.31%
Epoch [9/300], Step [90/391],                 Loss: 0.56910, Train_Acc:80.17%
Epoch [9/300], Step [100/391],                 Loss: 0.56832, Train_Acc:80.22%
Epoch [9/300], Step [110/391],                 Loss: 0.57071, Train_Acc:80.11%
Epoch [9/300], Step [120/391],                 Loss: 0.57026, Train_Acc:80.20%
Epoch [9/300], Step [130/391],                 Loss: 0.57024, Train_Acc:80.22%
Epoch [9/300], Step [140/391],                 Loss: 0.56596, Train_Acc:80.41%
Epoch [9/300], Step [150/391],                 Loss: 0.56565, Train_Acc:80.44%
Epoch [9/300], Step [160/391],                 Loss: 0.56479, Train_Acc:80.40%
Epoch [9/300], Step [170/391],                 Loss: 0.56445, Train_Acc:80.43%
Epoch [9/300], Step [180/391],                 Loss: 0.56518, Train_Acc:80.43%
Epoch [9/300], Step [190/391],                 Loss: 0.56488, Train_Acc:80.44%
Epoch [9/300], Step [200/391],                 Loss: 0.56348, Train_Acc:80.50%
Epoch [9/300], Step [210/391],                 Loss: 0.56250, Train_Acc:80.57%
Epoch [9/300], Step [220/391],                 Loss: 0.56325, Train_Acc:80.53%
Epoch [9/300], Step [230/391],                 Loss: 0.56245, Train_Acc:80.52%
Epoch [9/300], Step [240/391],                 Loss: 0.56060, Train_Acc:80.61%
Epoch [9/300], Step [250/391],                 Loss: 0.56078, Train_Acc:80.60%
Epoch [9/300], Step [260/391],                 Loss: 0.56300, Train_Acc:80.54%
Epoch [9/300], Step [270/391],                 Loss: 0.56403, Train_Acc:80.52%
Epoch [9/300], Step [280/391],                 Loss: 0.56323, Train_Acc:80.54%
Epoch [9/300], Step [290/391],                 Loss: 0.56287, Train_Acc:80.59%
Epoch [9/300], Step [300/391],                 Loss: 0.56254, Train_Acc:80.60%
Epoch [9/300], Step [310/391],                 Loss: 0.55967, Train_Acc:80.72%
Epoch [9/300], Step [320/391],                 Loss: 0.55956, Train_Acc:80.69%
Epoch [9/300], Step [330/391],                 Loss: 0.55807, Train_Acc:80.74%
Epoch [9/300], Step [340/391],                 Loss: 0.55638, Train_Acc:80.81%
Epoch [9/300], Step [350/391],                 Loss: 0.55493, Train_Acc:80.86%
Epoch [9/300], Step [360/391],                 Loss: 0.55431, Train_Acc:80.88%
Epoch [9/300], Step [370/391],                 Loss: 0.55292, Train_Acc:80.93%
Epoch [9/300], Step [380/391],                 Loss: 0.55219, Train_Acc:80.97%
Epoch [9/300], Step [390/391],                 Loss: 0.55183, Train_Acc:81.00%
Accuary on test images:75.06%
Epoch [10/300], Step [10/391],                 Loss: 0.55037, Train_Acc:81.02%
Epoch [10/300], Step [20/391],                 Loss: 0.51785, Train_Acc:82.46%
Epoch [10/300], Step [30/391],                 Loss: 0.50804, Train_Acc:82.66%
Epoch [10/300], Step [40/391],                 Loss: 0.50581, Train_Acc:83.01%
Epoch [10/300], Step [50/391],                 Loss: 0.50333, Train_Acc:83.03%
Epoch [10/300], Step [60/391],                 Loss: 0.51157, Train_Acc:82.75%
Epoch [10/300], Step [70/391],                 Loss: 0.51870, Train_Acc:82.49%
Epoch [10/300], Step [80/391],                 Loss: 0.52494, Train_Acc:82.29%
Epoch [10/300], Step [90/391],                 Loss: 0.52768, Train_Acc:82.13%
Epoch [10/300], Step [100/391],                 Loss: 0.53003, Train_Acc:82.03%
Epoch [10/300], Step [110/391],                 Loss: 0.53323, Train_Acc:81.85%
Epoch [10/300], Step [120/391],                 Loss: 0.53281, Train_Acc:81.89%
Epoch [10/300], Step [130/391],                 Loss: 0.53272, Train_Acc:81.86%
Epoch [10/300], Step [140/391],                 Loss: 0.53010, Train_Acc:81.90%
Epoch [10/300], Step [150/391],                 Loss: 0.53053, Train_Acc:81.83%
Epoch [10/300], Step [160/391],                 Loss: 0.52974, Train_Acc:81.91%
Epoch [10/300], Step [170/391],                 Loss: 0.53046, Train_Acc:81.90%
Epoch [10/300], Step [180/391],                 Loss: 0.53235, Train_Acc:81.84%
Epoch [10/300], Step [190/391],                 Loss: 0.53192, Train_Acc:81.81%
Epoch [10/300], Step [200/391],                 Loss: 0.53100, Train_Acc:81.85%
Epoch [10/300], Step [210/391],                 Loss: 0.52991, Train_Acc:81.88%
Epoch [10/300], Step [220/391],                 Loss: 0.53036, Train_Acc:81.86%
Epoch [10/300], Step [230/391],                 Loss: 0.52948, Train_Acc:81.88%
Epoch [10/300], Step [240/391],                 Loss: 0.52706, Train_Acc:81.99%
Epoch [10/300], Step [250/391],                 Loss: 0.52610, Train_Acc:82.03%
Epoch [10/300], Step [260/391],                 Loss: 0.52747, Train_Acc:82.01%
Epoch [10/300], Step [270/391],                 Loss: 0.52789, Train_Acc:81.95%
Epoch [10/300], Step [280/391],                 Loss: 0.52791, Train_Acc:81.94%
Epoch [10/300], Step [290/391],                 Loss: 0.52823, Train_Acc:81.94%
Epoch [10/300], Step [300/391],                 Loss: 0.52808, Train_Acc:81.97%
Epoch [10/300], Step [310/391],                 Loss: 0.52614, Train_Acc:82.04%
Epoch [10/300], Step [320/391],                 Loss: 0.52573, Train_Acc:82.03%
Epoch [10/300], Step [330/391],                 Loss: 0.52519, Train_Acc:82.02%
Epoch [10/300], Step [340/391],                 Loss: 0.52409, Train_Acc:82.07%
Epoch [10/300], Step [350/391],                 Loss: 0.52404, Train_Acc:82.07%
Epoch [10/300], Step [360/391],                 Loss: 0.52417, Train_Acc:82.03%
Epoch [10/300], Step [370/391],                 Loss: 0.52398, Train_Acc:82.04%
Epoch [10/300], Step [380/391],                 Loss: 0.52466, Train_Acc:82.01%
Epoch [10/300], Step [390/391],                 Loss: 0.52407, Train_Acc:82.02%
Accuary on test images:70.42%
Epoch [11/300], Step [10/391],                 Loss: 0.50210, Train_Acc:82.27%
Epoch [11/300], Step [20/391],                 Loss: 0.50119, Train_Acc:82.66%
Epoch [11/300], Step [30/391],                 Loss: 0.49485, Train_Acc:83.12%
Epoch [11/300], Step [40/391],                 Loss: 0.49749, Train_Acc:82.99%
Epoch [11/300], Step [50/391],                 Loss: 0.49513, Train_Acc:82.92%
Epoch [11/300], Step [60/391],                 Loss: 0.49464, Train_Acc:83.07%
Epoch [11/300], Step [70/391],                 Loss: 0.49983, Train_Acc:82.94%
Epoch [11/300], Step [80/391],                 Loss: 0.50112, Train_Acc:82.80%
Epoch [11/300], Step [90/391],                 Loss: 0.50513, Train_Acc:82.71%
Epoch [11/300], Step [100/391],                 Loss: 0.50660, Train_Acc:82.59%
Epoch [11/300], Step [110/391],                 Loss: 0.50795, Train_Acc:82.59%
Epoch [11/300], Step [120/391],                 Loss: 0.50731, Train_Acc:82.61%
Epoch [11/300], Step [130/391],                 Loss: 0.50798, Train_Acc:82.59%
Epoch [11/300], Step [140/391],                 Loss: 0.50290, Train_Acc:82.78%
Epoch [11/300], Step [150/391],                 Loss: 0.50100, Train_Acc:82.87%
Epoch [11/300], Step [160/391],                 Loss: 0.49992, Train_Acc:82.92%
Epoch [11/300], Step [170/391],                 Loss: 0.50177, Train_Acc:82.83%
Epoch [11/300], Step [180/391],                 Loss: 0.50351, Train_Acc:82.76%
Epoch [11/300], Step [190/391],                 Loss: 0.50315, Train_Acc:82.71%
Epoch [11/300], Step [200/391],                 Loss: 0.50350, Train_Acc:82.69%
Epoch [11/300], Step [210/391],                 Loss: 0.50226, Train_Acc:82.75%
Epoch [11/300], Step [220/391],                 Loss: 0.50286, Train_Acc:82.72%
Epoch [11/300], Step [230/391],                 Loss: 0.50199, Train_Acc:82.72%
Epoch [11/300], Step [240/391],                 Loss: 0.50032, Train_Acc:82.80%
Epoch [11/300], Step [250/391],                 Loss: 0.49960, Train_Acc:82.82%
Epoch [11/300], Step [260/391],                 Loss: 0.50212, Train_Acc:82.73%
Epoch [11/300], Step [270/391],                 Loss: 0.50338, Train_Acc:82.67%
Epoch [11/300], Step [280/391],                 Loss: 0.50356, Train_Acc:82.65%
Epoch [11/300], Step [290/391],                 Loss: 0.50451, Train_Acc:82.64%
Epoch [11/300], Step [300/391],                 Loss: 0.50527, Train_Acc:82.61%
Epoch [11/300], Step [310/391],                 Loss: 0.50545, Train_Acc:82.63%
Epoch [11/300], Step [320/391],                 Loss: 0.50720, Train_Acc:82.60%
Epoch [11/300], Step [330/391],                 Loss: 0.50674, Train_Acc:82.62%
Epoch [11/300], Step [340/391],                 Loss: 0.50672, Train_Acc:82.58%
Epoch [11/300], Step [350/391],                 Loss: 0.50645, Train_Acc:82.59%
Epoch [11/300], Step [360/391],                 Loss: 0.50602, Train_Acc:82.59%
Epoch [11/300], Step [370/391],                 Loss: 0.50529, Train_Acc:82.63%
Epoch [11/300], Step [380/391],                 Loss: 0.50501, Train_Acc:82.65%
Epoch [11/300], Step [390/391],                 Loss: 0.50436, Train_Acc:82.69%
Accuary on test images:65.12%
Epoch [12/300], Step [10/391],                 Loss: 0.51333, Train_Acc:82.19%
Epoch [12/300], Step [20/391],                 Loss: 0.48787, Train_Acc:83.32%
Epoch [12/300], Step [30/391],                 Loss: 0.48131, Train_Acc:83.41%
Epoch [12/300], Step [40/391],                 Loss: 0.48530, Train_Acc:83.20%
Epoch [12/300], Step [50/391],                 Loss: 0.48102, Train_Acc:83.25%
Epoch [12/300], Step [60/391],                 Loss: 0.48262, Train_Acc:83.11%
Epoch [12/300], Step [70/391],                 Loss: 0.48610, Train_Acc:83.14%
Epoch [12/300], Step [80/391],                 Loss: 0.48808, Train_Acc:83.18%
Epoch [12/300], Step [90/391],                 Loss: 0.49338, Train_Acc:83.07%
Epoch [12/300], Step [100/391],                 Loss: 0.49032, Train_Acc:83.14%
Epoch [12/300], Step [110/391],                 Loss: 0.49246, Train_Acc:83.11%
Epoch [12/300], Step [120/391],                 Loss: 0.49414, Train_Acc:83.07%
Epoch [12/300], Step [130/391],                 Loss: 0.49668, Train_Acc:83.00%
Epoch [12/300], Step [140/391],                 Loss: 0.49524, Train_Acc:83.09%
Epoch [12/300], Step [150/391],                 Loss: 0.49561, Train_Acc:82.97%
Epoch [12/300], Step [160/391],                 Loss: 0.49340, Train_Acc:83.04%
Epoch [12/300], Step [170/391],                 Loss: 0.49318, Train_Acc:83.02%
Epoch [12/300], Step [180/391],                 Loss: 0.49390, Train_Acc:82.97%
Epoch [12/300], Step [190/391],                 Loss: 0.49438, Train_Acc:82.96%
Epoch [12/300], Step [200/391],                 Loss: 0.49389, Train_Acc:82.97%
Epoch [12/300], Step [210/391],                 Loss: 0.49284, Train_Acc:83.05%
Epoch [12/300], Step [220/391],                 Loss: 0.49272, Train_Acc:83.01%
Epoch [12/300], Step [230/391],                 Loss: 0.49284, Train_Acc:82.97%
Epoch [12/300], Step [240/391],                 Loss: 0.49106, Train_Acc:83.04%
Epoch [12/300], Step [250/391],                 Loss: 0.49113, Train_Acc:83.00%
Epoch [12/300], Step [260/391],                 Loss: 0.49285, Train_Acc:82.94%
Epoch [12/300], Step [270/391],                 Loss: 0.49483, Train_Acc:82.79%
Epoch [12/300], Step [280/391],                 Loss: 0.49497, Train_Acc:82.81%
Epoch [12/300], Step [290/391],                 Loss: 0.49464, Train_Acc:82.85%
Epoch [12/300], Step [300/391],                 Loss: 0.49480, Train_Acc:82.87%
Epoch [12/300], Step [310/391],                 Loss: 0.49244, Train_Acc:82.94%
Epoch [12/300], Step [320/391],                 Loss: 0.49233, Train_Acc:82.97%
Epoch [12/300], Step [330/391],                 Loss: 0.49062, Train_Acc:83.03%
Epoch [12/300], Step [340/391],                 Loss: 0.48940, Train_Acc:83.07%
Epoch [12/300], Step [350/391],                 Loss: 0.48778, Train_Acc:83.17%
Epoch [12/300], Step [360/391],                 Loss: 0.48674, Train_Acc:83.19%
Epoch [12/300], Step [370/391],                 Loss: 0.48644, Train_Acc:83.19%
Epoch [12/300], Step [380/391],                 Loss: 0.48594, Train_Acc:83.19%
Epoch [12/300], Step [390/391],                 Loss: 0.48644, Train_Acc:83.20%
Accuary on test images:75.18%
Epoch [13/300], Step [10/391],                 Loss: 0.49721, Train_Acc:83.91%
Epoch [13/300], Step [20/391],                 Loss: 0.47245, Train_Acc:84.06%
Epoch [13/300], Step [30/391],                 Loss: 0.45727, Train_Acc:84.90%
Epoch [13/300], Step [40/391],                 Loss: 0.46197, Train_Acc:84.61%
Epoch [13/300], Step [50/391],                 Loss: 0.46058, Train_Acc:84.38%
Epoch [13/300], Step [60/391],                 Loss: 0.46040, Train_Acc:84.32%
Epoch [13/300], Step [70/391],                 Loss: 0.46564, Train_Acc:84.27%
Epoch [13/300], Step [80/391],                 Loss: 0.46884, Train_Acc:84.09%
Epoch [13/300], Step [90/391],                 Loss: 0.47440, Train_Acc:83.81%
Epoch [13/300], Step [100/391],                 Loss: 0.47546, Train_Acc:83.70%
Epoch [13/300], Step [110/391],                 Loss: 0.47933, Train_Acc:83.64%
Epoch [13/300], Step [120/391],                 Loss: 0.47987, Train_Acc:83.63%
Epoch [13/300], Step [130/391],                 Loss: 0.48594, Train_Acc:83.37%
Epoch [13/300], Step [140/391],                 Loss: 0.48355, Train_Acc:83.47%
Epoch [13/300], Step [150/391],                 Loss: 0.48572, Train_Acc:83.39%
Epoch [13/300], Step [160/391],                 Loss: 0.48400, Train_Acc:83.42%
Epoch [13/300], Step [170/391],                 Loss: 0.48363, Train_Acc:83.41%
Epoch [13/300], Step [180/391],                 Loss: 0.48354, Train_Acc:83.44%
Epoch [13/300], Step [190/391],                 Loss: 0.48468, Train_Acc:83.40%
Epoch [13/300], Step [200/391],                 Loss: 0.48491, Train_Acc:83.39%
Epoch [13/300], Step [210/391],                 Loss: 0.48364, Train_Acc:83.42%
Epoch [13/300], Step [220/391],                 Loss: 0.48210, Train_Acc:83.47%
Epoch [13/300], Step [230/391],                 Loss: 0.48018, Train_Acc:83.55%
Epoch [13/300], Step [240/391],                 Loss: 0.47794, Train_Acc:83.65%
Epoch [13/300], Step [250/391],                 Loss: 0.47767, Train_Acc:83.65%
Epoch [13/300], Step [260/391],                 Loss: 0.47928, Train_Acc:83.61%
Epoch [13/300], Step [270/391],                 Loss: 0.47991, Train_Acc:83.54%
Epoch [13/300], Step [280/391],                 Loss: 0.48061, Train_Acc:83.54%
Epoch [13/300], Step [290/391],                 Loss: 0.48083, Train_Acc:83.54%
Epoch [13/300], Step [300/391],                 Loss: 0.48106, Train_Acc:83.56%
Epoch [13/300], Step [310/391],                 Loss: 0.48016, Train_Acc:83.57%
Epoch [13/300], Step [320/391],                 Loss: 0.48040, Train_Acc:83.58%
Epoch [13/300], Step [330/391],                 Loss: 0.47909, Train_Acc:83.60%
Epoch [13/300], Step [340/391],                 Loss: 0.47697, Train_Acc:83.69%
Epoch [13/300], Step [350/391],                 Loss: 0.47641, Train_Acc:83.71%
Epoch [13/300], Step [360/391],                 Loss: 0.47676, Train_Acc:83.68%
Epoch [13/300], Step [370/391],                 Loss: 0.47687, Train_Acc:83.68%
Epoch [13/300], Step [380/391],                 Loss: 0.47726, Train_Acc:83.66%
Epoch [13/300], Step [390/391],                 Loss: 0.47656, Train_Acc:83.67%
Accuary on test images:74.42%
Epoch [14/300], Step [10/391],                 Loss: 0.46537, Train_Acc:83.91%
Epoch [14/300], Step [20/391],                 Loss: 0.45727, Train_Acc:83.63%
Epoch [14/300], Step [30/391],                 Loss: 0.44954, Train_Acc:83.96%
Epoch [14/300], Step [40/391],                 Loss: 0.45475, Train_Acc:84.08%
Epoch [14/300], Step [50/391],                 Loss: 0.45193, Train_Acc:84.34%
Epoch [14/300], Step [60/391],                 Loss: 0.45597, Train_Acc:84.11%
Epoch [14/300], Step [70/391],                 Loss: 0.45845, Train_Acc:84.10%
Epoch [14/300], Step [80/391],                 Loss: 0.46259, Train_Acc:83.96%
Epoch [14/300], Step [90/391],                 Loss: 0.46424, Train_Acc:83.96%
Epoch [14/300], Step [100/391],                 Loss: 0.46608, Train_Acc:83.99%
Epoch [14/300], Step [110/391],                 Loss: 0.46953, Train_Acc:83.87%
Epoch [14/300], Step [120/391],                 Loss: 0.46788, Train_Acc:83.88%
Epoch [14/300], Step [130/391],                 Loss: 0.46988, Train_Acc:83.88%
Epoch [14/300], Step [140/391],                 Loss: 0.46561, Train_Acc:84.03%
Epoch [14/300], Step [150/391],                 Loss: 0.46611, Train_Acc:84.01%
Epoch [14/300], Step [160/391],                 Loss: 0.46501, Train_Acc:84.03%
Epoch [14/300], Step [170/391],                 Loss: 0.46681, Train_Acc:83.98%
Epoch [14/300], Step [180/391],                 Loss: 0.46935, Train_Acc:83.88%
Epoch [14/300], Step [190/391],                 Loss: 0.46903, Train_Acc:83.92%
Epoch [14/300], Step [200/391],                 Loss: 0.46815, Train_Acc:83.89%
Epoch [14/300], Step [210/391],                 Loss: 0.46707, Train_Acc:83.92%
Epoch [14/300], Step [220/391],                 Loss: 0.46648, Train_Acc:83.94%
Epoch [14/300], Step [230/391],                 Loss: 0.46599, Train_Acc:83.94%
Epoch [14/300], Step [240/391],                 Loss: 0.46357, Train_Acc:84.01%
Epoch [14/300], Step [250/391],                 Loss: 0.46214, Train_Acc:84.05%
Epoch [14/300], Step [260/391],                 Loss: 0.46318, Train_Acc:84.04%
Epoch [14/300], Step [270/391],                 Loss: 0.46430, Train_Acc:84.01%
Epoch [14/300], Step [280/391],                 Loss: 0.46273, Train_Acc:84.04%
Epoch [14/300], Step [290/391],                 Loss: 0.46230, Train_Acc:84.10%
Epoch [14/300], Step [300/391],                 Loss: 0.46222, Train_Acc:84.11%
Epoch [14/300], Step [310/391],                 Loss: 0.46264, Train_Acc:84.06%
Epoch [14/300], Step [320/391],                 Loss: 0.46361, Train_Acc:84.02%
Epoch [14/300], Step [330/391],                 Loss: 0.46329, Train_Acc:84.03%
Epoch [14/300], Step [340/391],                 Loss: 0.46396, Train_Acc:83.99%
Epoch [14/300], Step [350/391],                 Loss: 0.46440, Train_Acc:83.99%
Epoch [14/300], Step [360/391],                 Loss: 0.46430, Train_Acc:83.97%
Epoch [14/300], Step [370/391],                 Loss: 0.46450, Train_Acc:83.97%
Epoch [14/300], Step [380/391],                 Loss: 0.46428, Train_Acc:83.96%
Epoch [14/300], Step [390/391],                 Loss: 0.46437, Train_Acc:83.97%
Accuary on test images:71.26%
Epoch [15/300], Step [10/391],                 Loss: 0.46627, Train_Acc:84.14%
Epoch [15/300], Step [20/391],                 Loss: 0.45070, Train_Acc:84.65%
Epoch [15/300], Step [30/391],                 Loss: 0.45419, Train_Acc:84.58%
Epoch [15/300], Step [40/391],                 Loss: 0.45956, Train_Acc:84.59%
Epoch [15/300], Step [50/391],                 Loss: 0.45498, Train_Acc:84.86%
Epoch [15/300], Step [60/391],                 Loss: 0.45403, Train_Acc:84.62%
Epoch [15/300], Step [70/391],                 Loss: 0.45351, Train_Acc:84.62%
Epoch [15/300], Step [80/391],                 Loss: 0.45024, Train_Acc:84.73%
Epoch [15/300], Step [90/391],                 Loss: 0.45431, Train_Acc:84.41%
Epoch [15/300], Step [100/391],                 Loss: 0.45397, Train_Acc:84.40%
Epoch [15/300], Step [110/391],                 Loss: 0.45753, Train_Acc:84.32%
Epoch [15/300], Step [120/391],                 Loss: 0.46084, Train_Acc:84.13%
Epoch [15/300], Step [130/391],                 Loss: 0.46183, Train_Acc:84.10%
Epoch [15/300], Step [140/391],                 Loss: 0.45892, Train_Acc:84.24%
Epoch [15/300], Step [150/391],                 Loss: 0.45799, Train_Acc:84.26%
Epoch [15/300], Step [160/391],                 Loss: 0.45740, Train_Acc:84.30%
Epoch [15/300], Step [170/391],                 Loss: 0.45758, Train_Acc:84.32%
Epoch [15/300], Step [180/391],                 Loss: 0.45794, Train_Acc:84.36%
Epoch [15/300], Step [190/391],                 Loss: 0.45673, Train_Acc:84.38%
Epoch [15/300], Step [200/391],                 Loss: 0.45473, Train_Acc:84.41%
Epoch [15/300], Step [210/391],                 Loss: 0.45424, Train_Acc:84.40%
Epoch [15/300], Step [220/391],                 Loss: 0.45337, Train_Acc:84.41%
Epoch [15/300], Step [230/391],                 Loss: 0.45410, Train_Acc:84.38%
Epoch [15/300], Step [240/391],                 Loss: 0.45265, Train_Acc:84.44%
Epoch [15/300], Step [250/391],                 Loss: 0.45265, Train_Acc:84.42%
Epoch [15/300], Step [260/391],                 Loss: 0.45506, Train_Acc:84.39%
Epoch [15/300], Step [270/391],                 Loss: 0.45631, Train_Acc:84.33%
Epoch [15/300], Step [280/391],                 Loss: 0.45582, Train_Acc:84.35%
Epoch [15/300], Step [290/391],                 Loss: 0.45572, Train_Acc:84.38%
Epoch [15/300], Step [300/391],                 Loss: 0.45607, Train_Acc:84.34%
Epoch [15/300], Step [310/391],                 Loss: 0.45522, Train_Acc:84.39%
Epoch [15/300], Step [320/391],                 Loss: 0.45601, Train_Acc:84.38%
Epoch [15/300], Step [330/391],                 Loss: 0.45494, Train_Acc:84.46%
Epoch [15/300], Step [340/391],                 Loss: 0.45337, Train_Acc:84.52%
Epoch [15/300], Step [350/391],                 Loss: 0.45339, Train_Acc:84.53%
Epoch [15/300], Step [360/391],                 Loss: 0.45350, Train_Acc:84.51%
Epoch [15/300], Step [370/391],                 Loss: 0.45369, Train_Acc:84.48%
Epoch [15/300], Step [380/391],                 Loss: 0.45376, Train_Acc:84.47%
Epoch [15/300], Step [390/391],                 Loss: 0.45303, Train_Acc:84.49%
Accuary on test images:76.50%
Epoch [16/300], Step [10/391],                 Loss: 0.47318, Train_Acc:83.36%
Epoch [16/300], Step [20/391],                 Loss: 0.46021, Train_Acc:84.18%
Epoch [16/300], Step [30/391],                 Loss: 0.45583, Train_Acc:84.61%
Epoch [16/300], Step [40/391],                 Loss: 0.45672, Train_Acc:84.67%
Epoch [16/300], Step [50/391],                 Loss: 0.45526, Train_Acc:84.77%
Epoch [16/300], Step [60/391],                 Loss: 0.45532, Train_Acc:84.65%
Epoch [16/300], Step [70/391],                 Loss: 0.45571, Train_Acc:84.62%
Epoch [16/300], Step [80/391],                 Loss: 0.45596, Train_Acc:84.73%
Epoch [16/300], Step [90/391],                 Loss: 0.45773, Train_Acc:84.64%
Epoch [16/300], Step [100/391],                 Loss: 0.45774, Train_Acc:84.45%
Epoch [16/300], Step [110/391],                 Loss: 0.46238, Train_Acc:84.33%
Epoch [16/300], Step [120/391],                 Loss: 0.46687, Train_Acc:84.10%
Epoch [16/300], Step [130/391],                 Loss: 0.47063, Train_Acc:84.00%
Epoch [16/300], Step [140/391],                 Loss: 0.46851, Train_Acc:84.14%
Epoch [16/300], Step [150/391],                 Loss: 0.46810, Train_Acc:84.08%
Epoch [16/300], Step [160/391],                 Loss: 0.46664, Train_Acc:84.07%
Epoch [16/300], Step [170/391],                 Loss: 0.46584, Train_Acc:84.05%
Epoch [16/300], Step [180/391],                 Loss: 0.46523, Train_Acc:84.11%
Epoch [16/300], Step [190/391],                 Loss: 0.46459, Train_Acc:84.13%
Epoch [16/300], Step [200/391],                 Loss: 0.46326, Train_Acc:84.11%
Epoch [16/300], Step [210/391],                 Loss: 0.46273, Train_Acc:84.13%
Epoch [16/300], Step [220/391],                 Loss: 0.46218, Train_Acc:84.17%
Epoch [16/300], Step [230/391],                 Loss: 0.46094, Train_Acc:84.22%
Epoch [16/300], Step [240/391],                 Loss: 0.45878, Train_Acc:84.29%
Epoch [16/300], Step [250/391],                 Loss: 0.46030, Train_Acc:84.22%
Epoch [16/300], Step [260/391],                 Loss: 0.46293, Train_Acc:84.16%
Epoch [16/300], Step [270/391],                 Loss: 0.46374, Train_Acc:84.11%
Epoch [16/300], Step [280/391],                 Loss: 0.46302, Train_Acc:84.15%
Epoch [16/300], Step [290/391],                 Loss: 0.46185, Train_Acc:84.19%
Epoch [16/300], Step [300/391],                 Loss: 0.46045, Train_Acc:84.24%
Epoch [16/300], Step [310/391],                 Loss: 0.45805, Train_Acc:84.31%
Epoch [16/300], Step [320/391],                 Loss: 0.45855, Train_Acc:84.30%
Epoch [16/300], Step [330/391],                 Loss: 0.45732, Train_Acc:84.34%
Epoch [16/300], Step [340/391],                 Loss: 0.45584, Train_Acc:84.40%
Epoch [16/300], Step [350/391],                 Loss: 0.45509, Train_Acc:84.44%
Epoch [16/300], Step [360/391],                 Loss: 0.45466, Train_Acc:84.43%
Epoch [16/300], Step [370/391],                 Loss: 0.45420, Train_Acc:84.45%
Epoch [16/300], Step [380/391],                 Loss: 0.45397, Train_Acc:84.44%
Epoch [16/300], Step [390/391],                 Loss: 0.45265, Train_Acc:84.47%
Accuary on test images:76.60%
Epoch [17/300], Step [10/391],                 Loss: 0.43343, Train_Acc:84.92%
Epoch [17/300], Step [20/391],                 Loss: 0.42842, Train_Acc:84.80%
Epoch [17/300], Step [30/391],                 Loss: 0.43215, Train_Acc:84.58%
Epoch [17/300], Step [40/391],                 Loss: 0.43374, Train_Acc:84.96%
Epoch [17/300], Step [50/391],                 Loss: 0.42940, Train_Acc:85.02%
Epoch [17/300], Step [60/391],                 Loss: 0.43662, Train_Acc:84.74%
Epoch [17/300], Step [70/391],                 Loss: 0.43659, Train_Acc:84.94%
Epoch [17/300], Step [80/391],                 Loss: 0.43641, Train_Acc:85.01%
Epoch [17/300], Step [90/391],                 Loss: 0.44015, Train_Acc:85.01%
Epoch [17/300], Step [100/391],                 Loss: 0.44054, Train_Acc:84.95%
Epoch [17/300], Step [110/391],                 Loss: 0.44002, Train_Acc:84.99%
Epoch [17/300], Step [120/391],                 Loss: 0.44133, Train_Acc:84.95%
Epoch [17/300], Step [130/391],                 Loss: 0.44320, Train_Acc:84.93%
Epoch [17/300], Step [140/391],                 Loss: 0.44127, Train_Acc:84.96%
Epoch [17/300], Step [150/391],                 Loss: 0.44037, Train_Acc:84.95%
Epoch [17/300], Step [160/391],                 Loss: 0.43729, Train_Acc:85.04%
Epoch [17/300], Step [170/391],                 Loss: 0.43753, Train_Acc:85.05%
Epoch [17/300], Step [180/391],                 Loss: 0.43781, Train_Acc:84.98%
Epoch [17/300], Step [190/391],                 Loss: 0.43779, Train_Acc:84.97%
Epoch [17/300], Step [200/391],                 Loss: 0.43870, Train_Acc:84.89%
Epoch [17/300], Step [210/391],                 Loss: 0.43754, Train_Acc:84.94%
Epoch [17/300], Step [220/391],                 Loss: 0.43624, Train_Acc:84.98%
Epoch [17/300], Step [230/391],                 Loss: 0.43518, Train_Acc:84.98%
Epoch [17/300], Step [240/391],                 Loss: 0.43351, Train_Acc:85.06%
Epoch [17/300], Step [250/391],                 Loss: 0.43479, Train_Acc:84.98%
Epoch [17/300], Step [260/391],                 Loss: 0.43683, Train_Acc:84.92%
Epoch [17/300], Step [270/391],                 Loss: 0.43856, Train_Acc:84.87%
Epoch [17/300], Step [280/391],                 Loss: 0.43907, Train_Acc:84.86%
Epoch [17/300], Step [290/391],                 Loss: 0.43947, Train_Acc:84.87%
Epoch [17/300], Step [300/391],                 Loss: 0.43824, Train_Acc:84.91%
Epoch [17/300], Step [310/391],                 Loss: 0.43685, Train_Acc:84.95%
Epoch [17/300], Step [320/391],                 Loss: 0.43772, Train_Acc:84.91%
Epoch [17/300], Step [330/391],                 Loss: 0.43653, Train_Acc:84.96%
Epoch [17/300], Step [340/391],                 Loss: 0.43535, Train_Acc:85.00%
Epoch [17/300], Step [350/391],                 Loss: 0.43546, Train_Acc:85.00%
Epoch [17/300], Step [360/391],                 Loss: 0.43585, Train_Acc:84.98%
Epoch [17/300], Step [370/391],                 Loss: 0.43498, Train_Acc:84.99%
Epoch [17/300], Step [380/391],                 Loss: 0.43454, Train_Acc:84.99%
Epoch [17/300], Step [390/391],                 Loss: 0.43371, Train_Acc:85.01%
Accuary on test images:72.98%
Epoch [18/300], Step [10/391],                 Loss: 0.45160, Train_Acc:84.22%
Epoch [18/300], Step [20/391],                 Loss: 0.44862, Train_Acc:84.38%
Epoch [18/300], Step [30/391],                 Loss: 0.44062, Train_Acc:84.71%
Epoch [18/300], Step [40/391],                 Loss: 0.44462, Train_Acc:84.86%
Epoch [18/300], Step [50/391],                 Loss: 0.44341, Train_Acc:84.69%
Epoch [18/300], Step [60/391],                 Loss: 0.44612, Train_Acc:84.54%
Epoch [18/300], Step [70/391],                 Loss: 0.43812, Train_Acc:84.84%
Epoch [18/300], Step [80/391],                 Loss: 0.43458, Train_Acc:85.05%
Epoch [18/300], Step [90/391],                 Loss: 0.43629, Train_Acc:85.14%
Epoch [18/300], Step [100/391],                 Loss: 0.43395, Train_Acc:85.21%
Epoch [18/300], Step [110/391],                 Loss: 0.43592, Train_Acc:85.27%
Epoch [18/300], Step [120/391],                 Loss: 0.43790, Train_Acc:85.16%
Epoch [18/300], Step [130/391],                 Loss: 0.44068, Train_Acc:85.12%
Epoch [18/300], Step [140/391],                 Loss: 0.43703, Train_Acc:85.27%
Epoch [18/300], Step [150/391],                 Loss: 0.43839, Train_Acc:85.18%
Epoch [18/300], Step [160/391],                 Loss: 0.43814, Train_Acc:85.21%
Epoch [18/300], Step [170/391],                 Loss: 0.43952, Train_Acc:85.08%
Epoch [18/300], Step [180/391],                 Loss: 0.43949, Train_Acc:85.06%
Epoch [18/300], Step [190/391],                 Loss: 0.44036, Train_Acc:84.98%
Epoch [18/300], Step [200/391],                 Loss: 0.44015, Train_Acc:84.93%
Epoch [18/300], Step [210/391],                 Loss: 0.43888, Train_Acc:85.00%
Epoch [18/300], Step [220/391],                 Loss: 0.43783, Train_Acc:85.05%
Epoch [18/300], Step [230/391],                 Loss: 0.43508, Train_Acc:85.12%
Epoch [18/300], Step [240/391],                 Loss: 0.43352, Train_Acc:85.16%
Epoch [18/300], Step [250/391],                 Loss: 0.43459, Train_Acc:85.14%
Epoch [18/300], Step [260/391],                 Loss: 0.43820, Train_Acc:85.06%
Epoch [18/300], Step [270/391],                 Loss: 0.43923, Train_Acc:84.97%
Epoch [18/300], Step [280/391],                 Loss: 0.43888, Train_Acc:85.01%
Epoch [18/300], Step [290/391],                 Loss: 0.43870, Train_Acc:85.01%
Epoch [18/300], Step [300/391],                 Loss: 0.43893, Train_Acc:84.98%
Epoch [18/300], Step [310/391],                 Loss: 0.43819, Train_Acc:85.01%
Epoch [18/300], Step [320/391],                 Loss: 0.43845, Train_Acc:85.00%
Epoch [18/300], Step [330/391],                 Loss: 0.43824, Train_Acc:85.03%
Epoch [18/300], Step [340/391],                 Loss: 0.43699, Train_Acc:85.10%
Epoch [18/300], Step [350/391],                 Loss: 0.43681, Train_Acc:85.13%
Epoch [18/300], Step [360/391],                 Loss: 0.43627, Train_Acc:85.13%
Epoch [18/300], Step [370/391],                 Loss: 0.43561, Train_Acc:85.14%
Epoch [18/300], Step [380/391],                 Loss: 0.43445, Train_Acc:85.17%
Epoch [18/300], Step [390/391],                 Loss: 0.43373, Train_Acc:85.20%
Accuary on test images:74.74%
Epoch [19/300], Step [10/391],                 Loss: 0.44025, Train_Acc:85.08%
Epoch [19/300], Step [20/391],                 Loss: 0.44321, Train_Acc:84.96%
Epoch [19/300], Step [30/391],                 Loss: 0.43587, Train_Acc:85.36%
Epoch [19/300], Step [40/391],                 Loss: 0.43845, Train_Acc:85.45%
Epoch [19/300], Step [50/391],                 Loss: 0.43476, Train_Acc:85.39%
Epoch [19/300], Step [60/391],                 Loss: 0.43607, Train_Acc:85.30%
Epoch [19/300], Step [70/391],                 Loss: 0.43157, Train_Acc:85.42%
Epoch [19/300], Step [80/391],                 Loss: 0.42755, Train_Acc:85.53%
Epoch [19/300], Step [90/391],                 Loss: 0.42919, Train_Acc:85.47%
Epoch [19/300], Step [100/391],                 Loss: 0.43058, Train_Acc:85.43%
Epoch [19/300], Step [110/391],                 Loss: 0.43323, Train_Acc:85.31%
Epoch [19/300], Step [120/391],                 Loss: 0.43122, Train_Acc:85.37%
Epoch [19/300], Step [130/391],                 Loss: 0.43214, Train_Acc:85.38%
Epoch [19/300], Step [140/391],                 Loss: 0.43081, Train_Acc:85.50%
Epoch [19/300], Step [150/391],                 Loss: 0.43230, Train_Acc:85.43%
Epoch [19/300], Step [160/391],                 Loss: 0.43240, Train_Acc:85.38%
Epoch [19/300], Step [170/391],                 Loss: 0.43409, Train_Acc:85.30%
Epoch [19/300], Step [180/391],                 Loss: 0.43258, Train_Acc:85.36%
Epoch [19/300], Step [190/391],                 Loss: 0.43207, Train_Acc:85.32%
Epoch [19/300], Step [200/391],                 Loss: 0.43213, Train_Acc:85.30%
Epoch [19/300], Step [210/391],                 Loss: 0.43109, Train_Acc:85.32%
Epoch [19/300], Step [220/391],                 Loss: 0.43135, Train_Acc:85.31%
Epoch [19/300], Step [230/391],                 Loss: 0.42854, Train_Acc:85.46%
Epoch [19/300], Step [240/391],                 Loss: 0.42648, Train_Acc:85.50%
Epoch [19/300], Step [250/391],                 Loss: 0.42632, Train_Acc:85.46%
Epoch [19/300], Step [260/391],                 Loss: 0.42997, Train_Acc:85.36%
Epoch [19/300], Step [270/391],                 Loss: 0.43178, Train_Acc:85.29%
Epoch [19/300], Step [280/391],                 Loss: 0.43299, Train_Acc:85.25%
Epoch [19/300], Step [290/391],                 Loss: 0.43307, Train_Acc:85.24%
Epoch [19/300], Step [300/391],                 Loss: 0.43305, Train_Acc:85.22%
Epoch [19/300], Step [310/391],                 Loss: 0.43260, Train_Acc:85.24%
Epoch [19/300], Step [320/391],                 Loss: 0.43368, Train_Acc:85.22%
Epoch [19/300], Step [330/391],                 Loss: 0.43259, Train_Acc:85.28%
Epoch [19/300], Step [340/391],                 Loss: 0.43142, Train_Acc:85.32%
Epoch [19/300], Step [350/391],                 Loss: 0.43136, Train_Acc:85.32%
Epoch [19/300], Step [360/391],                 Loss: 0.43212, Train_Acc:85.29%
Epoch [19/300], Step [370/391],                 Loss: 0.43257, Train_Acc:85.27%
Epoch [19/300], Step [380/391],                 Loss: 0.43290, Train_Acc:85.26%
Epoch [19/300], Step [390/391],                 Loss: 0.43204, Train_Acc:85.29%
Accuary on test images:77.48%
Epoch [20/300], Step [10/391],                 Loss: 0.42387, Train_Acc:84.92%
Epoch [20/300], Step [20/391],                 Loss: 0.42426, Train_Acc:85.08%
Epoch [20/300], Step [30/391],                 Loss: 0.41162, Train_Acc:85.44%
Epoch [20/300], Step [40/391],                 Loss: 0.40811, Train_Acc:85.68%
Epoch [20/300], Step [50/391],                 Loss: 0.40477, Train_Acc:85.70%
Epoch [20/300], Step [60/391],                 Loss: 0.40913, Train_Acc:85.72%
Epoch [20/300], Step [70/391],                 Loss: 0.40771, Train_Acc:85.84%
Epoch [20/300], Step [80/391],                 Loss: 0.41041, Train_Acc:85.70%
Epoch [20/300], Step [90/391],                 Loss: 0.41711, Train_Acc:85.48%
Epoch [20/300], Step [100/391],                 Loss: 0.41742, Train_Acc:85.43%
Epoch [20/300], Step [110/391],                 Loss: 0.42012, Train_Acc:85.40%
Epoch [20/300], Step [120/391],                 Loss: 0.42351, Train_Acc:85.23%
Epoch [20/300], Step [130/391],                 Loss: 0.42618, Train_Acc:85.18%
Epoch [20/300], Step [140/391],                 Loss: 0.42564, Train_Acc:85.23%
Epoch [20/300], Step [150/391],                 Loss: 0.42611, Train_Acc:85.22%
Epoch [20/300], Step [160/391],                 Loss: 0.42399, Train_Acc:85.31%
Epoch [20/300], Step [170/391],                 Loss: 0.42360, Train_Acc:85.30%
Epoch [20/300], Step [180/391],                 Loss: 0.42440, Train_Acc:85.32%
Epoch [20/300], Step [190/391],                 Loss: 0.42419, Train_Acc:85.33%
Epoch [20/300], Step [200/391],                 Loss: 0.42355, Train_Acc:85.36%
Epoch [20/300], Step [210/391],                 Loss: 0.42204, Train_Acc:85.43%
Epoch [20/300], Step [220/391],                 Loss: 0.42155, Train_Acc:85.47%
Epoch [20/300], Step [230/391],                 Loss: 0.42072, Train_Acc:85.51%
Epoch [20/300], Step [240/391],                 Loss: 0.41911, Train_Acc:85.53%
Epoch [20/300], Step [250/391],                 Loss: 0.41966, Train_Acc:85.48%
Epoch [20/300], Step [260/391],                 Loss: 0.42113, Train_Acc:85.50%
Epoch [20/300], Step [270/391],                 Loss: 0.42164, Train_Acc:85.48%
Epoch [20/300], Step [280/391],                 Loss: 0.42182, Train_Acc:85.46%
Epoch [20/300], Step [290/391],                 Loss: 0.42148, Train_Acc:85.51%
Epoch [20/300], Step [300/391],                 Loss: 0.42124, Train_Acc:85.52%
Epoch [20/300], Step [310/391],                 Loss: 0.42029, Train_Acc:85.58%
Epoch [20/300], Step [320/391],                 Loss: 0.42176, Train_Acc:85.56%
Epoch [20/300], Step [330/391],                 Loss: 0.42113, Train_Acc:85.57%
Epoch [20/300], Step [340/391],                 Loss: 0.42078, Train_Acc:85.56%
Epoch [20/300], Step [350/391],                 Loss: 0.42049, Train_Acc:85.58%
Epoch [20/300], Step [360/391],                 Loss: 0.42073, Train_Acc:85.52%
Epoch [20/300], Step [370/391],                 Loss: 0.42130, Train_Acc:85.51%
Epoch [20/300], Step [380/391],                 Loss: 0.42239, Train_Acc:85.48%
Epoch [20/300], Step [390/391],                 Loss: 0.42229, Train_Acc:85.48%
Accuary on test images:73.86%
Epoch [21/300], Step [10/391],                 Loss: 0.44660, Train_Acc:85.08%
Epoch [21/300], Step [20/391],                 Loss: 0.41855, Train_Acc:85.74%
Epoch [21/300], Step [30/391],                 Loss: 0.41192, Train_Acc:86.12%
Epoch [21/300], Step [40/391],                 Loss: 0.41525, Train_Acc:86.31%
Epoch [21/300], Step [50/391],                 Loss: 0.41293, Train_Acc:86.22%
Epoch [21/300], Step [60/391],                 Loss: 0.41944, Train_Acc:85.90%
Epoch [21/300], Step [70/391],                 Loss: 0.42034, Train_Acc:85.85%
Epoch [21/300], Step [80/391],                 Loss: 0.41948, Train_Acc:85.70%
Epoch [21/300], Step [90/391],                 Loss: 0.42247, Train_Acc:85.54%
Epoch [21/300], Step [100/391],                 Loss: 0.42130, Train_Acc:85.49%
Epoch [21/300], Step [110/391],                 Loss: 0.42215, Train_Acc:85.44%
Epoch [21/300], Step [120/391],                 Loss: 0.42182, Train_Acc:85.43%
Epoch [21/300], Step [130/391],                 Loss: 0.42229, Train_Acc:85.49%
Epoch [21/300], Step [140/391],                 Loss: 0.41984, Train_Acc:85.61%
Epoch [21/300], Step [150/391],                 Loss: 0.41987, Train_Acc:85.65%
Epoch [21/300], Step [160/391],                 Loss: 0.41838, Train_Acc:85.64%
Epoch [21/300], Step [170/391],                 Loss: 0.41842, Train_Acc:85.65%
Epoch [21/300], Step [180/391],                 Loss: 0.41716, Train_Acc:85.67%
Epoch [21/300], Step [190/391],                 Loss: 0.41711, Train_Acc:85.64%
Epoch [21/300], Step [200/391],                 Loss: 0.41994, Train_Acc:85.55%
Epoch [21/300], Step [210/391],                 Loss: 0.42084, Train_Acc:85.51%
Epoch [21/300], Step [220/391],                 Loss: 0.42147, Train_Acc:85.52%
Epoch [21/300], Step [230/391],                 Loss: 0.42086, Train_Acc:85.50%
Epoch [21/300], Step [240/391],                 Loss: 0.41862, Train_Acc:85.60%
Epoch [21/300], Step [250/391],                 Loss: 0.41955, Train_Acc:85.56%
Epoch [21/300], Step [260/391],                 Loss: 0.42272, Train_Acc:85.48%
Epoch [21/300], Step [270/391],                 Loss: 0.42383, Train_Acc:85.43%
Epoch [21/300], Step [280/391],                 Loss: 0.42383, Train_Acc:85.42%
Epoch [21/300], Step [290/391],                 Loss: 0.42440, Train_Acc:85.42%
Epoch [21/300], Step [300/391],                 Loss: 0.42408, Train_Acc:85.43%
Epoch [21/300], Step [310/391],                 Loss: 0.42265, Train_Acc:85.45%
Epoch [21/300], Step [320/391],                 Loss: 0.42248, Train_Acc:85.48%
Epoch [21/300], Step [330/391],                 Loss: 0.42197, Train_Acc:85.52%
Epoch [21/300], Step [340/391],                 Loss: 0.42085, Train_Acc:85.56%
Epoch [21/300], Step [350/391],                 Loss: 0.41966, Train_Acc:85.58%
Epoch [21/300], Step [360/391],                 Loss: 0.41987, Train_Acc:85.56%
Epoch [21/300], Step [370/391],                 Loss: 0.41992, Train_Acc:85.56%
Epoch [21/300], Step [380/391],                 Loss: 0.41993, Train_Acc:85.56%
Epoch [21/300], Step [390/391],                 Loss: 0.41950, Train_Acc:85.59%
Accuary on test images:72.62%
Epoch [22/300], Step [10/391],                 Loss: 0.40841, Train_Acc:85.47%
Epoch [22/300], Step [20/391],                 Loss: 0.40045, Train_Acc:86.21%
Epoch [22/300], Step [30/391],                 Loss: 0.40331, Train_Acc:86.25%
Epoch [22/300], Step [40/391],                 Loss: 0.40743, Train_Acc:86.17%
Epoch [22/300], Step [50/391],                 Loss: 0.40786, Train_Acc:86.17%
Epoch [22/300], Step [60/391],                 Loss: 0.41784, Train_Acc:85.69%
Epoch [22/300], Step [70/391],                 Loss: 0.42174, Train_Acc:85.50%
Epoch [22/300], Step [80/391],                 Loss: 0.41970, Train_Acc:85.52%
Epoch [22/300], Step [90/391],                 Loss: 0.42339, Train_Acc:85.27%
Epoch [22/300], Step [100/391],                 Loss: 0.42177, Train_Acc:85.30%
Epoch [22/300], Step [110/391],                 Loss: 0.42436, Train_Acc:85.20%
Epoch [22/300], Step [120/391],                 Loss: 0.42400, Train_Acc:85.16%
Epoch [22/300], Step [130/391],                 Loss: 0.42460, Train_Acc:85.17%
Epoch [22/300], Step [140/391],                 Loss: 0.42293, Train_Acc:85.24%
Epoch [22/300], Step [150/391],                 Loss: 0.42123, Train_Acc:85.33%
Epoch [22/300], Step [160/391],                 Loss: 0.41881, Train_Acc:85.39%
Epoch [22/300], Step [170/391],                 Loss: 0.41769, Train_Acc:85.47%
Epoch [22/300], Step [180/391],                 Loss: 0.41813, Train_Acc:85.42%
Epoch [22/300], Step [190/391],                 Loss: 0.41804, Train_Acc:85.38%
Epoch [22/300], Step [200/391],                 Loss: 0.41848, Train_Acc:85.38%
Epoch [22/300], Step [210/391],                 Loss: 0.41794, Train_Acc:85.43%
Epoch [22/300], Step [220/391],                 Loss: 0.41695, Train_Acc:85.49%
Epoch [22/300], Step [230/391],                 Loss: 0.41503, Train_Acc:85.59%
Epoch [22/300], Step [240/391],                 Loss: 0.41250, Train_Acc:85.66%
Epoch [22/300], Step [250/391],                 Loss: 0.41373, Train_Acc:85.62%
Epoch [22/300], Step [260/391],                 Loss: 0.41519, Train_Acc:85.60%
Epoch [22/300], Step [270/391],                 Loss: 0.41635, Train_Acc:85.61%
Epoch [22/300], Step [280/391],                 Loss: 0.41560, Train_Acc:85.64%
Epoch [22/300], Step [290/391],                 Loss: 0.41567, Train_Acc:85.64%
Epoch [22/300], Step [300/391],                 Loss: 0.41469, Train_Acc:85.65%
Epoch [22/300], Step [310/391],                 Loss: 0.41409, Train_Acc:85.67%
Epoch [22/300], Step [320/391],                 Loss: 0.41443, Train_Acc:85.65%
Epoch [22/300], Step [330/391],                 Loss: 0.41523, Train_Acc:85.64%
Epoch [22/300], Step [340/391],                 Loss: 0.41463, Train_Acc:85.67%
Epoch [22/300], Step [350/391],                 Loss: 0.41456, Train_Acc:85.67%
Epoch [22/300], Step [360/391],                 Loss: 0.41477, Train_Acc:85.66%
Epoch [22/300], Step [370/391],                 Loss: 0.41485, Train_Acc:85.68%
Epoch [22/300], Step [380/391],                 Loss: 0.41399, Train_Acc:85.72%
Epoch [22/300], Step [390/391],                 Loss: 0.41262, Train_Acc:85.77%
Accuary on test images:74.58%
Epoch [23/300], Step [10/391],                 Loss: 0.40470, Train_Acc:84.92%
Epoch [23/300], Step [20/391],                 Loss: 0.40413, Train_Acc:85.59%
Epoch [23/300], Step [30/391],                 Loss: 0.40470, Train_Acc:85.78%
Epoch [23/300], Step [40/391],                 Loss: 0.40381, Train_Acc:86.17%
Epoch [23/300], Step [50/391],                 Loss: 0.40126, Train_Acc:86.09%
Epoch [23/300], Step [60/391],                 Loss: 0.40923, Train_Acc:85.77%
Epoch [23/300], Step [70/391],                 Loss: 0.41013, Train_Acc:85.84%
Epoch [23/300], Step [80/391],                 Loss: 0.41540, Train_Acc:85.66%
Epoch [23/300], Step [90/391],                 Loss: 0.42125, Train_Acc:85.49%
Epoch [23/300], Step [100/391],                 Loss: 0.42258, Train_Acc:85.35%
Epoch [23/300], Step [110/391],                 Loss: 0.42964, Train_Acc:85.11%
Epoch [23/300], Step [120/391],                 Loss: 0.43107, Train_Acc:85.01%
Epoch [23/300], Step [130/391],                 Loss: 0.43223, Train_Acc:85.02%
Epoch [23/300], Step [140/391],                 Loss: 0.42901, Train_Acc:85.14%
Epoch [23/300], Step [150/391],                 Loss: 0.42990, Train_Acc:85.15%
Epoch [23/300], Step [160/391],                 Loss: 0.43001, Train_Acc:85.10%
Epoch [23/300], Step [170/391],                 Loss: 0.42968, Train_Acc:85.09%
Epoch [23/300], Step [180/391],                 Loss: 0.42692, Train_Acc:85.15%
Epoch [23/300], Step [190/391],                 Loss: 0.42507, Train_Acc:85.25%
Epoch [23/300], Step [200/391],                 Loss: 0.42347, Train_Acc:85.33%
Epoch [23/300], Step [210/391],                 Loss: 0.42305, Train_Acc:85.32%
Epoch [23/300], Step [220/391],                 Loss: 0.42178, Train_Acc:85.36%
Epoch [23/300], Step [230/391],                 Loss: 0.41962, Train_Acc:85.42%
Epoch [23/300], Step [240/391],                 Loss: 0.41728, Train_Acc:85.55%
Epoch [23/300], Step [250/391],                 Loss: 0.41664, Train_Acc:85.62%
Epoch [23/300], Step [260/391],                 Loss: 0.41811, Train_Acc:85.58%
Epoch [23/300], Step [270/391],                 Loss: 0.41934, Train_Acc:85.54%
Epoch [23/300], Step [280/391],                 Loss: 0.42001, Train_Acc:85.52%
Epoch [23/300], Step [290/391],                 Loss: 0.42042, Train_Acc:85.52%
Epoch [23/300], Step [300/391],                 Loss: 0.41994, Train_Acc:85.54%
Epoch [23/300], Step [310/391],                 Loss: 0.41827, Train_Acc:85.59%
Epoch [23/300], Step [320/391],                 Loss: 0.41866, Train_Acc:85.60%
Epoch [23/300], Step [330/391],                 Loss: 0.41573, Train_Acc:85.74%
Epoch [23/300], Step [340/391],                 Loss: 0.41485, Train_Acc:85.76%
Epoch [23/300], Step [350/391],                 Loss: 0.41524, Train_Acc:85.80%
Epoch [23/300], Step [360/391],                 Loss: 0.41507, Train_Acc:85.81%
Epoch [23/300], Step [370/391],                 Loss: 0.41506, Train_Acc:85.81%
Epoch [23/300], Step [380/391],                 Loss: 0.41475, Train_Acc:85.84%
Epoch [23/300], Step [390/391],                 Loss: 0.41431, Train_Acc:85.85%
Accuary on test images:70.38%
Epoch [24/300], Step [10/391],                 Loss: 0.41808, Train_Acc:85.00%
Epoch [24/300], Step [20/391],                 Loss: 0.39751, Train_Acc:86.25%
Epoch [24/300], Step [30/391],                 Loss: 0.39051, Train_Acc:86.72%
Epoch [24/300], Step [40/391],                 Loss: 0.39753, Train_Acc:86.48%
Epoch [24/300], Step [50/391],                 Loss: 0.38909, Train_Acc:86.55%
Epoch [24/300], Step [60/391],                 Loss: 0.39381, Train_Acc:86.32%
Epoch [24/300], Step [70/391],                 Loss: 0.39696, Train_Acc:86.23%
Epoch [24/300], Step [80/391],                 Loss: 0.39929, Train_Acc:86.19%
Epoch [24/300], Step [90/391],                 Loss: 0.40450, Train_Acc:86.05%
Epoch [24/300], Step [100/391],                 Loss: 0.40653, Train_Acc:85.89%
Epoch [24/300], Step [110/391],                 Loss: 0.40956, Train_Acc:85.82%
Epoch [24/300], Step [120/391],                 Loss: 0.41183, Train_Acc:85.79%
Epoch [24/300], Step [130/391],                 Loss: 0.41137, Train_Acc:85.82%
Epoch [24/300], Step [140/391],                 Loss: 0.40930, Train_Acc:85.85%
Epoch [24/300], Step [150/391],                 Loss: 0.41220, Train_Acc:85.78%
Epoch [24/300], Step [160/391],                 Loss: 0.41229, Train_Acc:85.79%
Epoch [24/300], Step [170/391],                 Loss: 0.41145, Train_Acc:85.82%
Epoch [24/300], Step [180/391],                 Loss: 0.41070, Train_Acc:85.83%
Epoch [24/300], Step [190/391],                 Loss: 0.41127, Train_Acc:85.80%
Epoch [24/300], Step [200/391],                 Loss: 0.41240, Train_Acc:85.80%
Epoch [24/300], Step [210/391],                 Loss: 0.41217, Train_Acc:85.86%
Epoch [24/300], Step [220/391],                 Loss: 0.41217, Train_Acc:85.87%
Epoch [24/300], Step [230/391],                 Loss: 0.41022, Train_Acc:85.92%
Epoch [24/300], Step [240/391],                 Loss: 0.40843, Train_Acc:85.98%
Epoch [24/300], Step [250/391],                 Loss: 0.41021, Train_Acc:85.88%
Epoch [24/300], Step [260/391],                 Loss: 0.41435, Train_Acc:85.73%
Epoch [24/300], Step [270/391],                 Loss: 0.41624, Train_Acc:85.69%
Epoch [24/300], Step [280/391],                 Loss: 0.41612, Train_Acc:85.65%
Epoch [24/300], Step [290/391],                 Loss: 0.41617, Train_Acc:85.63%
Epoch [24/300], Step [300/391],                 Loss: 0.41563, Train_Acc:85.64%
Epoch [24/300], Step [310/391],                 Loss: 0.41508, Train_Acc:85.67%
Epoch [24/300], Step [320/391],                 Loss: 0.41551, Train_Acc:85.67%
Epoch [24/300], Step [330/391],                 Loss: 0.41500, Train_Acc:85.68%
Epoch [24/300], Step [340/391],                 Loss: 0.41422, Train_Acc:85.71%
Epoch [24/300], Step [350/391],                 Loss: 0.41354, Train_Acc:85.76%
Epoch [24/300], Step [360/391],                 Loss: 0.41233, Train_Acc:85.81%
Epoch [24/300], Step [370/391],                 Loss: 0.41225, Train_Acc:85.81%
Epoch [24/300], Step [380/391],                 Loss: 0.41178, Train_Acc:85.84%
Epoch [24/300], Step [390/391],                 Loss: 0.41066, Train_Acc:85.90%
Accuary on test images:80.56%
Epoch [25/300], Step [10/391],                 Loss: 0.37005, Train_Acc:87.19%
Epoch [25/300], Step [20/391],                 Loss: 0.37888, Train_Acc:86.48%
Epoch [25/300], Step [30/391],                 Loss: 0.38246, Train_Acc:86.17%
Epoch [25/300], Step [40/391],                 Loss: 0.38391, Train_Acc:86.31%
Epoch [25/300], Step [50/391],                 Loss: 0.38699, Train_Acc:86.19%
Epoch [25/300], Step [60/391],                 Loss: 0.39538, Train_Acc:85.99%
Epoch [25/300], Step [70/391],                 Loss: 0.39767, Train_Acc:85.92%
Epoch [25/300], Step [80/391],                 Loss: 0.40256, Train_Acc:85.83%
Epoch [25/300], Step [90/391],                 Loss: 0.40922, Train_Acc:85.59%
Epoch [25/300], Step [100/391],                 Loss: 0.40995, Train_Acc:85.54%
Epoch [25/300], Step [110/391],                 Loss: 0.41029, Train_Acc:85.61%
Epoch [25/300], Step [120/391],                 Loss: 0.40870, Train_Acc:85.70%
Epoch [25/300], Step [130/391],                 Loss: 0.40807, Train_Acc:85.79%
Epoch [25/300], Step [140/391],                 Loss: 0.40705, Train_Acc:85.83%
Epoch [25/300], Step [150/391],                 Loss: 0.40934, Train_Acc:85.77%
Epoch [25/300], Step [160/391],                 Loss: 0.40906, Train_Acc:85.81%
Epoch [25/300], Step [170/391],                 Loss: 0.40939, Train_Acc:85.79%
Epoch [25/300], Step [180/391],                 Loss: 0.40908, Train_Acc:85.85%
Epoch [25/300], Step [190/391],                 Loss: 0.40842, Train_Acc:85.89%
Epoch [25/300], Step [200/391],                 Loss: 0.40982, Train_Acc:85.83%
Epoch [25/300], Step [210/391],                 Loss: 0.41138, Train_Acc:85.77%
Epoch [25/300], Step [220/391],                 Loss: 0.41068, Train_Acc:85.78%
Epoch [25/300], Step [230/391],                 Loss: 0.40853, Train_Acc:85.86%
Epoch [25/300], Step [240/391],                 Loss: 0.40678, Train_Acc:85.92%
Epoch [25/300], Step [250/391],                 Loss: 0.40595, Train_Acc:85.96%
Epoch [25/300], Step [260/391],                 Loss: 0.40789, Train_Acc:85.91%
Epoch [25/300], Step [270/391],                 Loss: 0.40951, Train_Acc:85.87%
Epoch [25/300], Step [280/391],                 Loss: 0.40916, Train_Acc:85.89%
Epoch [25/300], Step [290/391],                 Loss: 0.40827, Train_Acc:85.96%
Epoch [25/300], Step [300/391],                 Loss: 0.40707, Train_Acc:85.99%
Epoch [25/300], Step [310/391],                 Loss: 0.40512, Train_Acc:86.07%
Epoch [25/300], Step [320/391],                 Loss: 0.40494, Train_Acc:86.05%
Epoch [25/300], Step [330/391],                 Loss: 0.40470, Train_Acc:86.09%
Epoch [25/300], Step [340/391],                 Loss: 0.40375, Train_Acc:86.12%
Epoch [25/300], Step [350/391],                 Loss: 0.40344, Train_Acc:86.13%
Epoch [25/300], Step [360/391],                 Loss: 0.40401, Train_Acc:86.11%
Epoch [25/300], Step [370/391],                 Loss: 0.40495, Train_Acc:86.06%
Epoch [25/300], Step [380/391],                 Loss: 0.40564, Train_Acc:86.02%
Epoch [25/300], Step [390/391],                 Loss: 0.40608, Train_Acc:86.03%
Accuary on test images:77.32%
Epoch [26/300], Step [10/391],                 Loss: 0.38724, Train_Acc:85.86%
Epoch [26/300], Step [20/391],                 Loss: 0.37952, Train_Acc:86.76%
Epoch [26/300], Step [30/391],                 Loss: 0.37975, Train_Acc:86.67%
Epoch [26/300], Step [40/391],                 Loss: 0.39036, Train_Acc:86.43%
Epoch [26/300], Step [50/391],                 Loss: 0.38781, Train_Acc:86.44%
Epoch [26/300], Step [60/391],                 Loss: 0.39548, Train_Acc:86.05%
Epoch [26/300], Step [70/391],                 Loss: 0.40039, Train_Acc:86.09%
Epoch [26/300], Step [80/391],                 Loss: 0.40216, Train_Acc:86.08%
Epoch [26/300], Step [90/391],                 Loss: 0.40843, Train_Acc:85.87%
Epoch [26/300], Step [100/391],                 Loss: 0.40702, Train_Acc:85.88%
Epoch [26/300], Step [110/391],                 Loss: 0.40718, Train_Acc:85.99%
Epoch [26/300], Step [120/391],                 Loss: 0.40726, Train_Acc:85.96%
Epoch [26/300], Step [130/391],                 Loss: 0.40979, Train_Acc:85.87%
Epoch [26/300], Step [140/391],                 Loss: 0.41017, Train_Acc:85.81%
Epoch [26/300], Step [150/391],                 Loss: 0.41300, Train_Acc:85.68%
Epoch [26/300], Step [160/391],                 Loss: 0.41227, Train_Acc:85.79%
Epoch [26/300], Step [170/391],                 Loss: 0.41357, Train_Acc:85.72%
Epoch [26/300], Step [180/391],                 Loss: 0.41407, Train_Acc:85.71%
Epoch [26/300], Step [190/391],                 Loss: 0.41236, Train_Acc:85.80%
Epoch [26/300], Step [200/391],                 Loss: 0.41269, Train_Acc:85.77%
Epoch [26/300], Step [210/391],                 Loss: 0.41188, Train_Acc:85.80%
Epoch [26/300], Step [220/391],                 Loss: 0.41065, Train_Acc:85.86%
Epoch [26/300], Step [230/391],                 Loss: 0.40964, Train_Acc:85.91%
Epoch [26/300], Step [240/391],                 Loss: 0.40814, Train_Acc:85.96%
Epoch [26/300], Step [250/391],                 Loss: 0.40791, Train_Acc:86.00%
Epoch [26/300], Step [260/391],                 Loss: 0.40954, Train_Acc:85.98%
Epoch [26/300], Step [270/391],                 Loss: 0.41006, Train_Acc:85.97%
Epoch [26/300], Step [280/391],                 Loss: 0.41038, Train_Acc:85.96%
Epoch [26/300], Step [290/391],                 Loss: 0.41048, Train_Acc:85.99%
Epoch [26/300], Step [300/391],                 Loss: 0.40904, Train_Acc:86.01%
Epoch [26/300], Step [310/391],                 Loss: 0.40794, Train_Acc:86.03%
Epoch [26/300], Step [320/391],                 Loss: 0.40813, Train_Acc:86.04%
Epoch [26/300], Step [330/391],                 Loss: 0.40760, Train_Acc:86.06%
Epoch [26/300], Step [340/391],                 Loss: 0.40682, Train_Acc:86.11%
Epoch [26/300], Step [350/391],                 Loss: 0.40647, Train_Acc:86.10%
Epoch [26/300], Step [360/391],                 Loss: 0.40703, Train_Acc:86.08%
Epoch [26/300], Step [370/391],                 Loss: 0.40752, Train_Acc:86.06%
Epoch [26/300], Step [380/391],                 Loss: 0.40729, Train_Acc:86.07%
Epoch [26/300], Step [390/391],                 Loss: 0.40693, Train_Acc:86.07%
Accuary on test images:67.64%
Epoch [27/300], Step [10/391],                 Loss: 0.42211, Train_Acc:84.30%
Epoch [27/300], Step [20/391],                 Loss: 0.40518, Train_Acc:85.66%
Epoch [27/300], Step [30/391],                 Loss: 0.40106, Train_Acc:85.91%
Epoch [27/300], Step [40/391],                 Loss: 0.39988, Train_Acc:86.00%
Epoch [27/300], Step [50/391],                 Loss: 0.39215, Train_Acc:86.17%
Epoch [27/300], Step [60/391],                 Loss: 0.39396, Train_Acc:86.00%
Epoch [27/300], Step [70/391],                 Loss: 0.39295, Train_Acc:86.27%
Epoch [27/300], Step [80/391],                 Loss: 0.39505, Train_Acc:86.28%
Epoch [27/300], Step [90/391],                 Loss: 0.40041, Train_Acc:86.20%
Epoch [27/300], Step [100/391],                 Loss: 0.39765, Train_Acc:86.25%
Epoch [27/300], Step [110/391],                 Loss: 0.39996, Train_Acc:86.19%
Epoch [27/300], Step [120/391],                 Loss: 0.39970, Train_Acc:86.18%
Epoch [27/300], Step [130/391],                 Loss: 0.40206, Train_Acc:86.14%
Epoch [27/300], Step [140/391],                 Loss: 0.40053, Train_Acc:86.24%
Epoch [27/300], Step [150/391],                 Loss: 0.40109, Train_Acc:86.21%
Epoch [27/300], Step [160/391],                 Loss: 0.40078, Train_Acc:86.24%
Epoch [27/300], Step [170/391],                 Loss: 0.40105, Train_Acc:86.22%
Epoch [27/300], Step [180/391],                 Loss: 0.40393, Train_Acc:86.12%
Epoch [27/300], Step [190/391],                 Loss: 0.40609, Train_Acc:85.97%
Epoch [27/300], Step [200/391],                 Loss: 0.40689, Train_Acc:85.93%
Epoch [27/300], Step [210/391],                 Loss: 0.40698, Train_Acc:85.98%
Epoch [27/300], Step [220/391],                 Loss: 0.40844, Train_Acc:85.91%
Epoch [27/300], Step [230/391],                 Loss: 0.40794, Train_Acc:85.92%
Epoch [27/300], Step [240/391],                 Loss: 0.40606, Train_Acc:86.00%
Epoch [27/300], Step [250/391],                 Loss: 0.40540, Train_Acc:86.00%
Epoch [27/300], Step [260/391],                 Loss: 0.40833, Train_Acc:85.91%
Epoch [27/300], Step [270/391],                 Loss: 0.41049, Train_Acc:85.87%
Epoch [27/300], Step [280/391],                 Loss: 0.41048, Train_Acc:85.88%
Epoch [27/300], Step [290/391],                 Loss: 0.41046, Train_Acc:85.88%
Epoch [27/300], Step [300/391],                 Loss: 0.40993, Train_Acc:85.90%
Epoch [27/300], Step [310/391],                 Loss: 0.40937, Train_Acc:85.87%
Epoch [27/300], Step [320/391],                 Loss: 0.40883, Train_Acc:85.89%
Epoch [27/300], Step [330/391],                 Loss: 0.40784, Train_Acc:85.94%
Epoch [27/300], Step [340/391],                 Loss: 0.40649, Train_Acc:85.99%
Epoch [27/300], Step [350/391],                 Loss: 0.40566, Train_Acc:86.02%
Epoch [27/300], Step [360/391],                 Loss: 0.40635, Train_Acc:86.00%
Epoch [27/300], Step [370/391],                 Loss: 0.40661, Train_Acc:85.96%
Epoch [27/300], Step [380/391],                 Loss: 0.40639, Train_Acc:85.98%
Epoch [27/300], Step [390/391],                 Loss: 0.40603, Train_Acc:85.98%
Accuary on test images:70.74%
Epoch [28/300], Step [10/391],                 Loss: 0.42501, Train_Acc:85.08%
Epoch [28/300], Step [20/391],                 Loss: 0.42358, Train_Acc:85.00%
Epoch [28/300], Step [30/391],                 Loss: 0.42268, Train_Acc:85.49%
Epoch [28/300], Step [40/391],                 Loss: 0.41328, Train_Acc:86.19%
Epoch [28/300], Step [50/391],                 Loss: 0.41136, Train_Acc:86.23%
Epoch [28/300], Step [60/391],                 Loss: 0.41412, Train_Acc:86.09%
Epoch [28/300], Step [70/391],                 Loss: 0.40881, Train_Acc:86.25%
Epoch [28/300], Step [80/391],                 Loss: 0.40967, Train_Acc:86.13%
Epoch [28/300], Step [90/391],                 Loss: 0.41014, Train_Acc:86.08%
Epoch [28/300], Step [100/391],                 Loss: 0.40722, Train_Acc:86.13%
Epoch [28/300], Step [110/391],                 Loss: 0.40647, Train_Acc:86.16%
Epoch [28/300], Step [120/391],                 Loss: 0.40703, Train_Acc:86.01%
Epoch [28/300], Step [130/391],                 Loss: 0.40929, Train_Acc:85.99%
Epoch [28/300], Step [140/391],                 Loss: 0.40845, Train_Acc:86.08%
Epoch [28/300], Step [150/391],                 Loss: 0.41263, Train_Acc:85.96%
Epoch [28/300], Step [160/391],                 Loss: 0.41162, Train_Acc:86.04%
Epoch [28/300], Step [170/391],                 Loss: 0.41030, Train_Acc:86.08%
Epoch [28/300], Step [180/391],                 Loss: 0.40955, Train_Acc:86.06%
Epoch [28/300], Step [190/391],                 Loss: 0.40903, Train_Acc:86.10%
Epoch [28/300], Step [200/391],                 Loss: 0.40813, Train_Acc:86.17%
Epoch [28/300], Step [210/391],                 Loss: 0.40744, Train_Acc:86.21%
Epoch [28/300], Step [220/391],                 Loss: 0.40531, Train_Acc:86.27%
Epoch [28/300], Step [230/391],                 Loss: 0.40330, Train_Acc:86.29%
Epoch [28/300], Step [240/391],                 Loss: 0.40089, Train_Acc:86.42%
Epoch [28/300], Step [250/391],                 Loss: 0.39982, Train_Acc:86.42%
Epoch [28/300], Step [260/391],                 Loss: 0.40219, Train_Acc:86.36%
Epoch [28/300], Step [270/391],                 Loss: 0.40419, Train_Acc:86.29%
Epoch [28/300], Step [280/391],                 Loss: 0.40528, Train_Acc:86.23%
Epoch [28/300], Step [290/391],                 Loss: 0.40488, Train_Acc:86.23%
Epoch [28/300], Step [300/391],                 Loss: 0.40373, Train_Acc:86.22%
Epoch [28/300], Step [310/391],                 Loss: 0.40329, Train_Acc:86.22%
Epoch [28/300], Step [320/391],                 Loss: 0.40334, Train_Acc:86.23%
Epoch [28/300], Step [330/391],                 Loss: 0.40312, Train_Acc:86.24%
Epoch [28/300], Step [340/391],                 Loss: 0.40207, Train_Acc:86.28%
Epoch [28/300], Step [350/391],                 Loss: 0.40111, Train_Acc:86.31%
Epoch [28/300], Step [360/391],                 Loss: 0.40175, Train_Acc:86.24%
Epoch [28/300], Step [370/391],                 Loss: 0.40246, Train_Acc:86.24%
Epoch [28/300], Step [380/391],                 Loss: 0.40240, Train_Acc:86.26%
Epoch [28/300], Step [390/391],                 Loss: 0.40188, Train_Acc:86.28%
Accuary on test images:70.82%
Epoch [29/300], Step [10/391],                 Loss: 0.43048, Train_Acc:84.30%
Epoch [29/300], Step [20/391],                 Loss: 0.42434, Train_Acc:85.08%
Epoch [29/300], Step [30/391],                 Loss: 0.42124, Train_Acc:85.39%
Epoch [29/300], Step [40/391],                 Loss: 0.41929, Train_Acc:85.88%
Epoch [29/300], Step [50/391],                 Loss: 0.41162, Train_Acc:85.94%
Epoch [29/300], Step [60/391],                 Loss: 0.40820, Train_Acc:86.05%
Epoch [29/300], Step [70/391],                 Loss: 0.40461, Train_Acc:86.14%
Epoch [29/300], Step [80/391],                 Loss: 0.40155, Train_Acc:86.26%
Epoch [29/300], Step [90/391],                 Loss: 0.40168, Train_Acc:86.32%
Epoch [29/300], Step [100/391],                 Loss: 0.39696, Train_Acc:86.45%
Epoch [29/300], Step [110/391],                 Loss: 0.40081, Train_Acc:86.31%
Epoch [29/300], Step [120/391],                 Loss: 0.40089, Train_Acc:86.27%
Epoch [29/300], Step [130/391],                 Loss: 0.40470, Train_Acc:86.17%
Epoch [29/300], Step [140/391],                 Loss: 0.40598, Train_Acc:86.18%
Epoch [29/300], Step [150/391],                 Loss: 0.40734, Train_Acc:86.12%
Epoch [29/300], Step [160/391],                 Loss: 0.40723, Train_Acc:86.17%
Epoch [29/300], Step [170/391],                 Loss: 0.40800, Train_Acc:86.05%
Epoch [29/300], Step [180/391],                 Loss: 0.40789, Train_Acc:86.07%
Epoch [29/300], Step [190/391],                 Loss: 0.40651, Train_Acc:86.11%
Epoch [29/300], Step [200/391],                 Loss: 0.40557, Train_Acc:86.15%
Epoch [29/300], Step [210/391],                 Loss: 0.40480, Train_Acc:86.15%
Epoch [29/300], Step [220/391],                 Loss: 0.40379, Train_Acc:86.19%
Epoch [29/300], Step [230/391],                 Loss: 0.40133, Train_Acc:86.32%
Epoch [29/300], Step [240/391],                 Loss: 0.40008, Train_Acc:86.39%
Epoch [29/300], Step [250/391],                 Loss: 0.39792, Train_Acc:86.46%
Epoch [29/300], Step [260/391],                 Loss: 0.39931, Train_Acc:86.43%
Epoch [29/300], Step [270/391],                 Loss: 0.40038, Train_Acc:86.39%
Epoch [29/300], Step [280/391],                 Loss: 0.39972, Train_Acc:86.42%
Epoch [29/300], Step [290/391],                 Loss: 0.39880, Train_Acc:86.46%
Epoch [29/300], Step [300/391],                 Loss: 0.39864, Train_Acc:86.44%
Epoch [29/300], Step [310/391],                 Loss: 0.39824, Train_Acc:86.44%
Epoch [29/300], Step [320/391],                 Loss: 0.39961, Train_Acc:86.40%
Epoch [29/300], Step [330/391],                 Loss: 0.39866, Train_Acc:86.43%
Epoch [29/300], Step [340/391],                 Loss: 0.39784, Train_Acc:86.44%
Epoch [29/300], Step [350/391],                 Loss: 0.39707, Train_Acc:86.49%
Epoch [29/300], Step [360/391],                 Loss: 0.39679, Train_Acc:86.49%
Epoch [29/300], Step [370/391],                 Loss: 0.39714, Train_Acc:86.46%
Epoch [29/300], Step [380/391],                 Loss: 0.39762, Train_Acc:86.44%
Epoch [29/300], Step [390/391],                 Loss: 0.39860, Train_Acc:86.38%
Accuary on test images:73.86%
Epoch [30/300], Step [10/391],                 Loss: 0.39958, Train_Acc:85.16%
Epoch [30/300], Step [20/391],                 Loss: 0.39574, Train_Acc:85.62%
Epoch [30/300], Step [30/391],                 Loss: 0.39332, Train_Acc:85.89%
Epoch [30/300], Step [40/391],                 Loss: 0.39356, Train_Acc:86.21%
Epoch [30/300], Step [50/391],                 Loss: 0.38760, Train_Acc:86.47%
Epoch [30/300], Step [60/391],                 Loss: 0.38881, Train_Acc:86.43%
Epoch [30/300], Step [70/391],                 Loss: 0.38824, Train_Acc:86.53%
Epoch [30/300], Step [80/391],                 Loss: 0.38907, Train_Acc:86.65%
Epoch [30/300], Step [90/391],                 Loss: 0.39204, Train_Acc:86.63%
Epoch [30/300], Step [100/391],                 Loss: 0.38923, Train_Acc:86.62%
Epoch [30/300], Step [110/391],                 Loss: 0.39078, Train_Acc:86.51%
Epoch [30/300], Step [120/391],                 Loss: 0.39350, Train_Acc:86.41%
Epoch [30/300], Step [130/391],                 Loss: 0.39525, Train_Acc:86.42%
Epoch [30/300], Step [140/391],                 Loss: 0.39568, Train_Acc:86.37%
Epoch [30/300], Step [150/391],                 Loss: 0.39773, Train_Acc:86.31%
Epoch [30/300], Step [160/391],                 Loss: 0.39970, Train_Acc:86.21%
Epoch [30/300], Step [170/391],                 Loss: 0.40056, Train_Acc:86.13%
Epoch [30/300], Step [180/391],                 Loss: 0.40146, Train_Acc:86.04%
Epoch [30/300], Step [190/391],                 Loss: 0.40397, Train_Acc:85.93%
Epoch [30/300], Step [200/391],                 Loss: 0.40379, Train_Acc:85.96%
Epoch [30/300], Step [210/391],                 Loss: 0.40335, Train_Acc:85.99%
Epoch [30/300], Step [220/391],                 Loss: 0.40184, Train_Acc:86.08%
Epoch [30/300], Step [230/391],                 Loss: 0.39981, Train_Acc:86.14%
Epoch [30/300], Step [240/391],                 Loss: 0.39754, Train_Acc:86.22%
Epoch [30/300], Step [250/391],                 Loss: 0.39578, Train_Acc:86.26%
Epoch [30/300], Step [260/391],                 Loss: 0.39709, Train_Acc:86.21%
Epoch [30/300], Step [270/391],                 Loss: 0.39776, Train_Acc:86.15%
Epoch [30/300], Step [280/391],                 Loss: 0.39911, Train_Acc:86.10%
Epoch [30/300], Step [290/391],                 Loss: 0.40013, Train_Acc:86.08%
Epoch [30/300], Step [300/391],                 Loss: 0.40045, Train_Acc:86.04%
Epoch [30/300], Step [310/391],                 Loss: 0.39980, Train_Acc:86.06%
Epoch [30/300], Step [320/391],                 Loss: 0.39954, Train_Acc:86.09%
Epoch [30/300], Step [330/391],                 Loss: 0.39870, Train_Acc:86.15%
Epoch [30/300], Step [340/391],                 Loss: 0.39771, Train_Acc:86.16%
Epoch [30/300], Step [350/391],                 Loss: 0.39795, Train_Acc:86.15%
Epoch [30/300], Step [360/391],                 Loss: 0.39732, Train_Acc:86.15%
Epoch [30/300], Step [370/391],                 Loss: 0.39757, Train_Acc:86.15%
Epoch [30/300], Step [380/391],                 Loss: 0.39685, Train_Acc:86.19%
Epoch [30/300], Step [390/391],                 Loss: 0.39613, Train_Acc:86.21%
Accuary on test images:73.06%
Epoch [31/300], Step [10/391],                 Loss: 0.39357, Train_Acc:86.17%
Epoch [31/300], Step [20/391],                 Loss: 0.40437, Train_Acc:85.62%
Epoch [31/300], Step [30/391],                 Loss: 0.39995, Train_Acc:85.96%
Epoch [31/300], Step [40/391],                 Loss: 0.40416, Train_Acc:85.86%
Epoch [31/300], Step [50/391],                 Loss: 0.40189, Train_Acc:85.94%
Epoch [31/300], Step [60/391],                 Loss: 0.39944, Train_Acc:86.07%
Epoch [31/300], Step [70/391],                 Loss: 0.39960, Train_Acc:86.12%
Epoch [31/300], Step [80/391],                 Loss: 0.39586, Train_Acc:86.34%
Epoch [31/300], Step [90/391],                 Loss: 0.39780, Train_Acc:86.25%
Epoch [31/300], Step [100/391],                 Loss: 0.39659, Train_Acc:86.30%
Epoch [31/300], Step [110/391],                 Loss: 0.40060, Train_Acc:86.22%
Epoch [31/300], Step [120/391],                 Loss: 0.39819, Train_Acc:86.23%
Epoch [31/300], Step [130/391],                 Loss: 0.39868, Train_Acc:86.22%
Epoch [31/300], Step [140/391],                 Loss: 0.39600, Train_Acc:86.32%
Epoch [31/300], Step [150/391],                 Loss: 0.39675, Train_Acc:86.35%
Epoch [31/300], Step [160/391],                 Loss: 0.39477, Train_Acc:86.41%
Epoch [31/300], Step [170/391],                 Loss: 0.39642, Train_Acc:86.34%
Epoch [31/300], Step [180/391],                 Loss: 0.39631, Train_Acc:86.35%
Epoch [31/300], Step [190/391],                 Loss: 0.39644, Train_Acc:86.34%
Epoch [31/300], Step [200/391],                 Loss: 0.39619, Train_Acc:86.41%
Epoch [31/300], Step [210/391],                 Loss: 0.39470, Train_Acc:86.48%
Epoch [31/300], Step [220/391],                 Loss: 0.39214, Train_Acc:86.59%
Epoch [31/300], Step [230/391],                 Loss: 0.39124, Train_Acc:86.62%
Epoch [31/300], Step [240/391],                 Loss: 0.38850, Train_Acc:86.70%
Epoch [31/300], Step [250/391],                 Loss: 0.38806, Train_Acc:86.72%
Epoch [31/300], Step [260/391],                 Loss: 0.39125, Train_Acc:86.66%
Epoch [31/300], Step [270/391],                 Loss: 0.39286, Train_Acc:86.61%
Epoch [31/300], Step [280/391],                 Loss: 0.39440, Train_Acc:86.55%
Epoch [31/300], Step [290/391],                 Loss: 0.39553, Train_Acc:86.51%
Epoch [31/300], Step [300/391],                 Loss: 0.39419, Train_Acc:86.55%
Epoch [31/300], Step [310/391],                 Loss: 0.39416, Train_Acc:86.57%
Epoch [31/300], Step [320/391],                 Loss: 0.39615, Train_Acc:86.50%
Epoch [31/300], Step [330/391],                 Loss: 0.39502, Train_Acc:86.55%
Epoch [31/300], Step [340/391],                 Loss: 0.39438, Train_Acc:86.55%
Epoch [31/300], Step [350/391],                 Loss: 0.39342, Train_Acc:86.58%
Epoch [31/300], Step [360/391],                 Loss: 0.39371, Train_Acc:86.60%
Epoch [31/300], Step [370/391],                 Loss: 0.39402, Train_Acc:86.59%
Epoch [31/300], Step [380/391],                 Loss: 0.39376, Train_Acc:86.58%
Epoch [31/300], Step [390/391],                 Loss: 0.39328, Train_Acc:86.61%
Accuary on test images:71.54%
Epoch [32/300], Step [10/391],                 Loss: 0.38581, Train_Acc:86.56%
Epoch [32/300], Step [20/391],                 Loss: 0.37376, Train_Acc:87.34%
Epoch [32/300], Step [30/391],                 Loss: 0.36491, Train_Acc:87.76%
Epoch [32/300], Step [40/391],                 Loss: 0.37164, Train_Acc:87.75%
Epoch [32/300], Step [50/391],                 Loss: 0.37661, Train_Acc:87.47%
Epoch [32/300], Step [60/391],                 Loss: 0.38246, Train_Acc:87.23%
Epoch [32/300], Step [70/391],                 Loss: 0.38446, Train_Acc:87.20%
Epoch [32/300], Step [80/391],                 Loss: 0.39083, Train_Acc:86.94%
Epoch [32/300], Step [90/391],                 Loss: 0.39231, Train_Acc:86.86%
Epoch [32/300], Step [100/391],                 Loss: 0.39009, Train_Acc:86.89%
Epoch [32/300], Step [110/391],                 Loss: 0.39192, Train_Acc:86.84%
Epoch [32/300], Step [120/391],                 Loss: 0.38942, Train_Acc:86.89%
Epoch [32/300], Step [130/391],                 Loss: 0.39136, Train_Acc:86.84%
Epoch [32/300], Step [140/391],                 Loss: 0.39083, Train_Acc:86.84%
Epoch [32/300], Step [150/391],                 Loss: 0.39115, Train_Acc:86.79%
Epoch [32/300], Step [160/391],                 Loss: 0.39168, Train_Acc:86.75%
Epoch [32/300], Step [170/391],                 Loss: 0.39407, Train_Acc:86.68%
Epoch [32/300], Step [180/391],                 Loss: 0.39602, Train_Acc:86.55%
Epoch [32/300], Step [190/391],                 Loss: 0.39675, Train_Acc:86.47%
Epoch [32/300], Step [200/391],                 Loss: 0.39803, Train_Acc:86.43%
Epoch [32/300], Step [210/391],                 Loss: 0.39706, Train_Acc:86.48%
Epoch [32/300], Step [220/391],                 Loss: 0.39525, Train_Acc:86.53%
Epoch [32/300], Step [230/391],                 Loss: 0.39297, Train_Acc:86.64%
Epoch [32/300], Step [240/391],                 Loss: 0.39188, Train_Acc:86.67%
Epoch [32/300], Step [250/391],                 Loss: 0.39199, Train_Acc:86.65%
Epoch [32/300], Step [260/391],                 Loss: 0.39385, Train_Acc:86.61%
Epoch [32/300], Step [270/391],                 Loss: 0.39507, Train_Acc:86.56%
Epoch [32/300], Step [280/391],                 Loss: 0.39508, Train_Acc:86.55%
Epoch [32/300], Step [290/391],                 Loss: 0.39475, Train_Acc:86.57%
Epoch [32/300], Step [300/391],                 Loss: 0.39486, Train_Acc:86.57%
Epoch [32/300], Step [310/391],                 Loss: 0.39369, Train_Acc:86.61%
Epoch [32/300], Step [320/391],                 Loss: 0.39293, Train_Acc:86.64%
Epoch [32/300], Step [330/391],                 Loss: 0.39206, Train_Acc:86.65%
Epoch [32/300], Step [340/391],                 Loss: 0.39106, Train_Acc:86.67%
Epoch [32/300], Step [350/391],                 Loss: 0.39064, Train_Acc:86.68%
Epoch [32/300], Step [360/391],                 Loss: 0.39031, Train_Acc:86.68%
Epoch [32/300], Step [370/391],                 Loss: 0.39095, Train_Acc:86.66%
Epoch [32/300], Step [380/391],                 Loss: 0.39111, Train_Acc:86.66%
Epoch [32/300], Step [390/391],                 Loss: 0.39130, Train_Acc:86.65%
Accuary on test images:76.78%
Epoch [33/300], Step [10/391],                 Loss: 0.40990, Train_Acc:85.86%
Epoch [33/300], Step [20/391],                 Loss: 0.39766, Train_Acc:86.29%
Epoch [33/300], Step [30/391],                 Loss: 0.38156, Train_Acc:87.01%
Epoch [33/300], Step [40/391],                 Loss: 0.37996, Train_Acc:87.21%
Epoch [33/300], Step [50/391],                 Loss: 0.37696, Train_Acc:87.30%
Epoch [33/300], Step [60/391],                 Loss: 0.38195, Train_Acc:87.11%
Epoch [33/300], Step [70/391],                 Loss: 0.38409, Train_Acc:87.12%
Epoch [33/300], Step [80/391],                 Loss: 0.38207, Train_Acc:87.27%
Epoch [33/300], Step [90/391],                 Loss: 0.38485, Train_Acc:87.21%
Epoch [33/300], Step [100/391],                 Loss: 0.38198, Train_Acc:87.25%
Epoch [33/300], Step [110/391],                 Loss: 0.38482, Train_Acc:87.17%
Epoch [33/300], Step [120/391],                 Loss: 0.38604, Train_Acc:87.14%
Epoch [33/300], Step [130/391],                 Loss: 0.38664, Train_Acc:87.18%
Epoch [33/300], Step [140/391],                 Loss: 0.38470, Train_Acc:87.26%
Epoch [33/300], Step [150/391],                 Loss: 0.38525, Train_Acc:87.19%
Epoch [33/300], Step [160/391],                 Loss: 0.38509, Train_Acc:87.20%
Epoch [33/300], Step [170/391],                 Loss: 0.38515, Train_Acc:87.13%
Epoch [33/300], Step [180/391],                 Loss: 0.38716, Train_Acc:87.04%
Epoch [33/300], Step [190/391],                 Loss: 0.38681, Train_Acc:86.97%
Epoch [33/300], Step [200/391],                 Loss: 0.38714, Train_Acc:86.94%
Epoch [33/300], Step [210/391],                 Loss: 0.38582, Train_Acc:86.97%
Epoch [33/300], Step [220/391],                 Loss: 0.38611, Train_Acc:86.95%
Epoch [33/300], Step [230/391],                 Loss: 0.38653, Train_Acc:86.94%
Epoch [33/300], Step [240/391],                 Loss: 0.38585, Train_Acc:86.96%
Epoch [33/300], Step [250/391],                 Loss: 0.38817, Train_Acc:86.85%
Epoch [33/300], Step [260/391],                 Loss: 0.39124, Train_Acc:86.74%
Epoch [33/300], Step [270/391],                 Loss: 0.39221, Train_Acc:86.70%
Epoch [33/300], Step [280/391],                 Loss: 0.39243, Train_Acc:86.69%
Epoch [33/300], Step [290/391],                 Loss: 0.39303, Train_Acc:86.68%
Epoch [33/300], Step [300/391],                 Loss: 0.39298, Train_Acc:86.65%
Epoch [33/300], Step [310/391],                 Loss: 0.39155, Train_Acc:86.70%
Epoch [33/300], Step [320/391],                 Loss: 0.39172, Train_Acc:86.66%
Epoch [33/300], Step [330/391],                 Loss: 0.38999, Train_Acc:86.71%
Epoch [33/300], Step [340/391],                 Loss: 0.38889, Train_Acc:86.74%
Epoch [33/300], Step [350/391],                 Loss: 0.38817, Train_Acc:86.74%
Epoch [33/300], Step [360/391],                 Loss: 0.38860, Train_Acc:86.73%
Epoch [33/300], Step [370/391],                 Loss: 0.38922, Train_Acc:86.67%
Epoch [33/300], Step [380/391],                 Loss: 0.39074, Train_Acc:86.62%
Epoch [33/300], Step [390/391],                 Loss: 0.39047, Train_Acc:86.63%
Accuary on test images:77.22%
Epoch [34/300], Step [10/391],                 Loss: 0.39239, Train_Acc:85.86%
Epoch [34/300], Step [20/391],                 Loss: 0.38202, Train_Acc:86.41%
Epoch [34/300], Step [30/391],                 Loss: 0.36998, Train_Acc:86.85%
Epoch [34/300], Step [40/391],                 Loss: 0.37275, Train_Acc:87.09%
Epoch [34/300], Step [50/391],                 Loss: 0.37433, Train_Acc:86.91%
Epoch [34/300], Step [60/391],                 Loss: 0.38176, Train_Acc:86.63%
Epoch [34/300], Step [70/391],                 Loss: 0.38142, Train_Acc:86.75%
Epoch [34/300], Step [80/391],                 Loss: 0.38353, Train_Acc:86.70%
Epoch [34/300], Step [90/391],                 Loss: 0.38359, Train_Acc:86.77%
Epoch [34/300], Step [100/391],                 Loss: 0.37876, Train_Acc:86.86%
Epoch [34/300], Step [110/391],                 Loss: 0.38062, Train_Acc:86.92%
Epoch [34/300], Step [120/391],                 Loss: 0.38173, Train_Acc:86.91%
Epoch [34/300], Step [130/391],                 Loss: 0.38082, Train_Acc:86.95%
Epoch [34/300], Step [140/391],                 Loss: 0.37953, Train_Acc:87.03%
Epoch [34/300], Step [150/391],                 Loss: 0.38127, Train_Acc:86.90%
Epoch [34/300], Step [160/391],                 Loss: 0.38344, Train_Acc:86.80%
Epoch [34/300], Step [170/391],                 Loss: 0.38480, Train_Acc:86.82%
Epoch [34/300], Step [180/391],                 Loss: 0.38456, Train_Acc:86.82%
Epoch [34/300], Step [190/391],                 Loss: 0.38663, Train_Acc:86.77%
Epoch [34/300], Step [200/391],                 Loss: 0.38747, Train_Acc:86.73%
Epoch [34/300], Step [210/391],                 Loss: 0.38899, Train_Acc:86.70%
Epoch [34/300], Step [220/391],                 Loss: 0.38887, Train_Acc:86.70%
Epoch [34/300], Step [230/391],                 Loss: 0.38789, Train_Acc:86.73%
Epoch [34/300], Step [240/391],                 Loss: 0.38608, Train_Acc:86.80%
Epoch [34/300], Step [250/391],                 Loss: 0.38609, Train_Acc:86.78%
Epoch [34/300], Step [260/391],                 Loss: 0.38932, Train_Acc:86.68%
Epoch [34/300], Step [270/391],                 Loss: 0.39023, Train_Acc:86.61%
Epoch [34/300], Step [280/391],                 Loss: 0.39103, Train_Acc:86.58%
Epoch [34/300], Step [290/391],                 Loss: 0.39128, Train_Acc:86.59%
Epoch [34/300], Step [300/391],                 Loss: 0.39019, Train_Acc:86.62%
Epoch [34/300], Step [310/391],                 Loss: 0.38909, Train_Acc:86.63%
Epoch [34/300], Step [320/391],                 Loss: 0.38897, Train_Acc:86.63%
Epoch [34/300], Step [330/391],                 Loss: 0.38896, Train_Acc:86.64%
Epoch [34/300], Step [340/391],                 Loss: 0.38885, Train_Acc:86.62%
Epoch [34/300], Step [350/391],                 Loss: 0.38895, Train_Acc:86.61%
Epoch [34/300], Step [360/391],                 Loss: 0.38969, Train_Acc:86.57%
Epoch [34/300], Step [370/391],                 Loss: 0.39018, Train_Acc:86.54%
Epoch [34/300], Step [380/391],                 Loss: 0.38979, Train_Acc:86.54%
Epoch [34/300], Step [390/391],                 Loss: 0.38958, Train_Acc:86.58%
Accuary on test images:75.06%
Epoch [35/300], Step [10/391],                 Loss: 0.37836, Train_Acc:86.41%
Epoch [35/300], Step [20/391],                 Loss: 0.37179, Train_Acc:87.38%
Epoch [35/300], Step [30/391],                 Loss: 0.37419, Train_Acc:87.11%
Epoch [35/300], Step [40/391],                 Loss: 0.36526, Train_Acc:87.60%
Epoch [35/300], Step [50/391],                 Loss: 0.36302, Train_Acc:87.45%
Epoch [35/300], Step [60/391],                 Loss: 0.37135, Train_Acc:87.06%
Epoch [35/300], Step [70/391],                 Loss: 0.37236, Train_Acc:87.14%
Epoch [35/300], Step [80/391],                 Loss: 0.37501, Train_Acc:87.06%
Epoch [35/300], Step [90/391],                 Loss: 0.37513, Train_Acc:86.96%
Epoch [35/300], Step [100/391],                 Loss: 0.37586, Train_Acc:86.96%
Epoch [35/300], Step [110/391],                 Loss: 0.38158, Train_Acc:86.78%
Epoch [35/300], Step [120/391],                 Loss: 0.38490, Train_Acc:86.67%
Epoch [35/300], Step [130/391],                 Loss: 0.38659, Train_Acc:86.66%
Epoch [35/300], Step [140/391],                 Loss: 0.38460, Train_Acc:86.80%
Epoch [35/300], Step [150/391],                 Loss: 0.38602, Train_Acc:86.76%
Epoch [35/300], Step [160/391],                 Loss: 0.38229, Train_Acc:86.83%
Epoch [35/300], Step [170/391],                 Loss: 0.38166, Train_Acc:86.85%
Epoch [35/300], Step [180/391],                 Loss: 0.38116, Train_Acc:86.78%
Epoch [35/300], Step [190/391],                 Loss: 0.38186, Train_Acc:86.79%
Epoch [35/300], Step [200/391],                 Loss: 0.38381, Train_Acc:86.73%
Epoch [35/300], Step [210/391],                 Loss: 0.38350, Train_Acc:86.78%
Epoch [35/300], Step [220/391],                 Loss: 0.38325, Train_Acc:86.80%
Epoch [35/300], Step [230/391],                 Loss: 0.38265, Train_Acc:86.80%
Epoch [35/300], Step [240/391],                 Loss: 0.38231, Train_Acc:86.86%
Epoch [35/300], Step [250/391],                 Loss: 0.38284, Train_Acc:86.87%
Epoch [35/300], Step [260/391],                 Loss: 0.38703, Train_Acc:86.77%
Epoch [35/300], Step [270/391],                 Loss: 0.38897, Train_Acc:86.71%
Epoch [35/300], Step [280/391],                 Loss: 0.38889, Train_Acc:86.72%
Epoch [35/300], Step [290/391],                 Loss: 0.38851, Train_Acc:86.74%
Epoch [35/300], Step [300/391],                 Loss: 0.38901, Train_Acc:86.71%
Epoch [35/300], Step [310/391],                 Loss: 0.38842, Train_Acc:86.72%
Epoch [35/300], Step [320/391],                 Loss: 0.38908, Train_Acc:86.70%
Epoch [35/300], Step [330/391],                 Loss: 0.38922, Train_Acc:86.68%
Epoch [35/300], Step [340/391],                 Loss: 0.38869, Train_Acc:86.71%
Epoch [35/300], Step [350/391],                 Loss: 0.38833, Train_Acc:86.72%
Epoch [35/300], Step [360/391],                 Loss: 0.38813, Train_Acc:86.73%
Epoch [35/300], Step [370/391],                 Loss: 0.38773, Train_Acc:86.74%
Epoch [35/300], Step [380/391],                 Loss: 0.38693, Train_Acc:86.78%
Epoch [35/300], Step [390/391],                 Loss: 0.38647, Train_Acc:86.79%
Accuary on test images:71.94%
Epoch [36/300], Step [10/391],                 Loss: 0.41005, Train_Acc:86.17%
Epoch [36/300], Step [20/391],                 Loss: 0.40427, Train_Acc:86.37%
Epoch [36/300], Step [30/391],                 Loss: 0.39024, Train_Acc:86.82%
Epoch [36/300], Step [40/391],                 Loss: 0.39449, Train_Acc:86.58%
Epoch [36/300], Step [50/391],                 Loss: 0.38624, Train_Acc:86.80%
Epoch [36/300], Step [60/391],                 Loss: 0.38908, Train_Acc:86.59%
Epoch [36/300], Step [70/391],                 Loss: 0.38363, Train_Acc:86.76%
Epoch [36/300], Step [80/391],                 Loss: 0.38497, Train_Acc:86.66%
Epoch [36/300], Step [90/391],                 Loss: 0.38659, Train_Acc:86.58%
Epoch [36/300], Step [100/391],                 Loss: 0.38779, Train_Acc:86.52%
Epoch [36/300], Step [110/391],                 Loss: 0.39152, Train_Acc:86.48%
Epoch [36/300], Step [120/391],                 Loss: 0.39258, Train_Acc:86.46%
Epoch [36/300], Step [130/391],                 Loss: 0.39743, Train_Acc:86.32%
Epoch [36/300], Step [140/391],                 Loss: 0.39557, Train_Acc:86.38%
Epoch [36/300], Step [150/391],                 Loss: 0.39395, Train_Acc:86.41%
Epoch [36/300], Step [160/391],                 Loss: 0.39211, Train_Acc:86.48%
Epoch [36/300], Step [170/391],                 Loss: 0.39398, Train_Acc:86.44%
Epoch [36/300], Step [180/391],                 Loss: 0.39242, Train_Acc:86.51%
Epoch [36/300], Step [190/391],                 Loss: 0.39177, Train_Acc:86.49%
Epoch [36/300], Step [200/391],                 Loss: 0.39178, Train_Acc:86.50%
Epoch [36/300], Step [210/391],                 Loss: 0.39168, Train_Acc:86.50%
Epoch [36/300], Step [220/391],                 Loss: 0.39089, Train_Acc:86.56%
Epoch [36/300], Step [230/391],                 Loss: 0.38900, Train_Acc:86.60%
Epoch [36/300], Step [240/391],                 Loss: 0.38794, Train_Acc:86.64%
Epoch [36/300], Step [250/391],                 Loss: 0.38750, Train_Acc:86.66%
Epoch [36/300], Step [260/391],                 Loss: 0.39000, Train_Acc:86.60%
Epoch [36/300], Step [270/391],                 Loss: 0.39063, Train_Acc:86.54%
Epoch [36/300], Step [280/391],                 Loss: 0.39017, Train_Acc:86.56%
Epoch [36/300], Step [290/391],                 Loss: 0.38931, Train_Acc:86.62%
Epoch [36/300], Step [300/391],                 Loss: 0.38935, Train_Acc:86.60%
Epoch [36/300], Step [310/391],                 Loss: 0.38792, Train_Acc:86.63%
Epoch [36/300], Step [320/391],                 Loss: 0.38814, Train_Acc:86.63%
Epoch [36/300], Step [330/391],                 Loss: 0.38769, Train_Acc:86.66%
Epoch [36/300], Step [340/391],                 Loss: 0.38723, Train_Acc:86.69%
Epoch [36/300], Step [350/391],                 Loss: 0.38718, Train_Acc:86.70%
Epoch [36/300], Step [360/391],                 Loss: 0.38694, Train_Acc:86.71%
Epoch [36/300], Step [370/391],                 Loss: 0.38734, Train_Acc:86.67%
Epoch [36/300], Step [380/391],                 Loss: 0.38653, Train_Acc:86.73%
Epoch [36/300], Step [390/391],                 Loss: 0.38590, Train_Acc:86.76%
Accuary on test images:66.74%
Epoch [37/300], Step [10/391],                 Loss: 0.40984, Train_Acc:86.48%
Epoch [37/300], Step [20/391],                 Loss: 0.40273, Train_Acc:86.37%
Epoch [37/300], Step [30/391],                 Loss: 0.39165, Train_Acc:86.77%
Epoch [37/300], Step [40/391],                 Loss: 0.39507, Train_Acc:86.62%
Epoch [37/300], Step [50/391],                 Loss: 0.38715, Train_Acc:86.88%
Epoch [37/300], Step [60/391],                 Loss: 0.39133, Train_Acc:86.61%
Epoch [37/300], Step [70/391],                 Loss: 0.38445, Train_Acc:86.91%
Epoch [37/300], Step [80/391],                 Loss: 0.38237, Train_Acc:86.96%
Epoch [37/300], Step [90/391],                 Loss: 0.38599, Train_Acc:86.93%
Epoch [37/300], Step [100/391],                 Loss: 0.38604, Train_Acc:86.83%
Epoch [37/300], Step [110/391],                 Loss: 0.39024, Train_Acc:86.71%
Epoch [37/300], Step [120/391],                 Loss: 0.39416, Train_Acc:86.52%
Epoch [37/300], Step [130/391],                 Loss: 0.39556, Train_Acc:86.45%
Epoch [37/300], Step [140/391],                 Loss: 0.39409, Train_Acc:86.52%
Epoch [37/300], Step [150/391],                 Loss: 0.39486, Train_Acc:86.49%
Epoch [37/300], Step [160/391],                 Loss: 0.39400, Train_Acc:86.51%
Epoch [37/300], Step [170/391],                 Loss: 0.39395, Train_Acc:86.51%
Epoch [37/300], Step [180/391],                 Loss: 0.39311, Train_Acc:86.53%
Epoch [37/300], Step [190/391],                 Loss: 0.39092, Train_Acc:86.63%
Epoch [37/300], Step [200/391],                 Loss: 0.38973, Train_Acc:86.71%
Epoch [37/300], Step [210/391],                 Loss: 0.38865, Train_Acc:86.79%
Epoch [37/300], Step [220/391],                 Loss: 0.38698, Train_Acc:86.88%
Epoch [37/300], Step [230/391],                 Loss: 0.38643, Train_Acc:86.90%
Epoch [37/300], Step [240/391],                 Loss: 0.38705, Train_Acc:86.89%
Epoch [37/300], Step [250/391],                 Loss: 0.38756, Train_Acc:86.83%
Epoch [37/300], Step [260/391],                 Loss: 0.39144, Train_Acc:86.71%
Epoch [37/300], Step [270/391],                 Loss: 0.39372, Train_Acc:86.63%
Epoch [37/300], Step [280/391],                 Loss: 0.39390, Train_Acc:86.64%
Epoch [37/300], Step [290/391],                 Loss: 0.39395, Train_Acc:86.63%
Epoch [37/300], Step [300/391],                 Loss: 0.39251, Train_Acc:86.69%
Epoch [37/300], Step [310/391],                 Loss: 0.39192, Train_Acc:86.67%
Epoch [37/300], Step [320/391],                 Loss: 0.39189, Train_Acc:86.67%
Epoch [37/300], Step [330/391],                 Loss: 0.39134, Train_Acc:86.69%
Epoch [37/300], Step [340/391],                 Loss: 0.39035, Train_Acc:86.71%
Epoch [37/300], Step [350/391],                 Loss: 0.38981, Train_Acc:86.74%
Epoch [37/300], Step [360/391],                 Loss: 0.39013, Train_Acc:86.70%
Epoch [37/300], Step [370/391],                 Loss: 0.39040, Train_Acc:86.70%
Epoch [37/300], Step [380/391],                 Loss: 0.39018, Train_Acc:86.73%
Epoch [37/300], Step [390/391],                 Loss: 0.38872, Train_Acc:86.77%
Accuary on test images:76.40%
Epoch [38/300], Step [10/391],                 Loss: 0.37736, Train_Acc:87.73%
Epoch [38/300], Step [20/391],                 Loss: 0.37944, Train_Acc:87.46%
Epoch [38/300], Step [30/391],                 Loss: 0.37802, Train_Acc:87.53%
Epoch [38/300], Step [40/391],                 Loss: 0.38542, Train_Acc:87.23%
Epoch [38/300], Step [50/391],                 Loss: 0.38681, Train_Acc:87.05%
Epoch [38/300], Step [60/391],                 Loss: 0.39182, Train_Acc:86.73%
Epoch [38/300], Step [70/391],                 Loss: 0.38853, Train_Acc:86.69%
Epoch [38/300], Step [80/391],                 Loss: 0.38739, Train_Acc:86.74%
Epoch [38/300], Step [90/391],                 Loss: 0.38898, Train_Acc:86.57%
Epoch [38/300], Step [100/391],                 Loss: 0.38392, Train_Acc:86.74%
Epoch [38/300], Step [110/391],                 Loss: 0.38523, Train_Acc:86.72%
Epoch [38/300], Step [120/391],                 Loss: 0.38303, Train_Acc:86.78%
Epoch [38/300], Step [130/391],                 Loss: 0.38499, Train_Acc:86.78%
Epoch [38/300], Step [140/391],                 Loss: 0.38441, Train_Acc:86.81%
Epoch [38/300], Step [150/391],                 Loss: 0.38405, Train_Acc:86.82%
Epoch [38/300], Step [160/391],                 Loss: 0.38422, Train_Acc:86.82%
Epoch [38/300], Step [170/391],                 Loss: 0.38440, Train_Acc:86.83%
Epoch [38/300], Step [180/391],                 Loss: 0.38505, Train_Acc:86.78%
Epoch [38/300], Step [190/391],                 Loss: 0.38562, Train_Acc:86.78%
Epoch [38/300], Step [200/391],                 Loss: 0.38632, Train_Acc:86.77%
Epoch [38/300], Step [210/391],                 Loss: 0.38530, Train_Acc:86.88%
Epoch [38/300], Step [220/391],                 Loss: 0.38442, Train_Acc:86.90%
Epoch [38/300], Step [230/391],                 Loss: 0.38350, Train_Acc:86.95%
Epoch [38/300], Step [240/391],                 Loss: 0.38248, Train_Acc:86.98%
Epoch [38/300], Step [250/391],                 Loss: 0.38216, Train_Acc:86.98%
Epoch [38/300], Step [260/391],                 Loss: 0.38373, Train_Acc:86.92%
Epoch [38/300], Step [270/391],                 Loss: 0.38334, Train_Acc:86.97%
Epoch [38/300], Step [280/391],                 Loss: 0.38427, Train_Acc:86.91%
Epoch [38/300], Step [290/391],                 Loss: 0.38474, Train_Acc:86.92%
Epoch [38/300], Step [300/391],                 Loss: 0.38446, Train_Acc:86.91%
Epoch [38/300], Step [310/391],                 Loss: 0.38306, Train_Acc:86.94%
Epoch [38/300], Step [320/391],                 Loss: 0.38270, Train_Acc:86.96%
Epoch [38/300], Step [330/391],                 Loss: 0.38148, Train_Acc:87.00%
Epoch [38/300], Step [340/391],                 Loss: 0.38062, Train_Acc:87.06%
Epoch [38/300], Step [350/391],                 Loss: 0.38099, Train_Acc:87.03%
Epoch [38/300], Step [360/391],                 Loss: 0.38186, Train_Acc:86.98%
Epoch [38/300], Step [370/391],                 Loss: 0.38347, Train_Acc:86.91%
Epoch [38/300], Step [380/391],                 Loss: 0.38395, Train_Acc:86.88%
Epoch [38/300], Step [390/391],                 Loss: 0.38395, Train_Acc:86.89%
Accuary on test images:76.52%
Epoch [39/300], Step [10/391],                 Loss: 0.34931, Train_Acc:88.36%
Epoch [39/300], Step [20/391],                 Loss: 0.36092, Train_Acc:88.09%
Epoch [39/300], Step [30/391],                 Loss: 0.36617, Train_Acc:87.94%
Epoch [39/300], Step [40/391],                 Loss: 0.36781, Train_Acc:87.85%
Epoch [39/300], Step [50/391],                 Loss: 0.37918, Train_Acc:87.36%
Epoch [39/300], Step [60/391],                 Loss: 0.38066, Train_Acc:87.20%
Epoch [39/300], Step [70/391],                 Loss: 0.38096, Train_Acc:87.18%
Epoch [39/300], Step [80/391],                 Loss: 0.37982, Train_Acc:87.15%
Epoch [39/300], Step [90/391],                 Loss: 0.38283, Train_Acc:87.04%
Epoch [39/300], Step [100/391],                 Loss: 0.38228, Train_Acc:87.00%
Epoch [39/300], Step [110/391],                 Loss: 0.38374, Train_Acc:86.98%
Epoch [39/300], Step [120/391],                 Loss: 0.38127, Train_Acc:87.03%
Epoch [39/300], Step [130/391],                 Loss: 0.38322, Train_Acc:87.04%
Epoch [39/300], Step [140/391],                 Loss: 0.38090, Train_Acc:87.05%
Epoch [39/300], Step [150/391],                 Loss: 0.38173, Train_Acc:87.02%
Epoch [39/300], Step [160/391],                 Loss: 0.38128, Train_Acc:86.99%
Epoch [39/300], Step [170/391],                 Loss: 0.38287, Train_Acc:86.91%
Epoch [39/300], Step [180/391],                 Loss: 0.38407, Train_Acc:86.81%
Epoch [39/300], Step [190/391],                 Loss: 0.38350, Train_Acc:86.82%
Epoch [39/300], Step [200/391],                 Loss: 0.38408, Train_Acc:86.80%
Epoch [39/300], Step [210/391],                 Loss: 0.38347, Train_Acc:86.85%
Epoch [39/300], Step [220/391],                 Loss: 0.38259, Train_Acc:86.89%
Epoch [39/300], Step [230/391],                 Loss: 0.38115, Train_Acc:86.92%
Epoch [39/300], Step [240/391],                 Loss: 0.37928, Train_Acc:87.01%
Epoch [39/300], Step [250/391],                 Loss: 0.37915, Train_Acc:87.00%
Epoch [39/300], Step [260/391],                 Loss: 0.38076, Train_Acc:87.00%
Epoch [39/300], Step [270/391],                 Loss: 0.38117, Train_Acc:86.95%
Epoch [39/300], Step [280/391],                 Loss: 0.38123, Train_Acc:86.99%
Epoch [39/300], Step [290/391],                 Loss: 0.38113, Train_Acc:87.01%
Epoch [39/300], Step [300/391],                 Loss: 0.38141, Train_Acc:86.96%
Epoch [39/300], Step [310/391],                 Loss: 0.38164, Train_Acc:86.95%
Epoch [39/300], Step [320/391],                 Loss: 0.38293, Train_Acc:86.93%
Epoch [39/300], Step [330/391],                 Loss: 0.38214, Train_Acc:86.95%
Epoch [39/300], Step [340/391],                 Loss: 0.38056, Train_Acc:87.02%
Epoch [39/300], Step [350/391],                 Loss: 0.38016, Train_Acc:87.03%
Epoch [39/300], Step [360/391],                 Loss: 0.38026, Train_Acc:87.03%
Epoch [39/300], Step [370/391],                 Loss: 0.38129, Train_Acc:87.00%
Epoch [39/300], Step [380/391],                 Loss: 0.38071, Train_Acc:87.01%
Epoch [39/300], Step [390/391],                 Loss: 0.38030, Train_Acc:87.02%
Accuary on test images:73.96%
Epoch [40/300], Step [10/391],                 Loss: 0.39425, Train_Acc:85.62%
Epoch [40/300], Step [20/391],                 Loss: 0.38178, Train_Acc:86.52%
Epoch [40/300], Step [30/391],                 Loss: 0.37788, Train_Acc:86.77%
Epoch [40/300], Step [40/391],                 Loss: 0.37945, Train_Acc:86.76%
Epoch [40/300], Step [50/391],                 Loss: 0.37422, Train_Acc:87.19%
Epoch [40/300], Step [60/391],                 Loss: 0.37517, Train_Acc:87.17%
Epoch [40/300], Step [70/391],                 Loss: 0.37446, Train_Acc:87.20%
Epoch [40/300], Step [80/391],                 Loss: 0.37797, Train_Acc:87.16%
Epoch [40/300], Step [90/391],                 Loss: 0.38061, Train_Acc:87.06%
Epoch [40/300], Step [100/391],                 Loss: 0.37938, Train_Acc:87.10%
Epoch [40/300], Step [110/391],                 Loss: 0.38115, Train_Acc:87.10%
Epoch [40/300], Step [120/391],                 Loss: 0.38124, Train_Acc:87.05%
Epoch [40/300], Step [130/391],                 Loss: 0.38261, Train_Acc:87.00%
Epoch [40/300], Step [140/391],                 Loss: 0.37933, Train_Acc:87.12%
Epoch [40/300], Step [150/391],                 Loss: 0.38029, Train_Acc:87.08%
Epoch [40/300], Step [160/391],                 Loss: 0.37907, Train_Acc:87.10%
Epoch [40/300], Step [170/391],                 Loss: 0.37916, Train_Acc:87.07%
Epoch [40/300], Step [180/391],                 Loss: 0.37805, Train_Acc:87.06%
Epoch [40/300], Step [190/391],                 Loss: 0.37628, Train_Acc:87.13%
Epoch [40/300], Step [200/391],                 Loss: 0.37638, Train_Acc:87.12%
Epoch [40/300], Step [210/391],                 Loss: 0.37790, Train_Acc:87.01%
Epoch [40/300], Step [220/391],                 Loss: 0.37690, Train_Acc:87.04%
Epoch [40/300], Step [230/391],                 Loss: 0.37588, Train_Acc:87.08%
Epoch [40/300], Step [240/391],                 Loss: 0.37501, Train_Acc:87.12%
Epoch [40/300], Step [250/391],                 Loss: 0.37541, Train_Acc:87.10%
Epoch [40/300], Step [260/391],                 Loss: 0.37940, Train_Acc:87.00%
Epoch [40/300], Step [270/391],                 Loss: 0.38204, Train_Acc:86.89%
Epoch [40/300], Step [280/391],                 Loss: 0.38328, Train_Acc:86.81%
Epoch [40/300], Step [290/391],                 Loss: 0.38372, Train_Acc:86.78%
Epoch [40/300], Step [300/391],                 Loss: 0.38481, Train_Acc:86.74%
Epoch [40/300], Step [310/391],                 Loss: 0.38453, Train_Acc:86.73%
Epoch [40/300], Step [320/391],                 Loss: 0.38427, Train_Acc:86.77%
Epoch [40/300], Step [330/391],                 Loss: 0.38321, Train_Acc:86.83%
Epoch [40/300], Step [340/391],                 Loss: 0.38256, Train_Acc:86.84%
Epoch [40/300], Step [350/391],                 Loss: 0.38168, Train_Acc:86.88%
Epoch [40/300], Step [360/391],                 Loss: 0.38218, Train_Acc:86.87%
Epoch [40/300], Step [370/391],                 Loss: 0.38241, Train_Acc:86.85%
Epoch [40/300], Step [380/391],                 Loss: 0.38222, Train_Acc:86.85%
Epoch [40/300], Step [390/391],                 Loss: 0.38173, Train_Acc:86.88%
Accuary on test images:74.58%
Epoch [41/300], Step [10/391],                 Loss: 0.39863, Train_Acc:86.17%
Epoch [41/300], Step [20/391],                 Loss: 0.38787, Train_Acc:87.07%
Epoch [41/300], Step [30/391],                 Loss: 0.37935, Train_Acc:87.45%
Epoch [41/300], Step [40/391],                 Loss: 0.38373, Train_Acc:87.19%
Epoch [41/300], Step [50/391],                 Loss: 0.38070, Train_Acc:87.28%
Epoch [41/300], Step [60/391],                 Loss: 0.38217, Train_Acc:87.23%
Epoch [41/300], Step [70/391],                 Loss: 0.37836, Train_Acc:87.30%
Epoch [41/300], Step [80/391],                 Loss: 0.36984, Train_Acc:87.58%
Epoch [41/300], Step [90/391],                 Loss: 0.36984, Train_Acc:87.49%
Epoch [41/300], Step [100/391],                 Loss: 0.36990, Train_Acc:87.46%
Epoch [41/300], Step [110/391],                 Loss: 0.37434, Train_Acc:87.34%
Epoch [41/300], Step [120/391],                 Loss: 0.37355, Train_Acc:87.34%
Epoch [41/300], Step [130/391],                 Loss: 0.37569, Train_Acc:87.31%
Epoch [41/300], Step [140/391],                 Loss: 0.37872, Train_Acc:87.25%
Epoch [41/300], Step [150/391],                 Loss: 0.38269, Train_Acc:87.14%
Epoch [41/300], Step [160/391],                 Loss: 0.38368, Train_Acc:87.07%
Epoch [41/300], Step [170/391],                 Loss: 0.38192, Train_Acc:87.11%
Epoch [41/300], Step [180/391],                 Loss: 0.38242, Train_Acc:87.08%
Epoch [41/300], Step [190/391],                 Loss: 0.38255, Train_Acc:87.04%
Epoch [41/300], Step [200/391],                 Loss: 0.38298, Train_Acc:87.04%
Epoch [41/300], Step [210/391],                 Loss: 0.38375, Train_Acc:87.01%
Epoch [41/300], Step [220/391],                 Loss: 0.38104, Train_Acc:87.12%
Epoch [41/300], Step [230/391],                 Loss: 0.38056, Train_Acc:87.10%
Epoch [41/300], Step [240/391],                 Loss: 0.37969, Train_Acc:87.17%
Epoch [41/300], Step [250/391],                 Loss: 0.37988, Train_Acc:87.13%
Epoch [41/300], Step [260/391],                 Loss: 0.38216, Train_Acc:87.05%
Epoch [41/300], Step [270/391],                 Loss: 0.38301, Train_Acc:87.04%
Epoch [41/300], Step [280/391],                 Loss: 0.38281, Train_Acc:87.04%
Epoch [41/300], Step [290/391],                 Loss: 0.38235, Train_Acc:87.04%
Epoch [41/300], Step [300/391],                 Loss: 0.38309, Train_Acc:86.97%
Epoch [41/300], Step [310/391],                 Loss: 0.38197, Train_Acc:87.01%
Epoch [41/300], Step [320/391],                 Loss: 0.38234, Train_Acc:87.00%
Epoch [41/300], Step [330/391],                 Loss: 0.38123, Train_Acc:87.02%
Epoch [41/300], Step [340/391],                 Loss: 0.38058, Train_Acc:87.03%
Epoch [41/300], Step [350/391],                 Loss: 0.38134, Train_Acc:87.00%
Epoch [41/300], Step [360/391],                 Loss: 0.38131, Train_Acc:86.98%
Epoch [41/300], Step [370/391],                 Loss: 0.38075, Train_Acc:86.98%
Epoch [41/300], Step [380/391],                 Loss: 0.38053, Train_Acc:86.98%
Epoch [41/300], Step [390/391],                 Loss: 0.38057, Train_Acc:86.97%
Accuary on test images:78.88%
Epoch [42/300], Step [10/391],                 Loss: 0.39921, Train_Acc:86.17%
Epoch [42/300], Step [20/391],                 Loss: 0.38081, Train_Acc:86.99%
Epoch [42/300], Step [30/391],                 Loss: 0.37532, Train_Acc:86.93%
Epoch [42/300], Step [40/391],                 Loss: 0.37216, Train_Acc:87.30%
Epoch [42/300], Step [50/391],                 Loss: 0.37240, Train_Acc:87.14%
Epoch [42/300], Step [60/391],                 Loss: 0.38055, Train_Acc:87.02%
Epoch [42/300], Step [70/391],                 Loss: 0.38393, Train_Acc:86.82%
Epoch [42/300], Step [80/391],                 Loss: 0.38303, Train_Acc:86.80%
Epoch [42/300], Step [90/391],                 Loss: 0.38551, Train_Acc:86.68%
Epoch [42/300], Step [100/391],                 Loss: 0.38365, Train_Acc:86.77%
Epoch [42/300], Step [110/391],                 Loss: 0.38531, Train_Acc:86.73%
Epoch [42/300], Step [120/391],                 Loss: 0.38442, Train_Acc:86.72%
Epoch [42/300], Step [130/391],                 Loss: 0.38603, Train_Acc:86.70%
Epoch [42/300], Step [140/391],                 Loss: 0.38559, Train_Acc:86.77%
Epoch [42/300], Step [150/391],                 Loss: 0.38732, Train_Acc:86.75%
Epoch [42/300], Step [160/391],                 Loss: 0.38662, Train_Acc:86.77%
Epoch [42/300], Step [170/391],                 Loss: 0.38717, Train_Acc:86.76%
Epoch [42/300], Step [180/391],                 Loss: 0.38564, Train_Acc:86.81%
Epoch [42/300], Step [190/391],                 Loss: 0.38620, Train_Acc:86.79%
Epoch [42/300], Step [200/391],                 Loss: 0.38557, Train_Acc:86.80%
Epoch [42/300], Step [210/391],                 Loss: 0.38463, Train_Acc:86.80%
Epoch [42/300], Step [220/391],                 Loss: 0.38440, Train_Acc:86.82%
Epoch [42/300], Step [230/391],                 Loss: 0.38132, Train_Acc:86.92%
Epoch [42/300], Step [240/391],                 Loss: 0.37946, Train_Acc:86.98%
Epoch [42/300], Step [250/391],                 Loss: 0.37893, Train_Acc:87.01%
Epoch [42/300], Step [260/391],                 Loss: 0.38102, Train_Acc:86.92%
Epoch [42/300], Step [270/391],                 Loss: 0.38283, Train_Acc:86.86%
Epoch [42/300], Step [280/391],                 Loss: 0.38383, Train_Acc:86.81%
Epoch [42/300], Step [290/391],                 Loss: 0.38390, Train_Acc:86.84%
Epoch [42/300], Step [300/391],                 Loss: 0.38399, Train_Acc:86.84%
Epoch [42/300], Step [310/391],                 Loss: 0.38412, Train_Acc:86.83%
Epoch [42/300], Step [320/391],                 Loss: 0.38330, Train_Acc:86.89%
Epoch [42/300], Step [330/391],                 Loss: 0.38241, Train_Acc:86.92%
Epoch [42/300], Step [340/391],                 Loss: 0.38189, Train_Acc:86.94%
Epoch [42/300], Step [350/391],                 Loss: 0.38090, Train_Acc:86.99%
Epoch [42/300], Step [360/391],                 Loss: 0.38081, Train_Acc:87.00%
Epoch [42/300], Step [370/391],                 Loss: 0.38134, Train_Acc:86.96%
Epoch [42/300], Step [380/391],                 Loss: 0.38125, Train_Acc:86.96%
Epoch [42/300], Step [390/391],                 Loss: 0.38091, Train_Acc:86.98%
Accuary on test images:76.08%
Epoch [43/300], Step [10/391],                 Loss: 0.35404, Train_Acc:89.30%
Epoch [43/300], Step [20/391],                 Loss: 0.35368, Train_Acc:88.98%
Epoch [43/300], Step [30/391],                 Loss: 0.34684, Train_Acc:88.83%
Epoch [43/300], Step [40/391],                 Loss: 0.35252, Train_Acc:88.54%
Epoch [43/300], Step [50/391],                 Loss: 0.35575, Train_Acc:88.09%
Epoch [43/300], Step [60/391],                 Loss: 0.37165, Train_Acc:87.64%
Epoch [43/300], Step [70/391],                 Loss: 0.37702, Train_Acc:87.39%
Epoch [43/300], Step [80/391],                 Loss: 0.38337, Train_Acc:87.13%
Epoch [43/300], Step [90/391],                 Loss: 0.38452, Train_Acc:87.01%
Epoch [43/300], Step [100/391],                 Loss: 0.37957, Train_Acc:87.11%
Epoch [43/300], Step [110/391],                 Loss: 0.37970, Train_Acc:87.18%
Epoch [43/300], Step [120/391],                 Loss: 0.37741, Train_Acc:87.25%
Epoch [43/300], Step [130/391],                 Loss: 0.37612, Train_Acc:87.29%
Epoch [43/300], Step [140/391],                 Loss: 0.37671, Train_Acc:87.21%
Epoch [43/300], Step [150/391],                 Loss: 0.37758, Train_Acc:87.23%
Epoch [43/300], Step [160/391],                 Loss: 0.37736, Train_Acc:87.17%
Epoch [43/300], Step [170/391],                 Loss: 0.37610, Train_Acc:87.15%
Epoch [43/300], Step [180/391],                 Loss: 0.37697, Train_Acc:87.08%
Epoch [43/300], Step [190/391],                 Loss: 0.37653, Train_Acc:87.07%
Epoch [43/300], Step [200/391],                 Loss: 0.37857, Train_Acc:86.95%
Epoch [43/300], Step [210/391],                 Loss: 0.37828, Train_Acc:87.00%
Epoch [43/300], Step [220/391],                 Loss: 0.37911, Train_Acc:86.93%
Epoch [43/300], Step [230/391],                 Loss: 0.37856, Train_Acc:86.93%
Epoch [43/300], Step [240/391],                 Loss: 0.37843, Train_Acc:86.96%
Epoch [43/300], Step [250/391],                 Loss: 0.37857, Train_Acc:86.96%
Epoch [43/300], Step [260/391],                 Loss: 0.38041, Train_Acc:86.94%
Epoch [43/300], Step [270/391],                 Loss: 0.38154, Train_Acc:86.88%
Epoch [43/300], Step [280/391],                 Loss: 0.38018, Train_Acc:86.94%
Epoch [43/300], Step [290/391],                 Loss: 0.37982, Train_Acc:86.93%
Epoch [43/300], Step [300/391],                 Loss: 0.37899, Train_Acc:86.97%
Epoch [43/300], Step [310/391],                 Loss: 0.37771, Train_Acc:87.02%
Epoch [43/300], Step [320/391],                 Loss: 0.37907, Train_Acc:87.02%
Epoch [43/300], Step [330/391],                 Loss: 0.37959, Train_Acc:87.02%
Epoch [43/300], Step [340/391],                 Loss: 0.37917, Train_Acc:87.03%
Epoch [43/300], Step [350/391],                 Loss: 0.37870, Train_Acc:87.05%
Epoch [43/300], Step [360/391],                 Loss: 0.37880, Train_Acc:87.02%
Epoch [43/300], Step [370/391],                 Loss: 0.37990, Train_Acc:86.98%
Epoch [43/300], Step [380/391],                 Loss: 0.37995, Train_Acc:86.98%
Epoch [43/300], Step [390/391],                 Loss: 0.37899, Train_Acc:87.01%
Accuary on test images:77.34%
Epoch [44/300], Step [10/391],                 Loss: 0.34729, Train_Acc:87.58%
Epoch [44/300], Step [20/391],                 Loss: 0.37246, Train_Acc:86.95%
Epoch [44/300], Step [30/391],                 Loss: 0.36550, Train_Acc:86.98%
Epoch [44/300], Step [40/391],                 Loss: 0.36727, Train_Acc:86.93%
Epoch [44/300], Step [50/391],                 Loss: 0.37299, Train_Acc:86.91%
Epoch [44/300], Step [60/391],                 Loss: 0.37515, Train_Acc:86.71%
Epoch [44/300], Step [70/391],                 Loss: 0.37446, Train_Acc:86.73%
Epoch [44/300], Step [80/391],                 Loss: 0.37573, Train_Acc:86.73%
Epoch [44/300], Step [90/391],                 Loss: 0.38131, Train_Acc:86.48%
Epoch [44/300], Step [100/391],                 Loss: 0.37927, Train_Acc:86.66%
Epoch [44/300], Step [110/391],                 Loss: 0.38424, Train_Acc:86.56%
Epoch [44/300], Step [120/391],                 Loss: 0.38389, Train_Acc:86.59%
Epoch [44/300], Step [130/391],                 Loss: 0.38552, Train_Acc:86.61%
Epoch [44/300], Step [140/391],                 Loss: 0.38249, Train_Acc:86.72%
Epoch [44/300], Step [150/391],                 Loss: 0.38244, Train_Acc:86.68%
Epoch [44/300], Step [160/391],                 Loss: 0.38047, Train_Acc:86.75%
Epoch [44/300], Step [170/391],                 Loss: 0.37969, Train_Acc:86.78%
Epoch [44/300], Step [180/391],                 Loss: 0.38031, Train_Acc:86.79%
Epoch [44/300], Step [190/391],                 Loss: 0.38158, Train_Acc:86.71%
Epoch [44/300], Step [200/391],                 Loss: 0.38199, Train_Acc:86.70%
Epoch [44/300], Step [210/391],                 Loss: 0.38210, Train_Acc:86.70%
Epoch [44/300], Step [220/391],                 Loss: 0.38305, Train_Acc:86.72%
Epoch [44/300], Step [230/391],                 Loss: 0.38156, Train_Acc:86.78%
Epoch [44/300], Step [240/391],                 Loss: 0.37951, Train_Acc:86.88%
Epoch [44/300], Step [250/391],                 Loss: 0.37995, Train_Acc:86.85%
Epoch [44/300], Step [260/391],                 Loss: 0.38245, Train_Acc:86.80%
Epoch [44/300], Step [270/391],                 Loss: 0.38381, Train_Acc:86.75%
Epoch [44/300], Step [280/391],                 Loss: 0.38265, Train_Acc:86.80%
Epoch [44/300], Step [290/391],                 Loss: 0.38271, Train_Acc:86.79%
Epoch [44/300], Step [300/391],                 Loss: 0.38267, Train_Acc:86.79%
Epoch [44/300], Step [310/391],                 Loss: 0.38256, Train_Acc:86.81%
Epoch [44/300], Step [320/391],                 Loss: 0.38242, Train_Acc:86.82%
Epoch [44/300], Step [330/391],                 Loss: 0.38107, Train_Acc:86.87%
Epoch [44/300], Step [340/391],                 Loss: 0.38010, Train_Acc:86.90%
Epoch [44/300], Step [350/391],                 Loss: 0.37921, Train_Acc:86.97%
Epoch [44/300], Step [360/391],                 Loss: 0.37987, Train_Acc:86.93%
Epoch [44/300], Step [370/391],                 Loss: 0.38093, Train_Acc:86.90%
Epoch [44/300], Step [380/391],                 Loss: 0.38096, Train_Acc:86.90%
Epoch [44/300], Step [390/391],                 Loss: 0.38056, Train_Acc:86.91%
Accuary on test images:78.00%
Epoch [45/300], Step [10/391],                 Loss: 0.39877, Train_Acc:86.72%
Epoch [45/300], Step [20/391],                 Loss: 0.38897, Train_Acc:86.84%
Epoch [45/300], Step [30/391],                 Loss: 0.37726, Train_Acc:87.16%
Epoch [45/300], Step [40/391],                 Loss: 0.37263, Train_Acc:87.44%
Epoch [45/300], Step [50/391],                 Loss: 0.36779, Train_Acc:87.55%
Epoch [45/300], Step [60/391],                 Loss: 0.37480, Train_Acc:87.23%
Epoch [45/300], Step [70/391],                 Loss: 0.37528, Train_Acc:87.19%
Epoch [45/300], Step [80/391],                 Loss: 0.37613, Train_Acc:87.10%
Epoch [45/300], Step [90/391],                 Loss: 0.37816, Train_Acc:87.14%
Epoch [45/300], Step [100/391],                 Loss: 0.38080, Train_Acc:87.05%
Epoch [45/300], Step [110/391],                 Loss: 0.38449, Train_Acc:86.97%
Epoch [45/300], Step [120/391],                 Loss: 0.38580, Train_Acc:86.88%
Epoch [45/300], Step [130/391],                 Loss: 0.38584, Train_Acc:86.91%
Epoch [45/300], Step [140/391],                 Loss: 0.38474, Train_Acc:86.98%
Epoch [45/300], Step [150/391],                 Loss: 0.38465, Train_Acc:86.98%
Epoch [45/300], Step [160/391],                 Loss: 0.38415, Train_Acc:86.99%
Epoch [45/300], Step [170/391],                 Loss: 0.38454, Train_Acc:86.95%
Epoch [45/300], Step [180/391],                 Loss: 0.38272, Train_Acc:86.97%
Epoch [45/300], Step [190/391],                 Loss: 0.38374, Train_Acc:86.94%
Epoch [45/300], Step [200/391],                 Loss: 0.38372, Train_Acc:86.93%
Epoch [45/300], Step [210/391],                 Loss: 0.38229, Train_Acc:86.94%
Epoch [45/300], Step [220/391],                 Loss: 0.38077, Train_Acc:86.96%
Epoch [45/300], Step [230/391],                 Loss: 0.37905, Train_Acc:86.98%
Epoch [45/300], Step [240/391],                 Loss: 0.37826, Train_Acc:86.98%
Epoch [45/300], Step [250/391],                 Loss: 0.37938, Train_Acc:86.96%
Epoch [45/300], Step [260/391],                 Loss: 0.38259, Train_Acc:86.88%
Epoch [45/300], Step [270/391],                 Loss: 0.38241, Train_Acc:86.89%
Epoch [45/300], Step [280/391],                 Loss: 0.38155, Train_Acc:86.92%
Epoch [45/300], Step [290/391],                 Loss: 0.38190, Train_Acc:86.92%
Epoch [45/300], Step [300/391],                 Loss: 0.38141, Train_Acc:86.89%
Epoch [45/300], Step [310/391],                 Loss: 0.38049, Train_Acc:86.91%
Epoch [45/300], Step [320/391],                 Loss: 0.38051, Train_Acc:86.89%
Epoch [45/300], Step [330/391],                 Loss: 0.37932, Train_Acc:86.96%
Epoch [45/300], Step [340/391],                 Loss: 0.37774, Train_Acc:87.02%
Epoch [45/300], Step [350/391],                 Loss: 0.37745, Train_Acc:87.03%
Epoch [45/300], Step [360/391],                 Loss: 0.37778, Train_Acc:87.02%
Epoch [45/300], Step [370/391],                 Loss: 0.37762, Train_Acc:87.01%
Epoch [45/300], Step [380/391],                 Loss: 0.37753, Train_Acc:87.04%
Epoch [45/300], Step [390/391],                 Loss: 0.37715, Train_Acc:87.06%
Accuary on test images:75.06%
Epoch [46/300], Step [10/391],                 Loss: 0.38991, Train_Acc:86.48%
Epoch [46/300], Step [20/391],                 Loss: 0.38269, Train_Acc:87.11%
Epoch [46/300], Step [30/391],                 Loss: 0.37780, Train_Acc:87.37%
Epoch [46/300], Step [40/391],                 Loss: 0.38069, Train_Acc:87.19%
Epoch [46/300], Step [50/391],                 Loss: 0.38259, Train_Acc:86.95%
Epoch [46/300], Step [60/391],                 Loss: 0.39101, Train_Acc:86.56%
Epoch [46/300], Step [70/391],                 Loss: 0.39136, Train_Acc:86.71%
Epoch [46/300], Step [80/391],                 Loss: 0.38873, Train_Acc:86.83%
Epoch [46/300], Step [90/391],                 Loss: 0.38965, Train_Acc:86.86%
Epoch [46/300], Step [100/391],                 Loss: 0.38401, Train_Acc:87.02%
Epoch [46/300], Step [110/391],                 Loss: 0.38688, Train_Acc:86.95%
Epoch [46/300], Step [120/391],                 Loss: 0.38491, Train_Acc:86.99%
Epoch [46/300], Step [130/391],                 Loss: 0.38775, Train_Acc:86.89%
Epoch [46/300], Step [140/391],                 Loss: 0.38772, Train_Acc:86.90%
Epoch [46/300], Step [150/391],                 Loss: 0.38703, Train_Acc:86.89%
Epoch [46/300], Step [160/391],                 Loss: 0.38500, Train_Acc:86.91%
Epoch [46/300], Step [170/391],                 Loss: 0.38476, Train_Acc:86.88%
Epoch [46/300], Step [180/391],                 Loss: 0.38414, Train_Acc:86.90%
Epoch [46/300], Step [190/391],                 Loss: 0.38185, Train_Acc:86.92%
Epoch [46/300], Step [200/391],                 Loss: 0.38313, Train_Acc:86.82%
Epoch [46/300], Step [210/391],                 Loss: 0.38308, Train_Acc:86.85%
Epoch [46/300], Step [220/391],                 Loss: 0.38140, Train_Acc:86.86%
Epoch [46/300], Step [230/391],                 Loss: 0.37934, Train_Acc:86.93%
Epoch [46/300], Step [240/391],                 Loss: 0.37745, Train_Acc:87.00%
Epoch [46/300], Step [250/391],                 Loss: 0.37697, Train_Acc:87.02%
Epoch [46/300], Step [260/391],                 Loss: 0.37826, Train_Acc:86.96%
Epoch [46/300], Step [270/391],                 Loss: 0.37951, Train_Acc:86.94%
Epoch [46/300], Step [280/391],                 Loss: 0.38004, Train_Acc:86.91%
Epoch [46/300], Step [290/391],                 Loss: 0.37976, Train_Acc:86.92%
Epoch [46/300], Step [300/391],                 Loss: 0.37966, Train_Acc:86.88%
Epoch [46/300], Step [310/391],                 Loss: 0.37819, Train_Acc:86.95%
Epoch [46/300], Step [320/391],                 Loss: 0.37838, Train_Acc:86.96%
Epoch [46/300], Step [330/391],                 Loss: 0.37734, Train_Acc:87.01%
Epoch [46/300], Step [340/391],                 Loss: 0.37642, Train_Acc:87.04%
Epoch [46/300], Step [350/391],                 Loss: 0.37593, Train_Acc:87.06%
Epoch [46/300], Step [360/391],                 Loss: 0.37602, Train_Acc:87.06%
Epoch [46/300], Step [370/391],                 Loss: 0.37704, Train_Acc:87.02%
Epoch [46/300], Step [380/391],                 Loss: 0.37771, Train_Acc:87.00%
Epoch [46/300], Step [390/391],                 Loss: 0.37720, Train_Acc:87.02%
Accuary on test images:67.16%
Epoch [47/300], Step [10/391],                 Loss: 0.39155, Train_Acc:87.11%
Epoch [47/300], Step [20/391],                 Loss: 0.38118, Train_Acc:87.19%
Epoch [47/300], Step [30/391],                 Loss: 0.37338, Train_Acc:87.24%
Epoch [47/300], Step [40/391],                 Loss: 0.37459, Train_Acc:87.34%
Epoch [47/300], Step [50/391],                 Loss: 0.36968, Train_Acc:87.42%
Epoch [47/300], Step [60/391],                 Loss: 0.37343, Train_Acc:87.15%
Epoch [47/300], Step [70/391],                 Loss: 0.36689, Train_Acc:87.41%
Epoch [47/300], Step [80/391],                 Loss: 0.36744, Train_Acc:87.32%
Epoch [47/300], Step [90/391],                 Loss: 0.37236, Train_Acc:87.01%
Epoch [47/300], Step [100/391],                 Loss: 0.37342, Train_Acc:87.00%
Epoch [47/300], Step [110/391],                 Loss: 0.37623, Train_Acc:86.88%
Epoch [47/300], Step [120/391],                 Loss: 0.37665, Train_Acc:86.87%
Epoch [47/300], Step [130/391],                 Loss: 0.38072, Train_Acc:86.80%
Epoch [47/300], Step [140/391],                 Loss: 0.37936, Train_Acc:86.91%
Epoch [47/300], Step [150/391],                 Loss: 0.38283, Train_Acc:86.83%
Epoch [47/300], Step [160/391],                 Loss: 0.38176, Train_Acc:86.88%
Epoch [47/300], Step [170/391],                 Loss: 0.38411, Train_Acc:86.77%
Epoch [47/300], Step [180/391],                 Loss: 0.38489, Train_Acc:86.75%
Epoch [47/300], Step [190/391],                 Loss: 0.38532, Train_Acc:86.71%
Epoch [47/300], Step [200/391],                 Loss: 0.38606, Train_Acc:86.68%
Epoch [47/300], Step [210/391],                 Loss: 0.38594, Train_Acc:86.67%
Epoch [47/300], Step [220/391],                 Loss: 0.38468, Train_Acc:86.73%
Epoch [47/300], Step [230/391],                 Loss: 0.38228, Train_Acc:86.80%
Epoch [47/300], Step [240/391],                 Loss: 0.37987, Train_Acc:86.90%
Epoch [47/300], Step [250/391],                 Loss: 0.37777, Train_Acc:86.97%
Epoch [47/300], Step [260/391],                 Loss: 0.37857, Train_Acc:86.93%
Epoch [47/300], Step [270/391],                 Loss: 0.37893, Train_Acc:86.91%
Epoch [47/300], Step [280/391],                 Loss: 0.37770, Train_Acc:86.95%
Epoch [47/300], Step [290/391],                 Loss: 0.37821, Train_Acc:86.93%
Epoch [47/300], Step [300/391],                 Loss: 0.37752, Train_Acc:86.95%
Epoch [47/300], Step [310/391],                 Loss: 0.37709, Train_Acc:86.98%
Epoch [47/300], Step [320/391],                 Loss: 0.37695, Train_Acc:87.00%
Epoch [47/300], Step [330/391],                 Loss: 0.37582, Train_Acc:87.02%
Epoch [47/300], Step [340/391],                 Loss: 0.37599, Train_Acc:87.02%
Epoch [47/300], Step [350/391],                 Loss: 0.37573, Train_Acc:87.03%
Epoch [47/300], Step [360/391],                 Loss: 0.37574, Train_Acc:87.00%
Epoch [47/300], Step [370/391],                 Loss: 0.37632, Train_Acc:86.96%
Epoch [47/300], Step [380/391],                 Loss: 0.37663, Train_Acc:86.96%
Epoch [47/300], Step [390/391],                 Loss: 0.37621, Train_Acc:86.99%
Accuary on test images:75.92%
Epoch [48/300], Step [10/391],                 Loss: 0.37642, Train_Acc:86.48%
Epoch [48/300], Step [20/391],                 Loss: 0.38109, Train_Acc:86.45%
Epoch [48/300], Step [30/391],                 Loss: 0.37549, Train_Acc:86.98%
Epoch [48/300], Step [40/391],                 Loss: 0.37188, Train_Acc:87.23%
Epoch [48/300], Step [50/391],                 Loss: 0.37271, Train_Acc:87.06%
Epoch [48/300], Step [60/391],                 Loss: 0.37959, Train_Acc:86.88%
Epoch [48/300], Step [70/391],                 Loss: 0.38070, Train_Acc:86.98%
Epoch [48/300], Step [80/391],                 Loss: 0.37954, Train_Acc:87.05%
Epoch [48/300], Step [90/391],                 Loss: 0.38156, Train_Acc:87.03%
Epoch [48/300], Step [100/391],                 Loss: 0.38090, Train_Acc:86.99%
Epoch [48/300], Step [110/391],                 Loss: 0.38409, Train_Acc:86.83%
Epoch [48/300], Step [120/391],                 Loss: 0.38319, Train_Acc:86.88%
Epoch [48/300], Step [130/391],                 Loss: 0.38663, Train_Acc:86.78%
Epoch [48/300], Step [140/391],                 Loss: 0.38560, Train_Acc:86.88%
Epoch [48/300], Step [150/391],                 Loss: 0.38550, Train_Acc:86.91%
Epoch [48/300], Step [160/391],                 Loss: 0.38265, Train_Acc:87.03%
Epoch [48/300], Step [170/391],                 Loss: 0.38178, Train_Acc:87.05%
Epoch [48/300], Step [180/391],                 Loss: 0.38000, Train_Acc:87.09%
Epoch [48/300], Step [190/391],                 Loss: 0.37849, Train_Acc:87.15%
Epoch [48/300], Step [200/391],                 Loss: 0.38007, Train_Acc:87.06%
Epoch [48/300], Step [210/391],                 Loss: 0.37915, Train_Acc:87.07%
Epoch [48/300], Step [220/391],                 Loss: 0.37840, Train_Acc:87.08%
Epoch [48/300], Step [230/391],                 Loss: 0.37653, Train_Acc:87.11%
Epoch [48/300], Step [240/391],                 Loss: 0.37616, Train_Acc:87.10%
Epoch [48/300], Step [250/391],                 Loss: 0.37591, Train_Acc:87.10%
Epoch [48/300], Step [260/391],                 Loss: 0.37833, Train_Acc:87.02%
Epoch [48/300], Step [270/391],                 Loss: 0.37877, Train_Acc:87.03%
Epoch [48/300], Step [280/391],                 Loss: 0.37884, Train_Acc:87.01%
Epoch [48/300], Step [290/391],                 Loss: 0.37805, Train_Acc:87.04%
Epoch [48/300], Step [300/391],                 Loss: 0.37767, Train_Acc:87.05%
Epoch [48/300], Step [310/391],                 Loss: 0.37667, Train_Acc:87.07%
Epoch [48/300], Step [320/391],                 Loss: 0.37809, Train_Acc:87.02%
Epoch [48/300], Step [330/391],                 Loss: 0.37758, Train_Acc:87.04%
Epoch [48/300], Step [340/391],                 Loss: 0.37686, Train_Acc:87.08%
Epoch [48/300], Step [350/391],                 Loss: 0.37634, Train_Acc:87.12%
Epoch [48/300], Step [360/391],                 Loss: 0.37708, Train_Acc:87.09%
Epoch [48/300], Step [370/391],                 Loss: 0.37730, Train_Acc:87.07%
Epoch [48/300], Step [380/391],                 Loss: 0.37802, Train_Acc:87.03%
Epoch [48/300], Step [390/391],                 Loss: 0.37727, Train_Acc:87.04%
Accuary on test images:74.28%
Epoch [49/300], Step [10/391],                 Loss: 0.36990, Train_Acc:87.89%
Epoch [49/300], Step [20/391],                 Loss: 0.37157, Train_Acc:87.30%
Epoch [49/300], Step [30/391],                 Loss: 0.36649, Train_Acc:87.32%
Epoch [49/300], Step [40/391],                 Loss: 0.36985, Train_Acc:87.09%
Epoch [49/300], Step [50/391],                 Loss: 0.36489, Train_Acc:87.19%
Epoch [49/300], Step [60/391],                 Loss: 0.36369, Train_Acc:87.29%
Epoch [49/300], Step [70/391],                 Loss: 0.36282, Train_Acc:87.41%
Epoch [49/300], Step [80/391],                 Loss: 0.36123, Train_Acc:87.42%
Epoch [49/300], Step [90/391],                 Loss: 0.36710, Train_Acc:87.20%
Epoch [49/300], Step [100/391],                 Loss: 0.36480, Train_Acc:87.23%
Epoch [49/300], Step [110/391],                 Loss: 0.36687, Train_Acc:87.19%
Epoch [49/300], Step [120/391],                 Loss: 0.36778, Train_Acc:87.15%
Epoch [49/300], Step [130/391],                 Loss: 0.37083, Train_Acc:87.12%
Epoch [49/300], Step [140/391],                 Loss: 0.37057, Train_Acc:87.17%
Epoch [49/300], Step [150/391],                 Loss: 0.37234, Train_Acc:87.11%
Epoch [49/300], Step [160/391],                 Loss: 0.37257, Train_Acc:87.12%
Epoch [49/300], Step [170/391],                 Loss: 0.37254, Train_Acc:87.12%
Epoch [49/300], Step [180/391],                 Loss: 0.37184, Train_Acc:87.15%
Epoch [49/300], Step [190/391],                 Loss: 0.37387, Train_Acc:87.09%
Epoch [49/300], Step [200/391],                 Loss: 0.37659, Train_Acc:86.99%
Epoch [49/300], Step [210/391],                 Loss: 0.37717, Train_Acc:86.99%
Epoch [49/300], Step [220/391],                 Loss: 0.37547, Train_Acc:87.04%
Epoch [49/300], Step [230/391],                 Loss: 0.37440, Train_Acc:87.11%
Epoch [49/300], Step [240/391],                 Loss: 0.37364, Train_Acc:87.13%
Epoch [49/300], Step [250/391],                 Loss: 0.37280, Train_Acc:87.13%
Epoch [49/300], Step [260/391],                 Loss: 0.37628, Train_Acc:87.00%
Epoch [49/300], Step [270/391],                 Loss: 0.37808, Train_Acc:86.96%
Epoch [49/300], Step [280/391],                 Loss: 0.37915, Train_Acc:86.93%
Epoch [49/300], Step [290/391],                 Loss: 0.37980, Train_Acc:86.93%
Epoch [49/300], Step [300/391],                 Loss: 0.37916, Train_Acc:86.96%
Epoch [49/300], Step [310/391],                 Loss: 0.37938, Train_Acc:86.92%
Epoch [49/300], Step [320/391],                 Loss: 0.37917, Train_Acc:86.93%
Epoch [49/300], Step [330/391],                 Loss: 0.37836, Train_Acc:86.95%
Epoch [49/300], Step [340/391],                 Loss: 0.37790, Train_Acc:86.99%
Epoch [49/300], Step [350/391],                 Loss: 0.37750, Train_Acc:87.00%
Epoch [49/300], Step [360/391],                 Loss: 0.37773, Train_Acc:86.99%
Epoch [49/300], Step [370/391],                 Loss: 0.37864, Train_Acc:86.93%
Epoch [49/300], Step [380/391],                 Loss: 0.37805, Train_Acc:86.96%
Epoch [49/300], Step [390/391],                 Loss: 0.37717, Train_Acc:86.97%
Accuary on test images:78.66%
Epoch [50/300], Step [10/391],                 Loss: 0.37344, Train_Acc:86.95%
Epoch [50/300], Step [20/391],                 Loss: 0.35079, Train_Acc:87.93%
Epoch [50/300], Step [30/391],                 Loss: 0.34128, Train_Acc:88.44%
Epoch [50/300], Step [40/391],                 Loss: 0.35438, Train_Acc:88.07%
Epoch [50/300], Step [50/391],                 Loss: 0.36439, Train_Acc:87.78%
Epoch [50/300], Step [60/391],                 Loss: 0.36913, Train_Acc:87.73%
Epoch [50/300], Step [70/391],                 Loss: 0.36786, Train_Acc:87.79%
Epoch [50/300], Step [80/391],                 Loss: 0.36963, Train_Acc:87.80%
Epoch [50/300], Step [90/391],                 Loss: 0.37019, Train_Acc:87.70%
Epoch [50/300], Step [100/391],                 Loss: 0.36960, Train_Acc:87.66%
Epoch [50/300], Step [110/391],                 Loss: 0.37449, Train_Acc:87.46%
Epoch [50/300], Step [120/391],                 Loss: 0.37671, Train_Acc:87.39%
Epoch [50/300], Step [130/391],                 Loss: 0.38118, Train_Acc:87.23%
Epoch [50/300], Step [140/391],                 Loss: 0.38086, Train_Acc:87.31%
Epoch [50/300], Step [150/391],                 Loss: 0.38171, Train_Acc:87.22%
Epoch [50/300], Step [160/391],                 Loss: 0.38207, Train_Acc:87.20%
Epoch [50/300], Step [170/391],                 Loss: 0.38241, Train_Acc:87.13%
Epoch [50/300], Step [180/391],                 Loss: 0.38117, Train_Acc:87.12%
Epoch [50/300], Step [190/391],                 Loss: 0.38091, Train_Acc:87.06%
Epoch [50/300], Step [200/391],                 Loss: 0.37928, Train_Acc:87.07%
Epoch [50/300], Step [210/391],                 Loss: 0.37865, Train_Acc:87.12%
Epoch [50/300], Step [220/391],                 Loss: 0.37629, Train_Acc:87.19%
Epoch [50/300], Step [230/391],                 Loss: 0.37442, Train_Acc:87.25%
Epoch [50/300], Step [240/391],                 Loss: 0.37361, Train_Acc:87.26%
Epoch [50/300], Step [250/391],                 Loss: 0.37274, Train_Acc:87.25%
Epoch [50/300], Step [260/391],                 Loss: 0.37445, Train_Acc:87.22%
Epoch [50/300], Step [270/391],                 Loss: 0.37539, Train_Acc:87.21%
Epoch [50/300], Step [280/391],                 Loss: 0.37571, Train_Acc:87.18%
Epoch [50/300], Step [290/391],                 Loss: 0.37553, Train_Acc:87.19%
Epoch [50/300], Step [300/391],                 Loss: 0.37560, Train_Acc:87.18%
Epoch [50/300], Step [310/391],                 Loss: 0.37586, Train_Acc:87.18%
Epoch [50/300], Step [320/391],                 Loss: 0.37598, Train_Acc:87.17%
Epoch [50/300], Step [330/391],                 Loss: 0.37496, Train_Acc:87.22%
Epoch [50/300], Step [340/391],                 Loss: 0.37384, Train_Acc:87.26%
Epoch [50/300], Step [350/391],                 Loss: 0.37443, Train_Acc:87.23%
Epoch [50/300], Step [360/391],                 Loss: 0.37426, Train_Acc:87.23%
Epoch [50/300], Step [370/391],                 Loss: 0.37485, Train_Acc:87.20%
Epoch [50/300], Step [380/391],                 Loss: 0.37550, Train_Acc:87.20%
Epoch [50/300], Step [390/391],                 Loss: 0.37581, Train_Acc:87.20%
Accuary on test images:71.64%
Epoch [51/300], Step [10/391],                 Loss: 0.42405, Train_Acc:84.77%
Epoch [51/300], Step [20/391],                 Loss: 0.39950, Train_Acc:86.29%
Epoch [51/300], Step [30/391],                 Loss: 0.38251, Train_Acc:87.03%
Epoch [51/300], Step [40/391],                 Loss: 0.37158, Train_Acc:87.38%
Epoch [51/300], Step [50/391],                 Loss: 0.36651, Train_Acc:87.48%
Epoch [51/300], Step [60/391],                 Loss: 0.37068, Train_Acc:87.36%
Epoch [51/300], Step [70/391],                 Loss: 0.37206, Train_Acc:87.25%
Epoch [51/300], Step [80/391],                 Loss: 0.37077, Train_Acc:87.28%
Epoch [51/300], Step [90/391],                 Loss: 0.37194, Train_Acc:87.12%
Epoch [51/300], Step [100/391],                 Loss: 0.36940, Train_Acc:87.25%
Epoch [51/300], Step [110/391],                 Loss: 0.37168, Train_Acc:87.18%
Epoch [51/300], Step [120/391],                 Loss: 0.36940, Train_Acc:87.23%
Epoch [51/300], Step [130/391],                 Loss: 0.37171, Train_Acc:87.24%
Epoch [51/300], Step [140/391],                 Loss: 0.36868, Train_Acc:87.33%
Epoch [51/300], Step [150/391],                 Loss: 0.36862, Train_Acc:87.33%
Epoch [51/300], Step [160/391],                 Loss: 0.36777, Train_Acc:87.41%
Epoch [51/300], Step [170/391],                 Loss: 0.36862, Train_Acc:87.33%
Epoch [51/300], Step [180/391],                 Loss: 0.36840, Train_Acc:87.32%
Epoch [51/300], Step [190/391],                 Loss: 0.36871, Train_Acc:87.28%
Epoch [51/300], Step [200/391],                 Loss: 0.36929, Train_Acc:87.23%
Epoch [51/300], Step [210/391],                 Loss: 0.36984, Train_Acc:87.19%
Epoch [51/300], Step [220/391],                 Loss: 0.36980, Train_Acc:87.24%
Epoch [51/300], Step [230/391],                 Loss: 0.36984, Train_Acc:87.24%
Epoch [51/300], Step [240/391],                 Loss: 0.36965, Train_Acc:87.25%
Epoch [51/300], Step [250/391],                 Loss: 0.36957, Train_Acc:87.24%
Epoch [51/300], Step [260/391],                 Loss: 0.37175, Train_Acc:87.19%
Epoch [51/300], Step [270/391],                 Loss: 0.37245, Train_Acc:87.16%
Epoch [51/300], Step [280/391],                 Loss: 0.37292, Train_Acc:87.12%
Epoch [51/300], Step [290/391],                 Loss: 0.37302, Train_Acc:87.10%
Epoch [51/300], Step [300/391],                 Loss: 0.37312, Train_Acc:87.11%
Epoch [51/300], Step [310/391],                 Loss: 0.37270, Train_Acc:87.13%
Epoch [51/300], Step [320/391],                 Loss: 0.37399, Train_Acc:87.09%
Epoch [51/300], Step [330/391],                 Loss: 0.37307, Train_Acc:87.16%
Epoch [51/300], Step [340/391],                 Loss: 0.37235, Train_Acc:87.18%
Epoch [51/300], Step [350/391],                 Loss: 0.37227, Train_Acc:87.19%
Epoch [51/300], Step [360/391],                 Loss: 0.37247, Train_Acc:87.17%
Epoch [51/300], Step [370/391],                 Loss: 0.37258, Train_Acc:87.17%
Epoch [51/300], Step [380/391],                 Loss: 0.37220, Train_Acc:87.20%
Epoch [51/300], Step [390/391],                 Loss: 0.37226, Train_Acc:87.22%
Accuary on test images:70.38%
Epoch [52/300], Step [10/391],                 Loss: 0.39850, Train_Acc:87.03%
Epoch [52/300], Step [20/391],                 Loss: 0.39315, Train_Acc:86.88%
Epoch [52/300], Step [30/391],                 Loss: 0.37709, Train_Acc:87.06%
Epoch [52/300], Step [40/391],                 Loss: 0.37281, Train_Acc:87.19%
Epoch [52/300], Step [50/391],                 Loss: 0.37030, Train_Acc:87.19%
Epoch [52/300], Step [60/391],                 Loss: 0.37578, Train_Acc:86.99%
Epoch [52/300], Step [70/391],                 Loss: 0.36883, Train_Acc:87.27%
Epoch [52/300], Step [80/391],                 Loss: 0.37482, Train_Acc:87.01%
Epoch [52/300], Step [90/391],                 Loss: 0.37628, Train_Acc:86.85%
Epoch [52/300], Step [100/391],                 Loss: 0.37135, Train_Acc:87.02%
Epoch [52/300], Step [110/391],                 Loss: 0.37368, Train_Acc:87.02%
Epoch [52/300], Step [120/391],                 Loss: 0.37434, Train_Acc:86.96%
Epoch [52/300], Step [130/391],                 Loss: 0.37629, Train_Acc:86.98%
Epoch [52/300], Step [140/391],                 Loss: 0.37460, Train_Acc:87.08%
Epoch [52/300], Step [150/391],                 Loss: 0.37434, Train_Acc:87.07%
Epoch [52/300], Step [160/391],                 Loss: 0.37329, Train_Acc:87.13%
Epoch [52/300], Step [170/391],                 Loss: 0.37315, Train_Acc:87.09%
Epoch [52/300], Step [180/391],                 Loss: 0.37493, Train_Acc:86.97%
Epoch [52/300], Step [190/391],                 Loss: 0.37724, Train_Acc:86.85%
Epoch [52/300], Step [200/391],                 Loss: 0.37826, Train_Acc:86.83%
Epoch [52/300], Step [210/391],                 Loss: 0.37724, Train_Acc:86.89%
Epoch [52/300], Step [220/391],                 Loss: 0.37670, Train_Acc:86.92%
Epoch [52/300], Step [230/391],                 Loss: 0.37463, Train_Acc:87.00%
Epoch [52/300], Step [240/391],                 Loss: 0.37361, Train_Acc:87.02%
Epoch [52/300], Step [250/391],                 Loss: 0.37438, Train_Acc:87.00%
Epoch [52/300], Step [260/391],                 Loss: 0.37646, Train_Acc:86.95%
Epoch [52/300], Step [270/391],                 Loss: 0.37835, Train_Acc:86.88%
Epoch [52/300], Step [280/391],                 Loss: 0.37896, Train_Acc:86.88%
Epoch [52/300], Step [290/391],                 Loss: 0.37924, Train_Acc:86.89%
Epoch [52/300], Step [300/391],                 Loss: 0.37787, Train_Acc:86.95%
Epoch [52/300], Step [310/391],                 Loss: 0.37767, Train_Acc:86.96%
Epoch [52/300], Step [320/391],                 Loss: 0.37841, Train_Acc:86.94%
Epoch [52/300], Step [330/391],                 Loss: 0.37644, Train_Acc:87.02%
Epoch [52/300], Step [340/391],                 Loss: 0.37467, Train_Acc:87.09%
Epoch [52/300], Step [350/391],                 Loss: 0.37423, Train_Acc:87.11%
Epoch [52/300], Step [360/391],                 Loss: 0.37389, Train_Acc:87.11%
Epoch [52/300], Step [370/391],                 Loss: 0.37326, Train_Acc:87.14%
Epoch [52/300], Step [380/391],                 Loss: 0.37327, Train_Acc:87.16%
Epoch [52/300], Step [390/391],                 Loss: 0.37217, Train_Acc:87.21%
Accuary on test images:72.84%
Epoch [53/300], Step [10/391],                 Loss: 0.37566, Train_Acc:87.42%
Epoch [53/300], Step [20/391],                 Loss: 0.36756, Train_Acc:87.81%
Epoch [53/300], Step [30/391],                 Loss: 0.35903, Train_Acc:87.99%
Epoch [53/300], Step [40/391],                 Loss: 0.36240, Train_Acc:87.91%
Epoch [53/300], Step [50/391],                 Loss: 0.36946, Train_Acc:87.53%
Epoch [53/300], Step [60/391],                 Loss: 0.37648, Train_Acc:87.30%
Epoch [53/300], Step [70/391],                 Loss: 0.37462, Train_Acc:87.25%
Epoch [53/300], Step [80/391],                 Loss: 0.37596, Train_Acc:87.29%
Epoch [53/300], Step [90/391],                 Loss: 0.37981, Train_Acc:87.22%
Epoch [53/300], Step [100/391],                 Loss: 0.38108, Train_Acc:87.20%
Epoch [53/300], Step [110/391],                 Loss: 0.38290, Train_Acc:87.15%
Epoch [53/300], Step [120/391],                 Loss: 0.38339, Train_Acc:87.02%
Epoch [53/300], Step [130/391],                 Loss: 0.38397, Train_Acc:87.07%
Epoch [53/300], Step [140/391],                 Loss: 0.38303, Train_Acc:87.11%
Epoch [53/300], Step [150/391],                 Loss: 0.38254, Train_Acc:87.08%
Epoch [53/300], Step [160/391],                 Loss: 0.38170, Train_Acc:87.05%
Epoch [53/300], Step [170/391],                 Loss: 0.38157, Train_Acc:87.05%
Epoch [53/300], Step [180/391],                 Loss: 0.38227, Train_Acc:87.07%
Epoch [53/300], Step [190/391],                 Loss: 0.38208, Train_Acc:87.09%
Epoch [53/300], Step [200/391],                 Loss: 0.38118, Train_Acc:87.12%
Epoch [53/300], Step [210/391],                 Loss: 0.38011, Train_Acc:87.13%
Epoch [53/300], Step [220/391],                 Loss: 0.37826, Train_Acc:87.21%
Epoch [53/300], Step [230/391],                 Loss: 0.37707, Train_Acc:87.25%
Epoch [53/300], Step [240/391],                 Loss: 0.37502, Train_Acc:87.35%
Epoch [53/300], Step [250/391],                 Loss: 0.37531, Train_Acc:87.30%
Epoch [53/300], Step [260/391],                 Loss: 0.37778, Train_Acc:87.22%
Epoch [53/300], Step [270/391],                 Loss: 0.38036, Train_Acc:87.13%
Epoch [53/300], Step [280/391],                 Loss: 0.38030, Train_Acc:87.10%
Epoch [53/300], Step [290/391],                 Loss: 0.37980, Train_Acc:87.15%
Epoch [53/300], Step [300/391],                 Loss: 0.37834, Train_Acc:87.20%
Epoch [53/300], Step [310/391],                 Loss: 0.37698, Train_Acc:87.24%
Epoch [53/300], Step [320/391],                 Loss: 0.37715, Train_Acc:87.27%
Epoch [53/300], Step [330/391],                 Loss: 0.37581, Train_Acc:87.34%
Epoch [53/300], Step [340/391],                 Loss: 0.37469, Train_Acc:87.37%
Epoch [53/300], Step [350/391],                 Loss: 0.37362, Train_Acc:87.44%
Epoch [53/300], Step [360/391],                 Loss: 0.37477, Train_Acc:87.38%
Epoch [53/300], Step [370/391],                 Loss: 0.37583, Train_Acc:87.31%
Epoch [53/300], Step [380/391],                 Loss: 0.37602, Train_Acc:87.32%
Epoch [53/300], Step [390/391],                 Loss: 0.37591, Train_Acc:87.36%
Accuary on test images:71.00%
Epoch [54/300], Step [10/391],                 Loss: 0.36421, Train_Acc:87.03%
Epoch [54/300], Step [20/391],                 Loss: 0.34921, Train_Acc:87.97%
Epoch [54/300], Step [30/391],                 Loss: 0.35187, Train_Acc:87.97%
Epoch [54/300], Step [40/391],                 Loss: 0.35593, Train_Acc:87.91%
Epoch [54/300], Step [50/391],                 Loss: 0.35462, Train_Acc:87.86%
Epoch [54/300], Step [60/391],                 Loss: 0.35961, Train_Acc:87.49%
Epoch [54/300], Step [70/391],                 Loss: 0.35919, Train_Acc:87.51%
Epoch [54/300], Step [80/391],                 Loss: 0.36166, Train_Acc:87.52%
Epoch [54/300], Step [90/391],                 Loss: 0.36492, Train_Acc:87.25%
Epoch [54/300], Step [100/391],                 Loss: 0.36575, Train_Acc:87.26%
Epoch [54/300], Step [110/391],                 Loss: 0.36753, Train_Acc:87.22%
Epoch [54/300], Step [120/391],                 Loss: 0.36767, Train_Acc:87.19%
Epoch [54/300], Step [130/391],                 Loss: 0.36917, Train_Acc:87.12%
Epoch [54/300], Step [140/391],                 Loss: 0.36978, Train_Acc:87.19%
Epoch [54/300], Step [150/391],                 Loss: 0.37024, Train_Acc:87.24%
Epoch [54/300], Step [160/391],                 Loss: 0.36900, Train_Acc:87.28%
Epoch [54/300], Step [170/391],                 Loss: 0.36839, Train_Acc:87.29%
Epoch [54/300], Step [180/391],                 Loss: 0.36721, Train_Acc:87.32%
Epoch [54/300], Step [190/391],                 Loss: 0.36570, Train_Acc:87.41%
Epoch [54/300], Step [200/391],                 Loss: 0.36596, Train_Acc:87.42%
Epoch [54/300], Step [210/391],                 Loss: 0.36777, Train_Acc:87.38%
Epoch [54/300], Step [220/391],                 Loss: 0.36858, Train_Acc:87.37%
Epoch [54/300], Step [230/391],                 Loss: 0.36685, Train_Acc:87.44%
Epoch [54/300], Step [240/391],                 Loss: 0.36590, Train_Acc:87.44%
Epoch [54/300], Step [250/391],                 Loss: 0.36551, Train_Acc:87.47%
Epoch [54/300], Step [260/391],                 Loss: 0.36822, Train_Acc:87.38%
Epoch [54/300], Step [270/391],                 Loss: 0.36826, Train_Acc:87.36%
Epoch [54/300], Step [280/391],                 Loss: 0.36915, Train_Acc:87.35%
Epoch [54/300], Step [290/391],                 Loss: 0.36874, Train_Acc:87.37%
Epoch [54/300], Step [300/391],                 Loss: 0.36855, Train_Acc:87.38%
Epoch [54/300], Step [310/391],                 Loss: 0.36784, Train_Acc:87.41%
Epoch [54/300], Step [320/391],                 Loss: 0.36740, Train_Acc:87.41%
Epoch [54/300], Step [330/391],                 Loss: 0.36682, Train_Acc:87.46%
Epoch [54/300], Step [340/391],                 Loss: 0.36555, Train_Acc:87.53%
Epoch [54/300], Step [350/391],                 Loss: 0.36472, Train_Acc:87.56%
Epoch [54/300], Step [360/391],                 Loss: 0.36420, Train_Acc:87.60%
Epoch [54/300], Step [370/391],                 Loss: 0.36413, Train_Acc:87.59%
Epoch [54/300], Step [380/391],                 Loss: 0.36381, Train_Acc:87.62%
Epoch [54/300], Step [390/391],                 Loss: 0.36274, Train_Acc:87.65%
Accuary on test images:75.68%
Epoch [55/300], Step [10/391],                 Loss: 0.34994, Train_Acc:86.80%
Epoch [55/300], Step [20/391],                 Loss: 0.36343, Train_Acc:87.15%
Epoch [55/300], Step [30/391],                 Loss: 0.36318, Train_Acc:87.32%
Epoch [55/300], Step [40/391],                 Loss: 0.36648, Train_Acc:87.30%
Epoch [55/300], Step [50/391],                 Loss: 0.36563, Train_Acc:87.31%
Epoch [55/300], Step [60/391],                 Loss: 0.36978, Train_Acc:87.03%
Epoch [55/300], Step [70/391],                 Loss: 0.36579, Train_Acc:87.19%
Epoch [55/300], Step [80/391],                 Loss: 0.36666, Train_Acc:87.13%
Epoch [55/300], Step [90/391],                 Loss: 0.36728, Train_Acc:87.20%
Epoch [55/300], Step [100/391],                 Loss: 0.36692, Train_Acc:87.21%
Epoch [55/300], Step [110/391],                 Loss: 0.36765, Train_Acc:87.25%
Epoch [55/300], Step [120/391],                 Loss: 0.36817, Train_Acc:87.31%
Epoch [55/300], Step [130/391],                 Loss: 0.37284, Train_Acc:87.16%
Epoch [55/300], Step [140/391],                 Loss: 0.37123, Train_Acc:87.23%
Epoch [55/300], Step [150/391],                 Loss: 0.37083, Train_Acc:87.26%
Epoch [55/300], Step [160/391],                 Loss: 0.37056, Train_Acc:87.27%
Epoch [55/300], Step [170/391],                 Loss: 0.37464, Train_Acc:87.10%
Epoch [55/300], Step [180/391],                 Loss: 0.37532, Train_Acc:87.05%
Epoch [55/300], Step [190/391],                 Loss: 0.37575, Train_Acc:87.08%
Epoch [55/300], Step [200/391],                 Loss: 0.37537, Train_Acc:87.12%
Epoch [55/300], Step [210/391],                 Loss: 0.37656, Train_Acc:87.04%
Epoch [55/300], Step [220/391],                 Loss: 0.37742, Train_Acc:87.01%
Epoch [55/300], Step [230/391],                 Loss: 0.37538, Train_Acc:87.07%
Epoch [55/300], Step [240/391],                 Loss: 0.37261, Train_Acc:87.17%
Epoch [55/300], Step [250/391],                 Loss: 0.37294, Train_Acc:87.18%
Epoch [55/300], Step [260/391],                 Loss: 0.37600, Train_Acc:87.10%
Epoch [55/300], Step [270/391],                 Loss: 0.37700, Train_Acc:87.05%
Epoch [55/300], Step [280/391],                 Loss: 0.37729, Train_Acc:87.02%
Epoch [55/300], Step [290/391],                 Loss: 0.37684, Train_Acc:87.04%
Epoch [55/300], Step [300/391],                 Loss: 0.37734, Train_Acc:87.06%
Epoch [55/300], Step [310/391],                 Loss: 0.37702, Train_Acc:87.09%
Epoch [55/300], Step [320/391],                 Loss: 0.37690, Train_Acc:87.11%
Epoch [55/300], Step [330/391],                 Loss: 0.37603, Train_Acc:87.13%
Epoch [55/300], Step [340/391],                 Loss: 0.37547, Train_Acc:87.15%
Epoch [55/300], Step [350/391],                 Loss: 0.37454, Train_Acc:87.17%
Epoch [55/300], Step [360/391],                 Loss: 0.37406, Train_Acc:87.18%
Epoch [55/300], Step [370/391],                 Loss: 0.37488, Train_Acc:87.17%
Epoch [55/300], Step [380/391],                 Loss: 0.37449, Train_Acc:87.18%
Epoch [55/300], Step [390/391],                 Loss: 0.37357, Train_Acc:87.20%
Accuary on test images:76.42%
Epoch [56/300], Step [10/391],                 Loss: 0.40254, Train_Acc:85.78%
Epoch [56/300], Step [20/391],                 Loss: 0.37709, Train_Acc:87.03%
Epoch [56/300], Step [30/391],                 Loss: 0.36384, Train_Acc:87.47%
Epoch [56/300], Step [40/391],                 Loss: 0.35588, Train_Acc:87.91%
Epoch [56/300], Step [50/391],                 Loss: 0.35920, Train_Acc:87.61%
Epoch [56/300], Step [60/391],                 Loss: 0.36657, Train_Acc:87.55%
Epoch [56/300], Step [70/391],                 Loss: 0.36509, Train_Acc:87.59%
Epoch [56/300], Step [80/391],                 Loss: 0.36513, Train_Acc:87.59%
Epoch [56/300], Step [90/391],                 Loss: 0.36955, Train_Acc:87.48%
Epoch [56/300], Step [100/391],                 Loss: 0.37004, Train_Acc:87.45%
Epoch [56/300], Step [110/391],                 Loss: 0.37182, Train_Acc:87.34%
Epoch [56/300], Step [120/391],                 Loss: 0.36971, Train_Acc:87.30%
Epoch [56/300], Step [130/391],                 Loss: 0.37246, Train_Acc:87.21%
Epoch [56/300], Step [140/391],                 Loss: 0.37003, Train_Acc:87.37%
Epoch [56/300], Step [150/391],                 Loss: 0.37155, Train_Acc:87.30%
Epoch [56/300], Step [160/391],                 Loss: 0.37228, Train_Acc:87.30%
Epoch [56/300], Step [170/391],                 Loss: 0.37210, Train_Acc:87.31%
Epoch [56/300], Step [180/391],                 Loss: 0.37152, Train_Acc:87.32%
Epoch [56/300], Step [190/391],                 Loss: 0.37360, Train_Acc:87.26%
Epoch [56/300], Step [200/391],                 Loss: 0.37258, Train_Acc:87.30%
Epoch [56/300], Step [210/391],                 Loss: 0.37332, Train_Acc:87.28%
Epoch [56/300], Step [220/391],                 Loss: 0.37391, Train_Acc:87.23%
Epoch [56/300], Step [230/391],                 Loss: 0.37279, Train_Acc:87.28%
Epoch [56/300], Step [240/391],                 Loss: 0.37066, Train_Acc:87.33%
Epoch [56/300], Step [250/391],                 Loss: 0.36988, Train_Acc:87.34%
Epoch [56/300], Step [260/391],                 Loss: 0.37092, Train_Acc:87.37%
Epoch [56/300], Step [270/391],                 Loss: 0.37114, Train_Acc:87.37%
Epoch [56/300], Step [280/391],                 Loss: 0.37003, Train_Acc:87.37%
Epoch [56/300], Step [290/391],                 Loss: 0.36906, Train_Acc:87.42%
Epoch [56/300], Step [300/391],                 Loss: 0.36844, Train_Acc:87.43%
Epoch [56/300], Step [310/391],                 Loss: 0.36835, Train_Acc:87.45%
Epoch [56/300], Step [320/391],                 Loss: 0.36814, Train_Acc:87.47%
Epoch [56/300], Step [330/391],                 Loss: 0.36694, Train_Acc:87.52%
Epoch [56/300], Step [340/391],                 Loss: 0.36605, Train_Acc:87.54%
Epoch [56/300], Step [350/391],                 Loss: 0.36560, Train_Acc:87.56%
Epoch [56/300], Step [360/391],                 Loss: 0.36557, Train_Acc:87.57%
Epoch [56/300], Step [370/391],                 Loss: 0.36581, Train_Acc:87.55%
Epoch [56/300], Step [380/391],                 Loss: 0.36550, Train_Acc:87.57%
Epoch [56/300], Step [390/391],                 Loss: 0.36568, Train_Acc:87.58%
Accuary on test images:74.60%
Epoch [57/300], Step [10/391],                 Loss: 0.36273, Train_Acc:87.89%
Epoch [57/300], Step [20/391],                 Loss: 0.37489, Train_Acc:87.27%
Epoch [57/300], Step [30/391],                 Loss: 0.36948, Train_Acc:87.45%
Epoch [57/300], Step [40/391],                 Loss: 0.35836, Train_Acc:87.73%
Epoch [57/300], Step [50/391],                 Loss: 0.36282, Train_Acc:87.62%
Epoch [57/300], Step [60/391],                 Loss: 0.36538, Train_Acc:87.59%
Epoch [57/300], Step [70/391],                 Loss: 0.36735, Train_Acc:87.50%
Epoch [57/300], Step [80/391],                 Loss: 0.36384, Train_Acc:87.66%
Epoch [57/300], Step [90/391],                 Loss: 0.36451, Train_Acc:87.60%
Epoch [57/300], Step [100/391],                 Loss: 0.36387, Train_Acc:87.52%
Epoch [57/300], Step [110/391],                 Loss: 0.36470, Train_Acc:87.45%
Epoch [57/300], Step [120/391],                 Loss: 0.36513, Train_Acc:87.44%
Epoch [57/300], Step [130/391],                 Loss: 0.36700, Train_Acc:87.43%
Epoch [57/300], Step [140/391],                 Loss: 0.36773, Train_Acc:87.49%
Epoch [57/300], Step [150/391],                 Loss: 0.36906, Train_Acc:87.44%
Epoch [57/300], Step [160/391],                 Loss: 0.36737, Train_Acc:87.49%
Epoch [57/300], Step [170/391],                 Loss: 0.36828, Train_Acc:87.46%
Epoch [57/300], Step [180/391],                 Loss: 0.36696, Train_Acc:87.51%
Epoch [57/300], Step [190/391],                 Loss: 0.36660, Train_Acc:87.51%
Epoch [57/300], Step [200/391],                 Loss: 0.36585, Train_Acc:87.54%
Epoch [57/300], Step [210/391],                 Loss: 0.36627, Train_Acc:87.50%
Epoch [57/300], Step [220/391],                 Loss: 0.36534, Train_Acc:87.57%
Epoch [57/300], Step [230/391],                 Loss: 0.36494, Train_Acc:87.61%
Epoch [57/300], Step [240/391],                 Loss: 0.36428, Train_Acc:87.61%
Epoch [57/300], Step [250/391],                 Loss: 0.36409, Train_Acc:87.59%
Epoch [57/300], Step [260/391],                 Loss: 0.36770, Train_Acc:87.48%
Epoch [57/300], Step [270/391],                 Loss: 0.36890, Train_Acc:87.43%
Epoch [57/300], Step [280/391],                 Loss: 0.36958, Train_Acc:87.42%
Epoch [57/300], Step [290/391],                 Loss: 0.37007, Train_Acc:87.39%
Epoch [57/300], Step [300/391],                 Loss: 0.37017, Train_Acc:87.38%
Epoch [57/300], Step [310/391],                 Loss: 0.37015, Train_Acc:87.38%
Epoch [57/300], Step [320/391],                 Loss: 0.37126, Train_Acc:87.32%
Epoch [57/300], Step [330/391],                 Loss: 0.37017, Train_Acc:87.34%
Epoch [57/300], Step [340/391],                 Loss: 0.36960, Train_Acc:87.35%
Epoch [57/300], Step [350/391],                 Loss: 0.36840, Train_Acc:87.40%
Epoch [57/300], Step [360/391],                 Loss: 0.36830, Train_Acc:87.42%
Epoch [57/300], Step [370/391],                 Loss: 0.36816, Train_Acc:87.43%
Epoch [57/300], Step [380/391],                 Loss: 0.36788, Train_Acc:87.42%
Epoch [57/300], Step [390/391],                 Loss: 0.36787, Train_Acc:87.41%
Accuary on test images:76.68%
Epoch [58/300], Step [10/391],                 Loss: 0.37502, Train_Acc:86.56%
Epoch [58/300], Step [20/391],                 Loss: 0.37160, Train_Acc:86.80%
Epoch [58/300], Step [30/391],                 Loss: 0.36764, Train_Acc:87.01%
Epoch [58/300], Step [40/391],                 Loss: 0.36318, Train_Acc:87.44%
Epoch [58/300], Step [50/391],                 Loss: 0.35827, Train_Acc:87.59%
Epoch [58/300], Step [60/391],                 Loss: 0.36457, Train_Acc:87.41%
Epoch [58/300], Step [70/391],                 Loss: 0.36351, Train_Acc:87.60%
Epoch [58/300], Step [80/391],                 Loss: 0.36430, Train_Acc:87.55%
Epoch [58/300], Step [90/391],                 Loss: 0.36744, Train_Acc:87.43%
Epoch [58/300], Step [100/391],                 Loss: 0.36863, Train_Acc:87.38%
Epoch [58/300], Step [110/391],                 Loss: 0.37092, Train_Acc:87.37%
Epoch [58/300], Step [120/391],                 Loss: 0.37159, Train_Acc:87.27%
Epoch [58/300], Step [130/391],                 Loss: 0.37265, Train_Acc:87.22%
Epoch [58/300], Step [140/391],                 Loss: 0.37097, Train_Acc:87.29%
Epoch [58/300], Step [150/391],                 Loss: 0.36925, Train_Acc:87.35%
Epoch [58/300], Step [160/391],                 Loss: 0.36826, Train_Acc:87.38%
Epoch [58/300], Step [170/391],                 Loss: 0.36891, Train_Acc:87.37%
Epoch [58/300], Step [180/391],                 Loss: 0.36975, Train_Acc:87.37%
Epoch [58/300], Step [190/391],                 Loss: 0.37202, Train_Acc:87.25%
Epoch [58/300], Step [200/391],                 Loss: 0.37238, Train_Acc:87.25%
Epoch [58/300], Step [210/391],                 Loss: 0.37220, Train_Acc:87.25%
Epoch [58/300], Step [220/391],                 Loss: 0.37076, Train_Acc:87.33%
Epoch [58/300], Step [230/391],                 Loss: 0.36973, Train_Acc:87.33%
Epoch [58/300], Step [240/391],                 Loss: 0.36843, Train_Acc:87.39%
Epoch [58/300], Step [250/391],                 Loss: 0.36933, Train_Acc:87.38%
Epoch [58/300], Step [260/391],                 Loss: 0.37301, Train_Acc:87.31%
Epoch [58/300], Step [270/391],                 Loss: 0.37518, Train_Acc:87.26%
Epoch [58/300], Step [280/391],                 Loss: 0.37488, Train_Acc:87.25%
Epoch [58/300], Step [290/391],                 Loss: 0.37485, Train_Acc:87.25%
Epoch [58/300], Step [300/391],                 Loss: 0.37431, Train_Acc:87.28%
Epoch [58/300], Step [310/391],                 Loss: 0.37402, Train_Acc:87.28%
Epoch [58/300], Step [320/391],                 Loss: 0.37354, Train_Acc:87.30%
Epoch [58/300], Step [330/391],                 Loss: 0.37245, Train_Acc:87.34%
Epoch [58/300], Step [340/391],                 Loss: 0.37246, Train_Acc:87.33%
Epoch [58/300], Step [350/391],                 Loss: 0.37182, Train_Acc:87.37%
Epoch [58/300], Step [360/391],                 Loss: 0.37169, Train_Acc:87.37%
Epoch [58/300], Step [370/391],                 Loss: 0.37184, Train_Acc:87.36%
Epoch [58/300], Step [380/391],                 Loss: 0.37156, Train_Acc:87.38%
Epoch [58/300], Step [390/391],                 Loss: 0.37070, Train_Acc:87.42%
Accuary on test images:75.38%
Epoch [59/300], Step [10/391],                 Loss: 0.41178, Train_Acc:85.16%
Epoch [59/300], Step [20/391],                 Loss: 0.40771, Train_Acc:85.78%
Epoch [59/300], Step [30/391],                 Loss: 0.39273, Train_Acc:86.33%
Epoch [59/300], Step [40/391],                 Loss: 0.39640, Train_Acc:86.46%
Epoch [59/300], Step [50/391],                 Loss: 0.39358, Train_Acc:86.45%
Epoch [59/300], Step [60/391],                 Loss: 0.39832, Train_Acc:86.25%
Epoch [59/300], Step [70/391],                 Loss: 0.39564, Train_Acc:86.34%
Epoch [59/300], Step [80/391],                 Loss: 0.39345, Train_Acc:86.56%
Epoch [59/300], Step [90/391],                 Loss: 0.39173, Train_Acc:86.66%
Epoch [59/300], Step [100/391],                 Loss: 0.38658, Train_Acc:86.79%
Epoch [59/300], Step [110/391],                 Loss: 0.38808, Train_Acc:86.73%
Epoch [59/300], Step [120/391],                 Loss: 0.38801, Train_Acc:86.76%
Epoch [59/300], Step [130/391],                 Loss: 0.38955, Train_Acc:86.67%
Epoch [59/300], Step [140/391],                 Loss: 0.38749, Train_Acc:86.72%
Epoch [59/300], Step [150/391],                 Loss: 0.38729, Train_Acc:86.65%
Epoch [59/300], Step [160/391],                 Loss: 0.38497, Train_Acc:86.73%
Epoch [59/300], Step [170/391],                 Loss: 0.38580, Train_Acc:86.70%
Epoch [59/300], Step [180/391],                 Loss: 0.38553, Train_Acc:86.71%
Epoch [59/300], Step [190/391],                 Loss: 0.38620, Train_Acc:86.74%
Epoch [59/300], Step [200/391],                 Loss: 0.38616, Train_Acc:86.73%
Epoch [59/300], Step [210/391],                 Loss: 0.38576, Train_Acc:86.76%
Epoch [59/300], Step [220/391],                 Loss: 0.38427, Train_Acc:86.84%
Epoch [59/300], Step [230/391],                 Loss: 0.38229, Train_Acc:86.91%
Epoch [59/300], Step [240/391],                 Loss: 0.37951, Train_Acc:87.01%
Epoch [59/300], Step [250/391],                 Loss: 0.37853, Train_Acc:87.02%
Epoch [59/300], Step [260/391],                 Loss: 0.37937, Train_Acc:87.02%
Epoch [59/300], Step [270/391],                 Loss: 0.37998, Train_Acc:86.97%
Epoch [59/300], Step [280/391],                 Loss: 0.37945, Train_Acc:86.97%
Epoch [59/300], Step [290/391],                 Loss: 0.37972, Train_Acc:86.99%
Epoch [59/300], Step [300/391],                 Loss: 0.37934, Train_Acc:87.01%
Epoch [59/300], Step [310/391],                 Loss: 0.37962, Train_Acc:87.03%
Epoch [59/300], Step [320/391],                 Loss: 0.37925, Train_Acc:87.04%
Epoch [59/300], Step [330/391],                 Loss: 0.37917, Train_Acc:87.02%
Epoch [59/300], Step [340/391],                 Loss: 0.37811, Train_Acc:87.08%
Epoch [59/300], Step [350/391],                 Loss: 0.37871, Train_Acc:87.03%
Epoch [59/300], Step [360/391],                 Loss: 0.37791, Train_Acc:87.06%
Epoch [59/300], Step [370/391],                 Loss: 0.37780, Train_Acc:87.07%
Epoch [59/300], Step [380/391],                 Loss: 0.37673, Train_Acc:87.10%
Epoch [59/300], Step [390/391],                 Loss: 0.37556, Train_Acc:87.16%
Accuary on test images:78.90%
Epoch [60/300], Step [10/391],                 Loss: 0.35584, Train_Acc:87.42%
Epoch [60/300], Step [20/391],                 Loss: 0.34765, Train_Acc:88.20%
Epoch [60/300], Step [30/391],                 Loss: 0.35337, Train_Acc:87.92%
Epoch [60/300], Step [40/391],                 Loss: 0.35445, Train_Acc:88.07%
Epoch [60/300], Step [50/391],                 Loss: 0.36008, Train_Acc:87.83%
Epoch [60/300], Step [60/391],                 Loss: 0.36227, Train_Acc:87.80%
Epoch [60/300], Step [70/391],                 Loss: 0.35930, Train_Acc:88.01%
Epoch [60/300], Step [80/391],                 Loss: 0.35867, Train_Acc:88.00%
Epoch [60/300], Step [90/391],                 Loss: 0.36168, Train_Acc:87.86%
Epoch [60/300], Step [100/391],                 Loss: 0.35824, Train_Acc:87.98%
Epoch [60/300], Step [110/391],                 Loss: 0.35907, Train_Acc:87.97%
Epoch [60/300], Step [120/391],                 Loss: 0.36142, Train_Acc:87.85%
Epoch [60/300], Step [130/391],                 Loss: 0.36795, Train_Acc:87.63%
Epoch [60/300], Step [140/391],                 Loss: 0.36622, Train_Acc:87.69%
Epoch [60/300], Step [150/391],                 Loss: 0.36874, Train_Acc:87.53%
Epoch [60/300], Step [160/391],                 Loss: 0.36642, Train_Acc:87.62%
Epoch [60/300], Step [170/391],                 Loss: 0.36553, Train_Acc:87.58%
Epoch [60/300], Step [180/391],                 Loss: 0.36384, Train_Acc:87.64%
Epoch [60/300], Step [190/391],                 Loss: 0.36450, Train_Acc:87.62%
Epoch [60/300], Step [200/391],                 Loss: 0.36404, Train_Acc:87.66%
Epoch [60/300], Step [210/391],                 Loss: 0.36387, Train_Acc:87.67%
Epoch [60/300], Step [220/391],                 Loss: 0.36270, Train_Acc:87.73%
Epoch [60/300], Step [230/391],                 Loss: 0.36163, Train_Acc:87.71%
Epoch [60/300], Step [240/391],                 Loss: 0.36006, Train_Acc:87.77%
Epoch [60/300], Step [250/391],                 Loss: 0.36106, Train_Acc:87.74%
Epoch [60/300], Step [260/391],                 Loss: 0.36354, Train_Acc:87.69%
Epoch [60/300], Step [270/391],                 Loss: 0.36503, Train_Acc:87.63%
Epoch [60/300], Step [280/391],                 Loss: 0.36580, Train_Acc:87.59%
Epoch [60/300], Step [290/391],                 Loss: 0.36584, Train_Acc:87.59%
Epoch [60/300], Step [300/391],                 Loss: 0.36369, Train_Acc:87.65%
Epoch [60/300], Step [310/391],                 Loss: 0.36272, Train_Acc:87.66%
Epoch [60/300], Step [320/391],                 Loss: 0.36342, Train_Acc:87.63%
Epoch [60/300], Step [330/391],                 Loss: 0.36270, Train_Acc:87.63%
Epoch [60/300], Step [340/391],                 Loss: 0.36259, Train_Acc:87.62%
Epoch [60/300], Step [350/391],                 Loss: 0.36206, Train_Acc:87.64%
Epoch [60/300], Step [360/391],                 Loss: 0.36282, Train_Acc:87.59%
Epoch [60/300], Step [370/391],                 Loss: 0.36276, Train_Acc:87.59%
Epoch [60/300], Step [380/391],                 Loss: 0.36248, Train_Acc:87.58%
Epoch [60/300], Step [390/391],                 Loss: 0.36282, Train_Acc:87.59%
Accuary on test images:77.50%
Epoch [61/300], Step [10/391],                 Loss: 0.36576, Train_Acc:86.64%
Epoch [61/300], Step [20/391],                 Loss: 0.37826, Train_Acc:86.64%
Epoch [61/300], Step [30/391],                 Loss: 0.37366, Train_Acc:86.88%
Epoch [61/300], Step [40/391],                 Loss: 0.37490, Train_Acc:87.01%
Epoch [61/300], Step [50/391],                 Loss: 0.37471, Train_Acc:86.94%
Epoch [61/300], Step [60/391],                 Loss: 0.38598, Train_Acc:86.52%
Epoch [61/300], Step [70/391],                 Loss: 0.38418, Train_Acc:86.66%
Epoch [61/300], Step [80/391],                 Loss: 0.38184, Train_Acc:86.73%
Epoch [61/300], Step [90/391],                 Loss: 0.37708, Train_Acc:87.01%
Epoch [61/300], Step [100/391],                 Loss: 0.37133, Train_Acc:87.18%
Epoch [61/300], Step [110/391],                 Loss: 0.37300, Train_Acc:87.19%
Epoch [61/300], Step [120/391],                 Loss: 0.37156, Train_Acc:87.22%
Epoch [61/300], Step [130/391],                 Loss: 0.37296, Train_Acc:87.21%
Epoch [61/300], Step [140/391],                 Loss: 0.37183, Train_Acc:87.16%
Epoch [61/300], Step [150/391],                 Loss: 0.37159, Train_Acc:87.16%
Epoch [61/300], Step [160/391],                 Loss: 0.37065, Train_Acc:87.19%
Epoch [61/300], Step [170/391],                 Loss: 0.37095, Train_Acc:87.14%
Epoch [61/300], Step [180/391],                 Loss: 0.37205, Train_Acc:87.05%
Epoch [61/300], Step [190/391],                 Loss: 0.37261, Train_Acc:87.06%
Epoch [61/300], Step [200/391],                 Loss: 0.37371, Train_Acc:87.04%
Epoch [61/300], Step [210/391],                 Loss: 0.37412, Train_Acc:86.99%
Epoch [61/300], Step [220/391],                 Loss: 0.37459, Train_Acc:87.01%
Epoch [61/300], Step [230/391],                 Loss: 0.37270, Train_Acc:87.06%
Epoch [61/300], Step [240/391],                 Loss: 0.37109, Train_Acc:87.15%
Epoch [61/300], Step [250/391],                 Loss: 0.37139, Train_Acc:87.16%
Epoch [61/300], Step [260/391],                 Loss: 0.37218, Train_Acc:87.18%
Epoch [61/300], Step [270/391],                 Loss: 0.37221, Train_Acc:87.19%
Epoch [61/300], Step [280/391],                 Loss: 0.37166, Train_Acc:87.21%
Epoch [61/300], Step [290/391],                 Loss: 0.37109, Train_Acc:87.24%
Epoch [61/300], Step [300/391],                 Loss: 0.37098, Train_Acc:87.23%
Epoch [61/300], Step [310/391],                 Loss: 0.37094, Train_Acc:87.25%
Epoch [61/300], Step [320/391],                 Loss: 0.37162, Train_Acc:87.21%
Epoch [61/300], Step [330/391],                 Loss: 0.37122, Train_Acc:87.21%
Epoch [61/300], Step [340/391],                 Loss: 0.37074, Train_Acc:87.23%
Epoch [61/300], Step [350/391],                 Loss: 0.37003, Train_Acc:87.25%
Epoch [61/300], Step [360/391],                 Loss: 0.36993, Train_Acc:87.25%
Epoch [61/300], Step [370/391],                 Loss: 0.37054, Train_Acc:87.22%
Epoch [61/300], Step [380/391],                 Loss: 0.37031, Train_Acc:87.22%
Epoch [61/300], Step [390/391],                 Loss: 0.36995, Train_Acc:87.25%
Accuary on test images:69.44%
Epoch [62/300], Step [10/391],                 Loss: 0.39189, Train_Acc:86.33%
Epoch [62/300], Step [20/391],                 Loss: 0.37667, Train_Acc:86.80%
Epoch [62/300], Step [30/391],                 Loss: 0.35228, Train_Acc:87.55%
Epoch [62/300], Step [40/391],                 Loss: 0.34866, Train_Acc:88.01%
Epoch [62/300], Step [50/391],                 Loss: 0.34995, Train_Acc:87.95%
Epoch [62/300], Step [60/391],                 Loss: 0.35794, Train_Acc:87.73%
Epoch [62/300], Step [70/391],                 Loss: 0.35747, Train_Acc:87.70%
Epoch [62/300], Step [80/391],                 Loss: 0.35367, Train_Acc:87.82%
Epoch [62/300], Step [90/391],                 Loss: 0.35781, Train_Acc:87.74%
Epoch [62/300], Step [100/391],                 Loss: 0.35627, Train_Acc:87.77%
Epoch [62/300], Step [110/391],                 Loss: 0.35686, Train_Acc:87.73%
Epoch [62/300], Step [120/391],                 Loss: 0.35757, Train_Acc:87.72%
Epoch [62/300], Step [130/391],                 Loss: 0.36231, Train_Acc:87.66%
Epoch [62/300], Step [140/391],                 Loss: 0.36097, Train_Acc:87.65%
Epoch [62/300], Step [150/391],                 Loss: 0.36212, Train_Acc:87.64%
Epoch [62/300], Step [160/391],                 Loss: 0.36005, Train_Acc:87.72%
Epoch [62/300], Step [170/391],                 Loss: 0.35923, Train_Acc:87.77%
Epoch [62/300], Step [180/391],                 Loss: 0.35798, Train_Acc:87.78%
Epoch [62/300], Step [190/391],                 Loss: 0.35741, Train_Acc:87.75%
Epoch [62/300], Step [200/391],                 Loss: 0.35849, Train_Acc:87.71%
Epoch [62/300], Step [210/391],                 Loss: 0.35807, Train_Acc:87.73%
Epoch [62/300], Step [220/391],                 Loss: 0.35823, Train_Acc:87.74%
Epoch [62/300], Step [230/391],                 Loss: 0.35792, Train_Acc:87.75%
Epoch [62/300], Step [240/391],                 Loss: 0.35639, Train_Acc:87.80%
Epoch [62/300], Step [250/391],                 Loss: 0.35613, Train_Acc:87.80%
Epoch [62/300], Step [260/391],                 Loss: 0.35833, Train_Acc:87.77%
Epoch [62/300], Step [270/391],                 Loss: 0.35968, Train_Acc:87.71%
Epoch [62/300], Step [280/391],                 Loss: 0.36024, Train_Acc:87.67%
Epoch [62/300], Step [290/391],                 Loss: 0.35968, Train_Acc:87.71%
Epoch [62/300], Step [300/391],                 Loss: 0.35938, Train_Acc:87.70%
Epoch [62/300], Step [310/391],                 Loss: 0.35883, Train_Acc:87.72%
Epoch [62/300], Step [320/391],                 Loss: 0.35871, Train_Acc:87.73%
Epoch [62/300], Step [330/391],                 Loss: 0.35861, Train_Acc:87.74%
Epoch [62/300], Step [340/391],                 Loss: 0.35822, Train_Acc:87.76%
Epoch [62/300], Step [350/391],                 Loss: 0.35866, Train_Acc:87.74%
Epoch [62/300], Step [360/391],                 Loss: 0.35936, Train_Acc:87.70%
Epoch [62/300], Step [370/391],                 Loss: 0.36057, Train_Acc:87.67%
Epoch [62/300], Step [380/391],                 Loss: 0.36150, Train_Acc:87.63%
Epoch [62/300], Step [390/391],                 Loss: 0.36152, Train_Acc:87.64%
Accuary on test images:76.44%
Epoch [63/300], Step [10/391],                 Loss: 0.39500, Train_Acc:86.88%
Epoch [63/300], Step [20/391],                 Loss: 0.38447, Train_Acc:87.07%
Epoch [63/300], Step [30/391],                 Loss: 0.37003, Train_Acc:87.24%
Epoch [63/300], Step [40/391],                 Loss: 0.36512, Train_Acc:87.46%
Epoch [63/300], Step [50/391],                 Loss: 0.35809, Train_Acc:87.78%
Epoch [63/300], Step [60/391],                 Loss: 0.36333, Train_Acc:87.54%
Epoch [63/300], Step [70/391],                 Loss: 0.36668, Train_Acc:87.48%
Epoch [63/300], Step [80/391],                 Loss: 0.37172, Train_Acc:87.27%
Epoch [63/300], Step [90/391],                 Loss: 0.37746, Train_Acc:86.96%
Epoch [63/300], Step [100/391],                 Loss: 0.37775, Train_Acc:87.02%
Epoch [63/300], Step [110/391],                 Loss: 0.37936, Train_Acc:87.00%
Epoch [63/300], Step [120/391],                 Loss: 0.37978, Train_Acc:86.92%
Epoch [63/300], Step [130/391],                 Loss: 0.37870, Train_Acc:86.99%
Epoch [63/300], Step [140/391],                 Loss: 0.37690, Train_Acc:87.04%
Epoch [63/300], Step [150/391],                 Loss: 0.37655, Train_Acc:87.03%
Epoch [63/300], Step [160/391],                 Loss: 0.37584, Train_Acc:87.04%
Epoch [63/300], Step [170/391],                 Loss: 0.37622, Train_Acc:87.01%
Epoch [63/300], Step [180/391],                 Loss: 0.37638, Train_Acc:87.01%
Epoch [63/300], Step [190/391],                 Loss: 0.37615, Train_Acc:87.01%
Epoch [63/300], Step [200/391],                 Loss: 0.37669, Train_Acc:87.02%
Epoch [63/300], Step [210/391],                 Loss: 0.37666, Train_Acc:87.02%
Epoch [63/300], Step [220/391],                 Loss: 0.37604, Train_Acc:87.08%
Epoch [63/300], Step [230/391],                 Loss: 0.37496, Train_Acc:87.10%
Epoch [63/300], Step [240/391],                 Loss: 0.37243, Train_Acc:87.19%
Epoch [63/300], Step [250/391],                 Loss: 0.37188, Train_Acc:87.24%
Epoch [63/300], Step [260/391],                 Loss: 0.37293, Train_Acc:87.23%
Epoch [63/300], Step [270/391],                 Loss: 0.37498, Train_Acc:87.14%
Epoch [63/300], Step [280/391],                 Loss: 0.37425, Train_Acc:87.19%
Epoch [63/300], Step [290/391],                 Loss: 0.37394, Train_Acc:87.20%
Epoch [63/300], Step [300/391],                 Loss: 0.37235, Train_Acc:87.27%
Epoch [63/300], Step [310/391],                 Loss: 0.37115, Train_Acc:87.32%
Epoch [63/300], Step [320/391],                 Loss: 0.37144, Train_Acc:87.30%
Epoch [63/300], Step [330/391],                 Loss: 0.37094, Train_Acc:87.32%
Epoch [63/300], Step [340/391],                 Loss: 0.36983, Train_Acc:87.35%
Epoch [63/300], Step [350/391],                 Loss: 0.36952, Train_Acc:87.36%
Epoch [63/300], Step [360/391],                 Loss: 0.36959, Train_Acc:87.33%
Epoch [63/300], Step [370/391],                 Loss: 0.37016, Train_Acc:87.30%
Epoch [63/300], Step [380/391],                 Loss: 0.36973, Train_Acc:87.33%
Epoch [63/300], Step [390/391],                 Loss: 0.36912, Train_Acc:87.36%
Accuary on test images:75.88%
Epoch [64/300], Step [10/391],                 Loss: 0.34392, Train_Acc:87.66%
Epoch [64/300], Step [20/391],                 Loss: 0.33912, Train_Acc:88.12%
Epoch [64/300], Step [30/391],                 Loss: 0.33136, Train_Acc:88.67%
Epoch [64/300], Step [40/391],                 Loss: 0.33529, Train_Acc:88.67%
Epoch [64/300], Step [50/391],                 Loss: 0.33575, Train_Acc:88.64%
Epoch [64/300], Step [60/391],                 Loss: 0.34029, Train_Acc:88.39%
Epoch [64/300], Step [70/391],                 Loss: 0.33942, Train_Acc:88.36%
Epoch [64/300], Step [80/391],                 Loss: 0.34425, Train_Acc:88.13%
Epoch [64/300], Step [90/391],                 Loss: 0.34836, Train_Acc:88.03%
Epoch [64/300], Step [100/391],                 Loss: 0.35182, Train_Acc:87.86%
Epoch [64/300], Step [110/391],                 Loss: 0.35744, Train_Acc:87.76%
Epoch [64/300], Step [120/391],                 Loss: 0.35760, Train_Acc:87.71%
Epoch [64/300], Step [130/391],                 Loss: 0.36202, Train_Acc:87.61%
Epoch [64/300], Step [140/391],                 Loss: 0.36106, Train_Acc:87.62%
Epoch [64/300], Step [150/391],                 Loss: 0.36326, Train_Acc:87.61%
Epoch [64/300], Step [160/391],                 Loss: 0.36375, Train_Acc:87.57%
Epoch [64/300], Step [170/391],                 Loss: 0.36546, Train_Acc:87.54%
Epoch [64/300], Step [180/391],                 Loss: 0.36620, Train_Acc:87.53%
Epoch [64/300], Step [190/391],                 Loss: 0.36717, Train_Acc:87.45%
Epoch [64/300], Step [200/391],                 Loss: 0.36791, Train_Acc:87.49%
Epoch [64/300], Step [210/391],                 Loss: 0.36889, Train_Acc:87.47%
Epoch [64/300], Step [220/391],                 Loss: 0.36841, Train_Acc:87.49%
Epoch [64/300], Step [230/391],                 Loss: 0.36738, Train_Acc:87.51%
Epoch [64/300], Step [240/391],                 Loss: 0.36658, Train_Acc:87.54%
Epoch [64/300], Step [250/391],                 Loss: 0.36543, Train_Acc:87.58%
Epoch [64/300], Step [260/391],                 Loss: 0.36822, Train_Acc:87.49%
Epoch [64/300], Step [270/391],                 Loss: 0.36823, Train_Acc:87.49%
Epoch [64/300], Step [280/391],                 Loss: 0.36752, Train_Acc:87.50%
Epoch [64/300], Step [290/391],                 Loss: 0.36661, Train_Acc:87.54%
Epoch [64/300], Step [300/391],                 Loss: 0.36622, Train_Acc:87.53%
Epoch [64/300], Step [310/391],                 Loss: 0.36745, Train_Acc:87.47%
Epoch [64/300], Step [320/391],                 Loss: 0.36929, Train_Acc:87.41%
Epoch [64/300], Step [330/391],                 Loss: 0.36862, Train_Acc:87.44%
Epoch [64/300], Step [340/391],                 Loss: 0.36869, Train_Acc:87.41%
Epoch [64/300], Step [350/391],                 Loss: 0.36876, Train_Acc:87.44%
Epoch [64/300], Step [360/391],                 Loss: 0.36870, Train_Acc:87.45%
Epoch [64/300], Step [370/391],                 Loss: 0.36848, Train_Acc:87.44%
Epoch [64/300], Step [380/391],                 Loss: 0.36814, Train_Acc:87.45%
Epoch [64/300], Step [390/391],                 Loss: 0.36754, Train_Acc:87.45%
Accuary on test images:70.38%
Epoch [65/300], Step [10/391],                 Loss: 0.38423, Train_Acc:86.64%
Epoch [65/300], Step [20/391],                 Loss: 0.38299, Train_Acc:86.84%
Epoch [65/300], Step [30/391],                 Loss: 0.37197, Train_Acc:87.21%
Epoch [65/300], Step [40/391],                 Loss: 0.37307, Train_Acc:87.32%
Epoch [65/300], Step [50/391],                 Loss: 0.36800, Train_Acc:87.48%
Epoch [65/300], Step [60/391],                 Loss: 0.36555, Train_Acc:87.47%
Epoch [65/300], Step [70/391],                 Loss: 0.36204, Train_Acc:87.65%
Epoch [65/300], Step [80/391],                 Loss: 0.36384, Train_Acc:87.55%
Epoch [65/300], Step [90/391],                 Loss: 0.36313, Train_Acc:87.53%
Epoch [65/300], Step [100/391],                 Loss: 0.36278, Train_Acc:87.54%
Epoch [65/300], Step [110/391],                 Loss: 0.36727, Train_Acc:87.32%
Epoch [65/300], Step [120/391],                 Loss: 0.36965, Train_Acc:87.23%
Epoch [65/300], Step [130/391],                 Loss: 0.37227, Train_Acc:87.19%
Epoch [65/300], Step [140/391],                 Loss: 0.37098, Train_Acc:87.33%
Epoch [65/300], Step [150/391],                 Loss: 0.37035, Train_Acc:87.30%
Epoch [65/300], Step [160/391],                 Loss: 0.36853, Train_Acc:87.36%
Epoch [65/300], Step [170/391],                 Loss: 0.36903, Train_Acc:87.41%
Epoch [65/300], Step [180/391],                 Loss: 0.37000, Train_Acc:87.31%
Epoch [65/300], Step [190/391],                 Loss: 0.37039, Train_Acc:87.32%
Epoch [65/300], Step [200/391],                 Loss: 0.37110, Train_Acc:87.29%
Epoch [65/300], Step [210/391],                 Loss: 0.37072, Train_Acc:87.27%
Epoch [65/300], Step [220/391],                 Loss: 0.37036, Train_Acc:87.29%
Epoch [65/300], Step [230/391],                 Loss: 0.36913, Train_Acc:87.36%
Epoch [65/300], Step [240/391],                 Loss: 0.36744, Train_Acc:87.43%
Epoch [65/300], Step [250/391],                 Loss: 0.36740, Train_Acc:87.41%
Epoch [65/300], Step [260/391],                 Loss: 0.36904, Train_Acc:87.38%
Epoch [65/300], Step [270/391],                 Loss: 0.37144, Train_Acc:87.29%
Epoch [65/300], Step [280/391],                 Loss: 0.37232, Train_Acc:87.24%
Epoch [65/300], Step [290/391],                 Loss: 0.37340, Train_Acc:87.24%
Epoch [65/300], Step [300/391],                 Loss: 0.37272, Train_Acc:87.28%
Epoch [65/300], Step [310/391],                 Loss: 0.37233, Train_Acc:87.29%
Epoch [65/300], Step [320/391],                 Loss: 0.37183, Train_Acc:87.32%
Epoch [65/300], Step [330/391],                 Loss: 0.37146, Train_Acc:87.32%
Epoch [65/300], Step [340/391],                 Loss: 0.37106, Train_Acc:87.35%
Epoch [65/300], Step [350/391],                 Loss: 0.37079, Train_Acc:87.34%
Epoch [65/300], Step [360/391],                 Loss: 0.37125, Train_Acc:87.34%
Epoch [65/300], Step [370/391],                 Loss: 0.37117, Train_Acc:87.34%
Epoch [65/300], Step [380/391],                 Loss: 0.37112, Train_Acc:87.34%
Epoch [65/300], Step [390/391],                 Loss: 0.37002, Train_Acc:87.38%
Accuary on test images:75.72%
Epoch [66/300], Step [10/391],                 Loss: 0.34012, Train_Acc:88.67%
Epoch [66/300], Step [20/391],                 Loss: 0.34569, Train_Acc:88.12%
Epoch [66/300], Step [30/391],                 Loss: 0.34969, Train_Acc:87.99%
Epoch [66/300], Step [40/391],                 Loss: 0.35665, Train_Acc:87.54%
Epoch [66/300], Step [50/391],                 Loss: 0.36606, Train_Acc:87.27%
Epoch [66/300], Step [60/391],                 Loss: 0.37190, Train_Acc:87.06%
Epoch [66/300], Step [70/391],                 Loss: 0.37134, Train_Acc:87.15%
Epoch [66/300], Step [80/391],                 Loss: 0.36992, Train_Acc:87.29%
Epoch [66/300], Step [90/391],                 Loss: 0.37602, Train_Acc:87.06%
Epoch [66/300], Step [100/391],                 Loss: 0.37354, Train_Acc:87.04%
Epoch [66/300], Step [110/391],                 Loss: 0.37640, Train_Acc:87.00%
Epoch [66/300], Step [120/391],                 Loss: 0.37607, Train_Acc:87.02%
Epoch [66/300], Step [130/391],                 Loss: 0.37583, Train_Acc:87.01%
Epoch [66/300], Step [140/391],                 Loss: 0.37395, Train_Acc:87.10%
Epoch [66/300], Step [150/391],                 Loss: 0.37448, Train_Acc:87.08%
Epoch [66/300], Step [160/391],                 Loss: 0.37179, Train_Acc:87.10%
Epoch [66/300], Step [170/391],                 Loss: 0.37405, Train_Acc:87.02%
Epoch [66/300], Step [180/391],                 Loss: 0.37281, Train_Acc:87.08%
Epoch [66/300], Step [190/391],                 Loss: 0.37224, Train_Acc:87.12%
Epoch [66/300], Step [200/391],                 Loss: 0.37218, Train_Acc:87.16%
Epoch [66/300], Step [210/391],                 Loss: 0.37026, Train_Acc:87.21%
Epoch [66/300], Step [220/391],                 Loss: 0.36870, Train_Acc:87.28%
Epoch [66/300], Step [230/391],                 Loss: 0.36645, Train_Acc:87.34%
Epoch [66/300], Step [240/391],                 Loss: 0.36496, Train_Acc:87.37%
Epoch [66/300], Step [250/391],                 Loss: 0.36447, Train_Acc:87.40%
Epoch [66/300], Step [260/391],                 Loss: 0.36634, Train_Acc:87.32%
Epoch [66/300], Step [270/391],                 Loss: 0.36829, Train_Acc:87.26%
Epoch [66/300], Step [280/391],                 Loss: 0.36887, Train_Acc:87.26%
Epoch [66/300], Step [290/391],                 Loss: 0.36994, Train_Acc:87.22%
Epoch [66/300], Step [300/391],                 Loss: 0.37026, Train_Acc:87.20%
Epoch [66/300], Step [310/391],                 Loss: 0.37006, Train_Acc:87.19%
Epoch [66/300], Step [320/391],                 Loss: 0.37030, Train_Acc:87.18%
Epoch [66/300], Step [330/391],                 Loss: 0.36941, Train_Acc:87.20%
Epoch [66/300], Step [340/391],                 Loss: 0.36835, Train_Acc:87.25%
Epoch [66/300], Step [350/391],                 Loss: 0.36857, Train_Acc:87.25%
Epoch [66/300], Step [360/391],                 Loss: 0.36888, Train_Acc:87.24%
Epoch [66/300], Step [370/391],                 Loss: 0.36943, Train_Acc:87.20%
Epoch [66/300], Step [380/391],                 Loss: 0.36919, Train_Acc:87.23%
Epoch [66/300], Step [390/391],                 Loss: 0.36776, Train_Acc:87.30%
Accuary on test images:76.62%
Epoch [67/300], Step [10/391],                 Loss: 0.35388, Train_Acc:88.75%
Epoch [67/300], Step [20/391],                 Loss: 0.34582, Train_Acc:89.18%
Epoch [67/300], Step [30/391],                 Loss: 0.34547, Train_Acc:88.78%
Epoch [67/300], Step [40/391],                 Loss: 0.34458, Train_Acc:88.61%
Epoch [67/300], Step [50/391],                 Loss: 0.34969, Train_Acc:88.19%
Epoch [67/300], Step [60/391],                 Loss: 0.36060, Train_Acc:87.72%
Epoch [67/300], Step [70/391],                 Loss: 0.36302, Train_Acc:87.60%
Epoch [67/300], Step [80/391],                 Loss: 0.36128, Train_Acc:87.77%
Epoch [67/300], Step [90/391],                 Loss: 0.36410, Train_Acc:87.69%
Epoch [67/300], Step [100/391],                 Loss: 0.36270, Train_Acc:87.67%
Epoch [67/300], Step [110/391],                 Loss: 0.36643, Train_Acc:87.58%
Epoch [67/300], Step [120/391],                 Loss: 0.36706, Train_Acc:87.51%
Epoch [67/300], Step [130/391],                 Loss: 0.37150, Train_Acc:87.40%
Epoch [67/300], Step [140/391],                 Loss: 0.37259, Train_Acc:87.43%
Epoch [67/300], Step [150/391],                 Loss: 0.37394, Train_Acc:87.46%
Epoch [67/300], Step [160/391],                 Loss: 0.37147, Train_Acc:87.54%
Epoch [67/300], Step [170/391],                 Loss: 0.37290, Train_Acc:87.44%
Epoch [67/300], Step [180/391],                 Loss: 0.37213, Train_Acc:87.47%
Epoch [67/300], Step [190/391],                 Loss: 0.37210, Train_Acc:87.45%
Epoch [67/300], Step [200/391],                 Loss: 0.37317, Train_Acc:87.38%
Epoch [67/300], Step [210/391],                 Loss: 0.37194, Train_Acc:87.40%
Epoch [67/300], Step [220/391],                 Loss: 0.37108, Train_Acc:87.44%
Epoch [67/300], Step [230/391],                 Loss: 0.36856, Train_Acc:87.49%
Epoch [67/300], Step [240/391],                 Loss: 0.36633, Train_Acc:87.56%
Epoch [67/300], Step [250/391],                 Loss: 0.36599, Train_Acc:87.54%
Epoch [67/300], Step [260/391],                 Loss: 0.36817, Train_Acc:87.52%
Epoch [67/300], Step [270/391],                 Loss: 0.36926, Train_Acc:87.47%
Epoch [67/300], Step [280/391],                 Loss: 0.36952, Train_Acc:87.45%
Epoch [67/300], Step [290/391],                 Loss: 0.36965, Train_Acc:87.47%
Epoch [67/300], Step [300/391],                 Loss: 0.36876, Train_Acc:87.46%
Epoch [67/300], Step [310/391],                 Loss: 0.36843, Train_Acc:87.46%
Epoch [67/300], Step [320/391],                 Loss: 0.36909, Train_Acc:87.43%
Epoch [67/300], Step [330/391],                 Loss: 0.36836, Train_Acc:87.48%
Epoch [67/300], Step [340/391],                 Loss: 0.36766, Train_Acc:87.52%
Epoch [67/300], Step [350/391],                 Loss: 0.36673, Train_Acc:87.56%
Epoch [67/300], Step [360/391],                 Loss: 0.36672, Train_Acc:87.56%
Epoch [67/300], Step [370/391],                 Loss: 0.36604, Train_Acc:87.55%
Epoch [67/300], Step [380/391],                 Loss: 0.36540, Train_Acc:87.61%
Epoch [67/300], Step [390/391],                 Loss: 0.36467, Train_Acc:87.63%
Accuary on test images:73.00%
Epoch [68/300], Step [10/391],                 Loss: 0.37316, Train_Acc:86.64%
Epoch [68/300], Step [20/391],                 Loss: 0.35936, Train_Acc:87.93%
Epoch [68/300], Step [30/391],                 Loss: 0.35995, Train_Acc:87.92%
Epoch [68/300], Step [40/391],                 Loss: 0.35597, Train_Acc:88.28%
Epoch [68/300], Step [50/391],                 Loss: 0.35199, Train_Acc:88.38%
Epoch [68/300], Step [60/391],                 Loss: 0.36108, Train_Acc:87.94%
Epoch [68/300], Step [70/391],                 Loss: 0.36075, Train_Acc:87.98%
Epoch [68/300], Step [80/391],                 Loss: 0.36175, Train_Acc:88.05%
Epoch [68/300], Step [90/391],                 Loss: 0.36256, Train_Acc:87.97%
Epoch [68/300], Step [100/391],                 Loss: 0.36046, Train_Acc:88.12%
Epoch [68/300], Step [110/391],                 Loss: 0.36165, Train_Acc:88.05%
Epoch [68/300], Step [120/391],                 Loss: 0.36220, Train_Acc:87.99%
Epoch [68/300], Step [130/391],                 Loss: 0.36730, Train_Acc:87.79%
Epoch [68/300], Step [140/391],                 Loss: 0.36779, Train_Acc:87.80%
Epoch [68/300], Step [150/391],                 Loss: 0.37033, Train_Acc:87.65%
Epoch [68/300], Step [160/391],                 Loss: 0.36847, Train_Acc:87.73%
Epoch [68/300], Step [170/391],                 Loss: 0.37003, Train_Acc:87.65%
Epoch [68/300], Step [180/391],                 Loss: 0.36873, Train_Acc:87.65%
Epoch [68/300], Step [190/391],                 Loss: 0.36888, Train_Acc:87.63%
Epoch [68/300], Step [200/391],                 Loss: 0.37016, Train_Acc:87.65%
Epoch [68/300], Step [210/391],                 Loss: 0.37192, Train_Acc:87.59%
Epoch [68/300], Step [220/391],                 Loss: 0.37218, Train_Acc:87.56%
Epoch [68/300], Step [230/391],                 Loss: 0.37100, Train_Acc:87.57%
Epoch [68/300], Step [240/391],                 Loss: 0.36822, Train_Acc:87.65%
Epoch [68/300], Step [250/391],                 Loss: 0.36755, Train_Acc:87.71%
Epoch [68/300], Step [260/391],                 Loss: 0.36921, Train_Acc:87.68%
Epoch [68/300], Step [270/391],                 Loss: 0.36938, Train_Acc:87.62%
Epoch [68/300], Step [280/391],                 Loss: 0.36946, Train_Acc:87.61%
Epoch [68/300], Step [290/391],                 Loss: 0.36983, Train_Acc:87.61%
Epoch [68/300], Step [300/391],                 Loss: 0.36928, Train_Acc:87.60%
Epoch [68/300], Step [310/391],                 Loss: 0.36839, Train_Acc:87.62%
Epoch [68/300], Step [320/391],                 Loss: 0.36789, Train_Acc:87.63%
Epoch [68/300], Step [330/391],                 Loss: 0.36754, Train_Acc:87.64%
Epoch [68/300], Step [340/391],                 Loss: 0.36687, Train_Acc:87.64%
Epoch [68/300], Step [350/391],                 Loss: 0.36720, Train_Acc:87.62%
Epoch [68/300], Step [360/391],                 Loss: 0.36738, Train_Acc:87.62%
Epoch [68/300], Step [370/391],                 Loss: 0.36790, Train_Acc:87.61%
Epoch [68/300], Step [380/391],                 Loss: 0.36807, Train_Acc:87.59%
Epoch [68/300], Step [390/391],                 Loss: 0.36764, Train_Acc:87.59%
Accuary on test images:71.46%
Epoch [69/300], Step [10/391],                 Loss: 0.36451, Train_Acc:87.19%
Epoch [69/300], Step [20/391],                 Loss: 0.37851, Train_Acc:87.34%
Epoch [69/300], Step [30/391],                 Loss: 0.37250, Train_Acc:87.50%
Epoch [69/300], Step [40/391],                 Loss: 0.37535, Train_Acc:87.42%
Epoch [69/300], Step [50/391],                 Loss: 0.37174, Train_Acc:87.52%
Epoch [69/300], Step [60/391],                 Loss: 0.37218, Train_Acc:87.47%
Epoch [69/300], Step [70/391],                 Loss: 0.37376, Train_Acc:87.44%
Epoch [69/300], Step [80/391],                 Loss: 0.37391, Train_Acc:87.53%
Epoch [69/300], Step [90/391],                 Loss: 0.37611, Train_Acc:87.39%
Epoch [69/300], Step [100/391],                 Loss: 0.37266, Train_Acc:87.51%
Epoch [69/300], Step [110/391],                 Loss: 0.37740, Train_Acc:87.39%
Epoch [69/300], Step [120/391],                 Loss: 0.37740, Train_Acc:87.38%
Epoch [69/300], Step [130/391],                 Loss: 0.38177, Train_Acc:87.24%
Epoch [69/300], Step [140/391],                 Loss: 0.38033, Train_Acc:87.23%
Epoch [69/300], Step [150/391],                 Loss: 0.37962, Train_Acc:87.26%
Epoch [69/300], Step [160/391],                 Loss: 0.37940, Train_Acc:87.25%
Epoch [69/300], Step [170/391],                 Loss: 0.37879, Train_Acc:87.29%
Epoch [69/300], Step [180/391],                 Loss: 0.37826, Train_Acc:87.28%
Epoch [69/300], Step [190/391],                 Loss: 0.37769, Train_Acc:87.27%
Epoch [69/300], Step [200/391],                 Loss: 0.37753, Train_Acc:87.26%
Epoch [69/300], Step [210/391],                 Loss: 0.37582, Train_Acc:87.30%
Epoch [69/300], Step [220/391],                 Loss: 0.37587, Train_Acc:87.30%
Epoch [69/300], Step [230/391],                 Loss: 0.37435, Train_Acc:87.36%
Epoch [69/300], Step [240/391],                 Loss: 0.37293, Train_Acc:87.38%
Epoch [69/300], Step [250/391],                 Loss: 0.37212, Train_Acc:87.39%
Epoch [69/300], Step [260/391],                 Loss: 0.37531, Train_Acc:87.32%
Epoch [69/300], Step [270/391],                 Loss: 0.37589, Train_Acc:87.26%
Epoch [69/300], Step [280/391],                 Loss: 0.37459, Train_Acc:87.29%
Epoch [69/300], Step [290/391],                 Loss: 0.37486, Train_Acc:87.31%
Epoch [69/300], Step [300/391],                 Loss: 0.37384, Train_Acc:87.35%
Epoch [69/300], Step [310/391],                 Loss: 0.37319, Train_Acc:87.37%
Epoch [69/300], Step [320/391],                 Loss: 0.37312, Train_Acc:87.38%
Epoch [69/300], Step [330/391],                 Loss: 0.37167, Train_Acc:87.43%
Epoch [69/300], Step [340/391],                 Loss: 0.36999, Train_Acc:87.48%
Epoch [69/300], Step [350/391],                 Loss: 0.36913, Train_Acc:87.51%
Epoch [69/300], Step [360/391],                 Loss: 0.36890, Train_Acc:87.52%
Epoch [69/300], Step [370/391],                 Loss: 0.36855, Train_Acc:87.51%
Epoch [69/300], Step [380/391],                 Loss: 0.36852, Train_Acc:87.52%
Epoch [69/300], Step [390/391],                 Loss: 0.36853, Train_Acc:87.52%
Accuary on test images:75.38%
Epoch [70/300], Step [10/391],                 Loss: 0.36480, Train_Acc:87.73%
Epoch [70/300], Step [20/391],                 Loss: 0.35869, Train_Acc:88.24%
Epoch [70/300], Step [30/391],                 Loss: 0.36811, Train_Acc:87.71%
Epoch [70/300], Step [40/391],                 Loss: 0.36949, Train_Acc:87.50%
Epoch [70/300], Step [50/391],                 Loss: 0.36790, Train_Acc:87.42%
Epoch [70/300], Step [60/391],                 Loss: 0.36948, Train_Acc:87.36%
Epoch [70/300], Step [70/391],                 Loss: 0.36786, Train_Acc:87.43%
Epoch [70/300], Step [80/391],                 Loss: 0.36713, Train_Acc:87.54%
Epoch [70/300], Step [90/391],                 Loss: 0.36946, Train_Acc:87.48%
Epoch [70/300], Step [100/391],                 Loss: 0.37239, Train_Acc:87.38%
Epoch [70/300], Step [110/391],                 Loss: 0.37558, Train_Acc:87.14%
Epoch [70/300], Step [120/391],                 Loss: 0.37313, Train_Acc:87.23%
Epoch [70/300], Step [130/391],                 Loss: 0.37451, Train_Acc:87.21%
Epoch [70/300], Step [140/391],                 Loss: 0.37231, Train_Acc:87.34%
Epoch [70/300], Step [150/391],                 Loss: 0.37374, Train_Acc:87.27%
Epoch [70/300], Step [160/391],                 Loss: 0.37209, Train_Acc:87.36%
Epoch [70/300], Step [170/391],                 Loss: 0.37308, Train_Acc:87.37%
Epoch [70/300], Step [180/391],                 Loss: 0.37244, Train_Acc:87.36%
Epoch [70/300], Step [190/391],                 Loss: 0.37160, Train_Acc:87.39%
Epoch [70/300], Step [200/391],                 Loss: 0.37182, Train_Acc:87.36%
Epoch [70/300], Step [210/391],                 Loss: 0.37205, Train_Acc:87.38%
Epoch [70/300], Step [220/391],                 Loss: 0.37068, Train_Acc:87.45%
Epoch [70/300], Step [230/391],                 Loss: 0.36835, Train_Acc:87.49%
Epoch [70/300], Step [240/391],                 Loss: 0.36665, Train_Acc:87.57%
Epoch [70/300], Step [250/391],                 Loss: 0.36623, Train_Acc:87.58%
Epoch [70/300], Step [260/391],                 Loss: 0.36940, Train_Acc:87.50%
Epoch [70/300], Step [270/391],                 Loss: 0.37106, Train_Acc:87.42%
Epoch [70/300], Step [280/391],                 Loss: 0.37000, Train_Acc:87.45%
Epoch [70/300], Step [290/391],                 Loss: 0.36940, Train_Acc:87.46%
Epoch [70/300], Step [300/391],                 Loss: 0.36896, Train_Acc:87.46%
Epoch [70/300], Step [310/391],                 Loss: 0.36794, Train_Acc:87.48%
Epoch [70/300], Step [320/391],                 Loss: 0.36924, Train_Acc:87.44%
Epoch [70/300], Step [330/391],                 Loss: 0.36923, Train_Acc:87.43%
Epoch [70/300], Step [340/391],                 Loss: 0.36869, Train_Acc:87.41%
Epoch [70/300], Step [350/391],                 Loss: 0.36790, Train_Acc:87.45%
Epoch [70/300], Step [360/391],                 Loss: 0.36733, Train_Acc:87.46%
Epoch [70/300], Step [370/391],                 Loss: 0.36680, Train_Acc:87.46%
Epoch [70/300], Step [380/391],                 Loss: 0.36640, Train_Acc:87.47%
Epoch [70/300], Step [390/391],                 Loss: 0.36527, Train_Acc:87.50%
Accuary on test images:75.56%
Epoch [71/300], Step [10/391],                 Loss: 0.36131, Train_Acc:88.05%
Epoch [71/300], Step [20/391],                 Loss: 0.34246, Train_Acc:88.71%
Epoch [71/300], Step [30/391],                 Loss: 0.34151, Train_Acc:88.70%
Epoch [71/300], Step [40/391],                 Loss: 0.34550, Train_Acc:88.46%
Epoch [71/300], Step [50/391],                 Loss: 0.35098, Train_Acc:88.11%
Epoch [71/300], Step [60/391],                 Loss: 0.36246, Train_Acc:87.59%
Epoch [71/300], Step [70/391],                 Loss: 0.36272, Train_Acc:87.47%
Epoch [71/300], Step [80/391],                 Loss: 0.36684, Train_Acc:87.29%
Epoch [71/300], Step [90/391],                 Loss: 0.37065, Train_Acc:87.25%
Epoch [71/300], Step [100/391],                 Loss: 0.37377, Train_Acc:87.14%
Epoch [71/300], Step [110/391],                 Loss: 0.37511, Train_Acc:87.11%
Epoch [71/300], Step [120/391],                 Loss: 0.37662, Train_Acc:87.06%
Epoch [71/300], Step [130/391],                 Loss: 0.37473, Train_Acc:87.15%
Epoch [71/300], Step [140/391],                 Loss: 0.37144, Train_Acc:87.29%
Epoch [71/300], Step [150/391],                 Loss: 0.36949, Train_Acc:87.35%
Epoch [71/300], Step [160/391],                 Loss: 0.36726, Train_Acc:87.39%
Epoch [71/300], Step [170/391],                 Loss: 0.36776, Train_Acc:87.39%
Epoch [71/300], Step [180/391],                 Loss: 0.36762, Train_Acc:87.42%
Epoch [71/300], Step [190/391],                 Loss: 0.36859, Train_Acc:87.33%
Epoch [71/300], Step [200/391],                 Loss: 0.36892, Train_Acc:87.30%
Epoch [71/300], Step [210/391],                 Loss: 0.36904, Train_Acc:87.32%
Epoch [71/300], Step [220/391],                 Loss: 0.36884, Train_Acc:87.31%
Epoch [71/300], Step [230/391],                 Loss: 0.36755, Train_Acc:87.36%
Epoch [71/300], Step [240/391],                 Loss: 0.36595, Train_Acc:87.44%
Epoch [71/300], Step [250/391],                 Loss: 0.36606, Train_Acc:87.43%
Epoch [71/300], Step [260/391],                 Loss: 0.36848, Train_Acc:87.39%
Epoch [71/300], Step [270/391],                 Loss: 0.36955, Train_Acc:87.33%
Epoch [71/300], Step [280/391],                 Loss: 0.36934, Train_Acc:87.35%
Epoch [71/300], Step [290/391],                 Loss: 0.36932, Train_Acc:87.39%
Epoch [71/300], Step [300/391],                 Loss: 0.36907, Train_Acc:87.40%
Epoch [71/300], Step [310/391],                 Loss: 0.36894, Train_Acc:87.41%
Epoch [71/300], Step [320/391],                 Loss: 0.36914, Train_Acc:87.37%
Epoch [71/300], Step [330/391],                 Loss: 0.36774, Train_Acc:87.45%
Epoch [71/300], Step [340/391],                 Loss: 0.36635, Train_Acc:87.52%
Epoch [71/300], Step [350/391],                 Loss: 0.36670, Train_Acc:87.52%
Epoch [71/300], Step [360/391],                 Loss: 0.36795, Train_Acc:87.47%
Epoch [71/300], Step [370/391],                 Loss: 0.36780, Train_Acc:87.46%
Epoch [71/300], Step [380/391],                 Loss: 0.36721, Train_Acc:87.48%
Epoch [71/300], Step [390/391],                 Loss: 0.36710, Train_Acc:87.48%
Accuary on test images:77.66%
Epoch [72/300], Step [10/391],                 Loss: 0.35881, Train_Acc:87.58%
Epoch [72/300], Step [20/391],                 Loss: 0.34907, Train_Acc:88.09%
Epoch [72/300], Step [30/391],                 Loss: 0.33998, Train_Acc:88.26%
Epoch [72/300], Step [40/391],                 Loss: 0.34971, Train_Acc:88.03%
Epoch [72/300], Step [50/391],                 Loss: 0.35227, Train_Acc:88.00%
Epoch [72/300], Step [60/391],                 Loss: 0.36034, Train_Acc:87.68%
Epoch [72/300], Step [70/391],                 Loss: 0.35617, Train_Acc:87.92%
Epoch [72/300], Step [80/391],                 Loss: 0.35638, Train_Acc:87.91%
Epoch [72/300], Step [90/391],                 Loss: 0.36057, Train_Acc:87.74%
Epoch [72/300], Step [100/391],                 Loss: 0.36113, Train_Acc:87.67%
Epoch [72/300], Step [110/391],                 Loss: 0.36464, Train_Acc:87.55%
Epoch [72/300], Step [120/391],                 Loss: 0.36714, Train_Acc:87.40%
Epoch [72/300], Step [130/391],                 Loss: 0.37163, Train_Acc:87.32%
Epoch [72/300], Step [140/391],                 Loss: 0.37127, Train_Acc:87.35%
Epoch [72/300], Step [150/391],                 Loss: 0.37205, Train_Acc:87.33%
Epoch [72/300], Step [160/391],                 Loss: 0.36987, Train_Acc:87.41%
Epoch [72/300], Step [170/391],                 Loss: 0.36863, Train_Acc:87.45%
Epoch [72/300], Step [180/391],                 Loss: 0.36784, Train_Acc:87.49%
Epoch [72/300], Step [190/391],                 Loss: 0.36753, Train_Acc:87.48%
Epoch [72/300], Step [200/391],                 Loss: 0.36872, Train_Acc:87.43%
Epoch [72/300], Step [210/391],                 Loss: 0.37031, Train_Acc:87.38%
Epoch [72/300], Step [220/391],                 Loss: 0.36948, Train_Acc:87.37%
Epoch [72/300], Step [230/391],                 Loss: 0.36815, Train_Acc:87.41%
Epoch [72/300], Step [240/391],                 Loss: 0.36674, Train_Acc:87.48%
Epoch [72/300], Step [250/391],                 Loss: 0.36582, Train_Acc:87.50%
Epoch [72/300], Step [260/391],                 Loss: 0.36666, Train_Acc:87.50%
Epoch [72/300], Step [270/391],                 Loss: 0.36702, Train_Acc:87.47%
Epoch [72/300], Step [280/391],                 Loss: 0.36599, Train_Acc:87.49%
Epoch [72/300], Step [290/391],                 Loss: 0.36568, Train_Acc:87.51%
Epoch [72/300], Step [300/391],                 Loss: 0.36408, Train_Acc:87.54%
Epoch [72/300], Step [310/391],                 Loss: 0.36323, Train_Acc:87.54%
Epoch [72/300], Step [320/391],                 Loss: 0.36257, Train_Acc:87.60%
Epoch [72/300], Step [330/391],                 Loss: 0.36165, Train_Acc:87.65%
Epoch [72/300], Step [340/391],                 Loss: 0.36103, Train_Acc:87.69%
Epoch [72/300], Step [350/391],                 Loss: 0.36093, Train_Acc:87.73%
Epoch [72/300], Step [360/391],                 Loss: 0.36134, Train_Acc:87.68%
Epoch [72/300], Step [370/391],                 Loss: 0.36281, Train_Acc:87.64%
Epoch [72/300], Step [380/391],                 Loss: 0.36299, Train_Acc:87.62%
Epoch [72/300], Step [390/391],                 Loss: 0.36310, Train_Acc:87.61%
Accuary on test images:72.12%
Epoch [73/300], Step [10/391],                 Loss: 0.37276, Train_Acc:87.27%
Epoch [73/300], Step [20/391],                 Loss: 0.36948, Train_Acc:87.58%
Epoch [73/300], Step [30/391],                 Loss: 0.36511, Train_Acc:87.68%
Epoch [73/300], Step [40/391],                 Loss: 0.36578, Train_Acc:87.73%
Epoch [73/300], Step [50/391],                 Loss: 0.36026, Train_Acc:87.88%
Epoch [73/300], Step [60/391],                 Loss: 0.36785, Train_Acc:87.43%
Epoch [73/300], Step [70/391],                 Loss: 0.36461, Train_Acc:87.56%
Epoch [73/300], Step [80/391],                 Loss: 0.36282, Train_Acc:87.71%
Epoch [73/300], Step [90/391],                 Loss: 0.36347, Train_Acc:87.60%
Epoch [73/300], Step [100/391],                 Loss: 0.36560, Train_Acc:87.52%
Epoch [73/300], Step [110/391],                 Loss: 0.37025, Train_Acc:87.41%
Epoch [73/300], Step [120/391],                 Loss: 0.37238, Train_Acc:87.26%
Epoch [73/300], Step [130/391],                 Loss: 0.37414, Train_Acc:87.25%
Epoch [73/300], Step [140/391],                 Loss: 0.37288, Train_Acc:87.33%
Epoch [73/300], Step [150/391],                 Loss: 0.37317, Train_Acc:87.30%
Epoch [73/300], Step [160/391],                 Loss: 0.37048, Train_Acc:87.36%
Epoch [73/300], Step [170/391],                 Loss: 0.36839, Train_Acc:87.46%
Epoch [73/300], Step [180/391],                 Loss: 0.36567, Train_Acc:87.58%
Epoch [73/300], Step [190/391],                 Loss: 0.36424, Train_Acc:87.62%
Epoch [73/300], Step [200/391],                 Loss: 0.36412, Train_Acc:87.57%
Epoch [73/300], Step [210/391],                 Loss: 0.36182, Train_Acc:87.66%
Epoch [73/300], Step [220/391],                 Loss: 0.36140, Train_Acc:87.69%
Epoch [73/300], Step [230/391],                 Loss: 0.36091, Train_Acc:87.70%
Epoch [73/300], Step [240/391],                 Loss: 0.35985, Train_Acc:87.73%
Epoch [73/300], Step [250/391],                 Loss: 0.36035, Train_Acc:87.69%
Epoch [73/300], Step [260/391],                 Loss: 0.36419, Train_Acc:87.60%
Epoch [73/300], Step [270/391],                 Loss: 0.36484, Train_Acc:87.57%
Epoch [73/300], Step [280/391],                 Loss: 0.36422, Train_Acc:87.63%
Epoch [73/300], Step [290/391],                 Loss: 0.36425, Train_Acc:87.68%
Epoch [73/300], Step [300/391],                 Loss: 0.36461, Train_Acc:87.66%
Epoch [73/300], Step [310/391],                 Loss: 0.36528, Train_Acc:87.63%
Epoch [73/300], Step [320/391],                 Loss: 0.36427, Train_Acc:87.65%
Epoch [73/300], Step [330/391],                 Loss: 0.36333, Train_Acc:87.68%
Epoch [73/300], Step [340/391],                 Loss: 0.36338, Train_Acc:87.67%
Epoch [73/300], Step [350/391],                 Loss: 0.36350, Train_Acc:87.66%
Epoch [73/300], Step [360/391],                 Loss: 0.36439, Train_Acc:87.63%
Epoch [73/300], Step [370/391],                 Loss: 0.36493, Train_Acc:87.60%
Epoch [73/300], Step [380/391],                 Loss: 0.36494, Train_Acc:87.64%
Epoch [73/300], Step [390/391],                 Loss: 0.36377, Train_Acc:87.68%
Accuary on test images:78.26%
Epoch [74/300], Step [10/391],                 Loss: 0.35873, Train_Acc:87.73%
Epoch [74/300], Step [20/391],                 Loss: 0.35172, Train_Acc:87.62%
Epoch [74/300], Step [30/391],                 Loss: 0.33901, Train_Acc:87.97%
Epoch [74/300], Step [40/391],                 Loss: 0.33721, Train_Acc:88.26%
Epoch [74/300], Step [50/391],                 Loss: 0.34216, Train_Acc:88.20%
Epoch [74/300], Step [60/391],                 Loss: 0.35968, Train_Acc:87.41%
Epoch [74/300], Step [70/391],                 Loss: 0.36384, Train_Acc:87.31%
Epoch [74/300], Step [80/391],                 Loss: 0.37023, Train_Acc:87.12%
Epoch [74/300], Step [90/391],                 Loss: 0.37203, Train_Acc:87.08%
Epoch [74/300], Step [100/391],                 Loss: 0.36856, Train_Acc:87.23%
Epoch [74/300], Step [110/391],                 Loss: 0.36966, Train_Acc:87.28%
Epoch [74/300], Step [120/391],                 Loss: 0.37035, Train_Acc:87.26%
Epoch [74/300], Step [130/391],                 Loss: 0.37190, Train_Acc:87.21%
Epoch [74/300], Step [140/391],                 Loss: 0.37080, Train_Acc:87.29%
Epoch [74/300], Step [150/391],                 Loss: 0.37154, Train_Acc:87.31%
Epoch [74/300], Step [160/391],                 Loss: 0.37003, Train_Acc:87.35%
Epoch [74/300], Step [170/391],                 Loss: 0.37006, Train_Acc:87.36%
Epoch [74/300], Step [180/391],                 Loss: 0.36943, Train_Acc:87.37%
Epoch [74/300], Step [190/391],                 Loss: 0.36817, Train_Acc:87.37%
Epoch [74/300], Step [200/391],                 Loss: 0.36791, Train_Acc:87.38%
Epoch [74/300], Step [210/391],                 Loss: 0.36591, Train_Acc:87.49%
Epoch [74/300], Step [220/391],                 Loss: 0.36445, Train_Acc:87.56%
Epoch [74/300], Step [230/391],                 Loss: 0.36359, Train_Acc:87.54%
Epoch [74/300], Step [240/391],                 Loss: 0.36167, Train_Acc:87.61%
Epoch [74/300], Step [250/391],                 Loss: 0.36138, Train_Acc:87.61%
Epoch [74/300], Step [260/391],                 Loss: 0.36176, Train_Acc:87.63%
Epoch [74/300], Step [270/391],                 Loss: 0.36257, Train_Acc:87.59%
Epoch [74/300], Step [280/391],                 Loss: 0.36472, Train_Acc:87.52%
Epoch [74/300], Step [290/391],                 Loss: 0.36478, Train_Acc:87.54%
Epoch [74/300], Step [300/391],                 Loss: 0.36558, Train_Acc:87.48%
Epoch [74/300], Step [310/391],                 Loss: 0.36581, Train_Acc:87.47%
Epoch [74/300], Step [320/391],                 Loss: 0.36587, Train_Acc:87.50%
Epoch [74/300], Step [330/391],                 Loss: 0.36572, Train_Acc:87.52%
Epoch [74/300], Step [340/391],                 Loss: 0.36563, Train_Acc:87.52%
Epoch [74/300], Step [350/391],                 Loss: 0.36636, Train_Acc:87.53%
Epoch [74/300], Step [360/391],                 Loss: 0.36649, Train_Acc:87.51%
Epoch [74/300], Step [370/391],                 Loss: 0.36593, Train_Acc:87.53%
Epoch [74/300], Step [380/391],                 Loss: 0.36461, Train_Acc:87.58%
Epoch [74/300], Step [390/391],                 Loss: 0.36315, Train_Acc:87.63%
Accuary on test images:77.96%
Epoch [75/300], Step [10/391],                 Loss: 0.32733, Train_Acc:89.06%
Epoch [75/300], Step [20/391],                 Loss: 0.31847, Train_Acc:89.34%
Epoch [75/300], Step [30/391],                 Loss: 0.31699, Train_Acc:89.24%
Epoch [75/300], Step [40/391],                 Loss: 0.32904, Train_Acc:89.06%
Epoch [75/300], Step [50/391],                 Loss: 0.33341, Train_Acc:88.59%
Epoch [75/300], Step [60/391],                 Loss: 0.34775, Train_Acc:87.94%
Epoch [75/300], Step [70/391],                 Loss: 0.35061, Train_Acc:87.96%
Epoch [75/300], Step [80/391],                 Loss: 0.35466, Train_Acc:87.71%
Epoch [75/300], Step [90/391],                 Loss: 0.35925, Train_Acc:87.54%
Epoch [75/300], Step [100/391],                 Loss: 0.36046, Train_Acc:87.54%
Epoch [75/300], Step [110/391],                 Loss: 0.36250, Train_Acc:87.47%
Epoch [75/300], Step [120/391],                 Loss: 0.36342, Train_Acc:87.41%
Epoch [75/300], Step [130/391],                 Loss: 0.36596, Train_Acc:87.35%
Epoch [75/300], Step [140/391],                 Loss: 0.36671, Train_Acc:87.35%
Epoch [75/300], Step [150/391],                 Loss: 0.36822, Train_Acc:87.29%
Epoch [75/300], Step [160/391],                 Loss: 0.36639, Train_Acc:87.35%
Epoch [75/300], Step [170/391],                 Loss: 0.36627, Train_Acc:87.36%
Epoch [75/300], Step [180/391],                 Loss: 0.36632, Train_Acc:87.33%
Epoch [75/300], Step [190/391],                 Loss: 0.36766, Train_Acc:87.28%
Epoch [75/300], Step [200/391],                 Loss: 0.36911, Train_Acc:87.20%
Epoch [75/300], Step [210/391],                 Loss: 0.36866, Train_Acc:87.28%
Epoch [75/300], Step [220/391],                 Loss: 0.36897, Train_Acc:87.31%
Epoch [75/300], Step [230/391],                 Loss: 0.36709, Train_Acc:87.40%
Epoch [75/300], Step [240/391],                 Loss: 0.36520, Train_Acc:87.48%
Epoch [75/300], Step [250/391],                 Loss: 0.36584, Train_Acc:87.45%
Epoch [75/300], Step [260/391],                 Loss: 0.36747, Train_Acc:87.42%
Epoch [75/300], Step [270/391],                 Loss: 0.36866, Train_Acc:87.34%
Epoch [75/300], Step [280/391],                 Loss: 0.36830, Train_Acc:87.38%
Epoch [75/300], Step [290/391],                 Loss: 0.36850, Train_Acc:87.38%
Epoch [75/300], Step [300/391],                 Loss: 0.36909, Train_Acc:87.35%
Epoch [75/300], Step [310/391],                 Loss: 0.36852, Train_Acc:87.34%
Epoch [75/300], Step [320/391],                 Loss: 0.36810, Train_Acc:87.35%
Epoch [75/300], Step [330/391],                 Loss: 0.36743, Train_Acc:87.40%
Epoch [75/300], Step [340/391],                 Loss: 0.36619, Train_Acc:87.42%
Epoch [75/300], Step [350/391],                 Loss: 0.36563, Train_Acc:87.45%
Epoch [75/300], Step [360/391],                 Loss: 0.36572, Train_Acc:87.47%
Epoch [75/300], Step [370/391],                 Loss: 0.36535, Train_Acc:87.49%
Epoch [75/300], Step [380/391],                 Loss: 0.36492, Train_Acc:87.48%
Epoch [75/300], Step [390/391],                 Loss: 0.36445, Train_Acc:87.52%
Accuary on test images:72.96%
Epoch [76/300], Step [10/391],                 Loss: 0.36622, Train_Acc:87.89%
Epoch [76/300], Step [20/391],                 Loss: 0.36351, Train_Acc:87.93%
Epoch [76/300], Step [30/391],                 Loss: 0.34656, Train_Acc:88.46%
Epoch [76/300], Step [40/391],                 Loss: 0.34939, Train_Acc:88.34%
Epoch [76/300], Step [50/391],                 Loss: 0.34147, Train_Acc:88.50%
Epoch [76/300], Step [60/391],                 Loss: 0.34948, Train_Acc:88.31%
Epoch [76/300], Step [70/391],                 Loss: 0.35165, Train_Acc:88.31%
Epoch [76/300], Step [80/391],                 Loss: 0.35492, Train_Acc:88.18%
Epoch [76/300], Step [90/391],                 Loss: 0.35645, Train_Acc:88.07%
Epoch [76/300], Step [100/391],                 Loss: 0.35627, Train_Acc:88.02%
Epoch [76/300], Step [110/391],                 Loss: 0.35722, Train_Acc:88.08%
Epoch [76/300], Step [120/391],                 Loss: 0.35705, Train_Acc:88.01%
Epoch [76/300], Step [130/391],                 Loss: 0.36109, Train_Acc:87.87%
Epoch [76/300], Step [140/391],                 Loss: 0.36135, Train_Acc:87.88%
Epoch [76/300], Step [150/391],                 Loss: 0.36395, Train_Acc:87.80%
Epoch [76/300], Step [160/391],                 Loss: 0.36149, Train_Acc:87.87%
Epoch [76/300], Step [170/391],                 Loss: 0.36114, Train_Acc:87.85%
Epoch [76/300], Step [180/391],                 Loss: 0.36081, Train_Acc:87.85%
Epoch [76/300], Step [190/391],                 Loss: 0.36072, Train_Acc:87.86%
Epoch [76/300], Step [200/391],                 Loss: 0.36134, Train_Acc:87.78%
Epoch [76/300], Step [210/391],                 Loss: 0.36194, Train_Acc:87.76%
Epoch [76/300], Step [220/391],                 Loss: 0.36195, Train_Acc:87.77%
Epoch [76/300], Step [230/391],                 Loss: 0.35866, Train_Acc:87.87%
Epoch [76/300], Step [240/391],                 Loss: 0.35799, Train_Acc:87.90%
Epoch [76/300], Step [250/391],                 Loss: 0.35894, Train_Acc:87.89%
Epoch [76/300], Step [260/391],                 Loss: 0.36141, Train_Acc:87.83%
Epoch [76/300], Step [270/391],                 Loss: 0.36164, Train_Acc:87.79%
Epoch [76/300], Step [280/391],                 Loss: 0.36137, Train_Acc:87.80%
Epoch [76/300], Step [290/391],                 Loss: 0.36050, Train_Acc:87.82%
Epoch [76/300], Step [300/391],                 Loss: 0.35936, Train_Acc:87.83%
Epoch [76/300], Step [310/391],                 Loss: 0.35900, Train_Acc:87.85%
Epoch [76/300], Step [320/391],                 Loss: 0.35841, Train_Acc:87.85%
Epoch [76/300], Step [330/391],                 Loss: 0.35686, Train_Acc:87.93%
Epoch [76/300], Step [340/391],                 Loss: 0.35591, Train_Acc:87.97%
Epoch [76/300], Step [350/391],                 Loss: 0.35623, Train_Acc:87.91%
Epoch [76/300], Step [360/391],                 Loss: 0.35721, Train_Acc:87.88%
Epoch [76/300], Step [370/391],                 Loss: 0.35830, Train_Acc:87.83%
Epoch [76/300], Step [380/391],                 Loss: 0.35876, Train_Acc:87.80%
Epoch [76/300], Step [390/391],                 Loss: 0.35808, Train_Acc:87.81%
Accuary on test images:79.32%
Epoch [77/300], Step [10/391],                 Loss: 0.37829, Train_Acc:86.56%
Epoch [77/300], Step [20/391],                 Loss: 0.37905, Train_Acc:86.60%
Epoch [77/300], Step [30/391],                 Loss: 0.37593, Train_Acc:86.85%
Epoch [77/300], Step [40/391],                 Loss: 0.37587, Train_Acc:87.05%
Epoch [77/300], Step [50/391],                 Loss: 0.37478, Train_Acc:86.92%
Epoch [77/300], Step [60/391],                 Loss: 0.37484, Train_Acc:86.90%
Epoch [77/300], Step [70/391],                 Loss: 0.37009, Train_Acc:87.11%
Epoch [77/300], Step [80/391],                 Loss: 0.36697, Train_Acc:87.25%
Epoch [77/300], Step [90/391],                 Loss: 0.37090, Train_Acc:87.15%
Epoch [77/300], Step [100/391],                 Loss: 0.36879, Train_Acc:87.20%
Epoch [77/300], Step [110/391],                 Loss: 0.37141, Train_Acc:87.17%
Epoch [77/300], Step [120/391],                 Loss: 0.36962, Train_Acc:87.21%
Epoch [77/300], Step [130/391],                 Loss: 0.37320, Train_Acc:87.11%
Epoch [77/300], Step [140/391],                 Loss: 0.37251, Train_Acc:87.11%
Epoch [77/300], Step [150/391],                 Loss: 0.37376, Train_Acc:87.08%
Epoch [77/300], Step [160/391],                 Loss: 0.37138, Train_Acc:87.19%
Epoch [77/300], Step [170/391],                 Loss: 0.37207, Train_Acc:87.18%
Epoch [77/300], Step [180/391],                 Loss: 0.37108, Train_Acc:87.21%
Epoch [77/300], Step [190/391],                 Loss: 0.37110, Train_Acc:87.21%
Epoch [77/300], Step [200/391],                 Loss: 0.37152, Train_Acc:87.25%
Epoch [77/300], Step [210/391],                 Loss: 0.37183, Train_Acc:87.26%
Epoch [77/300], Step [220/391],                 Loss: 0.36982, Train_Acc:87.36%
Epoch [77/300], Step [230/391],                 Loss: 0.36928, Train_Acc:87.37%
Epoch [77/300], Step [240/391],                 Loss: 0.36676, Train_Acc:87.43%
Epoch [77/300], Step [250/391],                 Loss: 0.36752, Train_Acc:87.40%
Epoch [77/300], Step [260/391],                 Loss: 0.36905, Train_Acc:87.41%
Epoch [77/300], Step [270/391],                 Loss: 0.37009, Train_Acc:87.39%
Epoch [77/300], Step [280/391],                 Loss: 0.37020, Train_Acc:87.41%
Epoch [77/300], Step [290/391],                 Loss: 0.37015, Train_Acc:87.42%
Epoch [77/300], Step [300/391],                 Loss: 0.36979, Train_Acc:87.43%
Epoch [77/300], Step [310/391],                 Loss: 0.36879, Train_Acc:87.45%
Epoch [77/300], Step [320/391],                 Loss: 0.36892, Train_Acc:87.45%
Epoch [77/300], Step [330/391],                 Loss: 0.36779, Train_Acc:87.51%
Epoch [77/300], Step [340/391],                 Loss: 0.36751, Train_Acc:87.51%
Epoch [77/300], Step [350/391],                 Loss: 0.36658, Train_Acc:87.54%
Epoch [77/300], Step [360/391],                 Loss: 0.36637, Train_Acc:87.55%
Epoch [77/300], Step [370/391],                 Loss: 0.36733, Train_Acc:87.50%
Epoch [77/300], Step [380/391],                 Loss: 0.36708, Train_Acc:87.49%
Epoch [77/300], Step [390/391],                 Loss: 0.36659, Train_Acc:87.48%
Accuary on test images:69.64%
Epoch [78/300], Step [10/391],                 Loss: 0.36041, Train_Acc:86.95%
Epoch [78/300], Step [20/391],                 Loss: 0.36560, Train_Acc:87.15%
Epoch [78/300], Step [30/391],                 Loss: 0.35702, Train_Acc:87.53%
Epoch [78/300], Step [40/391],                 Loss: 0.35925, Train_Acc:87.38%
Epoch [78/300], Step [50/391],                 Loss: 0.35944, Train_Acc:87.30%
Epoch [78/300], Step [60/391],                 Loss: 0.36691, Train_Acc:87.17%
Epoch [78/300], Step [70/391],                 Loss: 0.36501, Train_Acc:87.23%
Epoch [78/300], Step [80/391],                 Loss: 0.36247, Train_Acc:87.33%
Epoch [78/300], Step [90/391],                 Loss: 0.36221, Train_Acc:87.41%
Epoch [78/300], Step [100/391],                 Loss: 0.35812, Train_Acc:87.55%
Epoch [78/300], Step [110/391],                 Loss: 0.36219, Train_Acc:87.48%
Epoch [78/300], Step [120/391],                 Loss: 0.36545, Train_Acc:87.38%
Epoch [78/300], Step [130/391],                 Loss: 0.36683, Train_Acc:87.40%
Epoch [78/300], Step [140/391],                 Loss: 0.36725, Train_Acc:87.35%
Epoch [78/300], Step [150/391],                 Loss: 0.36833, Train_Acc:87.26%
Epoch [78/300], Step [160/391],                 Loss: 0.36581, Train_Acc:87.33%
Epoch [78/300], Step [170/391],                 Loss: 0.36599, Train_Acc:87.29%
Epoch [78/300], Step [180/391],                 Loss: 0.36613, Train_Acc:87.25%
Epoch [78/300], Step [190/391],                 Loss: 0.36626, Train_Acc:87.27%
Epoch [78/300], Step [200/391],                 Loss: 0.36761, Train_Acc:87.20%
Epoch [78/300], Step [210/391],                 Loss: 0.36786, Train_Acc:87.19%
Epoch [78/300], Step [220/391],                 Loss: 0.36771, Train_Acc:87.21%
Epoch [78/300], Step [230/391],                 Loss: 0.36680, Train_Acc:87.23%
Epoch [78/300], Step [240/391],                 Loss: 0.36638, Train_Acc:87.27%
Epoch [78/300], Step [250/391],                 Loss: 0.36725, Train_Acc:87.23%
Epoch [78/300], Step [260/391],                 Loss: 0.36787, Train_Acc:87.25%
Epoch [78/300], Step [270/391],                 Loss: 0.36753, Train_Acc:87.27%
Epoch [78/300], Step [280/391],                 Loss: 0.36773, Train_Acc:87.30%
Epoch [78/300], Step [290/391],                 Loss: 0.36734, Train_Acc:87.32%
Epoch [78/300], Step [300/391],                 Loss: 0.36605, Train_Acc:87.34%
Epoch [78/300], Step [310/391],                 Loss: 0.36511, Train_Acc:87.38%
Epoch [78/300], Step [320/391],                 Loss: 0.36533, Train_Acc:87.39%
Epoch [78/300], Step [330/391],                 Loss: 0.36486, Train_Acc:87.41%
Epoch [78/300], Step [340/391],                 Loss: 0.36444, Train_Acc:87.41%
Epoch [78/300], Step [350/391],                 Loss: 0.36392, Train_Acc:87.46%
Epoch [78/300], Step [360/391],                 Loss: 0.36511, Train_Acc:87.42%
Epoch [78/300], Step [370/391],                 Loss: 0.36538, Train_Acc:87.43%
Epoch [78/300], Step [380/391],                 Loss: 0.36584, Train_Acc:87.43%
Epoch [78/300], Step [390/391],                 Loss: 0.36541, Train_Acc:87.45%
Accuary on test images:74.86%
Epoch [79/300], Step [10/391],                 Loss: 0.34261, Train_Acc:87.73%
Epoch [79/300], Step [20/391],                 Loss: 0.35124, Train_Acc:88.20%
Epoch [79/300], Step [30/391],                 Loss: 0.36276, Train_Acc:87.89%
Epoch [79/300], Step [40/391],                 Loss: 0.35929, Train_Acc:88.05%
Epoch [79/300], Step [50/391],                 Loss: 0.35353, Train_Acc:88.17%
Epoch [79/300], Step [60/391],                 Loss: 0.35498, Train_Acc:88.06%
Epoch [79/300], Step [70/391],                 Loss: 0.35687, Train_Acc:87.95%
Epoch [79/300], Step [80/391],                 Loss: 0.35509, Train_Acc:87.92%
Epoch [79/300], Step [90/391],                 Loss: 0.36077, Train_Acc:87.60%
Epoch [79/300], Step [100/391],                 Loss: 0.36295, Train_Acc:87.42%
Epoch [79/300], Step [110/391],                 Loss: 0.36664, Train_Acc:87.28%
Epoch [79/300], Step [120/391],                 Loss: 0.36831, Train_Acc:87.17%
Epoch [79/300], Step [130/391],                 Loss: 0.37186, Train_Acc:87.16%
Epoch [79/300], Step [140/391],                 Loss: 0.37238, Train_Acc:87.20%
Epoch [79/300], Step [150/391],                 Loss: 0.37241, Train_Acc:87.27%
Epoch [79/300], Step [160/391],                 Loss: 0.36937, Train_Acc:87.31%
Epoch [79/300], Step [170/391],                 Loss: 0.36857, Train_Acc:87.36%
Epoch [79/300], Step [180/391],                 Loss: 0.36812, Train_Acc:87.34%
Epoch [79/300], Step [190/391],                 Loss: 0.36932, Train_Acc:87.32%
Epoch [79/300], Step [200/391],                 Loss: 0.36919, Train_Acc:87.39%
Epoch [79/300], Step [210/391],                 Loss: 0.37005, Train_Acc:87.37%
Epoch [79/300], Step [220/391],                 Loss: 0.37020, Train_Acc:87.35%
Epoch [79/300], Step [230/391],                 Loss: 0.36821, Train_Acc:87.41%
Epoch [79/300], Step [240/391],                 Loss: 0.36581, Train_Acc:87.46%
Epoch [79/300], Step [250/391],                 Loss: 0.36597, Train_Acc:87.47%
Epoch [79/300], Step [260/391],                 Loss: 0.36798, Train_Acc:87.39%
Epoch [79/300], Step [270/391],                 Loss: 0.37018, Train_Acc:87.32%
Epoch [79/300], Step [280/391],                 Loss: 0.37096, Train_Acc:87.25%
Epoch [79/300], Step [290/391],                 Loss: 0.37141, Train_Acc:87.18%
Epoch [79/300], Step [300/391],                 Loss: 0.37148, Train_Acc:87.17%
Epoch [79/300], Step [310/391],                 Loss: 0.37032, Train_Acc:87.22%
Epoch [79/300], Step [320/391],                 Loss: 0.37016, Train_Acc:87.23%
Epoch [79/300], Step [330/391],                 Loss: 0.36901, Train_Acc:87.27%
Epoch [79/300], Step [340/391],                 Loss: 0.36795, Train_Acc:87.30%
Epoch [79/300], Step [350/391],                 Loss: 0.36733, Train_Acc:87.36%
Epoch [79/300], Step [360/391],                 Loss: 0.36710, Train_Acc:87.38%
Epoch [79/300], Step [370/391],                 Loss: 0.36682, Train_Acc:87.40%
Epoch [79/300], Step [380/391],                 Loss: 0.36587, Train_Acc:87.45%
Epoch [79/300], Step [390/391],                 Loss: 0.36429, Train_Acc:87.50%
Accuary on test images:77.24%
Epoch [80/300], Step [10/391],                 Loss: 0.33149, Train_Acc:87.97%
Epoch [80/300], Step [20/391],                 Loss: 0.33425, Train_Acc:88.12%
Epoch [80/300], Step [30/391],                 Loss: 0.33198, Train_Acc:88.75%
Epoch [80/300], Step [40/391],                 Loss: 0.33822, Train_Acc:88.65%
Epoch [80/300], Step [50/391],                 Loss: 0.34343, Train_Acc:88.53%
Epoch [80/300], Step [60/391],                 Loss: 0.35163, Train_Acc:88.11%
Epoch [80/300], Step [70/391],                 Loss: 0.35490, Train_Acc:87.90%
Epoch [80/300], Step [80/391],                 Loss: 0.35777, Train_Acc:87.79%
Epoch [80/300], Step [90/391],                 Loss: 0.35985, Train_Acc:87.80%
Epoch [80/300], Step [100/391],                 Loss: 0.36396, Train_Acc:87.58%
Epoch [80/300], Step [110/391],                 Loss: 0.36559, Train_Acc:87.48%
Epoch [80/300], Step [120/391],                 Loss: 0.36782, Train_Acc:87.34%
Epoch [80/300], Step [130/391],                 Loss: 0.37141, Train_Acc:87.30%
Epoch [80/300], Step [140/391],                 Loss: 0.36827, Train_Acc:87.41%
Epoch [80/300], Step [150/391],                 Loss: 0.36662, Train_Acc:87.49%
Epoch [80/300], Step [160/391],                 Loss: 0.36544, Train_Acc:87.52%
Epoch [80/300], Step [170/391],                 Loss: 0.36566, Train_Acc:87.51%
Epoch [80/300], Step [180/391],                 Loss: 0.36494, Train_Acc:87.47%
Epoch [80/300], Step [190/391],                 Loss: 0.36537, Train_Acc:87.41%
Epoch [80/300], Step [200/391],                 Loss: 0.36611, Train_Acc:87.39%
Epoch [80/300], Step [210/391],                 Loss: 0.36581, Train_Acc:87.40%
Epoch [80/300], Step [220/391],                 Loss: 0.36483, Train_Acc:87.47%
Epoch [80/300], Step [230/391],                 Loss: 0.36377, Train_Acc:87.51%
Epoch [80/300], Step [240/391],                 Loss: 0.36172, Train_Acc:87.58%
Epoch [80/300], Step [250/391],                 Loss: 0.36096, Train_Acc:87.60%
Epoch [80/300], Step [260/391],                 Loss: 0.36299, Train_Acc:87.52%
Epoch [80/300], Step [270/391],                 Loss: 0.36560, Train_Acc:87.41%
Epoch [80/300], Step [280/391],                 Loss: 0.36751, Train_Acc:87.35%
Epoch [80/300], Step [290/391],                 Loss: 0.36749, Train_Acc:87.34%
Epoch [80/300], Step [300/391],                 Loss: 0.36792, Train_Acc:87.32%
Epoch [80/300], Step [310/391],                 Loss: 0.36790, Train_Acc:87.31%
Epoch [80/300], Step [320/391],                 Loss: 0.36813, Train_Acc:87.29%
Epoch [80/300], Step [330/391],                 Loss: 0.36669, Train_Acc:87.36%
Epoch [80/300], Step [340/391],                 Loss: 0.36500, Train_Acc:87.41%
Epoch [80/300], Step [350/391],                 Loss: 0.36410, Train_Acc:87.46%
Epoch [80/300], Step [360/391],                 Loss: 0.36364, Train_Acc:87.49%
Epoch [80/300], Step [370/391],                 Loss: 0.36400, Train_Acc:87.48%
Epoch [80/300], Step [380/391],                 Loss: 0.36432, Train_Acc:87.44%
Epoch [80/300], Step [390/391],                 Loss: 0.36429, Train_Acc:87.43%
Accuary on test images:80.24%
Epoch [81/300], Step [10/391],                 Loss: 0.35861, Train_Acc:88.20%
Epoch [81/300], Step [20/391],                 Loss: 0.34871, Train_Acc:88.05%
Epoch [81/300], Step [30/391],                 Loss: 0.33942, Train_Acc:88.28%
Epoch [81/300], Step [40/391],                 Loss: 0.33945, Train_Acc:88.50%
Epoch [81/300], Step [50/391],                 Loss: 0.34315, Train_Acc:88.31%
Epoch [81/300], Step [60/391],                 Loss: 0.35425, Train_Acc:87.81%
Epoch [81/300], Step [70/391],                 Loss: 0.35813, Train_Acc:87.89%
Epoch [81/300], Step [80/391],                 Loss: 0.35691, Train_Acc:87.94%
Epoch [81/300], Step [90/391],                 Loss: 0.35873, Train_Acc:87.81%
Epoch [81/300], Step [100/391],                 Loss: 0.35484, Train_Acc:87.84%
Epoch [81/300], Step [110/391],                 Loss: 0.35595, Train_Acc:87.83%
Epoch [81/300], Step [120/391],                 Loss: 0.35943, Train_Acc:87.70%
Epoch [81/300], Step [130/391],                 Loss: 0.36103, Train_Acc:87.64%
Epoch [81/300], Step [140/391],                 Loss: 0.36067, Train_Acc:87.67%
Epoch [81/300], Step [150/391],                 Loss: 0.36226, Train_Acc:87.64%
Epoch [81/300], Step [160/391],                 Loss: 0.36128, Train_Acc:87.66%
Epoch [81/300], Step [170/391],                 Loss: 0.36056, Train_Acc:87.71%
Epoch [81/300], Step [180/391],                 Loss: 0.36071, Train_Acc:87.73%
Epoch [81/300], Step [190/391],                 Loss: 0.36200, Train_Acc:87.69%
Epoch [81/300], Step [200/391],                 Loss: 0.36399, Train_Acc:87.61%
Epoch [81/300], Step [210/391],                 Loss: 0.36480, Train_Acc:87.60%
Epoch [81/300], Step [220/391],                 Loss: 0.36373, Train_Acc:87.66%
Epoch [81/300], Step [230/391],                 Loss: 0.36103, Train_Acc:87.75%
Epoch [81/300], Step [240/391],                 Loss: 0.36029, Train_Acc:87.74%
Epoch [81/300], Step [250/391],                 Loss: 0.35981, Train_Acc:87.78%
Epoch [81/300], Step [260/391],                 Loss: 0.36276, Train_Acc:87.69%
Epoch [81/300], Step [270/391],                 Loss: 0.36343, Train_Acc:87.66%
Epoch [81/300], Step [280/391],                 Loss: 0.36373, Train_Acc:87.64%
Epoch [81/300], Step [290/391],                 Loss: 0.36530, Train_Acc:87.59%
Epoch [81/300], Step [300/391],                 Loss: 0.36516, Train_Acc:87.60%
Epoch [81/300], Step [310/391],                 Loss: 0.36489, Train_Acc:87.60%
Epoch [81/300], Step [320/391],                 Loss: 0.36475, Train_Acc:87.61%
Epoch [81/300], Step [330/391],                 Loss: 0.36385, Train_Acc:87.67%
Epoch [81/300], Step [340/391],                 Loss: 0.36292, Train_Acc:87.71%
Epoch [81/300], Step [350/391],                 Loss: 0.36158, Train_Acc:87.76%
Epoch [81/300], Step [360/391],                 Loss: 0.36133, Train_Acc:87.74%
Epoch [81/300], Step [370/391],                 Loss: 0.36241, Train_Acc:87.68%
Epoch [81/300], Step [380/391],                 Loss: 0.36317, Train_Acc:87.64%
Epoch [81/300], Step [390/391],                 Loss: 0.36283, Train_Acc:87.66%
Accuary on test images:69.38%
Epoch [82/300], Step [10/391],                 Loss: 0.35826, Train_Acc:87.81%
Epoch [82/300], Step [20/391],                 Loss: 0.34991, Train_Acc:88.44%
Epoch [82/300], Step [30/391],                 Loss: 0.33759, Train_Acc:88.75%
Epoch [82/300], Step [40/391],                 Loss: 0.34371, Train_Acc:88.59%
Epoch [82/300], Step [50/391],                 Loss: 0.34377, Train_Acc:88.58%
Epoch [82/300], Step [60/391],                 Loss: 0.35237, Train_Acc:88.19%
Epoch [82/300], Step [70/391],                 Loss: 0.35822, Train_Acc:87.95%
Epoch [82/300], Step [80/391],                 Loss: 0.36262, Train_Acc:87.77%
Epoch [82/300], Step [90/391],                 Loss: 0.36677, Train_Acc:87.63%
Epoch [82/300], Step [100/391],                 Loss: 0.36499, Train_Acc:87.70%
Epoch [82/300], Step [110/391],                 Loss: 0.36778, Train_Acc:87.56%
Epoch [82/300], Step [120/391],                 Loss: 0.36692, Train_Acc:87.64%
Epoch [82/300], Step [130/391],                 Loss: 0.36830, Train_Acc:87.63%
Epoch [82/300], Step [140/391],                 Loss: 0.36609, Train_Acc:87.67%
Epoch [82/300], Step [150/391],                 Loss: 0.36736, Train_Acc:87.64%
Epoch [82/300], Step [160/391],                 Loss: 0.36650, Train_Acc:87.67%
Epoch [82/300], Step [170/391],                 Loss: 0.36825, Train_Acc:87.56%
Epoch [82/300], Step [180/391],                 Loss: 0.36800, Train_Acc:87.56%
Epoch [82/300], Step [190/391],                 Loss: 0.36861, Train_Acc:87.47%
Epoch [82/300], Step [200/391],                 Loss: 0.37004, Train_Acc:87.42%
Epoch [82/300], Step [210/391],                 Loss: 0.36734, Train_Acc:87.53%
Epoch [82/300], Step [220/391],                 Loss: 0.36608, Train_Acc:87.60%
Epoch [82/300], Step [230/391],                 Loss: 0.36551, Train_Acc:87.61%
Epoch [82/300], Step [240/391],                 Loss: 0.36363, Train_Acc:87.65%
Epoch [82/300], Step [250/391],                 Loss: 0.36432, Train_Acc:87.60%
Epoch [82/300], Step [260/391],                 Loss: 0.36728, Train_Acc:87.52%
Epoch [82/300], Step [270/391],                 Loss: 0.36865, Train_Acc:87.50%
Epoch [82/300], Step [280/391],                 Loss: 0.36812, Train_Acc:87.51%
Epoch [82/300], Step [290/391],                 Loss: 0.36849, Train_Acc:87.50%
Epoch [82/300], Step [300/391],                 Loss: 0.36824, Train_Acc:87.49%
Epoch [82/300], Step [310/391],                 Loss: 0.36679, Train_Acc:87.56%
Epoch [82/300], Step [320/391],                 Loss: 0.36604, Train_Acc:87.59%
Epoch [82/300], Step [330/391],                 Loss: 0.36594, Train_Acc:87.58%
Epoch [82/300], Step [340/391],                 Loss: 0.36489, Train_Acc:87.62%
Epoch [82/300], Step [350/391],                 Loss: 0.36429, Train_Acc:87.64%
Epoch [82/300], Step [360/391],                 Loss: 0.36415, Train_Acc:87.65%
Epoch [82/300], Step [370/391],                 Loss: 0.36385, Train_Acc:87.64%
Epoch [82/300], Step [380/391],                 Loss: 0.36260, Train_Acc:87.68%
Epoch [82/300], Step [390/391],                 Loss: 0.36175, Train_Acc:87.72%
Accuary on test images:76.68%
Epoch [83/300], Step [10/391],                 Loss: 0.33946, Train_Acc:88.98%
Epoch [83/300], Step [20/391],                 Loss: 0.35629, Train_Acc:88.20%
Epoch [83/300], Step [30/391],                 Loss: 0.35722, Train_Acc:88.31%
Epoch [83/300], Step [40/391],                 Loss: 0.35594, Train_Acc:88.46%
Epoch [83/300], Step [50/391],                 Loss: 0.36249, Train_Acc:88.17%
Epoch [83/300], Step [60/391],                 Loss: 0.36602, Train_Acc:88.05%
Epoch [83/300], Step [70/391],                 Loss: 0.36390, Train_Acc:88.06%
Epoch [83/300], Step [80/391],                 Loss: 0.36366, Train_Acc:87.99%
Epoch [83/300], Step [90/391],                 Loss: 0.36789, Train_Acc:87.78%
Epoch [83/300], Step [100/391],                 Loss: 0.36502, Train_Acc:87.84%
Epoch [83/300], Step [110/391],                 Loss: 0.36251, Train_Acc:88.00%
Epoch [83/300], Step [120/391],                 Loss: 0.36066, Train_Acc:88.03%
Epoch [83/300], Step [130/391],                 Loss: 0.36189, Train_Acc:88.06%
Epoch [83/300], Step [140/391],                 Loss: 0.36057, Train_Acc:88.06%
Epoch [83/300], Step [150/391],                 Loss: 0.36201, Train_Acc:87.98%
Epoch [83/300], Step [160/391],                 Loss: 0.36008, Train_Acc:88.06%
Epoch [83/300], Step [170/391],                 Loss: 0.35869, Train_Acc:88.07%
Epoch [83/300], Step [180/391],                 Loss: 0.35996, Train_Acc:88.03%
Epoch [83/300], Step [190/391],                 Loss: 0.36099, Train_Acc:87.94%
Epoch [83/300], Step [200/391],                 Loss: 0.36186, Train_Acc:87.90%
Epoch [83/300], Step [210/391],                 Loss: 0.36169, Train_Acc:87.91%
Epoch [83/300], Step [220/391],                 Loss: 0.36206, Train_Acc:87.84%
Epoch [83/300], Step [230/391],                 Loss: 0.36174, Train_Acc:87.83%
Epoch [83/300], Step [240/391],                 Loss: 0.36158, Train_Acc:87.86%
Epoch [83/300], Step [250/391],                 Loss: 0.36238, Train_Acc:87.81%
Epoch [83/300], Step [260/391],                 Loss: 0.36488, Train_Acc:87.73%
Epoch [83/300], Step [270/391],                 Loss: 0.36622, Train_Acc:87.65%
Epoch [83/300], Step [280/391],                 Loss: 0.36577, Train_Acc:87.65%
Epoch [83/300], Step [290/391],                 Loss: 0.36499, Train_Acc:87.65%
Epoch [83/300], Step [300/391],                 Loss: 0.36435, Train_Acc:87.66%
Epoch [83/300], Step [310/391],                 Loss: 0.36420, Train_Acc:87.67%
Epoch [83/300], Step [320/391],                 Loss: 0.36458, Train_Acc:87.64%
Epoch [83/300], Step [330/391],                 Loss: 0.36344, Train_Acc:87.67%
Epoch [83/300], Step [340/391],                 Loss: 0.36321, Train_Acc:87.68%
Epoch [83/300], Step [350/391],                 Loss: 0.36308, Train_Acc:87.69%
Epoch [83/300], Step [360/391],                 Loss: 0.36324, Train_Acc:87.66%
Epoch [83/300], Step [370/391],                 Loss: 0.36399, Train_Acc:87.62%
Epoch [83/300], Step [380/391],                 Loss: 0.36400, Train_Acc:87.60%
Epoch [83/300], Step [390/391],                 Loss: 0.36351, Train_Acc:87.62%
Accuary on test images:74.68%
Epoch [84/300], Step [10/391],                 Loss: 0.38028, Train_Acc:86.88%
Epoch [84/300], Step [20/391],                 Loss: 0.37390, Train_Acc:87.03%
Epoch [84/300], Step [30/391],                 Loss: 0.36532, Train_Acc:87.27%
Epoch [84/300], Step [40/391],                 Loss: 0.35904, Train_Acc:87.60%
Epoch [84/300], Step [50/391],                 Loss: 0.35537, Train_Acc:87.84%
Epoch [84/300], Step [60/391],                 Loss: 0.35970, Train_Acc:87.54%
Epoch [84/300], Step [70/391],                 Loss: 0.35350, Train_Acc:87.80%
Epoch [84/300], Step [80/391],                 Loss: 0.35533, Train_Acc:87.73%
Epoch [84/300], Step [90/391],                 Loss: 0.35827, Train_Acc:87.55%
Epoch [84/300], Step [100/391],                 Loss: 0.36020, Train_Acc:87.52%
Epoch [84/300], Step [110/391],                 Loss: 0.36235, Train_Acc:87.47%
Epoch [84/300], Step [120/391],                 Loss: 0.36335, Train_Acc:87.45%
Epoch [84/300], Step [130/391],                 Loss: 0.36543, Train_Acc:87.42%
Epoch [84/300], Step [140/391],                 Loss: 0.36302, Train_Acc:87.47%
Epoch [84/300], Step [150/391],                 Loss: 0.36376, Train_Acc:87.45%
Epoch [84/300], Step [160/391],                 Loss: 0.36144, Train_Acc:87.52%
Epoch [84/300], Step [170/391],                 Loss: 0.36155, Train_Acc:87.54%
Epoch [84/300], Step [180/391],                 Loss: 0.36106, Train_Acc:87.50%
Epoch [84/300], Step [190/391],                 Loss: 0.36066, Train_Acc:87.52%
Epoch [84/300], Step [200/391],                 Loss: 0.36086, Train_Acc:87.47%
Epoch [84/300], Step [210/391],                 Loss: 0.36169, Train_Acc:87.46%
Epoch [84/300], Step [220/391],                 Loss: 0.36207, Train_Acc:87.46%
Epoch [84/300], Step [230/391],                 Loss: 0.36101, Train_Acc:87.53%
Epoch [84/300], Step [240/391],                 Loss: 0.35975, Train_Acc:87.59%
Epoch [84/300], Step [250/391],                 Loss: 0.36057, Train_Acc:87.57%
Epoch [84/300], Step [260/391],                 Loss: 0.36257, Train_Acc:87.52%
Epoch [84/300], Step [270/391],                 Loss: 0.36485, Train_Acc:87.43%
Epoch [84/300], Step [280/391],                 Loss: 0.36549, Train_Acc:87.41%
Epoch [84/300], Step [290/391],                 Loss: 0.36574, Train_Acc:87.41%
Epoch [84/300], Step [300/391],                 Loss: 0.36554, Train_Acc:87.42%
Epoch [84/300], Step [310/391],                 Loss: 0.36412, Train_Acc:87.47%
Epoch [84/300], Step [320/391],                 Loss: 0.36449, Train_Acc:87.47%
Epoch [84/300], Step [330/391],                 Loss: 0.36334, Train_Acc:87.52%
Epoch [84/300], Step [340/391],                 Loss: 0.36260, Train_Acc:87.55%
Epoch [84/300], Step [350/391],                 Loss: 0.36147, Train_Acc:87.58%
Epoch [84/300], Step [360/391],                 Loss: 0.36114, Train_Acc:87.58%
Epoch [84/300], Step [370/391],                 Loss: 0.36138, Train_Acc:87.57%
Epoch [84/300], Step [380/391],                 Loss: 0.36099, Train_Acc:87.57%
Epoch [84/300], Step [390/391],                 Loss: 0.36092, Train_Acc:87.58%
Accuary on test images:77.14%
Epoch [85/300], Step [10/391],                 Loss: 0.35204, Train_Acc:88.05%
Epoch [85/300], Step [20/391],                 Loss: 0.35308, Train_Acc:87.66%
Epoch [85/300], Step [30/391],                 Loss: 0.35607, Train_Acc:87.66%
Epoch [85/300], Step [40/391],                 Loss: 0.35521, Train_Acc:87.75%
Epoch [85/300], Step [50/391],                 Loss: 0.35881, Train_Acc:87.66%
Epoch [85/300], Step [60/391],                 Loss: 0.36687, Train_Acc:87.36%
Epoch [85/300], Step [70/391],                 Loss: 0.36302, Train_Acc:87.59%
Epoch [85/300], Step [80/391],                 Loss: 0.35879, Train_Acc:87.81%
Epoch [85/300], Step [90/391],                 Loss: 0.36503, Train_Acc:87.51%
Epoch [85/300], Step [100/391],                 Loss: 0.36233, Train_Acc:87.65%
Epoch [85/300], Step [110/391],                 Loss: 0.36361, Train_Acc:87.66%
Epoch [85/300], Step [120/391],                 Loss: 0.36418, Train_Acc:87.57%
Epoch [85/300], Step [130/391],                 Loss: 0.36673, Train_Acc:87.49%
Epoch [85/300], Step [140/391],                 Loss: 0.36449, Train_Acc:87.58%
Epoch [85/300], Step [150/391],                 Loss: 0.36686, Train_Acc:87.55%
Epoch [85/300], Step [160/391],                 Loss: 0.36500, Train_Acc:87.63%
Epoch [85/300], Step [170/391],                 Loss: 0.36603, Train_Acc:87.56%
Epoch [85/300], Step [180/391],                 Loss: 0.36773, Train_Acc:87.50%
Epoch [85/300], Step [190/391],                 Loss: 0.36689, Train_Acc:87.48%
Epoch [85/300], Step [200/391],                 Loss: 0.36560, Train_Acc:87.57%
Epoch [85/300], Step [210/391],                 Loss: 0.36548, Train_Acc:87.59%
Epoch [85/300], Step [220/391],                 Loss: 0.36385, Train_Acc:87.68%
Epoch [85/300], Step [230/391],                 Loss: 0.36322, Train_Acc:87.71%
Epoch [85/300], Step [240/391],                 Loss: 0.36120, Train_Acc:87.80%
Epoch [85/300], Step [250/391],                 Loss: 0.35953, Train_Acc:87.83%
Epoch [85/300], Step [260/391],                 Loss: 0.36069, Train_Acc:87.80%
Epoch [85/300], Step [270/391],                 Loss: 0.36084, Train_Acc:87.81%
Epoch [85/300], Step [280/391],                 Loss: 0.36024, Train_Acc:87.80%
Epoch [85/300], Step [290/391],                 Loss: 0.36027, Train_Acc:87.82%
Epoch [85/300], Step [300/391],                 Loss: 0.36027, Train_Acc:87.80%
Epoch [85/300], Step [310/391],                 Loss: 0.35970, Train_Acc:87.82%
Epoch [85/300], Step [320/391],                 Loss: 0.36021, Train_Acc:87.78%
Epoch [85/300], Step [330/391],                 Loss: 0.36161, Train_Acc:87.74%
Epoch [85/300], Step [340/391],                 Loss: 0.36252, Train_Acc:87.71%
Epoch [85/300], Step [350/391],                 Loss: 0.36343, Train_Acc:87.69%
Epoch [85/300], Step [360/391],                 Loss: 0.36411, Train_Acc:87.66%
Epoch [85/300], Step [370/391],                 Loss: 0.36422, Train_Acc:87.65%
Epoch [85/300], Step [380/391],                 Loss: 0.36394, Train_Acc:87.66%
Epoch [85/300], Step [390/391],                 Loss: 0.36412, Train_Acc:87.66%
Accuary on test images:75.30%
Epoch [86/300], Step [10/391],                 Loss: 0.35778, Train_Acc:88.20%
Epoch [86/300], Step [20/391],                 Loss: 0.34764, Train_Acc:88.16%
Epoch [86/300], Step [30/391],                 Loss: 0.34604, Train_Acc:88.18%
Epoch [86/300], Step [40/391],                 Loss: 0.34841, Train_Acc:88.24%
Epoch [86/300], Step [50/391],                 Loss: 0.34895, Train_Acc:88.17%
Epoch [86/300], Step [60/391],                 Loss: 0.35705, Train_Acc:87.84%
Epoch [86/300], Step [70/391],                 Loss: 0.35865, Train_Acc:87.79%
Epoch [86/300], Step [80/391],                 Loss: 0.36165, Train_Acc:87.66%
Epoch [86/300], Step [90/391],                 Loss: 0.36773, Train_Acc:87.48%
Epoch [86/300], Step [100/391],                 Loss: 0.36734, Train_Acc:87.53%
Epoch [86/300], Step [110/391],                 Loss: 0.37218, Train_Acc:87.39%
Epoch [86/300], Step [120/391],                 Loss: 0.37264, Train_Acc:87.42%
Epoch [86/300], Step [130/391],                 Loss: 0.37327, Train_Acc:87.37%
Epoch [86/300], Step [140/391],                 Loss: 0.36970, Train_Acc:87.50%
Epoch [86/300], Step [150/391],                 Loss: 0.36821, Train_Acc:87.57%
Epoch [86/300], Step [160/391],                 Loss: 0.36746, Train_Acc:87.57%
Epoch [86/300], Step [170/391],                 Loss: 0.36842, Train_Acc:87.49%
Epoch [86/300], Step [180/391],                 Loss: 0.36878, Train_Acc:87.44%
Epoch [86/300], Step [190/391],                 Loss: 0.36761, Train_Acc:87.45%
Epoch [86/300], Step [200/391],                 Loss: 0.36740, Train_Acc:87.48%
Epoch [86/300], Step [210/391],                 Loss: 0.36702, Train_Acc:87.46%
Epoch [86/300], Step [220/391],                 Loss: 0.36655, Train_Acc:87.45%
Epoch [86/300], Step [230/391],                 Loss: 0.36612, Train_Acc:87.43%
Epoch [86/300], Step [240/391],                 Loss: 0.36463, Train_Acc:87.47%
Epoch [86/300], Step [250/391],                 Loss: 0.36557, Train_Acc:87.38%
Epoch [86/300], Step [260/391],                 Loss: 0.36744, Train_Acc:87.32%
Epoch [86/300], Step [270/391],                 Loss: 0.36834, Train_Acc:87.32%
Epoch [86/300], Step [280/391],                 Loss: 0.37082, Train_Acc:87.22%
Epoch [86/300], Step [290/391],                 Loss: 0.37172, Train_Acc:87.19%
Epoch [86/300], Step [300/391],                 Loss: 0.37146, Train_Acc:87.22%
Epoch [86/300], Step [310/391],                 Loss: 0.37043, Train_Acc:87.24%
Epoch [86/300], Step [320/391],                 Loss: 0.37004, Train_Acc:87.26%
Epoch [86/300], Step [330/391],                 Loss: 0.36974, Train_Acc:87.31%
Epoch [86/300], Step [340/391],                 Loss: 0.36887, Train_Acc:87.34%
Epoch [86/300], Step [350/391],                 Loss: 0.36772, Train_Acc:87.39%
Epoch [86/300], Step [360/391],                 Loss: 0.36738, Train_Acc:87.40%
Epoch [86/300], Step [370/391],                 Loss: 0.36761, Train_Acc:87.39%
Epoch [86/300], Step [380/391],                 Loss: 0.36718, Train_Acc:87.41%
Epoch [86/300], Step [390/391],                 Loss: 0.36688, Train_Acc:87.41%
Accuary on test images:77.48%
Epoch [87/300], Step [10/391],                 Loss: 0.34852, Train_Acc:86.64%
Epoch [87/300], Step [20/391],                 Loss: 0.36010, Train_Acc:86.95%
Epoch [87/300], Step [30/391],                 Loss: 0.35713, Train_Acc:87.34%
Epoch [87/300], Step [40/391],                 Loss: 0.35620, Train_Acc:87.38%
Epoch [87/300], Step [50/391],                 Loss: 0.35068, Train_Acc:87.56%
Epoch [87/300], Step [60/391],                 Loss: 0.35588, Train_Acc:87.40%
Epoch [87/300], Step [70/391],                 Loss: 0.35302, Train_Acc:87.56%
Epoch [87/300], Step [80/391],                 Loss: 0.35506, Train_Acc:87.47%
Epoch [87/300], Step [90/391],                 Loss: 0.35525, Train_Acc:87.40%
Epoch [87/300], Step [100/391],                 Loss: 0.35589, Train_Acc:87.41%
Epoch [87/300], Step [110/391],                 Loss: 0.35861, Train_Acc:87.40%
Epoch [87/300], Step [120/391],                 Loss: 0.35713, Train_Acc:87.45%
Epoch [87/300], Step [130/391],                 Loss: 0.35888, Train_Acc:87.45%
Epoch [87/300], Step [140/391],                 Loss: 0.35694, Train_Acc:87.53%
Epoch [87/300], Step [150/391],                 Loss: 0.35820, Train_Acc:87.54%
Epoch [87/300], Step [160/391],                 Loss: 0.35517, Train_Acc:87.64%
Epoch [87/300], Step [170/391],                 Loss: 0.35521, Train_Acc:87.64%
Epoch [87/300], Step [180/391],                 Loss: 0.35596, Train_Acc:87.61%
Epoch [87/300], Step [190/391],                 Loss: 0.35812, Train_Acc:87.52%
Epoch [87/300], Step [200/391],                 Loss: 0.35741, Train_Acc:87.55%
Epoch [87/300], Step [210/391],                 Loss: 0.35780, Train_Acc:87.56%
Epoch [87/300], Step [220/391],                 Loss: 0.35685, Train_Acc:87.60%
Epoch [87/300], Step [230/391],                 Loss: 0.35506, Train_Acc:87.67%
Epoch [87/300], Step [240/391],                 Loss: 0.35361, Train_Acc:87.69%
Epoch [87/300], Step [250/391],                 Loss: 0.35310, Train_Acc:87.73%
Epoch [87/300], Step [260/391],                 Loss: 0.35527, Train_Acc:87.67%
Epoch [87/300], Step [270/391],                 Loss: 0.35537, Train_Acc:87.71%
Epoch [87/300], Step [280/391],                 Loss: 0.35477, Train_Acc:87.71%
Epoch [87/300], Step [290/391],                 Loss: 0.35421, Train_Acc:87.74%
Epoch [87/300], Step [300/391],                 Loss: 0.35296, Train_Acc:87.76%
Epoch [87/300], Step [310/391],                 Loss: 0.35221, Train_Acc:87.78%
Epoch [87/300], Step [320/391],                 Loss: 0.35216, Train_Acc:87.78%
Epoch [87/300], Step [330/391],                 Loss: 0.35168, Train_Acc:87.82%
Epoch [87/300], Step [340/391],                 Loss: 0.35114, Train_Acc:87.84%
Epoch [87/300], Step [350/391],                 Loss: 0.35178, Train_Acc:87.81%
Epoch [87/300], Step [360/391],                 Loss: 0.35264, Train_Acc:87.77%
Epoch [87/300], Step [370/391],                 Loss: 0.35276, Train_Acc:87.74%
Epoch [87/300], Step [380/391],                 Loss: 0.35260, Train_Acc:87.75%
Epoch [87/300], Step [390/391],                 Loss: 0.35159, Train_Acc:87.80%
Accuary on test images:69.58%
Epoch [88/300], Step [10/391],                 Loss: 0.36722, Train_Acc:87.27%
Epoch [88/300], Step [20/391],                 Loss: 0.35941, Train_Acc:88.12%
Epoch [88/300], Step [30/391],                 Loss: 0.35347, Train_Acc:88.12%
Epoch [88/300], Step [40/391],                 Loss: 0.35121, Train_Acc:88.09%
Epoch [88/300], Step [50/391],                 Loss: 0.34616, Train_Acc:88.11%
Epoch [88/300], Step [60/391],                 Loss: 0.35235, Train_Acc:87.84%
Epoch [88/300], Step [70/391],                 Loss: 0.35186, Train_Acc:87.86%
Epoch [88/300], Step [80/391],                 Loss: 0.35144, Train_Acc:87.85%
Epoch [88/300], Step [90/391],                 Loss: 0.35494, Train_Acc:87.68%
Epoch [88/300], Step [100/391],                 Loss: 0.35337, Train_Acc:87.72%
Epoch [88/300], Step [110/391],                 Loss: 0.35737, Train_Acc:87.66%
Epoch [88/300], Step [120/391],                 Loss: 0.36046, Train_Acc:87.52%
Epoch [88/300], Step [130/391],                 Loss: 0.36207, Train_Acc:87.50%
Epoch [88/300], Step [140/391],                 Loss: 0.36096, Train_Acc:87.57%
Epoch [88/300], Step [150/391],                 Loss: 0.36257, Train_Acc:87.53%
Epoch [88/300], Step [160/391],                 Loss: 0.36206, Train_Acc:87.54%
Epoch [88/300], Step [170/391],                 Loss: 0.36284, Train_Acc:87.60%
Epoch [88/300], Step [180/391],                 Loss: 0.36279, Train_Acc:87.62%
Epoch [88/300], Step [190/391],                 Loss: 0.36257, Train_Acc:87.60%
Epoch [88/300], Step [200/391],                 Loss: 0.36406, Train_Acc:87.55%
Epoch [88/300], Step [210/391],                 Loss: 0.36443, Train_Acc:87.57%
Epoch [88/300], Step [220/391],                 Loss: 0.36421, Train_Acc:87.62%
Epoch [88/300], Step [230/391],                 Loss: 0.36235, Train_Acc:87.66%
Epoch [88/300], Step [240/391],                 Loss: 0.35916, Train_Acc:87.80%
Epoch [88/300], Step [250/391],                 Loss: 0.35860, Train_Acc:87.82%
Epoch [88/300], Step [260/391],                 Loss: 0.35947, Train_Acc:87.81%
Epoch [88/300], Step [270/391],                 Loss: 0.35924, Train_Acc:87.80%
Epoch [88/300], Step [280/391],                 Loss: 0.35905, Train_Acc:87.79%
Epoch [88/300], Step [290/391],                 Loss: 0.35882, Train_Acc:87.78%
Epoch [88/300], Step [300/391],                 Loss: 0.35852, Train_Acc:87.79%
Epoch [88/300], Step [310/391],                 Loss: 0.35790, Train_Acc:87.81%
Epoch [88/300], Step [320/391],                 Loss: 0.35871, Train_Acc:87.78%
Epoch [88/300], Step [330/391],                 Loss: 0.35836, Train_Acc:87.79%
Epoch [88/300], Step [340/391],                 Loss: 0.35743, Train_Acc:87.81%
Epoch [88/300], Step [350/391],                 Loss: 0.35732, Train_Acc:87.82%
Epoch [88/300], Step [360/391],                 Loss: 0.35738, Train_Acc:87.80%
Epoch [88/300], Step [370/391],                 Loss: 0.35728, Train_Acc:87.79%
Epoch [88/300], Step [380/391],                 Loss: 0.35657, Train_Acc:87.81%
Epoch [88/300], Step [390/391],                 Loss: 0.35609, Train_Acc:87.84%
Accuary on test images:75.94%
Epoch [89/300], Step [10/391],                 Loss: 0.36249, Train_Acc:87.11%
Epoch [89/300], Step [20/391],                 Loss: 0.37122, Train_Acc:86.99%
Epoch [89/300], Step [30/391],                 Loss: 0.36027, Train_Acc:87.45%
Epoch [89/300], Step [40/391],                 Loss: 0.35556, Train_Acc:87.75%
Epoch [89/300], Step [50/391],                 Loss: 0.35934, Train_Acc:87.83%
Epoch [89/300], Step [60/391],                 Loss: 0.36236, Train_Acc:87.66%
Epoch [89/300], Step [70/391],                 Loss: 0.35786, Train_Acc:88.00%
Epoch [89/300], Step [80/391],                 Loss: 0.35626, Train_Acc:88.02%
Epoch [89/300], Step [90/391],                 Loss: 0.36013, Train_Acc:87.90%
Epoch [89/300], Step [100/391],                 Loss: 0.36249, Train_Acc:87.69%
Epoch [89/300], Step [110/391],                 Loss: 0.36295, Train_Acc:87.68%
Epoch [89/300], Step [120/391],                 Loss: 0.36312, Train_Acc:87.60%
Epoch [89/300], Step [130/391],                 Loss: 0.36687, Train_Acc:87.51%
Epoch [89/300], Step [140/391],                 Loss: 0.36545, Train_Acc:87.52%
Epoch [89/300], Step [150/391],                 Loss: 0.36681, Train_Acc:87.47%
Epoch [89/300], Step [160/391],                 Loss: 0.36661, Train_Acc:87.47%
Epoch [89/300], Step [170/391],                 Loss: 0.36781, Train_Acc:87.42%
Epoch [89/300], Step [180/391],                 Loss: 0.36908, Train_Acc:87.37%
Epoch [89/300], Step [190/391],                 Loss: 0.36823, Train_Acc:87.38%
Epoch [89/300], Step [200/391],                 Loss: 0.36732, Train_Acc:87.39%
Epoch [89/300], Step [210/391],                 Loss: 0.36545, Train_Acc:87.41%
Epoch [89/300], Step [220/391],                 Loss: 0.36403, Train_Acc:87.50%
Epoch [89/300], Step [230/391],                 Loss: 0.36214, Train_Acc:87.57%
Epoch [89/300], Step [240/391],                 Loss: 0.36018, Train_Acc:87.65%
Epoch [89/300], Step [250/391],                 Loss: 0.35922, Train_Acc:87.67%
Epoch [89/300], Step [260/391],                 Loss: 0.36096, Train_Acc:87.62%
Epoch [89/300], Step [270/391],                 Loss: 0.36281, Train_Acc:87.53%
Epoch [89/300], Step [280/391],                 Loss: 0.36332, Train_Acc:87.50%
Epoch [89/300], Step [290/391],                 Loss: 0.36299, Train_Acc:87.55%
Epoch [89/300], Step [300/391],                 Loss: 0.36307, Train_Acc:87.53%
Epoch [89/300], Step [310/391],                 Loss: 0.36215, Train_Acc:87.56%
Epoch [89/300], Step [320/391],                 Loss: 0.36206, Train_Acc:87.53%
Epoch [89/300], Step [330/391],                 Loss: 0.36150, Train_Acc:87.56%
Epoch [89/300], Step [340/391],                 Loss: 0.36050, Train_Acc:87.60%
Epoch [89/300], Step [350/391],                 Loss: 0.35896, Train_Acc:87.66%
Epoch [89/300], Step [360/391],                 Loss: 0.35905, Train_Acc:87.65%
Epoch [89/300], Step [370/391],                 Loss: 0.35959, Train_Acc:87.64%
Epoch [89/300], Step [380/391],                 Loss: 0.36054, Train_Acc:87.61%
Epoch [89/300], Step [390/391],                 Loss: 0.35974, Train_Acc:87.64%
Accuary on test images:76.78%
Epoch [90/300], Step [10/391],                 Loss: 0.33380, Train_Acc:88.36%
Epoch [90/300], Step [20/391],                 Loss: 0.34560, Train_Acc:88.12%
Epoch [90/300], Step [30/391],                 Loss: 0.33703, Train_Acc:88.41%
Epoch [90/300], Step [40/391],                 Loss: 0.33937, Train_Acc:88.48%
Epoch [90/300], Step [50/391],                 Loss: 0.34654, Train_Acc:88.25%
Epoch [90/300], Step [60/391],                 Loss: 0.35191, Train_Acc:88.01%
Epoch [90/300], Step [70/391],                 Loss: 0.35572, Train_Acc:87.90%
Epoch [90/300], Step [80/391],                 Loss: 0.35677, Train_Acc:87.90%
Epoch [90/300], Step [90/391],                 Loss: 0.35909, Train_Acc:87.73%
Epoch [90/300], Step [100/391],                 Loss: 0.35689, Train_Acc:87.78%
Epoch [90/300], Step [110/391],                 Loss: 0.35901, Train_Acc:87.76%
Epoch [90/300], Step [120/391],                 Loss: 0.36032, Train_Acc:87.71%
Epoch [90/300], Step [130/391],                 Loss: 0.36407, Train_Acc:87.61%
Epoch [90/300], Step [140/391],                 Loss: 0.36315, Train_Acc:87.63%
Epoch [90/300], Step [150/391],                 Loss: 0.36334, Train_Acc:87.61%
Epoch [90/300], Step [160/391],                 Loss: 0.36125, Train_Acc:87.71%
Epoch [90/300], Step [170/391],                 Loss: 0.36222, Train_Acc:87.69%
Epoch [90/300], Step [180/391],                 Loss: 0.36229, Train_Acc:87.71%
Epoch [90/300], Step [190/391],                 Loss: 0.36234, Train_Acc:87.70%
Epoch [90/300], Step [200/391],                 Loss: 0.36073, Train_Acc:87.74%
Epoch [90/300], Step [210/391],                 Loss: 0.36021, Train_Acc:87.77%
Epoch [90/300], Step [220/391],                 Loss: 0.36058, Train_Acc:87.79%
Epoch [90/300], Step [230/391],                 Loss: 0.35943, Train_Acc:87.81%
Epoch [90/300], Step [240/391],                 Loss: 0.35665, Train_Acc:87.87%
Epoch [90/300], Step [250/391],                 Loss: 0.35701, Train_Acc:87.84%
Epoch [90/300], Step [260/391],                 Loss: 0.36063, Train_Acc:87.76%
Epoch [90/300], Step [270/391],                 Loss: 0.36335, Train_Acc:87.69%
Epoch [90/300], Step [280/391],                 Loss: 0.36291, Train_Acc:87.70%
Epoch [90/300], Step [290/391],                 Loss: 0.36256, Train_Acc:87.71%
Epoch [90/300], Step [300/391],                 Loss: 0.36333, Train_Acc:87.67%
Epoch [90/300], Step [310/391],                 Loss: 0.36244, Train_Acc:87.67%
Epoch [90/300], Step [320/391],                 Loss: 0.36232, Train_Acc:87.67%
Epoch [90/300], Step [330/391],                 Loss: 0.36145, Train_Acc:87.69%
Epoch [90/300], Step [340/391],                 Loss: 0.35996, Train_Acc:87.76%
Epoch [90/300], Step [350/391],                 Loss: 0.35975, Train_Acc:87.77%
Epoch [90/300], Step [360/391],                 Loss: 0.36055, Train_Acc:87.72%
Epoch [90/300], Step [370/391],                 Loss: 0.36075, Train_Acc:87.69%
Epoch [90/300], Step [380/391],                 Loss: 0.36074, Train_Acc:87.69%
Epoch [90/300], Step [390/391],                 Loss: 0.36009, Train_Acc:87.72%
Accuary on test images:72.14%
Epoch [91/300], Step [10/391],                 Loss: 0.37850, Train_Acc:88.44%
Epoch [91/300], Step [20/391],                 Loss: 0.35553, Train_Acc:88.79%
Epoch [91/300], Step [30/391],                 Loss: 0.35760, Train_Acc:88.70%
Epoch [91/300], Step [40/391],                 Loss: 0.36478, Train_Acc:88.28%
Epoch [91/300], Step [50/391],                 Loss: 0.36468, Train_Acc:87.98%
Epoch [91/300], Step [60/391],                 Loss: 0.36916, Train_Acc:87.84%
Epoch [91/300], Step [70/391],                 Loss: 0.37105, Train_Acc:87.73%
Epoch [91/300], Step [80/391],                 Loss: 0.36978, Train_Acc:87.85%
Epoch [91/300], Step [90/391],                 Loss: 0.37043, Train_Acc:87.79%
Epoch [91/300], Step [100/391],                 Loss: 0.36754, Train_Acc:87.77%
Epoch [91/300], Step [110/391],                 Loss: 0.36748, Train_Acc:87.86%
Epoch [91/300], Step [120/391],                 Loss: 0.36729, Train_Acc:87.83%
Epoch [91/300], Step [130/391],                 Loss: 0.36764, Train_Acc:87.82%
Epoch [91/300], Step [140/391],                 Loss: 0.36460, Train_Acc:87.94%
Epoch [91/300], Step [150/391],                 Loss: 0.36540, Train_Acc:87.82%
Epoch [91/300], Step [160/391],                 Loss: 0.36410, Train_Acc:87.90%
Epoch [91/300], Step [170/391],                 Loss: 0.36389, Train_Acc:87.87%
Epoch [91/300], Step [180/391],                 Loss: 0.36401, Train_Acc:87.85%
Epoch [91/300], Step [190/391],                 Loss: 0.36526, Train_Acc:87.73%
Epoch [91/300], Step [200/391],                 Loss: 0.36679, Train_Acc:87.69%
Epoch [91/300], Step [210/391],                 Loss: 0.36734, Train_Acc:87.67%
Epoch [91/300], Step [220/391],                 Loss: 0.36700, Train_Acc:87.69%
Epoch [91/300], Step [230/391],                 Loss: 0.36778, Train_Acc:87.61%
Epoch [91/300], Step [240/391],                 Loss: 0.36581, Train_Acc:87.71%
Epoch [91/300], Step [250/391],                 Loss: 0.36445, Train_Acc:87.75%
Epoch [91/300], Step [260/391],                 Loss: 0.36666, Train_Acc:87.71%
Epoch [91/300], Step [270/391],                 Loss: 0.36648, Train_Acc:87.72%
Epoch [91/300], Step [280/391],                 Loss: 0.36651, Train_Acc:87.71%
Epoch [91/300], Step [290/391],                 Loss: 0.36694, Train_Acc:87.73%
Epoch [91/300], Step [300/391],                 Loss: 0.36621, Train_Acc:87.73%
Epoch [91/300], Step [310/391],                 Loss: 0.36532, Train_Acc:87.74%
Epoch [91/300], Step [320/391],                 Loss: 0.36518, Train_Acc:87.77%
Epoch [91/300], Step [330/391],                 Loss: 0.36500, Train_Acc:87.80%
Epoch [91/300], Step [340/391],                 Loss: 0.36375, Train_Acc:87.84%
Epoch [91/300], Step [350/391],                 Loss: 0.36256, Train_Acc:87.87%
Epoch [91/300], Step [360/391],                 Loss: 0.36271, Train_Acc:87.85%
Epoch [91/300], Step [370/391],                 Loss: 0.36244, Train_Acc:87.87%
Epoch [91/300], Step [380/391],                 Loss: 0.36204, Train_Acc:87.86%
Epoch [91/300], Step [390/391],                 Loss: 0.36142, Train_Acc:87.89%
Accuary on test images:74.38%
Epoch [92/300], Step [10/391],                 Loss: 0.35967, Train_Acc:87.73%
Epoch [92/300], Step [20/391],                 Loss: 0.36695, Train_Acc:87.70%
Epoch [92/300], Step [30/391],                 Loss: 0.35423, Train_Acc:87.97%
Epoch [92/300], Step [40/391],                 Loss: 0.35624, Train_Acc:87.87%
Epoch [92/300], Step [50/391],                 Loss: 0.35927, Train_Acc:87.75%
Epoch [92/300], Step [60/391],                 Loss: 0.36555, Train_Acc:87.50%
Epoch [92/300], Step [70/391],                 Loss: 0.35995, Train_Acc:87.72%
Epoch [92/300], Step [80/391],                 Loss: 0.36558, Train_Acc:87.62%
Epoch [92/300], Step [90/391],                 Loss: 0.36697, Train_Acc:87.53%
Epoch [92/300], Step [100/391],                 Loss: 0.36279, Train_Acc:87.61%
Epoch [92/300], Step [110/391],                 Loss: 0.36147, Train_Acc:87.67%
Epoch [92/300], Step [120/391],                 Loss: 0.36303, Train_Acc:87.58%
Epoch [92/300], Step [130/391],                 Loss: 0.36130, Train_Acc:87.69%
Epoch [92/300], Step [140/391],                 Loss: 0.35968, Train_Acc:87.77%
Epoch [92/300], Step [150/391],                 Loss: 0.35948, Train_Acc:87.74%
Epoch [92/300], Step [160/391],                 Loss: 0.35770, Train_Acc:87.83%
Epoch [92/300], Step [170/391],                 Loss: 0.35975, Train_Acc:87.76%
Epoch [92/300], Step [180/391],                 Loss: 0.36120, Train_Acc:87.68%
Epoch [92/300], Step [190/391],                 Loss: 0.36317, Train_Acc:87.60%
Epoch [92/300], Step [200/391],                 Loss: 0.36451, Train_Acc:87.58%
Epoch [92/300], Step [210/391],                 Loss: 0.36384, Train_Acc:87.60%
Epoch [92/300], Step [220/391],                 Loss: 0.36395, Train_Acc:87.62%
Epoch [92/300], Step [230/391],                 Loss: 0.36268, Train_Acc:87.66%
Epoch [92/300], Step [240/391],                 Loss: 0.36158, Train_Acc:87.67%
Epoch [92/300], Step [250/391],                 Loss: 0.36115, Train_Acc:87.64%
Epoch [92/300], Step [260/391],                 Loss: 0.36268, Train_Acc:87.60%
Epoch [92/300], Step [270/391],                 Loss: 0.36260, Train_Acc:87.59%
Epoch [92/300], Step [280/391],                 Loss: 0.36192, Train_Acc:87.62%
Epoch [92/300], Step [290/391],                 Loss: 0.36142, Train_Acc:87.65%
Epoch [92/300], Step [300/391],                 Loss: 0.36151, Train_Acc:87.64%
Epoch [92/300], Step [310/391],                 Loss: 0.36223, Train_Acc:87.62%
Epoch [92/300], Step [320/391],                 Loss: 0.36252, Train_Acc:87.61%
Epoch [92/300], Step [330/391],                 Loss: 0.36239, Train_Acc:87.62%
Epoch [92/300], Step [340/391],                 Loss: 0.36098, Train_Acc:87.66%
Epoch [92/300], Step [350/391],                 Loss: 0.36133, Train_Acc:87.69%
Epoch [92/300], Step [360/391],                 Loss: 0.36090, Train_Acc:87.74%
Epoch [92/300], Step [370/391],                 Loss: 0.36145, Train_Acc:87.73%
Epoch [92/300], Step [380/391],                 Loss: 0.36113, Train_Acc:87.73%
Epoch [92/300], Step [390/391],                 Loss: 0.36053, Train_Acc:87.76%
Accuary on test images:80.44%
Epoch [93/300], Step [10/391],                 Loss: 0.31077, Train_Acc:89.84%
Epoch [93/300], Step [20/391],                 Loss: 0.34104, Train_Acc:88.91%
Epoch [93/300], Step [30/391],                 Loss: 0.33335, Train_Acc:88.98%
Epoch [93/300], Step [40/391],                 Loss: 0.33932, Train_Acc:88.61%
Epoch [93/300], Step [50/391],                 Loss: 0.34272, Train_Acc:88.50%
Epoch [93/300], Step [60/391],                 Loss: 0.34842, Train_Acc:88.06%
Epoch [93/300], Step [70/391],                 Loss: 0.35174, Train_Acc:88.00%
Epoch [93/300], Step [80/391],                 Loss: 0.35850, Train_Acc:87.80%
Epoch [93/300], Step [90/391],                 Loss: 0.36451, Train_Acc:87.58%
Epoch [93/300], Step [100/391],                 Loss: 0.36635, Train_Acc:87.42%
Epoch [93/300], Step [110/391],                 Loss: 0.36877, Train_Acc:87.38%
Epoch [93/300], Step [120/391],                 Loss: 0.36555, Train_Acc:87.46%
Epoch [93/300], Step [130/391],                 Loss: 0.36803, Train_Acc:87.36%
Epoch [93/300], Step [140/391],                 Loss: 0.36794, Train_Acc:87.38%
Epoch [93/300], Step [150/391],                 Loss: 0.36830, Train_Acc:87.36%
Epoch [93/300], Step [160/391],                 Loss: 0.36810, Train_Acc:87.37%
Epoch [93/300], Step [170/391],                 Loss: 0.36818, Train_Acc:87.41%
Epoch [93/300], Step [180/391],                 Loss: 0.36781, Train_Acc:87.39%
Epoch [93/300], Step [190/391],                 Loss: 0.36823, Train_Acc:87.36%
Epoch [93/300], Step [200/391],                 Loss: 0.36752, Train_Acc:87.42%
Epoch [93/300], Step [210/391],                 Loss: 0.36773, Train_Acc:87.39%
Epoch [93/300], Step [220/391],                 Loss: 0.36626, Train_Acc:87.45%
Epoch [93/300], Step [230/391],                 Loss: 0.36590, Train_Acc:87.46%
Epoch [93/300], Step [240/391],                 Loss: 0.36526, Train_Acc:87.45%
Epoch [93/300], Step [250/391],                 Loss: 0.36486, Train_Acc:87.45%
Epoch [93/300], Step [260/391],                 Loss: 0.36666, Train_Acc:87.43%
Epoch [93/300], Step [270/391],                 Loss: 0.36684, Train_Acc:87.45%
Epoch [93/300], Step [280/391],                 Loss: 0.36789, Train_Acc:87.42%
Epoch [93/300], Step [290/391],                 Loss: 0.36807, Train_Acc:87.38%
Epoch [93/300], Step [300/391],                 Loss: 0.36726, Train_Acc:87.42%
Epoch [93/300], Step [310/391],                 Loss: 0.36721, Train_Acc:87.42%
Epoch [93/300], Step [320/391],                 Loss: 0.36730, Train_Acc:87.42%
Epoch [93/300], Step [330/391],                 Loss: 0.36556, Train_Acc:87.51%
Epoch [93/300], Step [340/391],                 Loss: 0.36480, Train_Acc:87.53%
Epoch [93/300], Step [350/391],                 Loss: 0.36334, Train_Acc:87.59%
Epoch [93/300], Step [360/391],                 Loss: 0.36323, Train_Acc:87.58%
Epoch [93/300], Step [370/391],                 Loss: 0.36340, Train_Acc:87.57%
Epoch [93/300], Step [380/391],                 Loss: 0.36348, Train_Acc:87.57%
Epoch [93/300], Step [390/391],                 Loss: 0.36265, Train_Acc:87.60%
Accuary on test images:68.10%
Epoch [94/300], Step [10/391],                 Loss: 0.35678, Train_Acc:87.81%
Epoch [94/300], Step [20/391],                 Loss: 0.35625, Train_Acc:88.01%
Epoch [94/300], Step [30/391],                 Loss: 0.35376, Train_Acc:87.92%
Epoch [94/300], Step [40/391],                 Loss: 0.35803, Train_Acc:87.89%
Epoch [94/300], Step [50/391],                 Loss: 0.36338, Train_Acc:87.81%
Epoch [94/300], Step [60/391],                 Loss: 0.36958, Train_Acc:87.38%
Epoch [94/300], Step [70/391],                 Loss: 0.37151, Train_Acc:87.30%
Epoch [94/300], Step [80/391],                 Loss: 0.37069, Train_Acc:87.37%
Epoch [94/300], Step [90/391],                 Loss: 0.37282, Train_Acc:87.37%
Epoch [94/300], Step [100/391],                 Loss: 0.37023, Train_Acc:87.47%
Epoch [94/300], Step [110/391],                 Loss: 0.37040, Train_Acc:87.41%
Epoch [94/300], Step [120/391],                 Loss: 0.37172, Train_Acc:87.36%
Epoch [94/300], Step [130/391],                 Loss: 0.37320, Train_Acc:87.34%
Epoch [94/300], Step [140/391],                 Loss: 0.37081, Train_Acc:87.36%
Epoch [94/300], Step [150/391],                 Loss: 0.37142, Train_Acc:87.40%
Epoch [94/300], Step [160/391],                 Loss: 0.36866, Train_Acc:87.48%
Epoch [94/300], Step [170/391],                 Loss: 0.36791, Train_Acc:87.50%
Epoch [94/300], Step [180/391],                 Loss: 0.36950, Train_Acc:87.42%
Epoch [94/300], Step [190/391],                 Loss: 0.36933, Train_Acc:87.45%
Epoch [94/300], Step [200/391],                 Loss: 0.36978, Train_Acc:87.45%
Epoch [94/300], Step [210/391],                 Loss: 0.36938, Train_Acc:87.48%
Epoch [94/300], Step [220/391],                 Loss: 0.36954, Train_Acc:87.48%
Epoch [94/300], Step [230/391],                 Loss: 0.36643, Train_Acc:87.59%
Epoch [94/300], Step [240/391],                 Loss: 0.36382, Train_Acc:87.69%
Epoch [94/300], Step [250/391],                 Loss: 0.36356, Train_Acc:87.70%
Epoch [94/300], Step [260/391],                 Loss: 0.36466, Train_Acc:87.68%
Epoch [94/300], Step [270/391],                 Loss: 0.36595, Train_Acc:87.64%
Epoch [94/300], Step [280/391],                 Loss: 0.36498, Train_Acc:87.67%
Epoch [94/300], Step [290/391],                 Loss: 0.36436, Train_Acc:87.70%
Epoch [94/300], Step [300/391],                 Loss: 0.36429, Train_Acc:87.67%
Epoch [94/300], Step [310/391],                 Loss: 0.36343, Train_Acc:87.71%
Epoch [94/300], Step [320/391],                 Loss: 0.36363, Train_Acc:87.71%
Epoch [94/300], Step [330/391],                 Loss: 0.36265, Train_Acc:87.76%
Epoch [94/300], Step [340/391],                 Loss: 0.36193, Train_Acc:87.78%
Epoch [94/300], Step [350/391],                 Loss: 0.36166, Train_Acc:87.79%
Epoch [94/300], Step [360/391],                 Loss: 0.36189, Train_Acc:87.76%
Epoch [94/300], Step [370/391],                 Loss: 0.36259, Train_Acc:87.70%
Epoch [94/300], Step [380/391],                 Loss: 0.36238, Train_Acc:87.71%
Epoch [94/300], Step [390/391],                 Loss: 0.36253, Train_Acc:87.71%
Accuary on test images:70.20%
Epoch [95/300], Step [10/391],                 Loss: 0.38569, Train_Acc:87.50%
Epoch [95/300], Step [20/391],                 Loss: 0.37631, Train_Acc:87.81%
Epoch [95/300], Step [30/391],                 Loss: 0.37242, Train_Acc:87.79%
Epoch [95/300], Step [40/391],                 Loss: 0.37089, Train_Acc:87.56%
Epoch [95/300], Step [50/391],                 Loss: 0.36764, Train_Acc:87.50%
Epoch [95/300], Step [60/391],                 Loss: 0.36998, Train_Acc:87.28%
Epoch [95/300], Step [70/391],                 Loss: 0.37370, Train_Acc:87.24%
Epoch [95/300], Step [80/391],                 Loss: 0.37311, Train_Acc:87.28%
Epoch [95/300], Step [90/391],                 Loss: 0.37477, Train_Acc:87.19%
Epoch [95/300], Step [100/391],                 Loss: 0.37333, Train_Acc:87.22%
Epoch [95/300], Step [110/391],                 Loss: 0.37470, Train_Acc:87.15%
Epoch [95/300], Step [120/391],                 Loss: 0.37426, Train_Acc:87.17%
Epoch [95/300], Step [130/391],                 Loss: 0.37546, Train_Acc:87.18%
Epoch [95/300], Step [140/391],                 Loss: 0.37435, Train_Acc:87.20%
Epoch [95/300], Step [150/391],                 Loss: 0.37511, Train_Acc:87.26%
Epoch [95/300], Step [160/391],                 Loss: 0.37363, Train_Acc:87.29%
Epoch [95/300], Step [170/391],                 Loss: 0.37244, Train_Acc:87.27%
Epoch [95/300], Step [180/391],                 Loss: 0.37197, Train_Acc:87.34%
Epoch [95/300], Step [190/391],                 Loss: 0.37044, Train_Acc:87.40%
Epoch [95/300], Step [200/391],                 Loss: 0.37063, Train_Acc:87.39%
Epoch [95/300], Step [210/391],                 Loss: 0.36996, Train_Acc:87.43%
Epoch [95/300], Step [220/391],                 Loss: 0.36976, Train_Acc:87.47%
Epoch [95/300], Step [230/391],                 Loss: 0.36680, Train_Acc:87.56%
Epoch [95/300], Step [240/391],                 Loss: 0.36571, Train_Acc:87.61%
Epoch [95/300], Step [250/391],                 Loss: 0.36404, Train_Acc:87.65%
Epoch [95/300], Step [260/391],                 Loss: 0.36646, Train_Acc:87.61%
Epoch [95/300], Step [270/391],                 Loss: 0.36591, Train_Acc:87.62%
Epoch [95/300], Step [280/391],                 Loss: 0.36576, Train_Acc:87.61%
Epoch [95/300], Step [290/391],                 Loss: 0.36510, Train_Acc:87.64%
Epoch [95/300], Step [300/391],                 Loss: 0.36602, Train_Acc:87.58%
Epoch [95/300], Step [310/391],                 Loss: 0.36624, Train_Acc:87.59%
Epoch [95/300], Step [320/391],                 Loss: 0.36559, Train_Acc:87.61%
Epoch [95/300], Step [330/391],                 Loss: 0.36470, Train_Acc:87.63%
Epoch [95/300], Step [340/391],                 Loss: 0.36368, Train_Acc:87.65%
Epoch [95/300], Step [350/391],                 Loss: 0.36331, Train_Acc:87.67%
Epoch [95/300], Step [360/391],                 Loss: 0.36360, Train_Acc:87.66%
Epoch [95/300], Step [370/391],                 Loss: 0.36455, Train_Acc:87.61%
Epoch [95/300], Step [380/391],                 Loss: 0.36407, Train_Acc:87.62%
Epoch [95/300], Step [390/391],                 Loss: 0.36401, Train_Acc:87.64%
Accuary on test images:72.54%
Epoch [96/300], Step [10/391],                 Loss: 0.37467, Train_Acc:86.72%
Epoch [96/300], Step [20/391],                 Loss: 0.36530, Train_Acc:87.50%
Epoch [96/300], Step [30/391],                 Loss: 0.36527, Train_Acc:87.84%
Epoch [96/300], Step [40/391],                 Loss: 0.36136, Train_Acc:88.11%
Epoch [96/300], Step [50/391],                 Loss: 0.36140, Train_Acc:87.95%
Epoch [96/300], Step [60/391],                 Loss: 0.36523, Train_Acc:87.68%
Epoch [96/300], Step [70/391],                 Loss: 0.36489, Train_Acc:87.72%
Epoch [96/300], Step [80/391],                 Loss: 0.36458, Train_Acc:87.70%
Epoch [96/300], Step [90/391],                 Loss: 0.36700, Train_Acc:87.52%
Epoch [96/300], Step [100/391],                 Loss: 0.36441, Train_Acc:87.53%
Epoch [96/300], Step [110/391],                 Loss: 0.36458, Train_Acc:87.56%
Epoch [96/300], Step [120/391],                 Loss: 0.36553, Train_Acc:87.53%
Epoch [96/300], Step [130/391],                 Loss: 0.36965, Train_Acc:87.42%
Epoch [96/300], Step [140/391],                 Loss: 0.36731, Train_Acc:87.49%
Epoch [96/300], Step [150/391],                 Loss: 0.36778, Train_Acc:87.47%
Epoch [96/300], Step [160/391],                 Loss: 0.36799, Train_Acc:87.44%
Epoch [96/300], Step [170/391],                 Loss: 0.36846, Train_Acc:87.42%
Epoch [96/300], Step [180/391],                 Loss: 0.36932, Train_Acc:87.38%
Epoch [96/300], Step [190/391],                 Loss: 0.37001, Train_Acc:87.38%
Epoch [96/300], Step [200/391],                 Loss: 0.36940, Train_Acc:87.42%
Epoch [96/300], Step [210/391],                 Loss: 0.36854, Train_Acc:87.47%
Epoch [96/300], Step [220/391],                 Loss: 0.36886, Train_Acc:87.46%
Epoch [96/300], Step [230/391],                 Loss: 0.36642, Train_Acc:87.57%
Epoch [96/300], Step [240/391],                 Loss: 0.36426, Train_Acc:87.62%
Epoch [96/300], Step [250/391],                 Loss: 0.36438, Train_Acc:87.61%
Epoch [96/300], Step [260/391],                 Loss: 0.36590, Train_Acc:87.58%
Epoch [96/300], Step [270/391],                 Loss: 0.36678, Train_Acc:87.53%
Epoch [96/300], Step [280/391],                 Loss: 0.36658, Train_Acc:87.56%
Epoch [96/300], Step [290/391],                 Loss: 0.36582, Train_Acc:87.61%
Epoch [96/300], Step [300/391],                 Loss: 0.36637, Train_Acc:87.56%
Epoch [96/300], Step [310/391],                 Loss: 0.36558, Train_Acc:87.59%
Epoch [96/300], Step [320/391],                 Loss: 0.36540, Train_Acc:87.60%
Epoch [96/300], Step [330/391],                 Loss: 0.36422, Train_Acc:87.68%
Epoch [96/300], Step [340/391],                 Loss: 0.36334, Train_Acc:87.75%
Epoch [96/300], Step [350/391],                 Loss: 0.36386, Train_Acc:87.73%
Epoch [96/300], Step [360/391],                 Loss: 0.36350, Train_Acc:87.74%
Epoch [96/300], Step [370/391],                 Loss: 0.36294, Train_Acc:87.74%
Epoch [96/300], Step [380/391],                 Loss: 0.36318, Train_Acc:87.72%
Epoch [96/300], Step [390/391],                 Loss: 0.36297, Train_Acc:87.73%
Accuary on test images:79.32%
Epoch [97/300], Step [10/391],                 Loss: 0.31326, Train_Acc:89.45%
Epoch [97/300], Step [20/391],                 Loss: 0.33025, Train_Acc:88.55%
Epoch [97/300], Step [30/391],                 Loss: 0.32939, Train_Acc:88.70%
Epoch [97/300], Step [40/391],                 Loss: 0.33248, Train_Acc:88.63%
Epoch [97/300], Step [50/391],                 Loss: 0.33174, Train_Acc:88.80%
Epoch [97/300], Step [60/391],                 Loss: 0.34102, Train_Acc:88.33%
Epoch [97/300], Step [70/391],                 Loss: 0.34470, Train_Acc:88.31%
Epoch [97/300], Step [80/391],                 Loss: 0.34595, Train_Acc:88.24%
Epoch [97/300], Step [90/391],                 Loss: 0.35183, Train_Acc:88.12%
Epoch [97/300], Step [100/391],                 Loss: 0.35094, Train_Acc:88.09%
Epoch [97/300], Step [110/391],                 Loss: 0.35386, Train_Acc:87.95%
Epoch [97/300], Step [120/391],                 Loss: 0.35475, Train_Acc:87.91%
Epoch [97/300], Step [130/391],                 Loss: 0.35482, Train_Acc:87.93%
Epoch [97/300], Step [140/391],                 Loss: 0.35317, Train_Acc:87.97%
Epoch [97/300], Step [150/391],                 Loss: 0.35289, Train_Acc:88.00%
Epoch [97/300], Step [160/391],                 Loss: 0.35252, Train_Acc:88.08%
Epoch [97/300], Step [170/391],                 Loss: 0.35326, Train_Acc:88.07%
Epoch [97/300], Step [180/391],                 Loss: 0.35313, Train_Acc:88.06%
Epoch [97/300], Step [190/391],                 Loss: 0.35387, Train_Acc:88.06%
Epoch [97/300], Step [200/391],                 Loss: 0.35667, Train_Acc:87.98%
Epoch [97/300], Step [210/391],                 Loss: 0.35729, Train_Acc:87.96%
Epoch [97/300], Step [220/391],                 Loss: 0.35781, Train_Acc:87.90%
Epoch [97/300], Step [230/391],                 Loss: 0.35701, Train_Acc:87.98%
Epoch [97/300], Step [240/391],                 Loss: 0.35600, Train_Acc:87.98%
Epoch [97/300], Step [250/391],                 Loss: 0.35502, Train_Acc:88.00%
Epoch [97/300], Step [260/391],                 Loss: 0.35624, Train_Acc:87.97%
Epoch [97/300], Step [270/391],                 Loss: 0.35800, Train_Acc:87.90%
Epoch [97/300], Step [280/391],                 Loss: 0.35940, Train_Acc:87.87%
Epoch [97/300], Step [290/391],                 Loss: 0.36038, Train_Acc:87.82%
Epoch [97/300], Step [300/391],                 Loss: 0.36037, Train_Acc:87.81%
Epoch [97/300], Step [310/391],                 Loss: 0.36009, Train_Acc:87.81%
Epoch [97/300], Step [320/391],                 Loss: 0.36070, Train_Acc:87.76%
Epoch [97/300], Step [330/391],                 Loss: 0.35897, Train_Acc:87.85%
Epoch [97/300], Step [340/391],                 Loss: 0.35858, Train_Acc:87.84%
Epoch [97/300], Step [350/391],                 Loss: 0.35818, Train_Acc:87.88%
Epoch [97/300], Step [360/391],                 Loss: 0.35825, Train_Acc:87.83%
Epoch [97/300], Step [370/391],                 Loss: 0.35846, Train_Acc:87.83%
Epoch [97/300], Step [380/391],                 Loss: 0.35861, Train_Acc:87.82%
Epoch [97/300], Step [390/391],                 Loss: 0.35827, Train_Acc:87.83%
Accuary on test images:76.28%
Epoch [98/300], Step [10/391],                 Loss: 0.38133, Train_Acc:86.17%
Epoch [98/300], Step [20/391],                 Loss: 0.38071, Train_Acc:86.37%
Epoch [98/300], Step [30/391],                 Loss: 0.38070, Train_Acc:86.56%
Epoch [98/300], Step [40/391],                 Loss: 0.38256, Train_Acc:86.43%
Epoch [98/300], Step [50/391],                 Loss: 0.37651, Train_Acc:86.67%
Epoch [98/300], Step [60/391],                 Loss: 0.37649, Train_Acc:86.72%
Epoch [98/300], Step [70/391],                 Loss: 0.36888, Train_Acc:87.09%
Epoch [98/300], Step [80/391],                 Loss: 0.36639, Train_Acc:87.16%
Epoch [98/300], Step [90/391],                 Loss: 0.36808, Train_Acc:87.08%
Epoch [98/300], Step [100/391],                 Loss: 0.36566, Train_Acc:87.23%
Epoch [98/300], Step [110/391],                 Loss: 0.36646, Train_Acc:87.24%
Epoch [98/300], Step [120/391],                 Loss: 0.36317, Train_Acc:87.29%
Epoch [98/300], Step [130/391],                 Loss: 0.36508, Train_Acc:87.33%
Epoch [98/300], Step [140/391],                 Loss: 0.36359, Train_Acc:87.39%
Epoch [98/300], Step [150/391],                 Loss: 0.36490, Train_Acc:87.43%
Epoch [98/300], Step [160/391],                 Loss: 0.36425, Train_Acc:87.39%
Epoch [98/300], Step [170/391],                 Loss: 0.36578, Train_Acc:87.32%
Epoch [98/300], Step [180/391],                 Loss: 0.36676, Train_Acc:87.25%
Epoch [98/300], Step [190/391],                 Loss: 0.36653, Train_Acc:87.27%
Epoch [98/300], Step [200/391],                 Loss: 0.36599, Train_Acc:87.32%
Epoch [98/300], Step [210/391],                 Loss: 0.36573, Train_Acc:87.32%
Epoch [98/300], Step [220/391],                 Loss: 0.36566, Train_Acc:87.33%
Epoch [98/300], Step [230/391],                 Loss: 0.36483, Train_Acc:87.37%
Epoch [98/300], Step [240/391],                 Loss: 0.36298, Train_Acc:87.45%
Epoch [98/300], Step [250/391],                 Loss: 0.36303, Train_Acc:87.42%
Epoch [98/300], Step [260/391],                 Loss: 0.36444, Train_Acc:87.37%
Epoch [98/300], Step [270/391],                 Loss: 0.36625, Train_Acc:87.34%
Epoch [98/300], Step [280/391],                 Loss: 0.36757, Train_Acc:87.33%
Epoch [98/300], Step [290/391],                 Loss: 0.36785, Train_Acc:87.35%
Epoch [98/300], Step [300/391],                 Loss: 0.36666, Train_Acc:87.37%
Epoch [98/300], Step [310/391],                 Loss: 0.36638, Train_Acc:87.37%
Epoch [98/300], Step [320/391],                 Loss: 0.36586, Train_Acc:87.40%
Epoch [98/300], Step [330/391],                 Loss: 0.36539, Train_Acc:87.43%
Epoch [98/300], Step [340/391],                 Loss: 0.36403, Train_Acc:87.49%
Epoch [98/300], Step [350/391],                 Loss: 0.36273, Train_Acc:87.54%
Epoch [98/300], Step [360/391],                 Loss: 0.36198, Train_Acc:87.55%
Epoch [98/300], Step [370/391],                 Loss: 0.36182, Train_Acc:87.54%
Epoch [98/300], Step [380/391],                 Loss: 0.36059, Train_Acc:87.60%
Epoch [98/300], Step [390/391],                 Loss: 0.35990, Train_Acc:87.64%
Accuary on test images:75.74%
Epoch [99/300], Step [10/391],                 Loss: 0.33927, Train_Acc:89.53%
Epoch [99/300], Step [20/391],                 Loss: 0.32889, Train_Acc:89.49%
Epoch [99/300], Step [30/391],                 Loss: 0.32988, Train_Acc:89.17%
Epoch [99/300], Step [40/391],                 Loss: 0.34017, Train_Acc:88.98%
Epoch [99/300], Step [50/391],                 Loss: 0.34144, Train_Acc:88.81%
Epoch [99/300], Step [60/391],                 Loss: 0.34950, Train_Acc:88.27%
Epoch [99/300], Step [70/391],                 Loss: 0.35336, Train_Acc:88.09%
Epoch [99/300], Step [80/391],                 Loss: 0.35710, Train_Acc:87.98%
Epoch [99/300], Step [90/391],                 Loss: 0.36082, Train_Acc:87.80%
Epoch [99/300], Step [100/391],                 Loss: 0.35991, Train_Acc:87.75%
Epoch [99/300], Step [110/391],                 Loss: 0.36234, Train_Acc:87.73%
Epoch [99/300], Step [120/391],                 Loss: 0.36267, Train_Acc:87.71%
Epoch [99/300], Step [130/391],                 Loss: 0.36449, Train_Acc:87.69%
Epoch [99/300], Step [140/391],                 Loss: 0.36273, Train_Acc:87.75%
Epoch [99/300], Step [150/391],                 Loss: 0.36254, Train_Acc:87.75%
Epoch [99/300], Step [160/391],                 Loss: 0.36078, Train_Acc:87.74%
Epoch [99/300], Step [170/391],                 Loss: 0.36047, Train_Acc:87.70%
Epoch [99/300], Step [180/391],                 Loss: 0.35885, Train_Acc:87.76%
Epoch [99/300], Step [190/391],                 Loss: 0.35828, Train_Acc:87.81%
Epoch [99/300], Step [200/391],                 Loss: 0.35691, Train_Acc:87.89%
Epoch [99/300], Step [210/391],                 Loss: 0.35685, Train_Acc:87.91%
Epoch [99/300], Step [220/391],                 Loss: 0.35834, Train_Acc:87.83%
Epoch [99/300], Step [230/391],                 Loss: 0.35865, Train_Acc:87.83%
Epoch [99/300], Step [240/391],                 Loss: 0.35766, Train_Acc:87.84%
Epoch [99/300], Step [250/391],                 Loss: 0.35777, Train_Acc:87.81%
Epoch [99/300], Step [260/391],                 Loss: 0.36015, Train_Acc:87.75%
Epoch [99/300], Step [270/391],                 Loss: 0.36114, Train_Acc:87.74%
Epoch [99/300], Step [280/391],                 Loss: 0.36139, Train_Acc:87.73%
Epoch [99/300], Step [290/391],                 Loss: 0.36120, Train_Acc:87.76%
Epoch [99/300], Step [300/391],                 Loss: 0.36015, Train_Acc:87.81%
Epoch [99/300], Step [310/391],                 Loss: 0.35895, Train_Acc:87.83%
Epoch [99/300], Step [320/391],                 Loss: 0.35957, Train_Acc:87.79%
Epoch [99/300], Step [330/391],                 Loss: 0.36076, Train_Acc:87.79%
Epoch [99/300], Step [340/391],                 Loss: 0.35954, Train_Acc:87.84%
Epoch [99/300], Step [350/391],                 Loss: 0.35857, Train_Acc:87.88%
Epoch [99/300], Step [360/391],                 Loss: 0.35819, Train_Acc:87.88%
Epoch [99/300], Step [370/391],                 Loss: 0.35853, Train_Acc:87.87%
Epoch [99/300], Step [380/391],                 Loss: 0.35836, Train_Acc:87.86%
Epoch [99/300], Step [390/391],                 Loss: 0.35816, Train_Acc:87.85%
Accuary on test images:77.72%
Epoch [100/300], Step [10/391],                 Loss: 0.37666, Train_Acc:86.48%
Epoch [100/300], Step [20/391],                 Loss: 0.35263, Train_Acc:87.62%
Epoch [100/300], Step [30/391],                 Loss: 0.34706, Train_Acc:88.07%
Epoch [100/300], Step [40/391],                 Loss: 0.35167, Train_Acc:87.95%
Epoch [100/300], Step [50/391],                 Loss: 0.34941, Train_Acc:88.02%
Epoch [100/300], Step [60/391],                 Loss: 0.35436, Train_Acc:87.64%
Epoch [100/300], Step [70/391],                 Loss: 0.35703, Train_Acc:87.57%
Epoch [100/300], Step [80/391],                 Loss: 0.35675, Train_Acc:87.56%
Epoch [100/300], Step [90/391],                 Loss: 0.36159, Train_Acc:87.39%
Epoch [100/300], Step [100/391],                 Loss: 0.36048, Train_Acc:87.37%
Epoch [100/300], Step [110/391],                 Loss: 0.36316, Train_Acc:87.26%
Epoch [100/300], Step [120/391],                 Loss: 0.36396, Train_Acc:87.27%
Epoch [100/300], Step [130/391],                 Loss: 0.36936, Train_Acc:87.10%
Epoch [100/300], Step [140/391],                 Loss: 0.37054, Train_Acc:87.06%
Epoch [100/300], Step [150/391],                 Loss: 0.37270, Train_Acc:86.98%
Epoch [100/300], Step [160/391],                 Loss: 0.37071, Train_Acc:87.06%
Epoch [100/300], Step [170/391],                 Loss: 0.37201, Train_Acc:86.98%
Epoch [100/300], Step [180/391],                 Loss: 0.37097, Train_Acc:87.00%
Epoch [100/300], Step [190/391],                 Loss: 0.37148, Train_Acc:87.02%
Epoch [100/300], Step [200/391],                 Loss: 0.37180, Train_Acc:87.06%
Epoch [100/300], Step [210/391],                 Loss: 0.37135, Train_Acc:87.08%
Epoch [100/300], Step [220/391],                 Loss: 0.37201, Train_Acc:87.11%
Epoch [100/300], Step [230/391],                 Loss: 0.37077, Train_Acc:87.16%
Epoch [100/300], Step [240/391],                 Loss: 0.36862, Train_Acc:87.25%
Epoch [100/300], Step [250/391],                 Loss: 0.36713, Train_Acc:87.29%
Epoch [100/300], Step [260/391],                 Loss: 0.36853, Train_Acc:87.27%
Epoch [100/300], Step [270/391],                 Loss: 0.36797, Train_Acc:87.29%
Epoch [100/300], Step [280/391],                 Loss: 0.36787, Train_Acc:87.33%
Epoch [100/300], Step [290/391],                 Loss: 0.36825, Train_Acc:87.33%
Epoch [100/300], Step [300/391],                 Loss: 0.36752, Train_Acc:87.37%
Epoch [100/300], Step [310/391],                 Loss: 0.36576, Train_Acc:87.43%
Epoch [100/300], Step [320/391],                 Loss: 0.36545, Train_Acc:87.46%
Epoch [100/300], Step [330/391],                 Loss: 0.36442, Train_Acc:87.50%
Epoch [100/300], Step [340/391],                 Loss: 0.36306, Train_Acc:87.54%
Epoch [100/300], Step [350/391],                 Loss: 0.36357, Train_Acc:87.52%
Epoch [100/300], Step [360/391],                 Loss: 0.36395, Train_Acc:87.50%
Epoch [100/300], Step [370/391],                 Loss: 0.36425, Train_Acc:87.50%
Epoch [100/300], Step [380/391],                 Loss: 0.36444, Train_Acc:87.49%
Epoch [100/300], Step [390/391],                 Loss: 0.36439, Train_Acc:87.51%
Accuary on test images:76.60%
Epoch [101/300], Step [10/391],                 Loss: 0.34236, Train_Acc:88.67%
Epoch [101/300], Step [20/391],                 Loss: 0.35574, Train_Acc:88.05%
Epoch [101/300], Step [30/391],                 Loss: 0.34487, Train_Acc:88.23%
Epoch [101/300], Step [40/391],                 Loss: 0.34809, Train_Acc:88.16%
Epoch [101/300], Step [50/391],                 Loss: 0.35076, Train_Acc:88.08%
Epoch [101/300], Step [60/391],                 Loss: 0.35436, Train_Acc:87.97%
Epoch [101/300], Step [70/391],                 Loss: 0.35483, Train_Acc:87.95%
Epoch [101/300], Step [80/391],                 Loss: 0.35915, Train_Acc:87.73%
Epoch [101/300], Step [90/391],                 Loss: 0.36416, Train_Acc:87.66%
Epoch [101/300], Step [100/391],                 Loss: 0.36334, Train_Acc:87.71%
Epoch [101/300], Step [110/391],                 Loss: 0.36357, Train_Acc:87.67%
Epoch [101/300], Step [120/391],                 Loss: 0.36372, Train_Acc:87.69%
Epoch [101/300], Step [130/391],                 Loss: 0.36404, Train_Acc:87.68%
Epoch [101/300], Step [140/391],                 Loss: 0.36082, Train_Acc:87.71%
Epoch [101/300], Step [150/391],                 Loss: 0.35992, Train_Acc:87.74%
Epoch [101/300], Step [160/391],                 Loss: 0.35771, Train_Acc:87.79%
Epoch [101/300], Step [170/391],                 Loss: 0.35702, Train_Acc:87.80%
Epoch [101/300], Step [180/391],                 Loss: 0.35650, Train_Acc:87.81%
Epoch [101/300], Step [190/391],                 Loss: 0.35540, Train_Acc:87.84%
Epoch [101/300], Step [200/391],                 Loss: 0.35599, Train_Acc:87.82%
Epoch [101/300], Step [210/391],                 Loss: 0.35711, Train_Acc:87.78%
Epoch [101/300], Step [220/391],                 Loss: 0.35805, Train_Acc:87.77%
Epoch [101/300], Step [230/391],                 Loss: 0.35913, Train_Acc:87.72%
Epoch [101/300], Step [240/391],                 Loss: 0.35939, Train_Acc:87.70%
Epoch [101/300], Step [250/391],                 Loss: 0.36013, Train_Acc:87.67%
Epoch [101/300], Step [260/391],                 Loss: 0.36216, Train_Acc:87.63%
Epoch [101/300], Step [270/391],                 Loss: 0.36374, Train_Acc:87.58%
Epoch [101/300], Step [280/391],                 Loss: 0.36560, Train_Acc:87.51%
Epoch [101/300], Step [290/391],                 Loss: 0.36579, Train_Acc:87.51%
Epoch [101/300], Step [300/391],                 Loss: 0.36496, Train_Acc:87.52%
Epoch [101/300], Step [310/391],                 Loss: 0.36392, Train_Acc:87.56%
Epoch [101/300], Step [320/391],                 Loss: 0.36453, Train_Acc:87.55%
Epoch [101/300], Step [330/391],                 Loss: 0.36391, Train_Acc:87.59%
Epoch [101/300], Step [340/391],                 Loss: 0.36252, Train_Acc:87.65%
Epoch [101/300], Step [350/391],                 Loss: 0.36221, Train_Acc:87.66%
Epoch [101/300], Step [360/391],                 Loss: 0.36208, Train_Acc:87.67%
Epoch [101/300], Step [370/391],                 Loss: 0.36239, Train_Acc:87.64%
Epoch [101/300], Step [380/391],                 Loss: 0.36245, Train_Acc:87.64%
Epoch [101/300], Step [390/391],                 Loss: 0.36135, Train_Acc:87.69%
Accuary on test images:80.96%
Epoch [102/300], Step [10/391],                 Loss: 0.34235, Train_Acc:88.75%
Epoch [102/300], Step [20/391],                 Loss: 0.33943, Train_Acc:88.48%
Epoch [102/300], Step [30/391],                 Loss: 0.33166, Train_Acc:88.70%
Epoch [102/300], Step [40/391],                 Loss: 0.33716, Train_Acc:88.61%
Epoch [102/300], Step [50/391],                 Loss: 0.33781, Train_Acc:88.62%
Epoch [102/300], Step [60/391],                 Loss: 0.34394, Train_Acc:88.42%
Epoch [102/300], Step [70/391],                 Loss: 0.34558, Train_Acc:88.43%
Epoch [102/300], Step [80/391],                 Loss: 0.34616, Train_Acc:88.36%
Epoch [102/300], Step [90/391],                 Loss: 0.35007, Train_Acc:88.20%
Epoch [102/300], Step [100/391],                 Loss: 0.35482, Train_Acc:88.00%
Epoch [102/300], Step [110/391],                 Loss: 0.35950, Train_Acc:87.91%
Epoch [102/300], Step [120/391],                 Loss: 0.36258, Train_Acc:87.72%
Epoch [102/300], Step [130/391],                 Loss: 0.36481, Train_Acc:87.67%
Epoch [102/300], Step [140/391],                 Loss: 0.36324, Train_Acc:87.72%
Epoch [102/300], Step [150/391],                 Loss: 0.36371, Train_Acc:87.69%
Epoch [102/300], Step [160/391],                 Loss: 0.36295, Train_Acc:87.76%
Epoch [102/300], Step [170/391],                 Loss: 0.36473, Train_Acc:87.71%
Epoch [102/300], Step [180/391],                 Loss: 0.36642, Train_Acc:87.66%
Epoch [102/300], Step [190/391],                 Loss: 0.36783, Train_Acc:87.56%
Epoch [102/300], Step [200/391],                 Loss: 0.36781, Train_Acc:87.53%
Epoch [102/300], Step [210/391],                 Loss: 0.36677, Train_Acc:87.58%
Epoch [102/300], Step [220/391],                 Loss: 0.36486, Train_Acc:87.64%
Epoch [102/300], Step [230/391],                 Loss: 0.36204, Train_Acc:87.75%
Epoch [102/300], Step [240/391],                 Loss: 0.35984, Train_Acc:87.85%
Epoch [102/300], Step [250/391],                 Loss: 0.35921, Train_Acc:87.87%
Epoch [102/300], Step [260/391],                 Loss: 0.36123, Train_Acc:87.77%
Epoch [102/300], Step [270/391],                 Loss: 0.36148, Train_Acc:87.78%
Epoch [102/300], Step [280/391],                 Loss: 0.36240, Train_Acc:87.73%
Epoch [102/300], Step [290/391],                 Loss: 0.36290, Train_Acc:87.76%
Epoch [102/300], Step [300/391],                 Loss: 0.36334, Train_Acc:87.73%
Epoch [102/300], Step [310/391],                 Loss: 0.36367, Train_Acc:87.71%
Epoch [102/300], Step [320/391],                 Loss: 0.36351, Train_Acc:87.70%
Epoch [102/300], Step [330/391],                 Loss: 0.36332, Train_Acc:87.66%
Epoch [102/300], Step [340/391],                 Loss: 0.36258, Train_Acc:87.70%
Epoch [102/300], Step [350/391],                 Loss: 0.36205, Train_Acc:87.72%
Epoch [102/300], Step [360/391],                 Loss: 0.36237, Train_Acc:87.70%
Epoch [102/300], Step [370/391],                 Loss: 0.36257, Train_Acc:87.69%
Epoch [102/300], Step [380/391],                 Loss: 0.36245, Train_Acc:87.70%
Epoch [102/300], Step [390/391],                 Loss: 0.36181, Train_Acc:87.71%
Accuary on test images:80.54%
Epoch [103/300], Step [10/391],                 Loss: 0.38385, Train_Acc:87.50%
Epoch [103/300], Step [20/391],                 Loss: 0.36594, Train_Acc:87.85%
Epoch [103/300], Step [30/391],                 Loss: 0.36122, Train_Acc:87.99%
Epoch [103/300], Step [40/391],                 Loss: 0.35767, Train_Acc:88.12%
Epoch [103/300], Step [50/391],                 Loss: 0.35552, Train_Acc:88.08%
Epoch [103/300], Step [60/391],                 Loss: 0.35597, Train_Acc:87.92%
Epoch [103/300], Step [70/391],                 Loss: 0.35410, Train_Acc:87.78%
Epoch [103/300], Step [80/391],                 Loss: 0.35388, Train_Acc:87.83%
Epoch [103/300], Step [90/391],                 Loss: 0.36079, Train_Acc:87.58%
Epoch [103/300], Step [100/391],                 Loss: 0.36110, Train_Acc:87.60%
Epoch [103/300], Step [110/391],                 Loss: 0.36357, Train_Acc:87.50%
Epoch [103/300], Step [120/391],                 Loss: 0.36275, Train_Acc:87.53%
Epoch [103/300], Step [130/391],                 Loss: 0.36622, Train_Acc:87.43%
Epoch [103/300], Step [140/391],                 Loss: 0.36641, Train_Acc:87.45%
Epoch [103/300], Step [150/391],                 Loss: 0.36758, Train_Acc:87.45%
Epoch [103/300], Step [160/391],                 Loss: 0.36476, Train_Acc:87.55%
Epoch [103/300], Step [170/391],                 Loss: 0.36453, Train_Acc:87.56%
Epoch [103/300], Step [180/391],                 Loss: 0.36420, Train_Acc:87.56%
Epoch [103/300], Step [190/391],                 Loss: 0.36410, Train_Acc:87.56%
Epoch [103/300], Step [200/391],                 Loss: 0.36436, Train_Acc:87.55%
Epoch [103/300], Step [210/391],                 Loss: 0.36446, Train_Acc:87.56%
Epoch [103/300], Step [220/391],                 Loss: 0.36398, Train_Acc:87.56%
Epoch [103/300], Step [230/391],                 Loss: 0.36190, Train_Acc:87.63%
Epoch [103/300], Step [240/391],                 Loss: 0.36008, Train_Acc:87.71%
Epoch [103/300], Step [250/391],                 Loss: 0.36036, Train_Acc:87.71%
Epoch [103/300], Step [260/391],                 Loss: 0.36170, Train_Acc:87.69%
Epoch [103/300], Step [270/391],                 Loss: 0.36189, Train_Acc:87.67%
Epoch [103/300], Step [280/391],                 Loss: 0.36179, Train_Acc:87.69%
Epoch [103/300], Step [290/391],                 Loss: 0.36230, Train_Acc:87.68%
Epoch [103/300], Step [300/391],                 Loss: 0.36133, Train_Acc:87.69%
Epoch [103/300], Step [310/391],                 Loss: 0.36049, Train_Acc:87.70%
Epoch [103/300], Step [320/391],                 Loss: 0.35993, Train_Acc:87.72%
Epoch [103/300], Step [330/391],                 Loss: 0.35880, Train_Acc:87.77%
Epoch [103/300], Step [340/391],                 Loss: 0.35736, Train_Acc:87.80%
Epoch [103/300], Step [350/391],                 Loss: 0.35683, Train_Acc:87.81%
Epoch [103/300], Step [360/391],                 Loss: 0.35776, Train_Acc:87.76%
Epoch [103/300], Step [370/391],                 Loss: 0.35844, Train_Acc:87.75%
Epoch [103/300], Step [380/391],                 Loss: 0.35848, Train_Acc:87.76%
Epoch [103/300], Step [390/391],                 Loss: 0.35900, Train_Acc:87.74%
Accuary on test images:74.24%
Epoch [104/300], Step [10/391],                 Loss: 0.37669, Train_Acc:87.89%
Epoch [104/300], Step [20/391],                 Loss: 0.34486, Train_Acc:88.36%
Epoch [104/300], Step [30/391],                 Loss: 0.34479, Train_Acc:88.65%
Epoch [104/300], Step [40/391],                 Loss: 0.35465, Train_Acc:88.30%
Epoch [104/300], Step [50/391],                 Loss: 0.34809, Train_Acc:88.41%
Epoch [104/300], Step [60/391],                 Loss: 0.35221, Train_Acc:88.26%
Epoch [104/300], Step [70/391],                 Loss: 0.34863, Train_Acc:88.38%
Epoch [104/300], Step [80/391],                 Loss: 0.35056, Train_Acc:88.20%
Epoch [104/300], Step [90/391],                 Loss: 0.35309, Train_Acc:88.11%
Epoch [104/300], Step [100/391],                 Loss: 0.35296, Train_Acc:88.12%
Epoch [104/300], Step [110/391],                 Loss: 0.35551, Train_Acc:88.02%
Epoch [104/300], Step [120/391],                 Loss: 0.35423, Train_Acc:87.97%
Epoch [104/300], Step [130/391],                 Loss: 0.35505, Train_Acc:87.91%
Epoch [104/300], Step [140/391],                 Loss: 0.35539, Train_Acc:87.93%
Epoch [104/300], Step [150/391],                 Loss: 0.35452, Train_Acc:88.01%
Epoch [104/300], Step [160/391],                 Loss: 0.35331, Train_Acc:88.06%
Epoch [104/300], Step [170/391],                 Loss: 0.35583, Train_Acc:87.95%
Epoch [104/300], Step [180/391],                 Loss: 0.35549, Train_Acc:87.92%
Epoch [104/300], Step [190/391],                 Loss: 0.35557, Train_Acc:87.90%
Epoch [104/300], Step [200/391],                 Loss: 0.35609, Train_Acc:87.88%
Epoch [104/300], Step [210/391],                 Loss: 0.35552, Train_Acc:87.89%
Epoch [104/300], Step [220/391],                 Loss: 0.35611, Train_Acc:87.88%
Epoch [104/300], Step [230/391],                 Loss: 0.35518, Train_Acc:87.93%
Epoch [104/300], Step [240/391],                 Loss: 0.35300, Train_Acc:87.99%
Epoch [104/300], Step [250/391],                 Loss: 0.35232, Train_Acc:88.00%
Epoch [104/300], Step [260/391],                 Loss: 0.35555, Train_Acc:87.88%
Epoch [104/300], Step [270/391],                 Loss: 0.35741, Train_Acc:87.82%
Epoch [104/300], Step [280/391],                 Loss: 0.35819, Train_Acc:87.80%
Epoch [104/300], Step [290/391],                 Loss: 0.35840, Train_Acc:87.80%
Epoch [104/300], Step [300/391],                 Loss: 0.35829, Train_Acc:87.78%
Epoch [104/300], Step [310/391],                 Loss: 0.35893, Train_Acc:87.75%
Epoch [104/300], Step [320/391],                 Loss: 0.35943, Train_Acc:87.75%
Epoch [104/300], Step [330/391],                 Loss: 0.35857, Train_Acc:87.78%
Epoch [104/300], Step [340/391],                 Loss: 0.35773, Train_Acc:87.80%
Epoch [104/300], Step [350/391],                 Loss: 0.35673, Train_Acc:87.83%
Epoch [104/300], Step [360/391],                 Loss: 0.35776, Train_Acc:87.77%
Epoch [104/300], Step [370/391],                 Loss: 0.35905, Train_Acc:87.72%
Epoch [104/300], Step [380/391],                 Loss: 0.35970, Train_Acc:87.71%
Epoch [104/300], Step [390/391],                 Loss: 0.35916, Train_Acc:87.75%
Accuary on test images:81.76%
Epoch [105/300], Step [10/391],                 Loss: 0.34736, Train_Acc:87.66%
Epoch [105/300], Step [20/391],                 Loss: 0.34113, Train_Acc:87.62%
Epoch [105/300], Step [30/391],                 Loss: 0.33485, Train_Acc:88.31%
Epoch [105/300], Step [40/391],                 Loss: 0.33744, Train_Acc:88.42%
Epoch [105/300], Step [50/391],                 Loss: 0.34714, Train_Acc:88.16%
Epoch [105/300], Step [60/391],                 Loss: 0.35215, Train_Acc:87.98%
Epoch [105/300], Step [70/391],                 Loss: 0.35110, Train_Acc:88.07%
Epoch [105/300], Step [80/391],                 Loss: 0.35662, Train_Acc:87.85%
Epoch [105/300], Step [90/391],                 Loss: 0.35934, Train_Acc:87.82%
Epoch [105/300], Step [100/391],                 Loss: 0.35859, Train_Acc:87.79%
Epoch [105/300], Step [110/391],                 Loss: 0.36050, Train_Acc:87.82%
Epoch [105/300], Step [120/391],                 Loss: 0.35792, Train_Acc:87.86%
Epoch [105/300], Step [130/391],                 Loss: 0.36076, Train_Acc:87.85%
Epoch [105/300], Step [140/391],                 Loss: 0.36112, Train_Acc:87.83%
Epoch [105/300], Step [150/391],                 Loss: 0.36224, Train_Acc:87.80%
Epoch [105/300], Step [160/391],                 Loss: 0.36185, Train_Acc:87.78%
Epoch [105/300], Step [170/391],                 Loss: 0.36388, Train_Acc:87.68%
Epoch [105/300], Step [180/391],                 Loss: 0.36581, Train_Acc:87.55%
Epoch [105/300], Step [190/391],                 Loss: 0.36646, Train_Acc:87.49%
Epoch [105/300], Step [200/391],                 Loss: 0.36721, Train_Acc:87.46%
Epoch [105/300], Step [210/391],                 Loss: 0.36643, Train_Acc:87.49%
Epoch [105/300], Step [220/391],                 Loss: 0.36503, Train_Acc:87.55%
Epoch [105/300], Step [230/391],                 Loss: 0.36339, Train_Acc:87.61%
Epoch [105/300], Step [240/391],                 Loss: 0.36230, Train_Acc:87.61%
Epoch [105/300], Step [250/391],                 Loss: 0.36312, Train_Acc:87.57%
Epoch [105/300], Step [260/391],                 Loss: 0.36441, Train_Acc:87.54%
Epoch [105/300], Step [270/391],                 Loss: 0.36477, Train_Acc:87.54%
Epoch [105/300], Step [280/391],                 Loss: 0.36441, Train_Acc:87.54%
Epoch [105/300], Step [290/391],                 Loss: 0.36388, Train_Acc:87.57%
Epoch [105/300], Step [300/391],                 Loss: 0.36392, Train_Acc:87.54%
Epoch [105/300], Step [310/391],                 Loss: 0.36331, Train_Acc:87.55%
Epoch [105/300], Step [320/391],                 Loss: 0.36321, Train_Acc:87.56%
Epoch [105/300], Step [330/391],                 Loss: 0.36197, Train_Acc:87.59%
Epoch [105/300], Step [340/391],                 Loss: 0.36093, Train_Acc:87.63%
Epoch [105/300], Step [350/391],                 Loss: 0.36032, Train_Acc:87.64%
Epoch [105/300], Step [360/391],                 Loss: 0.35999, Train_Acc:87.63%
Epoch [105/300], Step [370/391],                 Loss: 0.36082, Train_Acc:87.60%
Epoch [105/300], Step [380/391],                 Loss: 0.36085, Train_Acc:87.58%
Epoch [105/300], Step [390/391],                 Loss: 0.36021, Train_Acc:87.62%
Accuary on test images:75.90%
Epoch [106/300], Step [10/391],                 Loss: 0.33459, Train_Acc:88.28%
Epoch [106/300], Step [20/391],                 Loss: 0.34331, Train_Acc:88.05%
Epoch [106/300], Step [30/391],                 Loss: 0.34675, Train_Acc:87.76%
Epoch [106/300], Step [40/391],                 Loss: 0.35955, Train_Acc:87.71%
Epoch [106/300], Step [50/391],                 Loss: 0.36953, Train_Acc:87.27%
Epoch [106/300], Step [60/391],                 Loss: 0.37343, Train_Acc:87.06%
Epoch [106/300], Step [70/391],                 Loss: 0.37337, Train_Acc:87.28%
Epoch [106/300], Step [80/391],                 Loss: 0.37228, Train_Acc:87.28%
Epoch [106/300], Step [90/391],                 Loss: 0.37340, Train_Acc:87.23%
Epoch [106/300], Step [100/391],                 Loss: 0.37210, Train_Acc:87.25%
Epoch [106/300], Step [110/391],                 Loss: 0.37314, Train_Acc:87.27%
Epoch [106/300], Step [120/391],                 Loss: 0.37085, Train_Acc:87.38%
Epoch [106/300], Step [130/391],                 Loss: 0.37227, Train_Acc:87.31%
Epoch [106/300], Step [140/391],                 Loss: 0.36988, Train_Acc:87.40%
Epoch [106/300], Step [150/391],                 Loss: 0.37148, Train_Acc:87.32%
Epoch [106/300], Step [160/391],                 Loss: 0.37188, Train_Acc:87.29%
Epoch [106/300], Step [170/391],                 Loss: 0.37036, Train_Acc:87.37%
Epoch [106/300], Step [180/391],                 Loss: 0.36910, Train_Acc:87.41%
Epoch [106/300], Step [190/391],                 Loss: 0.36767, Train_Acc:87.41%
Epoch [106/300], Step [200/391],                 Loss: 0.36916, Train_Acc:87.41%
Epoch [106/300], Step [210/391],                 Loss: 0.36827, Train_Acc:87.41%
Epoch [106/300], Step [220/391],                 Loss: 0.36756, Train_Acc:87.44%
Epoch [106/300], Step [230/391],                 Loss: 0.36597, Train_Acc:87.51%
Epoch [106/300], Step [240/391],                 Loss: 0.36439, Train_Acc:87.54%
Epoch [106/300], Step [250/391],                 Loss: 0.36426, Train_Acc:87.54%
Epoch [106/300], Step [260/391],                 Loss: 0.36589, Train_Acc:87.53%
Epoch [106/300], Step [270/391],                 Loss: 0.36633, Train_Acc:87.52%
Epoch [106/300], Step [280/391],                 Loss: 0.36606, Train_Acc:87.54%
Epoch [106/300], Step [290/391],                 Loss: 0.36601, Train_Acc:87.55%
Epoch [106/300], Step [300/391],                 Loss: 0.36639, Train_Acc:87.53%
Epoch [106/300], Step [310/391],                 Loss: 0.36594, Train_Acc:87.56%
Epoch [106/300], Step [320/391],                 Loss: 0.36594, Train_Acc:87.56%
Epoch [106/300], Step [330/391],                 Loss: 0.36570, Train_Acc:87.59%
Epoch [106/300], Step [340/391],                 Loss: 0.36493, Train_Acc:87.60%
Epoch [106/300], Step [350/391],                 Loss: 0.36513, Train_Acc:87.61%
Epoch [106/300], Step [360/391],                 Loss: 0.36511, Train_Acc:87.60%
Epoch [106/300], Step [370/391],                 Loss: 0.36611, Train_Acc:87.56%
Epoch [106/300], Step [380/391],                 Loss: 0.36578, Train_Acc:87.56%
Epoch [106/300], Step [390/391],                 Loss: 0.36569, Train_Acc:87.58%
Accuary on test images:76.80%
Epoch [107/300], Step [10/391],                 Loss: 0.32014, Train_Acc:88.67%
Epoch [107/300], Step [20/391],                 Loss: 0.33081, Train_Acc:88.32%
Epoch [107/300], Step [30/391],                 Loss: 0.33306, Train_Acc:88.49%
Epoch [107/300], Step [40/391],                 Loss: 0.34523, Train_Acc:88.30%
Epoch [107/300], Step [50/391],                 Loss: 0.35132, Train_Acc:88.12%
Epoch [107/300], Step [60/391],                 Loss: 0.35589, Train_Acc:88.02%
Epoch [107/300], Step [70/391],                 Loss: 0.35251, Train_Acc:88.12%
Epoch [107/300], Step [80/391],                 Loss: 0.35447, Train_Acc:87.99%
Epoch [107/300], Step [90/391],                 Loss: 0.35777, Train_Acc:87.91%
Epoch [107/300], Step [100/391],                 Loss: 0.35575, Train_Acc:88.00%
Epoch [107/300], Step [110/391],                 Loss: 0.35737, Train_Acc:87.94%
Epoch [107/300], Step [120/391],                 Loss: 0.35421, Train_Acc:88.08%
Epoch [107/300], Step [130/391],                 Loss: 0.35795, Train_Acc:87.95%
Epoch [107/300], Step [140/391],                 Loss: 0.35868, Train_Acc:87.89%
Epoch [107/300], Step [150/391],                 Loss: 0.35892, Train_Acc:87.90%
Epoch [107/300], Step [160/391],                 Loss: 0.35896, Train_Acc:87.90%
Epoch [107/300], Step [170/391],                 Loss: 0.35978, Train_Acc:87.90%
Epoch [107/300], Step [180/391],                 Loss: 0.35784, Train_Acc:87.93%
Epoch [107/300], Step [190/391],                 Loss: 0.35732, Train_Acc:87.97%
Epoch [107/300], Step [200/391],                 Loss: 0.35764, Train_Acc:87.96%
Epoch [107/300], Step [210/391],                 Loss: 0.35664, Train_Acc:87.98%
Epoch [107/300], Step [220/391],                 Loss: 0.35514, Train_Acc:88.01%
Epoch [107/300], Step [230/391],                 Loss: 0.35353, Train_Acc:88.04%
Epoch [107/300], Step [240/391],                 Loss: 0.35148, Train_Acc:88.12%
Epoch [107/300], Step [250/391],                 Loss: 0.35328, Train_Acc:88.03%
Epoch [107/300], Step [260/391],                 Loss: 0.35621, Train_Acc:87.91%
Epoch [107/300], Step [270/391],                 Loss: 0.35784, Train_Acc:87.86%
Epoch [107/300], Step [280/391],                 Loss: 0.35899, Train_Acc:87.79%
Epoch [107/300], Step [290/391],                 Loss: 0.35873, Train_Acc:87.81%
Epoch [107/300], Step [300/391],                 Loss: 0.35845, Train_Acc:87.81%
Epoch [107/300], Step [310/391],                 Loss: 0.35838, Train_Acc:87.78%
Epoch [107/300], Step [320/391],                 Loss: 0.35886, Train_Acc:87.75%
Epoch [107/300], Step [330/391],                 Loss: 0.35864, Train_Acc:87.77%
Epoch [107/300], Step [340/391],                 Loss: 0.35775, Train_Acc:87.80%
Epoch [107/300], Step [350/391],                 Loss: 0.35669, Train_Acc:87.84%
Epoch [107/300], Step [360/391],                 Loss: 0.35801, Train_Acc:87.80%
Epoch [107/300], Step [370/391],                 Loss: 0.35854, Train_Acc:87.79%
Epoch [107/300], Step [380/391],                 Loss: 0.35846, Train_Acc:87.82%
Epoch [107/300], Step [390/391],                 Loss: 0.35782, Train_Acc:87.83%
Accuary on test images:72.24%
Epoch [108/300], Step [10/391],                 Loss: 0.35804, Train_Acc:87.58%
Epoch [108/300], Step [20/391],                 Loss: 0.33110, Train_Acc:88.75%
Epoch [108/300], Step [30/391],                 Loss: 0.31566, Train_Acc:89.38%
Epoch [108/300], Step [40/391],                 Loss: 0.32445, Train_Acc:89.16%
Epoch [108/300], Step [50/391],                 Loss: 0.33639, Train_Acc:88.62%
Epoch [108/300], Step [60/391],                 Loss: 0.34534, Train_Acc:88.36%
Epoch [108/300], Step [70/391],                 Loss: 0.35289, Train_Acc:88.09%
Epoch [108/300], Step [80/391],                 Loss: 0.35636, Train_Acc:87.91%
Epoch [108/300], Step [90/391],                 Loss: 0.36498, Train_Acc:87.59%
Epoch [108/300], Step [100/391],                 Loss: 0.36397, Train_Acc:87.59%
Epoch [108/300], Step [110/391],                 Loss: 0.36732, Train_Acc:87.47%
Epoch [108/300], Step [120/391],                 Loss: 0.36673, Train_Acc:87.45%
Epoch [108/300], Step [130/391],                 Loss: 0.36938, Train_Acc:87.45%
Epoch [108/300], Step [140/391],                 Loss: 0.36845, Train_Acc:87.52%
Epoch [108/300], Step [150/391],                 Loss: 0.36774, Train_Acc:87.55%
Epoch [108/300], Step [160/391],                 Loss: 0.36477, Train_Acc:87.61%
Epoch [108/300], Step [170/391],                 Loss: 0.36501, Train_Acc:87.60%
Epoch [108/300], Step [180/391],                 Loss: 0.36438, Train_Acc:87.60%
Epoch [108/300], Step [190/391],                 Loss: 0.36393, Train_Acc:87.64%
Epoch [108/300], Step [200/391],                 Loss: 0.36468, Train_Acc:87.56%
Epoch [108/300], Step [210/391],                 Loss: 0.36510, Train_Acc:87.55%
Epoch [108/300], Step [220/391],                 Loss: 0.36514, Train_Acc:87.57%
Epoch [108/300], Step [230/391],                 Loss: 0.36311, Train_Acc:87.62%
Epoch [108/300], Step [240/391],                 Loss: 0.36181, Train_Acc:87.65%
Epoch [108/300], Step [250/391],                 Loss: 0.36221, Train_Acc:87.59%
Epoch [108/300], Step [260/391],                 Loss: 0.36468, Train_Acc:87.53%
Epoch [108/300], Step [270/391],                 Loss: 0.36553, Train_Acc:87.51%
Epoch [108/300], Step [280/391],                 Loss: 0.36456, Train_Acc:87.56%
Epoch [108/300], Step [290/391],                 Loss: 0.36418, Train_Acc:87.57%
Epoch [108/300], Step [300/391],                 Loss: 0.36411, Train_Acc:87.59%
Epoch [108/300], Step [310/391],                 Loss: 0.36365, Train_Acc:87.63%
Epoch [108/300], Step [320/391],                 Loss: 0.36334, Train_Acc:87.63%
Epoch [108/300], Step [330/391],                 Loss: 0.36209, Train_Acc:87.68%
Epoch [108/300], Step [340/391],                 Loss: 0.36091, Train_Acc:87.73%
Epoch [108/300], Step [350/391],                 Loss: 0.36073, Train_Acc:87.76%
Epoch [108/300], Step [360/391],                 Loss: 0.36046, Train_Acc:87.74%
Epoch [108/300], Step [370/391],                 Loss: 0.36189, Train_Acc:87.69%
Epoch [108/300], Step [380/391],                 Loss: 0.36242, Train_Acc:87.66%
Epoch [108/300], Step [390/391],                 Loss: 0.36214, Train_Acc:87.68%
Accuary on test images:77.84%
Epoch [109/300], Step [10/391],                 Loss: 0.38146, Train_Acc:86.33%
Epoch [109/300], Step [20/391],                 Loss: 0.37276, Train_Acc:86.68%
Epoch [109/300], Step [30/391],                 Loss: 0.35556, Train_Acc:87.86%
Epoch [109/300], Step [40/391],                 Loss: 0.35641, Train_Acc:87.81%
Epoch [109/300], Step [50/391],                 Loss: 0.35544, Train_Acc:87.92%
Epoch [109/300], Step [60/391],                 Loss: 0.36552, Train_Acc:87.46%
Epoch [109/300], Step [70/391],                 Loss: 0.36410, Train_Acc:87.54%
Epoch [109/300], Step [80/391],                 Loss: 0.36844, Train_Acc:87.34%
Epoch [109/300], Step [90/391],                 Loss: 0.36956, Train_Acc:87.31%
Epoch [109/300], Step [100/391],                 Loss: 0.36891, Train_Acc:87.29%
Epoch [109/300], Step [110/391],                 Loss: 0.36848, Train_Acc:87.26%
Epoch [109/300], Step [120/391],                 Loss: 0.36870, Train_Acc:87.25%
Epoch [109/300], Step [130/391],                 Loss: 0.37354, Train_Acc:87.10%
Epoch [109/300], Step [140/391],                 Loss: 0.37449, Train_Acc:87.06%
Epoch [109/300], Step [150/391],                 Loss: 0.37449, Train_Acc:87.07%
Epoch [109/300], Step [160/391],                 Loss: 0.37392, Train_Acc:87.05%
Epoch [109/300], Step [170/391],                 Loss: 0.37332, Train_Acc:87.09%
Epoch [109/300], Step [180/391],                 Loss: 0.37134, Train_Acc:87.19%
Epoch [109/300], Step [190/391],                 Loss: 0.36916, Train_Acc:87.27%
Epoch [109/300], Step [200/391],                 Loss: 0.36779, Train_Acc:87.37%
Epoch [109/300], Step [210/391],                 Loss: 0.36720, Train_Acc:87.38%
Epoch [109/300], Step [220/391],                 Loss: 0.36728, Train_Acc:87.43%
Epoch [109/300], Step [230/391],                 Loss: 0.36628, Train_Acc:87.46%
Epoch [109/300], Step [240/391],                 Loss: 0.36321, Train_Acc:87.56%
Epoch [109/300], Step [250/391],                 Loss: 0.36233, Train_Acc:87.57%
Epoch [109/300], Step [260/391],                 Loss: 0.36408, Train_Acc:87.53%
Epoch [109/300], Step [270/391],                 Loss: 0.36563, Train_Acc:87.49%
Epoch [109/300], Step [280/391],                 Loss: 0.36557, Train_Acc:87.47%
Epoch [109/300], Step [290/391],                 Loss: 0.36605, Train_Acc:87.45%
Epoch [109/300], Step [300/391],                 Loss: 0.36627, Train_Acc:87.45%
Epoch [109/300], Step [310/391],                 Loss: 0.36607, Train_Acc:87.46%
Epoch [109/300], Step [320/391],                 Loss: 0.36591, Train_Acc:87.49%
Epoch [109/300], Step [330/391],                 Loss: 0.36526, Train_Acc:87.50%
Epoch [109/300], Step [340/391],                 Loss: 0.36443, Train_Acc:87.54%
Epoch [109/300], Step [350/391],                 Loss: 0.36484, Train_Acc:87.51%
Epoch [109/300], Step [360/391],                 Loss: 0.36425, Train_Acc:87.53%
Epoch [109/300], Step [370/391],                 Loss: 0.36447, Train_Acc:87.51%
Epoch [109/300], Step [380/391],                 Loss: 0.36540, Train_Acc:87.49%
Epoch [109/300], Step [390/391],                 Loss: 0.36409, Train_Acc:87.52%
Accuary on test images:78.84%
Epoch [110/300], Step [10/391],                 Loss: 0.35350, Train_Acc:87.81%
Epoch [110/300], Step [20/391],                 Loss: 0.36530, Train_Acc:87.62%
Epoch [110/300], Step [30/391],                 Loss: 0.36362, Train_Acc:87.58%
Epoch [110/300], Step [40/391],                 Loss: 0.35690, Train_Acc:87.85%
Epoch [110/300], Step [50/391],                 Loss: 0.35949, Train_Acc:87.83%
Epoch [110/300], Step [60/391],                 Loss: 0.36521, Train_Acc:87.57%
Epoch [110/300], Step [70/391],                 Loss: 0.36451, Train_Acc:87.54%
Epoch [110/300], Step [80/391],                 Loss: 0.36967, Train_Acc:87.42%
Epoch [110/300], Step [90/391],                 Loss: 0.37292, Train_Acc:87.38%
Epoch [110/300], Step [100/391],                 Loss: 0.36685, Train_Acc:87.59%
Epoch [110/300], Step [110/391],                 Loss: 0.36891, Train_Acc:87.51%
Epoch [110/300], Step [120/391],                 Loss: 0.36810, Train_Acc:87.47%
Epoch [110/300], Step [130/391],                 Loss: 0.36767, Train_Acc:87.51%
Epoch [110/300], Step [140/391],                 Loss: 0.36518, Train_Acc:87.59%
Epoch [110/300], Step [150/391],                 Loss: 0.36375, Train_Acc:87.59%
Epoch [110/300], Step [160/391],                 Loss: 0.36264, Train_Acc:87.65%
Epoch [110/300], Step [170/391],                 Loss: 0.36264, Train_Acc:87.61%
Epoch [110/300], Step [180/391],                 Loss: 0.36385, Train_Acc:87.62%
Epoch [110/300], Step [190/391],                 Loss: 0.36469, Train_Acc:87.59%
Epoch [110/300], Step [200/391],                 Loss: 0.36452, Train_Acc:87.62%
Epoch [110/300], Step [210/391],                 Loss: 0.36407, Train_Acc:87.65%
Epoch [110/300], Step [220/391],                 Loss: 0.36318, Train_Acc:87.69%
Epoch [110/300], Step [230/391],                 Loss: 0.36108, Train_Acc:87.74%
Epoch [110/300], Step [240/391],                 Loss: 0.35883, Train_Acc:87.85%
Epoch [110/300], Step [250/391],                 Loss: 0.35805, Train_Acc:87.88%
Epoch [110/300], Step [260/391],                 Loss: 0.36086, Train_Acc:87.78%
Epoch [110/300], Step [270/391],                 Loss: 0.36131, Train_Acc:87.77%
Epoch [110/300], Step [280/391],                 Loss: 0.35973, Train_Acc:87.87%
Epoch [110/300], Step [290/391],                 Loss: 0.35893, Train_Acc:87.91%
Epoch [110/300], Step [300/391],                 Loss: 0.35777, Train_Acc:87.97%
Epoch [110/300], Step [310/391],                 Loss: 0.35698, Train_Acc:87.98%
Epoch [110/300], Step [320/391],                 Loss: 0.35632, Train_Acc:88.00%
Epoch [110/300], Step [330/391],                 Loss: 0.35587, Train_Acc:88.00%
Epoch [110/300], Step [340/391],                 Loss: 0.35458, Train_Acc:88.03%
Epoch [110/300], Step [350/391],                 Loss: 0.35449, Train_Acc:88.02%
Epoch [110/300], Step [360/391],                 Loss: 0.35498, Train_Acc:87.98%
Epoch [110/300], Step [370/391],                 Loss: 0.35593, Train_Acc:87.93%
Epoch [110/300], Step [380/391],                 Loss: 0.35627, Train_Acc:87.92%
Epoch [110/300], Step [390/391],                 Loss: 0.35605, Train_Acc:87.94%
Accuary on test images:77.54%
Epoch [111/300], Step [10/391],                 Loss: 0.32111, Train_Acc:88.20%
Epoch [111/300], Step [20/391],                 Loss: 0.33145, Train_Acc:88.01%
Epoch [111/300], Step [30/391],                 Loss: 0.33556, Train_Acc:87.92%
Epoch [111/300], Step [40/391],                 Loss: 0.33955, Train_Acc:87.93%
Epoch [111/300], Step [50/391],                 Loss: 0.33832, Train_Acc:88.03%
Epoch [111/300], Step [60/391],                 Loss: 0.34722, Train_Acc:87.62%
Epoch [111/300], Step [70/391],                 Loss: 0.35184, Train_Acc:87.68%
Epoch [111/300], Step [80/391],                 Loss: 0.35504, Train_Acc:87.59%
Epoch [111/300], Step [90/391],                 Loss: 0.35550, Train_Acc:87.51%
Epoch [111/300], Step [100/391],                 Loss: 0.35609, Train_Acc:87.38%
Epoch [111/300], Step [110/391],                 Loss: 0.35762, Train_Acc:87.27%
Epoch [111/300], Step [120/391],                 Loss: 0.35940, Train_Acc:87.21%
Epoch [111/300], Step [130/391],                 Loss: 0.36227, Train_Acc:87.17%
Epoch [111/300], Step [140/391],                 Loss: 0.36074, Train_Acc:87.28%
Epoch [111/300], Step [150/391],                 Loss: 0.36156, Train_Acc:87.26%
Epoch [111/300], Step [160/391],                 Loss: 0.36049, Train_Acc:87.33%
Epoch [111/300], Step [170/391],                 Loss: 0.36291, Train_Acc:87.24%
Epoch [111/300], Step [180/391],                 Loss: 0.36424, Train_Acc:87.22%
Epoch [111/300], Step [190/391],                 Loss: 0.36590, Train_Acc:87.11%
Epoch [111/300], Step [200/391],                 Loss: 0.36721, Train_Acc:87.12%
Epoch [111/300], Step [210/391],                 Loss: 0.36823, Train_Acc:87.16%
Epoch [111/300], Step [220/391],                 Loss: 0.36729, Train_Acc:87.22%
Epoch [111/300], Step [230/391],                 Loss: 0.36410, Train_Acc:87.33%
Epoch [111/300], Step [240/391],                 Loss: 0.36266, Train_Acc:87.38%
Epoch [111/300], Step [250/391],                 Loss: 0.36288, Train_Acc:87.34%
Epoch [111/300], Step [260/391],                 Loss: 0.36489, Train_Acc:87.29%
Epoch [111/300], Step [270/391],                 Loss: 0.36634, Train_Acc:87.26%
Epoch [111/300], Step [280/391],                 Loss: 0.36629, Train_Acc:87.26%
Epoch [111/300], Step [290/391],                 Loss: 0.36686, Train_Acc:87.26%
Epoch [111/300], Step [300/391],                 Loss: 0.36700, Train_Acc:87.26%
Epoch [111/300], Step [310/391],                 Loss: 0.36506, Train_Acc:87.34%
Epoch [111/300], Step [320/391],                 Loss: 0.36516, Train_Acc:87.36%
Epoch [111/300], Step [330/391],                 Loss: 0.36417, Train_Acc:87.40%
Epoch [111/300], Step [340/391],                 Loss: 0.36272, Train_Acc:87.44%
Epoch [111/300], Step [350/391],                 Loss: 0.36225, Train_Acc:87.48%
Epoch [111/300], Step [360/391],                 Loss: 0.36217, Train_Acc:87.47%
Epoch [111/300], Step [370/391],                 Loss: 0.36237, Train_Acc:87.46%
Epoch [111/300], Step [380/391],                 Loss: 0.36234, Train_Acc:87.47%
Epoch [111/300], Step [390/391],                 Loss: 0.36201, Train_Acc:87.52%
Accuary on test images:74.16%
Epoch [112/300], Step [10/391],                 Loss: 0.34442, Train_Acc:88.20%
Epoch [112/300], Step [20/391],                 Loss: 0.35280, Train_Acc:88.48%
Epoch [112/300], Step [30/391],                 Loss: 0.34381, Train_Acc:88.80%
Epoch [112/300], Step [40/391],                 Loss: 0.33831, Train_Acc:88.98%
Epoch [112/300], Step [50/391],                 Loss: 0.34824, Train_Acc:88.61%
Epoch [112/300], Step [60/391],                 Loss: 0.35680, Train_Acc:88.29%
Epoch [112/300], Step [70/391],                 Loss: 0.35333, Train_Acc:88.37%
Epoch [112/300], Step [80/391],                 Loss: 0.35554, Train_Acc:88.32%
Epoch [112/300], Step [90/391],                 Loss: 0.35961, Train_Acc:88.10%
Epoch [112/300], Step [100/391],                 Loss: 0.35730, Train_Acc:88.05%
Epoch [112/300], Step [110/391],                 Loss: 0.35942, Train_Acc:88.00%
Epoch [112/300], Step [120/391],                 Loss: 0.36071, Train_Acc:87.88%
Epoch [112/300], Step [130/391],                 Loss: 0.36384, Train_Acc:87.85%
Epoch [112/300], Step [140/391],                 Loss: 0.36196, Train_Acc:87.91%
Epoch [112/300], Step [150/391],                 Loss: 0.36125, Train_Acc:87.95%
Epoch [112/300], Step [160/391],                 Loss: 0.35837, Train_Acc:88.06%
Epoch [112/300], Step [170/391],                 Loss: 0.35818, Train_Acc:88.01%
Epoch [112/300], Step [180/391],                 Loss: 0.35922, Train_Acc:87.99%
Epoch [112/300], Step [190/391],                 Loss: 0.36075, Train_Acc:87.94%
Epoch [112/300], Step [200/391],                 Loss: 0.36108, Train_Acc:87.90%
Epoch [112/300], Step [210/391],                 Loss: 0.35965, Train_Acc:87.94%
Epoch [112/300], Step [220/391],                 Loss: 0.35921, Train_Acc:87.95%
Epoch [112/300], Step [230/391],                 Loss: 0.35823, Train_Acc:87.99%
Epoch [112/300], Step [240/391],                 Loss: 0.35691, Train_Acc:88.03%
Epoch [112/300], Step [250/391],                 Loss: 0.35602, Train_Acc:88.04%
Epoch [112/300], Step [260/391],                 Loss: 0.35811, Train_Acc:87.94%
Epoch [112/300], Step [270/391],                 Loss: 0.35823, Train_Acc:87.93%
Epoch [112/300], Step [280/391],                 Loss: 0.35759, Train_Acc:87.94%
Epoch [112/300], Step [290/391],                 Loss: 0.35799, Train_Acc:87.96%
Epoch [112/300], Step [300/391],                 Loss: 0.35810, Train_Acc:87.91%
Epoch [112/300], Step [310/391],                 Loss: 0.35783, Train_Acc:87.91%
Epoch [112/300], Step [320/391],                 Loss: 0.35834, Train_Acc:87.89%
Epoch [112/300], Step [330/391],                 Loss: 0.35730, Train_Acc:87.93%
Epoch [112/300], Step [340/391],                 Loss: 0.35640, Train_Acc:87.95%
Epoch [112/300], Step [350/391],                 Loss: 0.35688, Train_Acc:87.92%
Epoch [112/300], Step [360/391],                 Loss: 0.35633, Train_Acc:87.93%
Epoch [112/300], Step [370/391],                 Loss: 0.35648, Train_Acc:87.93%
Epoch [112/300], Step [380/391],                 Loss: 0.35606, Train_Acc:87.94%
Epoch [112/300], Step [390/391],                 Loss: 0.35607, Train_Acc:87.93%
Accuary on test images:69.04%
Epoch [113/300], Step [10/391],                 Loss: 0.36923, Train_Acc:87.27%
Epoch [113/300], Step [20/391],                 Loss: 0.36897, Train_Acc:87.62%
Epoch [113/300], Step [30/391],                 Loss: 0.35825, Train_Acc:87.76%
Epoch [113/300], Step [40/391],                 Loss: 0.36487, Train_Acc:87.70%
Epoch [113/300], Step [50/391],                 Loss: 0.36338, Train_Acc:87.86%
Epoch [113/300], Step [60/391],                 Loss: 0.36649, Train_Acc:87.60%
Epoch [113/300], Step [70/391],                 Loss: 0.36181, Train_Acc:87.82%
Epoch [113/300], Step [80/391],                 Loss: 0.36457, Train_Acc:87.61%
Epoch [113/300], Step [90/391],                 Loss: 0.36956, Train_Acc:87.54%
Epoch [113/300], Step [100/391],                 Loss: 0.36585, Train_Acc:87.71%
Epoch [113/300], Step [110/391],                 Loss: 0.36216, Train_Acc:87.78%
Epoch [113/300], Step [120/391],                 Loss: 0.36047, Train_Acc:87.83%
Epoch [113/300], Step [130/391],                 Loss: 0.36463, Train_Acc:87.69%
Epoch [113/300], Step [140/391],                 Loss: 0.36616, Train_Acc:87.61%
Epoch [113/300], Step [150/391],                 Loss: 0.36488, Train_Acc:87.69%
Epoch [113/300], Step [160/391],                 Loss: 0.36182, Train_Acc:87.82%
Epoch [113/300], Step [170/391],                 Loss: 0.36079, Train_Acc:87.82%
Epoch [113/300], Step [180/391],                 Loss: 0.36181, Train_Acc:87.75%
Epoch [113/300], Step [190/391],                 Loss: 0.36305, Train_Acc:87.69%
Epoch [113/300], Step [200/391],                 Loss: 0.36423, Train_Acc:87.60%
Epoch [113/300], Step [210/391],                 Loss: 0.36619, Train_Acc:87.51%
Epoch [113/300], Step [220/391],                 Loss: 0.36580, Train_Acc:87.56%
Epoch [113/300], Step [230/391],                 Loss: 0.36420, Train_Acc:87.55%
Epoch [113/300], Step [240/391],                 Loss: 0.36260, Train_Acc:87.57%
Epoch [113/300], Step [250/391],                 Loss: 0.36129, Train_Acc:87.63%
Epoch [113/300], Step [260/391],                 Loss: 0.36276, Train_Acc:87.63%
Epoch [113/300], Step [270/391],                 Loss: 0.36303, Train_Acc:87.62%
Epoch [113/300], Step [280/391],                 Loss: 0.36303, Train_Acc:87.63%
Epoch [113/300], Step [290/391],                 Loss: 0.36326, Train_Acc:87.62%
Epoch [113/300], Step [300/391],                 Loss: 0.36282, Train_Acc:87.61%
Epoch [113/300], Step [310/391],                 Loss: 0.36320, Train_Acc:87.59%
Epoch [113/300], Step [320/391],                 Loss: 0.36298, Train_Acc:87.59%
Epoch [113/300], Step [330/391],                 Loss: 0.36168, Train_Acc:87.67%
Epoch [113/300], Step [340/391],                 Loss: 0.36114, Train_Acc:87.68%
Epoch [113/300], Step [350/391],                 Loss: 0.36079, Train_Acc:87.69%
Epoch [113/300], Step [360/391],                 Loss: 0.36052, Train_Acc:87.68%
Epoch [113/300], Step [370/391],                 Loss: 0.36099, Train_Acc:87.68%
Epoch [113/300], Step [380/391],                 Loss: 0.36079, Train_Acc:87.68%
Epoch [113/300], Step [390/391],                 Loss: 0.36037, Train_Acc:87.73%
Accuary on test images:75.90%
Epoch [114/300], Step [10/391],                 Loss: 0.34184, Train_Acc:88.44%
Epoch [114/300], Step [20/391],                 Loss: 0.32075, Train_Acc:89.10%
Epoch [114/300], Step [30/391],                 Loss: 0.32468, Train_Acc:88.91%
Epoch [114/300], Step [40/391],                 Loss: 0.34000, Train_Acc:88.54%
Epoch [114/300], Step [50/391],                 Loss: 0.33793, Train_Acc:88.64%
Epoch [114/300], Step [60/391],                 Loss: 0.34249, Train_Acc:88.40%
Epoch [114/300], Step [70/391],                 Loss: 0.34279, Train_Acc:88.33%
Epoch [114/300], Step [80/391],                 Loss: 0.34663, Train_Acc:88.19%
Epoch [114/300], Step [90/391],                 Loss: 0.35298, Train_Acc:88.01%
Epoch [114/300], Step [100/391],                 Loss: 0.35169, Train_Acc:87.93%
Epoch [114/300], Step [110/391],                 Loss: 0.35448, Train_Acc:87.84%
Epoch [114/300], Step [120/391],                 Loss: 0.35752, Train_Acc:87.79%
Epoch [114/300], Step [130/391],                 Loss: 0.36317, Train_Acc:87.63%
Epoch [114/300], Step [140/391],                 Loss: 0.36305, Train_Acc:87.62%
Epoch [114/300], Step [150/391],                 Loss: 0.36404, Train_Acc:87.58%
Epoch [114/300], Step [160/391],                 Loss: 0.36195, Train_Acc:87.69%
Epoch [114/300], Step [170/391],                 Loss: 0.36125, Train_Acc:87.69%
Epoch [114/300], Step [180/391],                 Loss: 0.35977, Train_Acc:87.72%
Epoch [114/300], Step [190/391],                 Loss: 0.35785, Train_Acc:87.78%
Epoch [114/300], Step [200/391],                 Loss: 0.35618, Train_Acc:87.81%
Epoch [114/300], Step [210/391],                 Loss: 0.35507, Train_Acc:87.81%
Epoch [114/300], Step [220/391],                 Loss: 0.35433, Train_Acc:87.85%
Epoch [114/300], Step [230/391],                 Loss: 0.35284, Train_Acc:87.91%
Epoch [114/300], Step [240/391],                 Loss: 0.35205, Train_Acc:87.92%
Epoch [114/300], Step [250/391],                 Loss: 0.35408, Train_Acc:87.82%
Epoch [114/300], Step [260/391],                 Loss: 0.35590, Train_Acc:87.79%
Epoch [114/300], Step [270/391],                 Loss: 0.35723, Train_Acc:87.75%
Epoch [114/300], Step [280/391],                 Loss: 0.35811, Train_Acc:87.73%
Epoch [114/300], Step [290/391],                 Loss: 0.35707, Train_Acc:87.74%
Epoch [114/300], Step [300/391],                 Loss: 0.35712, Train_Acc:87.76%
Epoch [114/300], Step [310/391],                 Loss: 0.35615, Train_Acc:87.79%
Epoch [114/300], Step [320/391],                 Loss: 0.35676, Train_Acc:87.77%
Epoch [114/300], Step [330/391],                 Loss: 0.35591, Train_Acc:87.83%
Epoch [114/300], Step [340/391],                 Loss: 0.35544, Train_Acc:87.86%
Epoch [114/300], Step [350/391],                 Loss: 0.35447, Train_Acc:87.89%
Epoch [114/300], Step [360/391],                 Loss: 0.35395, Train_Acc:87.89%
Epoch [114/300], Step [370/391],                 Loss: 0.35419, Train_Acc:87.89%
Epoch [114/300], Step [380/391],                 Loss: 0.35391, Train_Acc:87.90%
Epoch [114/300], Step [390/391],                 Loss: 0.35355, Train_Acc:87.89%
Accuary on test images:74.90%
Epoch [115/300], Step [10/391],                 Loss: 0.33372, Train_Acc:89.06%
Epoch [115/300], Step [20/391],                 Loss: 0.33618, Train_Acc:89.10%
Epoch [115/300], Step [30/391],                 Loss: 0.33319, Train_Acc:88.91%
Epoch [115/300], Step [40/391],                 Loss: 0.34593, Train_Acc:88.61%
Epoch [115/300], Step [50/391],                 Loss: 0.34876, Train_Acc:88.58%
Epoch [115/300], Step [60/391],                 Loss: 0.35464, Train_Acc:88.19%
Epoch [115/300], Step [70/391],                 Loss: 0.35014, Train_Acc:88.35%
Epoch [115/300], Step [80/391],                 Loss: 0.34997, Train_Acc:88.37%
Epoch [115/300], Step [90/391],                 Loss: 0.35297, Train_Acc:88.31%
Epoch [115/300], Step [100/391],                 Loss: 0.34912, Train_Acc:88.41%
Epoch [115/300], Step [110/391],                 Loss: 0.35273, Train_Acc:88.30%
Epoch [115/300], Step [120/391],                 Loss: 0.35390, Train_Acc:88.26%
Epoch [115/300], Step [130/391],                 Loss: 0.35726, Train_Acc:88.11%
Epoch [115/300], Step [140/391],                 Loss: 0.35715, Train_Acc:88.10%
Epoch [115/300], Step [150/391],                 Loss: 0.35823, Train_Acc:88.04%
Epoch [115/300], Step [160/391],                 Loss: 0.35577, Train_Acc:88.11%
Epoch [115/300], Step [170/391],                 Loss: 0.35467, Train_Acc:88.10%
Epoch [115/300], Step [180/391],                 Loss: 0.35454, Train_Acc:88.11%
Epoch [115/300], Step [190/391],                 Loss: 0.35438, Train_Acc:88.11%
Epoch [115/300], Step [200/391],                 Loss: 0.35385, Train_Acc:88.10%
Epoch [115/300], Step [210/391],                 Loss: 0.35440, Train_Acc:88.09%
Epoch [115/300], Step [220/391],                 Loss: 0.35373, Train_Acc:88.10%
Epoch [115/300], Step [230/391],                 Loss: 0.35281, Train_Acc:88.09%
Epoch [115/300], Step [240/391],                 Loss: 0.35120, Train_Acc:88.17%
Epoch [115/300], Step [250/391],                 Loss: 0.35126, Train_Acc:88.17%
Epoch [115/300], Step [260/391],                 Loss: 0.35286, Train_Acc:88.09%
Epoch [115/300], Step [270/391],                 Loss: 0.35337, Train_Acc:88.11%
Epoch [115/300], Step [280/391],                 Loss: 0.35324, Train_Acc:88.12%
Epoch [115/300], Step [290/391],                 Loss: 0.35330, Train_Acc:88.13%
Epoch [115/300], Step [300/391],                 Loss: 0.35352, Train_Acc:88.11%
Epoch [115/300], Step [310/391],                 Loss: 0.35290, Train_Acc:88.14%
Epoch [115/300], Step [320/391],                 Loss: 0.35364, Train_Acc:88.10%
Epoch [115/300], Step [330/391],                 Loss: 0.35322, Train_Acc:88.14%
Epoch [115/300], Step [340/391],                 Loss: 0.35319, Train_Acc:88.14%
Epoch [115/300], Step [350/391],                 Loss: 0.35438, Train_Acc:88.09%
Epoch [115/300], Step [360/391],                 Loss: 0.35554, Train_Acc:88.00%
Epoch [115/300], Step [370/391],                 Loss: 0.35682, Train_Acc:87.94%
Epoch [115/300], Step [380/391],                 Loss: 0.35670, Train_Acc:87.95%
Epoch [115/300], Step [390/391],                 Loss: 0.35592, Train_Acc:88.00%
Accuary on test images:77.98%
Epoch [116/300], Step [10/391],                 Loss: 0.32715, Train_Acc:89.84%
Epoch [116/300], Step [20/391],                 Loss: 0.33201, Train_Acc:89.73%
Epoch [116/300], Step [30/391],                 Loss: 0.34118, Train_Acc:88.65%
Epoch [116/300], Step [40/391],                 Loss: 0.34715, Train_Acc:88.46%
Epoch [116/300], Step [50/391],                 Loss: 0.35004, Train_Acc:88.20%
Epoch [116/300], Step [60/391],                 Loss: 0.35327, Train_Acc:88.27%
Epoch [116/300], Step [70/391],                 Loss: 0.35229, Train_Acc:88.24%
Epoch [116/300], Step [80/391],                 Loss: 0.35096, Train_Acc:88.17%
Epoch [116/300], Step [90/391],                 Loss: 0.35424, Train_Acc:88.05%
Epoch [116/300], Step [100/391],                 Loss: 0.35314, Train_Acc:88.02%
Epoch [116/300], Step [110/391],                 Loss: 0.35886, Train_Acc:87.90%
Epoch [116/300], Step [120/391],                 Loss: 0.36057, Train_Acc:87.82%
Epoch [116/300], Step [130/391],                 Loss: 0.36025, Train_Acc:87.85%
Epoch [116/300], Step [140/391],                 Loss: 0.35960, Train_Acc:87.85%
Epoch [116/300], Step [150/391],                 Loss: 0.36119, Train_Acc:87.79%
Epoch [116/300], Step [160/391],                 Loss: 0.36222, Train_Acc:87.71%
Epoch [116/300], Step [170/391],                 Loss: 0.36348, Train_Acc:87.71%
Epoch [116/300], Step [180/391],                 Loss: 0.36056, Train_Acc:87.78%
Epoch [116/300], Step [190/391],                 Loss: 0.35969, Train_Acc:87.82%
Epoch [116/300], Step [200/391],                 Loss: 0.35995, Train_Acc:87.78%
Epoch [116/300], Step [210/391],                 Loss: 0.36074, Train_Acc:87.77%
Epoch [116/300], Step [220/391],                 Loss: 0.35971, Train_Acc:87.82%
Epoch [116/300], Step [230/391],                 Loss: 0.35870, Train_Acc:87.83%
Epoch [116/300], Step [240/391],                 Loss: 0.35771, Train_Acc:87.89%
Epoch [116/300], Step [250/391],                 Loss: 0.35746, Train_Acc:87.89%
Epoch [116/300], Step [260/391],                 Loss: 0.35939, Train_Acc:87.81%
Epoch [116/300], Step [270/391],                 Loss: 0.35936, Train_Acc:87.81%
Epoch [116/300], Step [280/391],                 Loss: 0.35830, Train_Acc:87.83%
Epoch [116/300], Step [290/391],                 Loss: 0.35757, Train_Acc:87.87%
Epoch [116/300], Step [300/391],                 Loss: 0.35633, Train_Acc:87.88%
Epoch [116/300], Step [310/391],                 Loss: 0.35515, Train_Acc:87.91%
Epoch [116/300], Step [320/391],                 Loss: 0.35619, Train_Acc:87.88%
Epoch [116/300], Step [330/391],                 Loss: 0.35522, Train_Acc:87.94%
Epoch [116/300], Step [340/391],                 Loss: 0.35488, Train_Acc:87.93%
Epoch [116/300], Step [350/391],                 Loss: 0.35494, Train_Acc:87.96%
Epoch [116/300], Step [360/391],                 Loss: 0.35503, Train_Acc:87.96%
Epoch [116/300], Step [370/391],                 Loss: 0.35636, Train_Acc:87.89%
Epoch [116/300], Step [380/391],                 Loss: 0.35592, Train_Acc:87.92%
Epoch [116/300], Step [390/391],                 Loss: 0.35533, Train_Acc:87.96%
Accuary on test images:75.46%
Epoch [117/300], Step [10/391],                 Loss: 0.36631, Train_Acc:86.95%
Epoch [117/300], Step [20/391],                 Loss: 0.36371, Train_Acc:87.50%
Epoch [117/300], Step [30/391],                 Loss: 0.35531, Train_Acc:87.53%
Epoch [117/300], Step [40/391],                 Loss: 0.34855, Train_Acc:87.85%
Epoch [117/300], Step [50/391],                 Loss: 0.35268, Train_Acc:87.75%
Epoch [117/300], Step [60/391],                 Loss: 0.36160, Train_Acc:87.36%
Epoch [117/300], Step [70/391],                 Loss: 0.36333, Train_Acc:87.35%
Epoch [117/300], Step [80/391],                 Loss: 0.36422, Train_Acc:87.26%
Epoch [117/300], Step [90/391],                 Loss: 0.36431, Train_Acc:87.29%
Epoch [117/300], Step [100/391],                 Loss: 0.36250, Train_Acc:87.36%
Epoch [117/300], Step [110/391],                 Loss: 0.36361, Train_Acc:87.39%
Epoch [117/300], Step [120/391],                 Loss: 0.36586, Train_Acc:87.29%
Epoch [117/300], Step [130/391],                 Loss: 0.36621, Train_Acc:87.29%
Epoch [117/300], Step [140/391],                 Loss: 0.36557, Train_Acc:87.38%
Epoch [117/300], Step [150/391],                 Loss: 0.36518, Train_Acc:87.42%
Epoch [117/300], Step [160/391],                 Loss: 0.36343, Train_Acc:87.51%
Epoch [117/300], Step [170/391],                 Loss: 0.36369, Train_Acc:87.51%
Epoch [117/300], Step [180/391],                 Loss: 0.36317, Train_Acc:87.53%
Epoch [117/300], Step [190/391],                 Loss: 0.36255, Train_Acc:87.56%
Epoch [117/300], Step [200/391],                 Loss: 0.36215, Train_Acc:87.55%
Epoch [117/300], Step [210/391],                 Loss: 0.36065, Train_Acc:87.63%
Epoch [117/300], Step [220/391],                 Loss: 0.35956, Train_Acc:87.71%
Epoch [117/300], Step [230/391],                 Loss: 0.35793, Train_Acc:87.76%
Epoch [117/300], Step [240/391],                 Loss: 0.35592, Train_Acc:87.83%
Epoch [117/300], Step [250/391],                 Loss: 0.35490, Train_Acc:87.85%
Epoch [117/300], Step [260/391],                 Loss: 0.35706, Train_Acc:87.76%
Epoch [117/300], Step [270/391],                 Loss: 0.35878, Train_Acc:87.71%
Epoch [117/300], Step [280/391],                 Loss: 0.36063, Train_Acc:87.66%
Epoch [117/300], Step [290/391],                 Loss: 0.36158, Train_Acc:87.64%
Epoch [117/300], Step [300/391],                 Loss: 0.36211, Train_Acc:87.62%
Epoch [117/300], Step [310/391],                 Loss: 0.36172, Train_Acc:87.66%
Epoch [117/300], Step [320/391],                 Loss: 0.36196, Train_Acc:87.64%
Epoch [117/300], Step [330/391],                 Loss: 0.36054, Train_Acc:87.71%
Epoch [117/300], Step [340/391],                 Loss: 0.35952, Train_Acc:87.74%
Epoch [117/300], Step [350/391],                 Loss: 0.35954, Train_Acc:87.76%
Epoch [117/300], Step [360/391],                 Loss: 0.35982, Train_Acc:87.73%
Epoch [117/300], Step [370/391],                 Loss: 0.36032, Train_Acc:87.71%
Epoch [117/300], Step [380/391],                 Loss: 0.36018, Train_Acc:87.73%
Epoch [117/300], Step [390/391],                 Loss: 0.35863, Train_Acc:87.80%
Accuary on test images:74.12%
Epoch [118/300], Step [10/391],                 Loss: 0.35672, Train_Acc:88.28%
Epoch [118/300], Step [20/391],                 Loss: 0.35257, Train_Acc:88.59%
Epoch [118/300], Step [30/391],                 Loss: 0.35045, Train_Acc:88.20%
Epoch [118/300], Step [40/391],                 Loss: 0.34842, Train_Acc:88.18%
Epoch [118/300], Step [50/391],                 Loss: 0.35362, Train_Acc:87.88%
Epoch [118/300], Step [60/391],                 Loss: 0.35902, Train_Acc:87.47%
Epoch [118/300], Step [70/391],                 Loss: 0.35741, Train_Acc:87.62%
Epoch [118/300], Step [80/391],                 Loss: 0.36008, Train_Acc:87.56%
Epoch [118/300], Step [90/391],                 Loss: 0.36023, Train_Acc:87.65%
Epoch [118/300], Step [100/391],                 Loss: 0.35773, Train_Acc:87.75%
Epoch [118/300], Step [110/391],                 Loss: 0.36062, Train_Acc:87.70%
Epoch [118/300], Step [120/391],                 Loss: 0.36021, Train_Acc:87.71%
Epoch [118/300], Step [130/391],                 Loss: 0.36238, Train_Acc:87.61%
Epoch [118/300], Step [140/391],                 Loss: 0.36121, Train_Acc:87.67%
Epoch [118/300], Step [150/391],                 Loss: 0.36266, Train_Acc:87.59%
Epoch [118/300], Step [160/391],                 Loss: 0.36160, Train_Acc:87.62%
Epoch [118/300], Step [170/391],                 Loss: 0.36213, Train_Acc:87.64%
Epoch [118/300], Step [180/391],                 Loss: 0.36380, Train_Acc:87.60%
Epoch [118/300], Step [190/391],                 Loss: 0.36457, Train_Acc:87.59%
Epoch [118/300], Step [200/391],                 Loss: 0.36542, Train_Acc:87.57%
Epoch [118/300], Step [210/391],                 Loss: 0.36631, Train_Acc:87.55%
Epoch [118/300], Step [220/391],                 Loss: 0.36643, Train_Acc:87.53%
Epoch [118/300], Step [230/391],                 Loss: 0.36479, Train_Acc:87.59%
Epoch [118/300], Step [240/391],                 Loss: 0.36181, Train_Acc:87.70%
Epoch [118/300], Step [250/391],                 Loss: 0.36011, Train_Acc:87.76%
Epoch [118/300], Step [260/391],                 Loss: 0.36211, Train_Acc:87.76%
Epoch [118/300], Step [270/391],                 Loss: 0.36269, Train_Acc:87.73%
Epoch [118/300], Step [280/391],                 Loss: 0.36304, Train_Acc:87.72%
Epoch [118/300], Step [290/391],                 Loss: 0.36285, Train_Acc:87.74%
Epoch [118/300], Step [300/391],                 Loss: 0.36224, Train_Acc:87.73%
Epoch [118/300], Step [310/391],                 Loss: 0.36246, Train_Acc:87.74%
Epoch [118/300], Step [320/391],                 Loss: 0.36269, Train_Acc:87.73%
Epoch [118/300], Step [330/391],                 Loss: 0.36196, Train_Acc:87.76%
Epoch [118/300], Step [340/391],                 Loss: 0.36046, Train_Acc:87.81%
Epoch [118/300], Step [350/391],                 Loss: 0.35922, Train_Acc:87.88%
Epoch [118/300], Step [360/391],                 Loss: 0.35910, Train_Acc:87.86%
Epoch [118/300], Step [370/391],                 Loss: 0.36010, Train_Acc:87.83%
Epoch [118/300], Step [380/391],                 Loss: 0.36008, Train_Acc:87.83%
Epoch [118/300], Step [390/391],                 Loss: 0.35955, Train_Acc:87.84%
Accuary on test images:69.06%
Epoch [119/300], Step [10/391],                 Loss: 0.36342, Train_Acc:87.34%
Epoch [119/300], Step [20/391],                 Loss: 0.36495, Train_Acc:87.58%
Epoch [119/300], Step [30/391],                 Loss: 0.35984, Train_Acc:87.71%
Epoch [119/300], Step [40/391],                 Loss: 0.36803, Train_Acc:87.54%
Epoch [119/300], Step [50/391],                 Loss: 0.37094, Train_Acc:87.41%
Epoch [119/300], Step [60/391],                 Loss: 0.37201, Train_Acc:87.29%
Epoch [119/300], Step [70/391],                 Loss: 0.36335, Train_Acc:87.59%
Epoch [119/300], Step [80/391],                 Loss: 0.36295, Train_Acc:87.53%
Epoch [119/300], Step [90/391],                 Loss: 0.36720, Train_Acc:87.34%
Epoch [119/300], Step [100/391],                 Loss: 0.36469, Train_Acc:87.47%
Epoch [119/300], Step [110/391],                 Loss: 0.36369, Train_Acc:87.54%
Epoch [119/300], Step [120/391],                 Loss: 0.36206, Train_Acc:87.57%
Epoch [119/300], Step [130/391],                 Loss: 0.36227, Train_Acc:87.60%
Epoch [119/300], Step [140/391],                 Loss: 0.36184, Train_Acc:87.63%
Epoch [119/300], Step [150/391],                 Loss: 0.36081, Train_Acc:87.64%
Epoch [119/300], Step [160/391],                 Loss: 0.36017, Train_Acc:87.63%
Epoch [119/300], Step [170/391],                 Loss: 0.36311, Train_Acc:87.52%
Epoch [119/300], Step [180/391],                 Loss: 0.36467, Train_Acc:87.42%
Epoch [119/300], Step [190/391],                 Loss: 0.36412, Train_Acc:87.43%
Epoch [119/300], Step [200/391],                 Loss: 0.36356, Train_Acc:87.46%
Epoch [119/300], Step [210/391],                 Loss: 0.36261, Train_Acc:87.51%
Epoch [119/300], Step [220/391],                 Loss: 0.36339, Train_Acc:87.51%
Epoch [119/300], Step [230/391],                 Loss: 0.36195, Train_Acc:87.61%
Epoch [119/300], Step [240/391],                 Loss: 0.36144, Train_Acc:87.64%
Epoch [119/300], Step [250/391],                 Loss: 0.36037, Train_Acc:87.65%
Epoch [119/300], Step [260/391],                 Loss: 0.36182, Train_Acc:87.62%
Epoch [119/300], Step [270/391],                 Loss: 0.36275, Train_Acc:87.58%
Epoch [119/300], Step [280/391],                 Loss: 0.36305, Train_Acc:87.59%
Epoch [119/300], Step [290/391],                 Loss: 0.36379, Train_Acc:87.61%
Epoch [119/300], Step [300/391],                 Loss: 0.36416, Train_Acc:87.57%
Epoch [119/300], Step [310/391],                 Loss: 0.36320, Train_Acc:87.61%
Epoch [119/300], Step [320/391],                 Loss: 0.36304, Train_Acc:87.63%
Epoch [119/300], Step [330/391],                 Loss: 0.36227, Train_Acc:87.67%
Epoch [119/300], Step [340/391],                 Loss: 0.36091, Train_Acc:87.72%
Epoch [119/300], Step [350/391],                 Loss: 0.35960, Train_Acc:87.78%
Epoch [119/300], Step [360/391],                 Loss: 0.35865, Train_Acc:87.80%
Epoch [119/300], Step [370/391],                 Loss: 0.35904, Train_Acc:87.80%
Epoch [119/300], Step [380/391],                 Loss: 0.35841, Train_Acc:87.82%
Epoch [119/300], Step [390/391],                 Loss: 0.35777, Train_Acc:87.84%
Accuary on test images:80.14%
Epoch [120/300], Step [10/391],                 Loss: 0.35136, Train_Acc:88.67%
Epoch [120/300], Step [20/391],                 Loss: 0.35847, Train_Acc:88.28%
Epoch [120/300], Step [30/391],                 Loss: 0.35478, Train_Acc:88.23%
Epoch [120/300], Step [40/391],                 Loss: 0.35520, Train_Acc:88.09%
Epoch [120/300], Step [50/391],                 Loss: 0.35995, Train_Acc:88.11%
Epoch [120/300], Step [60/391],                 Loss: 0.37003, Train_Acc:87.66%
Epoch [120/300], Step [70/391],                 Loss: 0.36629, Train_Acc:87.83%
Epoch [120/300], Step [80/391],                 Loss: 0.36525, Train_Acc:87.71%
Epoch [120/300], Step [90/391],                 Loss: 0.36923, Train_Acc:87.55%
Epoch [120/300], Step [100/391],                 Loss: 0.36752, Train_Acc:87.56%
Epoch [120/300], Step [110/391],                 Loss: 0.36840, Train_Acc:87.56%
Epoch [120/300], Step [120/391],                 Loss: 0.36834, Train_Acc:87.50%
Epoch [120/300], Step [130/391],                 Loss: 0.36702, Train_Acc:87.51%
Epoch [120/300], Step [140/391],                 Loss: 0.36557, Train_Acc:87.60%
Epoch [120/300], Step [150/391],                 Loss: 0.36673, Train_Acc:87.56%
Epoch [120/300], Step [160/391],                 Loss: 0.36527, Train_Acc:87.60%
Epoch [120/300], Step [170/391],                 Loss: 0.36735, Train_Acc:87.52%
Epoch [120/300], Step [180/391],                 Loss: 0.36967, Train_Acc:87.43%
Epoch [120/300], Step [190/391],                 Loss: 0.36945, Train_Acc:87.40%
Epoch [120/300], Step [200/391],                 Loss: 0.36895, Train_Acc:87.41%
Epoch [120/300], Step [210/391],                 Loss: 0.36774, Train_Acc:87.45%
Epoch [120/300], Step [220/391],                 Loss: 0.36487, Train_Acc:87.50%
Epoch [120/300], Step [230/391],                 Loss: 0.36282, Train_Acc:87.53%
Epoch [120/300], Step [240/391],                 Loss: 0.36023, Train_Acc:87.64%
Epoch [120/300], Step [250/391],                 Loss: 0.36104, Train_Acc:87.61%
Epoch [120/300], Step [260/391],                 Loss: 0.36425, Train_Acc:87.54%
Epoch [120/300], Step [270/391],                 Loss: 0.36439, Train_Acc:87.55%
Epoch [120/300], Step [280/391],                 Loss: 0.36415, Train_Acc:87.55%
Epoch [120/300], Step [290/391],                 Loss: 0.36439, Train_Acc:87.55%
Epoch [120/300], Step [300/391],                 Loss: 0.36416, Train_Acc:87.54%
Epoch [120/300], Step [310/391],                 Loss: 0.36453, Train_Acc:87.55%
Epoch [120/300], Step [320/391],                 Loss: 0.36499, Train_Acc:87.53%
Epoch [120/300], Step [330/391],                 Loss: 0.36544, Train_Acc:87.49%
Epoch [120/300], Step [340/391],                 Loss: 0.36463, Train_Acc:87.51%
Epoch [120/300], Step [350/391],                 Loss: 0.36393, Train_Acc:87.57%
Epoch [120/300], Step [360/391],                 Loss: 0.36281, Train_Acc:87.62%
Epoch [120/300], Step [370/391],                 Loss: 0.36274, Train_Acc:87.62%
Epoch [120/300], Step [380/391],                 Loss: 0.36201, Train_Acc:87.65%
Epoch [120/300], Step [390/391],                 Loss: 0.36069, Train_Acc:87.69%
Accuary on test images:76.86%
Epoch [121/300], Step [10/391],                 Loss: 0.34473, Train_Acc:88.44%
Epoch [121/300], Step [20/391],                 Loss: 0.34765, Train_Acc:88.01%
Epoch [121/300], Step [30/391],                 Loss: 0.34529, Train_Acc:88.07%
Epoch [121/300], Step [40/391],                 Loss: 0.34706, Train_Acc:88.26%
Epoch [121/300], Step [50/391],                 Loss: 0.34875, Train_Acc:88.09%
Epoch [121/300], Step [60/391],                 Loss: 0.35109, Train_Acc:87.96%
Epoch [121/300], Step [70/391],                 Loss: 0.35310, Train_Acc:87.85%
Epoch [121/300], Step [80/391],                 Loss: 0.35253, Train_Acc:87.85%
Epoch [121/300], Step [90/391],                 Loss: 0.35560, Train_Acc:87.82%
Epoch [121/300], Step [100/391],                 Loss: 0.35424, Train_Acc:87.82%
Epoch [121/300], Step [110/391],                 Loss: 0.35670, Train_Acc:87.73%
Epoch [121/300], Step [120/391],                 Loss: 0.35832, Train_Acc:87.68%
Epoch [121/300], Step [130/391],                 Loss: 0.36242, Train_Acc:87.59%
Epoch [121/300], Step [140/391],                 Loss: 0.36193, Train_Acc:87.58%
Epoch [121/300], Step [150/391],                 Loss: 0.36338, Train_Acc:87.46%
Epoch [121/300], Step [160/391],                 Loss: 0.36107, Train_Acc:87.50%
Epoch [121/300], Step [170/391],                 Loss: 0.36118, Train_Acc:87.49%
Epoch [121/300], Step [180/391],                 Loss: 0.36256, Train_Acc:87.43%
Epoch [121/300], Step [190/391],                 Loss: 0.36217, Train_Acc:87.42%
Epoch [121/300], Step [200/391],                 Loss: 0.36197, Train_Acc:87.41%
Epoch [121/300], Step [210/391],                 Loss: 0.36109, Train_Acc:87.44%
Epoch [121/300], Step [220/391],                 Loss: 0.35975, Train_Acc:87.49%
Epoch [121/300], Step [230/391],                 Loss: 0.35733, Train_Acc:87.57%
Epoch [121/300], Step [240/391],                 Loss: 0.35472, Train_Acc:87.66%
Epoch [121/300], Step [250/391],                 Loss: 0.35477, Train_Acc:87.65%
Epoch [121/300], Step [260/391],                 Loss: 0.35699, Train_Acc:87.62%
Epoch [121/300], Step [270/391],                 Loss: 0.35872, Train_Acc:87.57%
Epoch [121/300], Step [280/391],                 Loss: 0.35830, Train_Acc:87.58%
Epoch [121/300], Step [290/391],                 Loss: 0.35913, Train_Acc:87.56%
Epoch [121/300], Step [300/391],                 Loss: 0.35916, Train_Acc:87.54%
Epoch [121/300], Step [310/391],                 Loss: 0.35847, Train_Acc:87.58%
Epoch [121/300], Step [320/391],                 Loss: 0.35809, Train_Acc:87.59%
Epoch [121/300], Step [330/391],                 Loss: 0.35691, Train_Acc:87.64%
Epoch [121/300], Step [340/391],                 Loss: 0.35667, Train_Acc:87.66%
Epoch [121/300], Step [350/391],                 Loss: 0.35727, Train_Acc:87.66%
Epoch [121/300], Step [360/391],                 Loss: 0.35776, Train_Acc:87.64%
Epoch [121/300], Step [370/391],                 Loss: 0.35795, Train_Acc:87.64%
Epoch [121/300], Step [380/391],                 Loss: 0.35721, Train_Acc:87.65%
Epoch [121/300], Step [390/391],                 Loss: 0.35652, Train_Acc:87.67%
Accuary on test images:79.60%
Epoch [122/300], Step [10/391],                 Loss: 0.34458, Train_Acc:88.44%
Epoch [122/300], Step [20/391],                 Loss: 0.34542, Train_Acc:88.36%
Epoch [122/300], Step [30/391],                 Loss: 0.33492, Train_Acc:88.72%
Epoch [122/300], Step [40/391],                 Loss: 0.33491, Train_Acc:88.73%
Epoch [122/300], Step [50/391],                 Loss: 0.33201, Train_Acc:88.64%
Epoch [122/300], Step [60/391],                 Loss: 0.33817, Train_Acc:88.15%
Epoch [122/300], Step [70/391],                 Loss: 0.34681, Train_Acc:87.91%
Epoch [122/300], Step [80/391],                 Loss: 0.34982, Train_Acc:87.74%
Epoch [122/300], Step [90/391],                 Loss: 0.35723, Train_Acc:87.38%
Epoch [122/300], Step [100/391],                 Loss: 0.35604, Train_Acc:87.45%
Epoch [122/300], Step [110/391],                 Loss: 0.35933, Train_Acc:87.33%
Epoch [122/300], Step [120/391],                 Loss: 0.36213, Train_Acc:87.25%
Epoch [122/300], Step [130/391],                 Loss: 0.36403, Train_Acc:87.24%
Epoch [122/300], Step [140/391],                 Loss: 0.36217, Train_Acc:87.35%
Epoch [122/300], Step [150/391],                 Loss: 0.36141, Train_Acc:87.41%
Epoch [122/300], Step [160/391],                 Loss: 0.35849, Train_Acc:87.51%
Epoch [122/300], Step [170/391],                 Loss: 0.35829, Train_Acc:87.45%
Epoch [122/300], Step [180/391],                 Loss: 0.35874, Train_Acc:87.45%
Epoch [122/300], Step [190/391],                 Loss: 0.35752, Train_Acc:87.54%
Epoch [122/300], Step [200/391],                 Loss: 0.35664, Train_Acc:87.59%
Epoch [122/300], Step [210/391],                 Loss: 0.35750, Train_Acc:87.57%
Epoch [122/300], Step [220/391],                 Loss: 0.35849, Train_Acc:87.54%
Epoch [122/300], Step [230/391],                 Loss: 0.35796, Train_Acc:87.56%
Epoch [122/300], Step [240/391],                 Loss: 0.35744, Train_Acc:87.58%
Epoch [122/300], Step [250/391],                 Loss: 0.35708, Train_Acc:87.61%
Epoch [122/300], Step [260/391],                 Loss: 0.35881, Train_Acc:87.52%
Epoch [122/300], Step [270/391],                 Loss: 0.35948, Train_Acc:87.50%
Epoch [122/300], Step [280/391],                 Loss: 0.35848, Train_Acc:87.54%
Epoch [122/300], Step [290/391],                 Loss: 0.35808, Train_Acc:87.56%
Epoch [122/300], Step [300/391],                 Loss: 0.35764, Train_Acc:87.54%
Epoch [122/300], Step [310/391],                 Loss: 0.35745, Train_Acc:87.54%
Epoch [122/300], Step [320/391],                 Loss: 0.35816, Train_Acc:87.53%
Epoch [122/300], Step [330/391],                 Loss: 0.35747, Train_Acc:87.58%
Epoch [122/300], Step [340/391],                 Loss: 0.35643, Train_Acc:87.62%
Epoch [122/300], Step [350/391],                 Loss: 0.35715, Train_Acc:87.61%
Epoch [122/300], Step [360/391],                 Loss: 0.35652, Train_Acc:87.60%
Epoch [122/300], Step [370/391],                 Loss: 0.35668, Train_Acc:87.60%
Epoch [122/300], Step [380/391],                 Loss: 0.35687, Train_Acc:87.58%
Epoch [122/300], Step [390/391],                 Loss: 0.35619, Train_Acc:87.61%
Accuary on test images:75.60%
Epoch [123/300], Step [10/391],                 Loss: 0.34617, Train_Acc:88.20%
Epoch [123/300], Step [20/391],                 Loss: 0.35631, Train_Acc:87.93%
Epoch [123/300], Step [30/391],                 Loss: 0.35118, Train_Acc:88.18%
Epoch [123/300], Step [40/391],                 Loss: 0.35192, Train_Acc:88.09%
Epoch [123/300], Step [50/391],                 Loss: 0.35493, Train_Acc:87.94%
Epoch [123/300], Step [60/391],                 Loss: 0.35901, Train_Acc:87.80%
Epoch [123/300], Step [70/391],                 Loss: 0.35510, Train_Acc:87.91%
Epoch [123/300], Step [80/391],                 Loss: 0.35308, Train_Acc:88.00%
Epoch [123/300], Step [90/391],                 Loss: 0.35551, Train_Acc:87.89%
Epoch [123/300], Step [100/391],                 Loss: 0.35364, Train_Acc:87.86%
Epoch [123/300], Step [110/391],                 Loss: 0.35547, Train_Acc:87.85%
Epoch [123/300], Step [120/391],                 Loss: 0.35706, Train_Acc:87.73%
Epoch [123/300], Step [130/391],                 Loss: 0.36322, Train_Acc:87.53%
Epoch [123/300], Step [140/391],                 Loss: 0.36308, Train_Acc:87.52%
Epoch [123/300], Step [150/391],                 Loss: 0.36282, Train_Acc:87.56%
Epoch [123/300], Step [160/391],                 Loss: 0.36168, Train_Acc:87.62%
Epoch [123/300], Step [170/391],                 Loss: 0.35958, Train_Acc:87.69%
Epoch [123/300], Step [180/391],                 Loss: 0.36009, Train_Acc:87.69%
Epoch [123/300], Step [190/391],                 Loss: 0.35993, Train_Acc:87.69%
Epoch [123/300], Step [200/391],                 Loss: 0.36204, Train_Acc:87.59%
Epoch [123/300], Step [210/391],                 Loss: 0.36130, Train_Acc:87.59%
Epoch [123/300], Step [220/391],                 Loss: 0.36077, Train_Acc:87.62%
Epoch [123/300], Step [230/391],                 Loss: 0.36030, Train_Acc:87.66%
Epoch [123/300], Step [240/391],                 Loss: 0.35893, Train_Acc:87.70%
Epoch [123/300], Step [250/391],                 Loss: 0.35860, Train_Acc:87.69%
Epoch [123/300], Step [260/391],                 Loss: 0.36062, Train_Acc:87.68%
Epoch [123/300], Step [270/391],                 Loss: 0.36125, Train_Acc:87.69%
Epoch [123/300], Step [280/391],                 Loss: 0.36158, Train_Acc:87.67%
Epoch [123/300], Step [290/391],                 Loss: 0.36124, Train_Acc:87.69%
Epoch [123/300], Step [300/391],                 Loss: 0.36125, Train_Acc:87.67%
Epoch [123/300], Step [310/391],                 Loss: 0.35983, Train_Acc:87.74%
Epoch [123/300], Step [320/391],                 Loss: 0.36012, Train_Acc:87.75%
Epoch [123/300], Step [330/391],                 Loss: 0.35906, Train_Acc:87.77%
Epoch [123/300], Step [340/391],                 Loss: 0.35855, Train_Acc:87.78%
Epoch [123/300], Step [350/391],                 Loss: 0.35827, Train_Acc:87.78%
Epoch [123/300], Step [360/391],                 Loss: 0.35796, Train_Acc:87.78%
Epoch [123/300], Step [370/391],                 Loss: 0.35864, Train_Acc:87.75%
Epoch [123/300], Step [380/391],                 Loss: 0.35813, Train_Acc:87.76%
Epoch [123/300], Step [390/391],                 Loss: 0.35654, Train_Acc:87.81%
Accuary on test images:77.58%
Epoch [124/300], Step [10/391],                 Loss: 0.32887, Train_Acc:89.22%
Epoch [124/300], Step [20/391],                 Loss: 0.33803, Train_Acc:88.83%
Epoch [124/300], Step [30/391],                 Loss: 0.33739, Train_Acc:88.62%
Epoch [124/300], Step [40/391],                 Loss: 0.34752, Train_Acc:88.30%
Epoch [124/300], Step [50/391],                 Loss: 0.35309, Train_Acc:88.16%
Epoch [124/300], Step [60/391],                 Loss: 0.36821, Train_Acc:87.67%
Epoch [124/300], Step [70/391],                 Loss: 0.37278, Train_Acc:87.49%
Epoch [124/300], Step [80/391],                 Loss: 0.37014, Train_Acc:87.55%
Epoch [124/300], Step [90/391],                 Loss: 0.37406, Train_Acc:87.35%
Epoch [124/300], Step [100/391],                 Loss: 0.37212, Train_Acc:87.43%
Epoch [124/300], Step [110/391],                 Loss: 0.37102, Train_Acc:87.46%
Epoch [124/300], Step [120/391],                 Loss: 0.37006, Train_Acc:87.49%
Epoch [124/300], Step [130/391],                 Loss: 0.36920, Train_Acc:87.52%
Epoch [124/300], Step [140/391],                 Loss: 0.36766, Train_Acc:87.61%
Epoch [124/300], Step [150/391],                 Loss: 0.36792, Train_Acc:87.62%
Epoch [124/300], Step [160/391],                 Loss: 0.36540, Train_Acc:87.71%
Epoch [124/300], Step [170/391],                 Loss: 0.36368, Train_Acc:87.74%
Epoch [124/300], Step [180/391],                 Loss: 0.36240, Train_Acc:87.75%
Epoch [124/300], Step [190/391],                 Loss: 0.36171, Train_Acc:87.75%
Epoch [124/300], Step [200/391],                 Loss: 0.36147, Train_Acc:87.75%
Epoch [124/300], Step [210/391],                 Loss: 0.36378, Train_Acc:87.68%
Epoch [124/300], Step [220/391],                 Loss: 0.36393, Train_Acc:87.68%
Epoch [124/300], Step [230/391],                 Loss: 0.36180, Train_Acc:87.73%
Epoch [124/300], Step [240/391],                 Loss: 0.35934, Train_Acc:87.82%
Epoch [124/300], Step [250/391],                 Loss: 0.35782, Train_Acc:87.86%
Epoch [124/300], Step [260/391],                 Loss: 0.35804, Train_Acc:87.86%
Epoch [124/300], Step [270/391],                 Loss: 0.35840, Train_Acc:87.81%
Epoch [124/300], Step [280/391],                 Loss: 0.35757, Train_Acc:87.82%
Epoch [124/300], Step [290/391],                 Loss: 0.35791, Train_Acc:87.82%
Epoch [124/300], Step [300/391],                 Loss: 0.35824, Train_Acc:87.79%
Epoch [124/300], Step [310/391],                 Loss: 0.35836, Train_Acc:87.79%
Epoch [124/300], Step [320/391],                 Loss: 0.35800, Train_Acc:87.79%
Epoch [124/300], Step [330/391],                 Loss: 0.35628, Train_Acc:87.84%
Epoch [124/300], Step [340/391],                 Loss: 0.35518, Train_Acc:87.90%
Epoch [124/300], Step [350/391],                 Loss: 0.35466, Train_Acc:87.95%
Epoch [124/300], Step [360/391],                 Loss: 0.35472, Train_Acc:87.94%
Epoch [124/300], Step [370/391],                 Loss: 0.35579, Train_Acc:87.90%
Epoch [124/300], Step [380/391],                 Loss: 0.35591, Train_Acc:87.89%
Epoch [124/300], Step [390/391],                 Loss: 0.35596, Train_Acc:87.85%
Accuary on test images:75.44%
Epoch [125/300], Step [10/391],                 Loss: 0.31619, Train_Acc:89.45%
Epoch [125/300], Step [20/391],                 Loss: 0.34034, Train_Acc:88.40%
Epoch [125/300], Step [30/391],                 Loss: 0.34741, Train_Acc:87.79%
Epoch [125/300], Step [40/391],                 Loss: 0.34594, Train_Acc:88.03%
Epoch [125/300], Step [50/391],                 Loss: 0.34241, Train_Acc:88.14%
Epoch [125/300], Step [60/391],                 Loss: 0.34686, Train_Acc:88.16%
Epoch [125/300], Step [70/391],                 Loss: 0.34858, Train_Acc:88.12%
Epoch [125/300], Step [80/391],                 Loss: 0.35169, Train_Acc:87.90%
Epoch [125/300], Step [90/391],                 Loss: 0.35744, Train_Acc:87.73%
Epoch [125/300], Step [100/391],                 Loss: 0.35850, Train_Acc:87.73%
Epoch [125/300], Step [110/391],                 Loss: 0.36449, Train_Acc:87.54%
Epoch [125/300], Step [120/391],                 Loss: 0.36296, Train_Acc:87.57%
Epoch [125/300], Step [130/391],                 Loss: 0.36394, Train_Acc:87.47%
Epoch [125/300], Step [140/391],                 Loss: 0.36239, Train_Acc:87.53%
Epoch [125/300], Step [150/391],                 Loss: 0.36261, Train_Acc:87.52%
Epoch [125/300], Step [160/391],                 Loss: 0.36121, Train_Acc:87.57%
Epoch [125/300], Step [170/391],                 Loss: 0.36278, Train_Acc:87.50%
Epoch [125/300], Step [180/391],                 Loss: 0.36259, Train_Acc:87.53%
Epoch [125/300], Step [190/391],                 Loss: 0.36306, Train_Acc:87.51%
Epoch [125/300], Step [200/391],                 Loss: 0.36274, Train_Acc:87.50%
Epoch [125/300], Step [210/391],                 Loss: 0.36324, Train_Acc:87.47%
Epoch [125/300], Step [220/391],                 Loss: 0.36288, Train_Acc:87.51%
Epoch [125/300], Step [230/391],                 Loss: 0.36157, Train_Acc:87.58%
Epoch [125/300], Step [240/391],                 Loss: 0.35971, Train_Acc:87.62%
Epoch [125/300], Step [250/391],                 Loss: 0.35967, Train_Acc:87.62%
Epoch [125/300], Step [260/391],                 Loss: 0.36108, Train_Acc:87.63%
Epoch [125/300], Step [270/391],                 Loss: 0.36089, Train_Acc:87.64%
Epoch [125/300], Step [280/391],                 Loss: 0.35996, Train_Acc:87.67%
Epoch [125/300], Step [290/391],                 Loss: 0.35961, Train_Acc:87.69%
Epoch [125/300], Step [300/391],                 Loss: 0.35954, Train_Acc:87.69%
Epoch [125/300], Step [310/391],                 Loss: 0.35916, Train_Acc:87.70%
Epoch [125/300], Step [320/391],                 Loss: 0.36006, Train_Acc:87.67%
Epoch [125/300], Step [330/391],                 Loss: 0.35948, Train_Acc:87.69%
Epoch [125/300], Step [340/391],                 Loss: 0.35806, Train_Acc:87.74%
Epoch [125/300], Step [350/391],                 Loss: 0.35710, Train_Acc:87.76%
Epoch [125/300], Step [360/391],                 Loss: 0.35779, Train_Acc:87.75%
Epoch [125/300], Step [370/391],                 Loss: 0.35781, Train_Acc:87.76%
Epoch [125/300], Step [380/391],                 Loss: 0.35818, Train_Acc:87.76%
Epoch [125/300], Step [390/391],                 Loss: 0.35791, Train_Acc:87.75%
Accuary on test images:74.92%
Epoch [126/300], Step [10/391],                 Loss: 0.34571, Train_Acc:88.44%
Epoch [126/300], Step [20/391],                 Loss: 0.33487, Train_Acc:88.44%
Epoch [126/300], Step [30/391],                 Loss: 0.32170, Train_Acc:89.09%
Epoch [126/300], Step [40/391],                 Loss: 0.32913, Train_Acc:88.93%
Epoch [126/300], Step [50/391],                 Loss: 0.33023, Train_Acc:88.64%
Epoch [126/300], Step [60/391],                 Loss: 0.33763, Train_Acc:88.31%
Epoch [126/300], Step [70/391],                 Loss: 0.33997, Train_Acc:88.27%
Epoch [126/300], Step [80/391],                 Loss: 0.34415, Train_Acc:88.12%
Epoch [126/300], Step [90/391],                 Loss: 0.34826, Train_Acc:87.94%
Epoch [126/300], Step [100/391],                 Loss: 0.34789, Train_Acc:87.95%
Epoch [126/300], Step [110/391],                 Loss: 0.35402, Train_Acc:87.76%
Epoch [126/300], Step [120/391],                 Loss: 0.35313, Train_Acc:87.79%
Epoch [126/300], Step [130/391],                 Loss: 0.35301, Train_Acc:87.87%
Epoch [126/300], Step [140/391],                 Loss: 0.35079, Train_Acc:87.95%
Epoch [126/300], Step [150/391],                 Loss: 0.35044, Train_Acc:88.03%
Epoch [126/300], Step [160/391],                 Loss: 0.34954, Train_Acc:88.02%
Epoch [126/300], Step [170/391],                 Loss: 0.35075, Train_Acc:88.01%
Epoch [126/300], Step [180/391],                 Loss: 0.35122, Train_Acc:88.00%
Epoch [126/300], Step [190/391],                 Loss: 0.35260, Train_Acc:87.95%
Epoch [126/300], Step [200/391],                 Loss: 0.35307, Train_Acc:87.87%
Epoch [126/300], Step [210/391],                 Loss: 0.35321, Train_Acc:87.86%
Epoch [126/300], Step [220/391],                 Loss: 0.35213, Train_Acc:87.89%
Epoch [126/300], Step [230/391],                 Loss: 0.35086, Train_Acc:87.93%
Epoch [126/300], Step [240/391],                 Loss: 0.34818, Train_Acc:88.05%
Epoch [126/300], Step [250/391],                 Loss: 0.34820, Train_Acc:88.04%
Epoch [126/300], Step [260/391],                 Loss: 0.35037, Train_Acc:87.98%
Epoch [126/300], Step [270/391],                 Loss: 0.35251, Train_Acc:87.90%
Epoch [126/300], Step [280/391],                 Loss: 0.35370, Train_Acc:87.83%
Epoch [126/300], Step [290/391],                 Loss: 0.35290, Train_Acc:87.87%
Epoch [126/300], Step [300/391],                 Loss: 0.35182, Train_Acc:87.86%
Epoch [126/300], Step [310/391],                 Loss: 0.35085, Train_Acc:87.92%
Epoch [126/300], Step [320/391],                 Loss: 0.35051, Train_Acc:87.93%
Epoch [126/300], Step [330/391],                 Loss: 0.34937, Train_Acc:87.98%
Epoch [126/300], Step [340/391],                 Loss: 0.34932, Train_Acc:88.00%
Epoch [126/300], Step [350/391],                 Loss: 0.34880, Train_Acc:88.03%
Epoch [126/300], Step [360/391],                 Loss: 0.34877, Train_Acc:87.99%
Epoch [126/300], Step [370/391],                 Loss: 0.35003, Train_Acc:87.95%
Epoch [126/300], Step [380/391],                 Loss: 0.35038, Train_Acc:87.95%
Epoch [126/300], Step [390/391],                 Loss: 0.35039, Train_Acc:87.97%
Accuary on test images:74.56%
Epoch [127/300], Step [10/391],                 Loss: 0.34976, Train_Acc:89.06%
Epoch [127/300], Step [20/391],                 Loss: 0.33724, Train_Acc:89.10%
Epoch [127/300], Step [30/391],                 Loss: 0.33003, Train_Acc:89.06%
Epoch [127/300], Step [40/391],                 Loss: 0.33523, Train_Acc:88.93%
Epoch [127/300], Step [50/391],                 Loss: 0.34247, Train_Acc:88.61%
Epoch [127/300], Step [60/391],                 Loss: 0.34592, Train_Acc:88.55%
Epoch [127/300], Step [70/391],                 Loss: 0.34658, Train_Acc:88.57%
Epoch [127/300], Step [80/391],                 Loss: 0.34955, Train_Acc:88.42%
Epoch [127/300], Step [90/391],                 Loss: 0.35490, Train_Acc:88.15%
Epoch [127/300], Step [100/391],                 Loss: 0.35429, Train_Acc:88.09%
Epoch [127/300], Step [110/391],                 Loss: 0.35807, Train_Acc:87.93%
Epoch [127/300], Step [120/391],                 Loss: 0.36158, Train_Acc:87.84%
Epoch [127/300], Step [130/391],                 Loss: 0.36080, Train_Acc:87.86%
Epoch [127/300], Step [140/391],                 Loss: 0.35769, Train_Acc:87.91%
Epoch [127/300], Step [150/391],                 Loss: 0.35946, Train_Acc:87.81%
Epoch [127/300], Step [160/391],                 Loss: 0.35945, Train_Acc:87.85%
Epoch [127/300], Step [170/391],                 Loss: 0.35881, Train_Acc:87.89%
Epoch [127/300], Step [180/391],                 Loss: 0.35777, Train_Acc:87.88%
Epoch [127/300], Step [190/391],                 Loss: 0.35727, Train_Acc:87.86%
Epoch [127/300], Step [200/391],                 Loss: 0.35724, Train_Acc:87.89%
Epoch [127/300], Step [210/391],                 Loss: 0.35761, Train_Acc:87.86%
Epoch [127/300], Step [220/391],                 Loss: 0.35672, Train_Acc:87.89%
Epoch [127/300], Step [230/391],                 Loss: 0.35496, Train_Acc:87.96%
Epoch [127/300], Step [240/391],                 Loss: 0.35164, Train_Acc:88.07%
Epoch [127/300], Step [250/391],                 Loss: 0.35093, Train_Acc:88.11%
Epoch [127/300], Step [260/391],                 Loss: 0.35401, Train_Acc:88.03%
Epoch [127/300], Step [270/391],                 Loss: 0.35618, Train_Acc:87.95%
Epoch [127/300], Step [280/391],                 Loss: 0.35664, Train_Acc:87.90%
Epoch [127/300], Step [290/391],                 Loss: 0.35661, Train_Acc:87.93%
Epoch [127/300], Step [300/391],                 Loss: 0.35628, Train_Acc:87.91%
Epoch [127/300], Step [310/391],                 Loss: 0.35525, Train_Acc:87.95%
Epoch [127/300], Step [320/391],                 Loss: 0.35603, Train_Acc:87.94%
Epoch [127/300], Step [330/391],                 Loss: 0.35479, Train_Acc:87.97%
Epoch [127/300], Step [340/391],                 Loss: 0.35417, Train_Acc:87.96%
Epoch [127/300], Step [350/391],                 Loss: 0.35384, Train_Acc:88.01%
Epoch [127/300], Step [360/391],                 Loss: 0.35359, Train_Acc:88.03%
Epoch [127/300], Step [370/391],                 Loss: 0.35374, Train_Acc:88.03%
Epoch [127/300], Step [380/391],                 Loss: 0.35415, Train_Acc:88.00%
Epoch [127/300], Step [390/391],                 Loss: 0.35395, Train_Acc:88.00%
Accuary on test images:76.62%
Epoch [128/300], Step [10/391],                 Loss: 0.37252, Train_Acc:87.11%
Epoch [128/300], Step [20/391],                 Loss: 0.37275, Train_Acc:87.66%
Epoch [128/300], Step [30/391],                 Loss: 0.37473, Train_Acc:87.68%
Epoch [128/300], Step [40/391],                 Loss: 0.37374, Train_Acc:87.68%
Epoch [128/300], Step [50/391],                 Loss: 0.37277, Train_Acc:87.62%
Epoch [128/300], Step [60/391],                 Loss: 0.37064, Train_Acc:87.53%
Epoch [128/300], Step [70/391],                 Loss: 0.36606, Train_Acc:87.73%
Epoch [128/300], Step [80/391],                 Loss: 0.36530, Train_Acc:87.68%
Epoch [128/300], Step [90/391],                 Loss: 0.37055, Train_Acc:87.36%
Epoch [128/300], Step [100/391],                 Loss: 0.37140, Train_Acc:87.32%
Epoch [128/300], Step [110/391],                 Loss: 0.37293, Train_Acc:87.32%
Epoch [128/300], Step [120/391],                 Loss: 0.37229, Train_Acc:87.33%
Epoch [128/300], Step [130/391],                 Loss: 0.37411, Train_Acc:87.29%
Epoch [128/300], Step [140/391],                 Loss: 0.37206, Train_Acc:87.41%
Epoch [128/300], Step [150/391],                 Loss: 0.37080, Train_Acc:87.47%
Epoch [128/300], Step [160/391],                 Loss: 0.36901, Train_Acc:87.51%
Epoch [128/300], Step [170/391],                 Loss: 0.36853, Train_Acc:87.50%
Epoch [128/300], Step [180/391],                 Loss: 0.36654, Train_Acc:87.51%
Epoch [128/300], Step [190/391],                 Loss: 0.36628, Train_Acc:87.52%
Epoch [128/300], Step [200/391],                 Loss: 0.36810, Train_Acc:87.45%
Epoch [128/300], Step [210/391],                 Loss: 0.36734, Train_Acc:87.46%
Epoch [128/300], Step [220/391],                 Loss: 0.36728, Train_Acc:87.46%
Epoch [128/300], Step [230/391],                 Loss: 0.36510, Train_Acc:87.51%
Epoch [128/300], Step [240/391],                 Loss: 0.36225, Train_Acc:87.60%
Epoch [128/300], Step [250/391],                 Loss: 0.36125, Train_Acc:87.63%
Epoch [128/300], Step [260/391],                 Loss: 0.36301, Train_Acc:87.50%
Epoch [128/300], Step [270/391],                 Loss: 0.36511, Train_Acc:87.43%
Epoch [128/300], Step [280/391],                 Loss: 0.36489, Train_Acc:87.42%
Epoch [128/300], Step [290/391],                 Loss: 0.36447, Train_Acc:87.43%
Epoch [128/300], Step [300/391],                 Loss: 0.36429, Train_Acc:87.42%
Epoch [128/300], Step [310/391],                 Loss: 0.36472, Train_Acc:87.40%
Epoch [128/300], Step [320/391],                 Loss: 0.36603, Train_Acc:87.39%
Epoch [128/300], Step [330/391],                 Loss: 0.36571, Train_Acc:87.40%
Epoch [128/300], Step [340/391],                 Loss: 0.36515, Train_Acc:87.42%
Epoch [128/300], Step [350/391],                 Loss: 0.36556, Train_Acc:87.40%
Epoch [128/300], Step [360/391],                 Loss: 0.36539, Train_Acc:87.43%
Epoch [128/300], Step [370/391],                 Loss: 0.36595, Train_Acc:87.39%
Epoch [128/300], Step [380/391],                 Loss: 0.36487, Train_Acc:87.43%
Epoch [128/300], Step [390/391],                 Loss: 0.36459, Train_Acc:87.43%
Accuary on test images:69.82%
Epoch [129/300], Step [10/391],                 Loss: 0.32092, Train_Acc:89.22%
Epoch [129/300], Step [20/391],                 Loss: 0.32877, Train_Acc:88.98%
Epoch [129/300], Step [30/391],                 Loss: 0.33292, Train_Acc:88.78%
Epoch [129/300], Step [40/391],                 Loss: 0.33485, Train_Acc:88.89%
Epoch [129/300], Step [50/391],                 Loss: 0.33271, Train_Acc:88.91%
Epoch [129/300], Step [60/391],                 Loss: 0.33746, Train_Acc:88.70%
Epoch [129/300], Step [70/391],                 Loss: 0.33797, Train_Acc:88.67%
Epoch [129/300], Step [80/391],                 Loss: 0.34049, Train_Acc:88.54%
Epoch [129/300], Step [90/391],                 Loss: 0.34207, Train_Acc:88.45%
Epoch [129/300], Step [100/391],                 Loss: 0.34196, Train_Acc:88.37%
Epoch [129/300], Step [110/391],                 Loss: 0.34486, Train_Acc:88.27%
Epoch [129/300], Step [120/391],                 Loss: 0.34604, Train_Acc:88.15%
Epoch [129/300], Step [130/391],                 Loss: 0.35025, Train_Acc:87.99%
Epoch [129/300], Step [140/391],                 Loss: 0.35396, Train_Acc:87.91%
Epoch [129/300], Step [150/391],                 Loss: 0.35620, Train_Acc:87.84%
Epoch [129/300], Step [160/391],                 Loss: 0.35676, Train_Acc:87.85%
Epoch [129/300], Step [170/391],                 Loss: 0.35845, Train_Acc:87.86%
Epoch [129/300], Step [180/391],                 Loss: 0.35848, Train_Acc:87.86%
Epoch [129/300], Step [190/391],                 Loss: 0.35749, Train_Acc:87.88%
Epoch [129/300], Step [200/391],                 Loss: 0.35821, Train_Acc:87.84%
Epoch [129/300], Step [210/391],                 Loss: 0.35785, Train_Acc:87.82%
Epoch [129/300], Step [220/391],                 Loss: 0.35747, Train_Acc:87.84%
Epoch [129/300], Step [230/391],                 Loss: 0.35625, Train_Acc:87.86%
Epoch [129/300], Step [240/391],                 Loss: 0.35695, Train_Acc:87.83%
Epoch [129/300], Step [250/391],                 Loss: 0.35759, Train_Acc:87.82%
Epoch [129/300], Step [260/391],                 Loss: 0.35896, Train_Acc:87.80%
Epoch [129/300], Step [270/391],                 Loss: 0.36036, Train_Acc:87.74%
Epoch [129/300], Step [280/391],                 Loss: 0.36095, Train_Acc:87.70%
Epoch [129/300], Step [290/391],                 Loss: 0.35972, Train_Acc:87.72%
Epoch [129/300], Step [300/391],                 Loss: 0.35897, Train_Acc:87.74%
Epoch [129/300], Step [310/391],                 Loss: 0.35858, Train_Acc:87.76%
Epoch [129/300], Step [320/391],                 Loss: 0.35894, Train_Acc:87.76%
Epoch [129/300], Step [330/391],                 Loss: 0.35806, Train_Acc:87.80%
Epoch [129/300], Step [340/391],                 Loss: 0.35782, Train_Acc:87.81%
Epoch [129/300], Step [350/391],                 Loss: 0.35692, Train_Acc:87.84%
Epoch [129/300], Step [360/391],                 Loss: 0.35562, Train_Acc:87.86%
Epoch [129/300], Step [370/391],                 Loss: 0.35561, Train_Acc:87.83%
Epoch [129/300], Step [380/391],                 Loss: 0.35534, Train_Acc:87.85%
Epoch [129/300], Step [390/391],                 Loss: 0.35520, Train_Acc:87.86%
Accuary on test images:76.64%
Epoch [130/300], Step [10/391],                 Loss: 0.35372, Train_Acc:87.81%
Epoch [130/300], Step [20/391],                 Loss: 0.35139, Train_Acc:88.01%
Epoch [130/300], Step [30/391],                 Loss: 0.33418, Train_Acc:88.62%
Epoch [130/300], Step [40/391],                 Loss: 0.34099, Train_Acc:88.46%
Epoch [130/300], Step [50/391],                 Loss: 0.34326, Train_Acc:88.33%
Epoch [130/300], Step [60/391],                 Loss: 0.34798, Train_Acc:88.23%
Epoch [130/300], Step [70/391],                 Loss: 0.34586, Train_Acc:88.45%
Epoch [130/300], Step [80/391],                 Loss: 0.34372, Train_Acc:88.45%
Epoch [130/300], Step [90/391],                 Loss: 0.34865, Train_Acc:88.32%
Epoch [130/300], Step [100/391],                 Loss: 0.34869, Train_Acc:88.34%
Epoch [130/300], Step [110/391],                 Loss: 0.35753, Train_Acc:88.07%
Epoch [130/300], Step [120/391],                 Loss: 0.36083, Train_Acc:87.88%
Epoch [130/300], Step [130/391],                 Loss: 0.36480, Train_Acc:87.79%
Epoch [130/300], Step [140/391],                 Loss: 0.36263, Train_Acc:87.88%
Epoch [130/300], Step [150/391],                 Loss: 0.36351, Train_Acc:87.82%
Epoch [130/300], Step [160/391],                 Loss: 0.36480, Train_Acc:87.76%
Epoch [130/300], Step [170/391],                 Loss: 0.36582, Train_Acc:87.74%
Epoch [130/300], Step [180/391],                 Loss: 0.36537, Train_Acc:87.75%
Epoch [130/300], Step [190/391],                 Loss: 0.36634, Train_Acc:87.66%
Epoch [130/300], Step [200/391],                 Loss: 0.36714, Train_Acc:87.66%
Epoch [130/300], Step [210/391],                 Loss: 0.36807, Train_Acc:87.66%
Epoch [130/300], Step [220/391],                 Loss: 0.36767, Train_Acc:87.65%
Epoch [130/300], Step [230/391],                 Loss: 0.36566, Train_Acc:87.72%
Epoch [130/300], Step [240/391],                 Loss: 0.36357, Train_Acc:87.79%
Epoch [130/300], Step [250/391],                 Loss: 0.36304, Train_Acc:87.84%
Epoch [130/300], Step [260/391],                 Loss: 0.36511, Train_Acc:87.81%
Epoch [130/300], Step [270/391],                 Loss: 0.36441, Train_Acc:87.82%
Epoch [130/300], Step [280/391],                 Loss: 0.36332, Train_Acc:87.85%
Epoch [130/300], Step [290/391],                 Loss: 0.36302, Train_Acc:87.83%
Epoch [130/300], Step [300/391],                 Loss: 0.36155, Train_Acc:87.85%
Epoch [130/300], Step [310/391],                 Loss: 0.36084, Train_Acc:87.89%
Epoch [130/300], Step [320/391],                 Loss: 0.36067, Train_Acc:87.88%
Epoch [130/300], Step [330/391],                 Loss: 0.35884, Train_Acc:87.95%
Epoch [130/300], Step [340/391],                 Loss: 0.35820, Train_Acc:87.97%
Epoch [130/300], Step [350/391],                 Loss: 0.35816, Train_Acc:87.96%
Epoch [130/300], Step [360/391],                 Loss: 0.35830, Train_Acc:87.92%
Epoch [130/300], Step [370/391],                 Loss: 0.35860, Train_Acc:87.89%
Epoch [130/300], Step [380/391],                 Loss: 0.35937, Train_Acc:87.88%
Epoch [130/300], Step [390/391],                 Loss: 0.35892, Train_Acc:87.89%
Accuary on test images:69.16%
Epoch [131/300], Step [10/391],                 Loss: 0.36266, Train_Acc:88.28%
Epoch [131/300], Step [20/391],                 Loss: 0.34878, Train_Acc:88.71%
Epoch [131/300], Step [30/391],                 Loss: 0.33606, Train_Acc:88.72%
Epoch [131/300], Step [40/391],                 Loss: 0.34726, Train_Acc:88.40%
Epoch [131/300], Step [50/391],                 Loss: 0.34288, Train_Acc:88.44%
Epoch [131/300], Step [60/391],                 Loss: 0.34580, Train_Acc:88.40%
Epoch [131/300], Step [70/391],                 Loss: 0.34736, Train_Acc:88.30%
Epoch [131/300], Step [80/391],                 Loss: 0.35000, Train_Acc:88.18%
Epoch [131/300], Step [90/391],                 Loss: 0.35255, Train_Acc:88.14%
Epoch [131/300], Step [100/391],                 Loss: 0.34730, Train_Acc:88.32%
Epoch [131/300], Step [110/391],                 Loss: 0.35006, Train_Acc:88.17%
Epoch [131/300], Step [120/391],                 Loss: 0.35104, Train_Acc:88.11%
Epoch [131/300], Step [130/391],                 Loss: 0.35571, Train_Acc:88.00%
Epoch [131/300], Step [140/391],                 Loss: 0.35621, Train_Acc:88.02%
Epoch [131/300], Step [150/391],                 Loss: 0.35863, Train_Acc:87.92%
Epoch [131/300], Step [160/391],                 Loss: 0.35934, Train_Acc:87.90%
Epoch [131/300], Step [170/391],                 Loss: 0.36097, Train_Acc:87.80%
Epoch [131/300], Step [180/391],                 Loss: 0.36196, Train_Acc:87.76%
Epoch [131/300], Step [190/391],                 Loss: 0.36121, Train_Acc:87.77%
Epoch [131/300], Step [200/391],                 Loss: 0.36074, Train_Acc:87.80%
Epoch [131/300], Step [210/391],                 Loss: 0.35906, Train_Acc:87.88%
Epoch [131/300], Step [220/391],                 Loss: 0.35667, Train_Acc:88.00%
Epoch [131/300], Step [230/391],                 Loss: 0.35691, Train_Acc:87.97%
Epoch [131/300], Step [240/391],                 Loss: 0.35577, Train_Acc:87.99%
Epoch [131/300], Step [250/391],                 Loss: 0.35646, Train_Acc:87.95%
Epoch [131/300], Step [260/391],                 Loss: 0.35862, Train_Acc:87.90%
Epoch [131/300], Step [270/391],                 Loss: 0.35933, Train_Acc:87.88%
Epoch [131/300], Step [280/391],                 Loss: 0.35937, Train_Acc:87.88%
Epoch [131/300], Step [290/391],                 Loss: 0.36090, Train_Acc:87.82%
Epoch [131/300], Step [300/391],                 Loss: 0.36155, Train_Acc:87.77%
Epoch [131/300], Step [310/391],                 Loss: 0.36116, Train_Acc:87.77%
Epoch [131/300], Step [320/391],                 Loss: 0.36141, Train_Acc:87.76%
Epoch [131/300], Step [330/391],                 Loss: 0.36001, Train_Acc:87.83%
Epoch [131/300], Step [340/391],                 Loss: 0.35858, Train_Acc:87.89%
Epoch [131/300], Step [350/391],                 Loss: 0.35781, Train_Acc:87.93%
Epoch [131/300], Step [360/391],                 Loss: 0.35737, Train_Acc:87.92%
Epoch [131/300], Step [370/391],                 Loss: 0.35768, Train_Acc:87.91%
Epoch [131/300], Step [380/391],                 Loss: 0.35725, Train_Acc:87.91%
Epoch [131/300], Step [390/391],                 Loss: 0.35678, Train_Acc:87.93%
Accuary on test images:73.58%
Epoch [132/300], Step [10/391],                 Loss: 0.36286, Train_Acc:87.66%
Epoch [132/300], Step [20/391],                 Loss: 0.34019, Train_Acc:88.12%
Epoch [132/300], Step [30/391],                 Loss: 0.33355, Train_Acc:88.67%
Epoch [132/300], Step [40/391],                 Loss: 0.34317, Train_Acc:88.11%
Epoch [132/300], Step [50/391],                 Loss: 0.33899, Train_Acc:88.23%
Epoch [132/300], Step [60/391],                 Loss: 0.34845, Train_Acc:88.07%
Epoch [132/300], Step [70/391],                 Loss: 0.34637, Train_Acc:88.04%
Epoch [132/300], Step [80/391],                 Loss: 0.35122, Train_Acc:87.93%
Epoch [132/300], Step [90/391],                 Loss: 0.35511, Train_Acc:87.73%
Epoch [132/300], Step [100/391],                 Loss: 0.35148, Train_Acc:87.91%
Epoch [132/300], Step [110/391],                 Loss: 0.35227, Train_Acc:87.81%
Epoch [132/300], Step [120/391],                 Loss: 0.35443, Train_Acc:87.74%
Epoch [132/300], Step [130/391],                 Loss: 0.35845, Train_Acc:87.60%
Epoch [132/300], Step [140/391],                 Loss: 0.35945, Train_Acc:87.58%
Epoch [132/300], Step [150/391],                 Loss: 0.35889, Train_Acc:87.67%
Epoch [132/300], Step [160/391],                 Loss: 0.35674, Train_Acc:87.74%
Epoch [132/300], Step [170/391],                 Loss: 0.35738, Train_Acc:87.70%
Epoch [132/300], Step [180/391],                 Loss: 0.35746, Train_Acc:87.70%
Epoch [132/300], Step [190/391],                 Loss: 0.35757, Train_Acc:87.74%
Epoch [132/300], Step [200/391],                 Loss: 0.35758, Train_Acc:87.74%
Epoch [132/300], Step [210/391],                 Loss: 0.35770, Train_Acc:87.75%
Epoch [132/300], Step [220/391],                 Loss: 0.35747, Train_Acc:87.72%
Epoch [132/300], Step [230/391],                 Loss: 0.35571, Train_Acc:87.77%
Epoch [132/300], Step [240/391],                 Loss: 0.35215, Train_Acc:87.91%
Epoch [132/300], Step [250/391],                 Loss: 0.35069, Train_Acc:87.99%
Epoch [132/300], Step [260/391],                 Loss: 0.35419, Train_Acc:87.89%
Epoch [132/300], Step [270/391],                 Loss: 0.35529, Train_Acc:87.85%
Epoch [132/300], Step [280/391],                 Loss: 0.35592, Train_Acc:87.85%
Epoch [132/300], Step [290/391],                 Loss: 0.35586, Train_Acc:87.84%
Epoch [132/300], Step [300/391],                 Loss: 0.35559, Train_Acc:87.86%
Epoch [132/300], Step [310/391],                 Loss: 0.35549, Train_Acc:87.88%
Epoch [132/300], Step [320/391],                 Loss: 0.35492, Train_Acc:87.90%
Epoch [132/300], Step [330/391],                 Loss: 0.35377, Train_Acc:87.95%
Epoch [132/300], Step [340/391],                 Loss: 0.35250, Train_Acc:87.98%
Epoch [132/300], Step [350/391],                 Loss: 0.35191, Train_Acc:87.99%
Epoch [132/300], Step [360/391],                 Loss: 0.35127, Train_Acc:88.01%
Epoch [132/300], Step [370/391],                 Loss: 0.35173, Train_Acc:88.00%
Epoch [132/300], Step [380/391],                 Loss: 0.35126, Train_Acc:88.02%
Epoch [132/300], Step [390/391],                 Loss: 0.35126, Train_Acc:88.02%
Accuary on test images:73.30%
Epoch [133/300], Step [10/391],                 Loss: 0.36162, Train_Acc:86.95%
Epoch [133/300], Step [20/391],                 Loss: 0.34629, Train_Acc:88.24%
Epoch [133/300], Step [30/391],                 Loss: 0.33907, Train_Acc:88.70%
Epoch [133/300], Step [40/391],                 Loss: 0.33910, Train_Acc:88.52%
Epoch [133/300], Step [50/391],                 Loss: 0.33827, Train_Acc:88.50%
Epoch [133/300], Step [60/391],                 Loss: 0.34347, Train_Acc:88.11%
Epoch [133/300], Step [70/391],                 Loss: 0.34126, Train_Acc:88.11%
Epoch [133/300], Step [80/391],                 Loss: 0.34503, Train_Acc:87.99%
Epoch [133/300], Step [90/391],                 Loss: 0.34956, Train_Acc:87.80%
Epoch [133/300], Step [100/391],                 Loss: 0.35020, Train_Acc:87.82%
Epoch [133/300], Step [110/391],                 Loss: 0.35178, Train_Acc:87.79%
Epoch [133/300], Step [120/391],                 Loss: 0.34942, Train_Acc:87.82%
Epoch [133/300], Step [130/391],                 Loss: 0.35006, Train_Acc:87.83%
Epoch [133/300], Step [140/391],                 Loss: 0.35038, Train_Acc:87.83%
Epoch [133/300], Step [150/391],                 Loss: 0.35252, Train_Acc:87.80%
Epoch [133/300], Step [160/391],                 Loss: 0.35141, Train_Acc:87.79%
Epoch [133/300], Step [170/391],                 Loss: 0.35336, Train_Acc:87.75%
Epoch [133/300], Step [180/391],                 Loss: 0.35527, Train_Acc:87.64%
Epoch [133/300], Step [190/391],                 Loss: 0.35661, Train_Acc:87.64%
Epoch [133/300], Step [200/391],                 Loss: 0.35770, Train_Acc:87.63%
Epoch [133/300], Step [210/391],                 Loss: 0.35654, Train_Acc:87.70%
Epoch [133/300], Step [220/391],                 Loss: 0.35420, Train_Acc:87.78%
Epoch [133/300], Step [230/391],                 Loss: 0.35220, Train_Acc:87.84%
Epoch [133/300], Step [240/391],                 Loss: 0.35047, Train_Acc:87.90%
Epoch [133/300], Step [250/391],                 Loss: 0.35065, Train_Acc:87.92%
Epoch [133/300], Step [260/391],                 Loss: 0.35337, Train_Acc:87.85%
Epoch [133/300], Step [270/391],                 Loss: 0.35391, Train_Acc:87.81%
Epoch [133/300], Step [280/391],                 Loss: 0.35534, Train_Acc:87.75%
Epoch [133/300], Step [290/391],                 Loss: 0.35663, Train_Acc:87.71%
Epoch [133/300], Step [300/391],                 Loss: 0.35669, Train_Acc:87.72%
Epoch [133/300], Step [310/391],                 Loss: 0.35597, Train_Acc:87.74%
Epoch [133/300], Step [320/391],                 Loss: 0.35548, Train_Acc:87.79%
Epoch [133/300], Step [330/391],                 Loss: 0.35398, Train_Acc:87.86%
Epoch [133/300], Step [340/391],                 Loss: 0.35257, Train_Acc:87.91%
Epoch [133/300], Step [350/391],                 Loss: 0.35165, Train_Acc:87.94%
Epoch [133/300], Step [360/391],                 Loss: 0.35202, Train_Acc:87.91%
Epoch [133/300], Step [370/391],                 Loss: 0.35238, Train_Acc:87.89%
Epoch [133/300], Step [380/391],                 Loss: 0.35279, Train_Acc:87.86%
Epoch [133/300], Step [390/391],                 Loss: 0.35175, Train_Acc:87.90%
Accuary on test images:72.74%
Epoch [134/300], Step [10/391],                 Loss: 0.36595, Train_Acc:87.73%
Epoch [134/300], Step [20/391],                 Loss: 0.36256, Train_Acc:87.73%
Epoch [134/300], Step [30/391],                 Loss: 0.35520, Train_Acc:87.81%
Epoch [134/300], Step [40/391],                 Loss: 0.35813, Train_Acc:87.77%
Epoch [134/300], Step [50/391],                 Loss: 0.35993, Train_Acc:87.83%
Epoch [134/300], Step [60/391],                 Loss: 0.36107, Train_Acc:87.76%
Epoch [134/300], Step [70/391],                 Loss: 0.35942, Train_Acc:87.90%
Epoch [134/300], Step [80/391],                 Loss: 0.35817, Train_Acc:87.92%
Epoch [134/300], Step [90/391],                 Loss: 0.35920, Train_Acc:87.81%
Epoch [134/300], Step [100/391],                 Loss: 0.35749, Train_Acc:87.89%
Epoch [134/300], Step [110/391],                 Loss: 0.35737, Train_Acc:87.88%
Epoch [134/300], Step [120/391],                 Loss: 0.36095, Train_Acc:87.68%
Epoch [134/300], Step [130/391],                 Loss: 0.36141, Train_Acc:87.67%
Epoch [134/300], Step [140/391],                 Loss: 0.35920, Train_Acc:87.80%
Epoch [134/300], Step [150/391],                 Loss: 0.36003, Train_Acc:87.75%
Epoch [134/300], Step [160/391],                 Loss: 0.35786, Train_Acc:87.79%
Epoch [134/300], Step [170/391],                 Loss: 0.35920, Train_Acc:87.73%
Epoch [134/300], Step [180/391],                 Loss: 0.35921, Train_Acc:87.68%
Epoch [134/300], Step [190/391],                 Loss: 0.35826, Train_Acc:87.70%
Epoch [134/300], Step [200/391],                 Loss: 0.35839, Train_Acc:87.73%
Epoch [134/300], Step [210/391],                 Loss: 0.35820, Train_Acc:87.72%
Epoch [134/300], Step [220/391],                 Loss: 0.35797, Train_Acc:87.76%
Epoch [134/300], Step [230/391],                 Loss: 0.35747, Train_Acc:87.78%
Epoch [134/300], Step [240/391],                 Loss: 0.35603, Train_Acc:87.84%
Epoch [134/300], Step [250/391],                 Loss: 0.35676, Train_Acc:87.82%
Epoch [134/300], Step [260/391],                 Loss: 0.35912, Train_Acc:87.74%
Epoch [134/300], Step [270/391],                 Loss: 0.36066, Train_Acc:87.68%
Epoch [134/300], Step [280/391],                 Loss: 0.36048, Train_Acc:87.68%
Epoch [134/300], Step [290/391],                 Loss: 0.35974, Train_Acc:87.69%
Epoch [134/300], Step [300/391],                 Loss: 0.35852, Train_Acc:87.71%
Epoch [134/300], Step [310/391],                 Loss: 0.35788, Train_Acc:87.72%
Epoch [134/300], Step [320/391],                 Loss: 0.35865, Train_Acc:87.70%
Epoch [134/300], Step [330/391],                 Loss: 0.35773, Train_Acc:87.74%
Epoch [134/300], Step [340/391],                 Loss: 0.35685, Train_Acc:87.75%
Epoch [134/300], Step [350/391],                 Loss: 0.35631, Train_Acc:87.75%
Epoch [134/300], Step [360/391],                 Loss: 0.35636, Train_Acc:87.73%
Epoch [134/300], Step [370/391],                 Loss: 0.35699, Train_Acc:87.70%
Epoch [134/300], Step [380/391],                 Loss: 0.35805, Train_Acc:87.67%
Epoch [134/300], Step [390/391],                 Loss: 0.35757, Train_Acc:87.70%
Accuary on test images:66.02%
Epoch [135/300], Step [10/391],                 Loss: 0.34648, Train_Acc:88.05%
Epoch [135/300], Step [20/391],                 Loss: 0.35908, Train_Acc:87.77%
Epoch [135/300], Step [30/391],                 Loss: 0.35211, Train_Acc:88.23%
Epoch [135/300], Step [40/391],                 Loss: 0.35568, Train_Acc:88.03%
Epoch [135/300], Step [50/391],                 Loss: 0.35536, Train_Acc:88.03%
Epoch [135/300], Step [60/391],                 Loss: 0.35854, Train_Acc:87.77%
Epoch [135/300], Step [70/391],                 Loss: 0.35821, Train_Acc:87.77%
Epoch [135/300], Step [80/391],                 Loss: 0.35950, Train_Acc:87.63%
Epoch [135/300], Step [90/391],                 Loss: 0.36140, Train_Acc:87.42%
Epoch [135/300], Step [100/391],                 Loss: 0.35930, Train_Acc:87.52%
Epoch [135/300], Step [110/391],                 Loss: 0.36115, Train_Acc:87.47%
Epoch [135/300], Step [120/391],                 Loss: 0.36223, Train_Acc:87.45%
Epoch [135/300], Step [130/391],                 Loss: 0.36338, Train_Acc:87.45%
Epoch [135/300], Step [140/391],                 Loss: 0.36249, Train_Acc:87.59%
Epoch [135/300], Step [150/391],                 Loss: 0.36508, Train_Acc:87.52%
Epoch [135/300], Step [160/391],                 Loss: 0.36341, Train_Acc:87.57%
Epoch [135/300], Step [170/391],                 Loss: 0.36240, Train_Acc:87.61%
Epoch [135/300], Step [180/391],                 Loss: 0.36194, Train_Acc:87.66%
Epoch [135/300], Step [190/391],                 Loss: 0.36135, Train_Acc:87.66%
Epoch [135/300], Step [200/391],                 Loss: 0.36122, Train_Acc:87.68%
Epoch [135/300], Step [210/391],                 Loss: 0.36175, Train_Acc:87.67%
Epoch [135/300], Step [220/391],                 Loss: 0.36147, Train_Acc:87.68%
Epoch [135/300], Step [230/391],                 Loss: 0.36011, Train_Acc:87.71%
Epoch [135/300], Step [240/391],                 Loss: 0.35899, Train_Acc:87.73%
Epoch [135/300], Step [250/391],                 Loss: 0.35842, Train_Acc:87.71%
Epoch [135/300], Step [260/391],                 Loss: 0.35997, Train_Acc:87.69%
Epoch [135/300], Step [270/391],                 Loss: 0.36084, Train_Acc:87.67%
Epoch [135/300], Step [280/391],                 Loss: 0.36119, Train_Acc:87.65%
Epoch [135/300], Step [290/391],                 Loss: 0.36072, Train_Acc:87.67%
Epoch [135/300], Step [300/391],                 Loss: 0.36035, Train_Acc:87.70%
Epoch [135/300], Step [310/391],                 Loss: 0.35875, Train_Acc:87.73%
Epoch [135/300], Step [320/391],                 Loss: 0.35827, Train_Acc:87.78%
Epoch [135/300], Step [330/391],                 Loss: 0.35742, Train_Acc:87.82%
Epoch [135/300], Step [340/391],                 Loss: 0.35653, Train_Acc:87.84%
Epoch [135/300], Step [350/391],                 Loss: 0.35642, Train_Acc:87.85%
Epoch [135/300], Step [360/391],                 Loss: 0.35688, Train_Acc:87.81%
Epoch [135/300], Step [370/391],                 Loss: 0.35638, Train_Acc:87.81%
Epoch [135/300], Step [380/391],                 Loss: 0.35537, Train_Acc:87.85%
Epoch [135/300], Step [390/391],                 Loss: 0.35589, Train_Acc:87.85%
Accuary on test images:74.24%
Epoch [136/300], Step [10/391],                 Loss: 0.36670, Train_Acc:87.34%
Epoch [136/300], Step [20/391],                 Loss: 0.35624, Train_Acc:87.54%
Epoch [136/300], Step [30/391],                 Loss: 0.35749, Train_Acc:87.60%
Epoch [136/300], Step [40/391],                 Loss: 0.36388, Train_Acc:87.58%
Epoch [136/300], Step [50/391],                 Loss: 0.36218, Train_Acc:87.78%
Epoch [136/300], Step [60/391],                 Loss: 0.36795, Train_Acc:87.60%
Epoch [136/300], Step [70/391],                 Loss: 0.36164, Train_Acc:87.87%
Epoch [136/300], Step [80/391],                 Loss: 0.36063, Train_Acc:87.94%
Epoch [136/300], Step [90/391],                 Loss: 0.36299, Train_Acc:87.73%
Epoch [136/300], Step [100/391],                 Loss: 0.35904, Train_Acc:87.79%
Epoch [136/300], Step [110/391],                 Loss: 0.35905, Train_Acc:87.83%
Epoch [136/300], Step [120/391],                 Loss: 0.36041, Train_Acc:87.70%
Epoch [136/300], Step [130/391],                 Loss: 0.36327, Train_Acc:87.63%
Epoch [136/300], Step [140/391],                 Loss: 0.36158, Train_Acc:87.64%
Epoch [136/300], Step [150/391],                 Loss: 0.36202, Train_Acc:87.62%
Epoch [136/300], Step [160/391],                 Loss: 0.36101, Train_Acc:87.60%
Epoch [136/300], Step [170/391],                 Loss: 0.36111, Train_Acc:87.60%
Epoch [136/300], Step [180/391],                 Loss: 0.36182, Train_Acc:87.53%
Epoch [136/300], Step [190/391],                 Loss: 0.36361, Train_Acc:87.48%
Epoch [136/300], Step [200/391],                 Loss: 0.36506, Train_Acc:87.46%
Epoch [136/300], Step [210/391],                 Loss: 0.36543, Train_Acc:87.47%
Epoch [136/300], Step [220/391],                 Loss: 0.36306, Train_Acc:87.57%
Epoch [136/300], Step [230/391],                 Loss: 0.36108, Train_Acc:87.62%
Epoch [136/300], Step [240/391],                 Loss: 0.35942, Train_Acc:87.67%
Epoch [136/300], Step [250/391],                 Loss: 0.35783, Train_Acc:87.72%
Epoch [136/300], Step [260/391],                 Loss: 0.35765, Train_Acc:87.74%
Epoch [136/300], Step [270/391],                 Loss: 0.35795, Train_Acc:87.73%
Epoch [136/300], Step [280/391],                 Loss: 0.35795, Train_Acc:87.73%
Epoch [136/300], Step [290/391],                 Loss: 0.35815, Train_Acc:87.74%
Epoch [136/300], Step [300/391],                 Loss: 0.35837, Train_Acc:87.72%
Epoch [136/300], Step [310/391],                 Loss: 0.35819, Train_Acc:87.73%
Epoch [136/300], Step [320/391],                 Loss: 0.35841, Train_Acc:87.71%
Epoch [136/300], Step [330/391],                 Loss: 0.35861, Train_Acc:87.70%
Epoch [136/300], Step [340/391],                 Loss: 0.35802, Train_Acc:87.72%
Epoch [136/300], Step [350/391],                 Loss: 0.35742, Train_Acc:87.76%
Epoch [136/300], Step [360/391],                 Loss: 0.35707, Train_Acc:87.78%
Epoch [136/300], Step [370/391],                 Loss: 0.35730, Train_Acc:87.76%
Epoch [136/300], Step [380/391],                 Loss: 0.35741, Train_Acc:87.76%
Epoch [136/300], Step [390/391],                 Loss: 0.35713, Train_Acc:87.78%
Accuary on test images:76.68%
Epoch [137/300], Step [10/391],                 Loss: 0.38535, Train_Acc:86.72%
Epoch [137/300], Step [20/391],                 Loss: 0.36992, Train_Acc:87.66%
Epoch [137/300], Step [30/391],                 Loss: 0.36107, Train_Acc:87.94%
Epoch [137/300], Step [40/391],                 Loss: 0.34954, Train_Acc:88.40%
Epoch [137/300], Step [50/391],                 Loss: 0.35319, Train_Acc:88.19%
Epoch [137/300], Step [60/391],                 Loss: 0.35981, Train_Acc:87.85%
Epoch [137/300], Step [70/391],                 Loss: 0.35712, Train_Acc:88.04%
Epoch [137/300], Step [80/391],                 Loss: 0.36118, Train_Acc:87.90%
Epoch [137/300], Step [90/391],                 Loss: 0.36185, Train_Acc:87.77%
Epoch [137/300], Step [100/391],                 Loss: 0.35904, Train_Acc:87.81%
Epoch [137/300], Step [110/391],                 Loss: 0.35932, Train_Acc:87.83%
Epoch [137/300], Step [120/391],                 Loss: 0.36008, Train_Acc:87.75%
Epoch [137/300], Step [130/391],                 Loss: 0.36257, Train_Acc:87.67%
Epoch [137/300], Step [140/391],                 Loss: 0.36290, Train_Acc:87.66%
Epoch [137/300], Step [150/391],                 Loss: 0.36396, Train_Acc:87.61%
Epoch [137/300], Step [160/391],                 Loss: 0.36473, Train_Acc:87.68%
Epoch [137/300], Step [170/391],                 Loss: 0.36493, Train_Acc:87.64%
Epoch [137/300], Step [180/391],                 Loss: 0.36454, Train_Acc:87.63%
Epoch [137/300], Step [190/391],                 Loss: 0.36473, Train_Acc:87.68%
Epoch [137/300], Step [200/391],                 Loss: 0.36368, Train_Acc:87.63%
Epoch [137/300], Step [210/391],                 Loss: 0.36305, Train_Acc:87.67%
Epoch [137/300], Step [220/391],                 Loss: 0.36143, Train_Acc:87.72%
Epoch [137/300], Step [230/391],                 Loss: 0.35921, Train_Acc:87.79%
Epoch [137/300], Step [240/391],                 Loss: 0.35837, Train_Acc:87.83%
Epoch [137/300], Step [250/391],                 Loss: 0.35788, Train_Acc:87.81%
Epoch [137/300], Step [260/391],                 Loss: 0.35922, Train_Acc:87.79%
Epoch [137/300], Step [270/391],                 Loss: 0.35853, Train_Acc:87.84%
Epoch [137/300], Step [280/391],                 Loss: 0.35932, Train_Acc:87.81%
Epoch [137/300], Step [290/391],                 Loss: 0.36010, Train_Acc:87.80%
Epoch [137/300], Step [300/391],                 Loss: 0.35895, Train_Acc:87.84%
Epoch [137/300], Step [310/391],                 Loss: 0.35848, Train_Acc:87.86%
Epoch [137/300], Step [320/391],                 Loss: 0.35961, Train_Acc:87.80%
Epoch [137/300], Step [330/391],                 Loss: 0.35901, Train_Acc:87.81%
Epoch [137/300], Step [340/391],                 Loss: 0.35788, Train_Acc:87.84%
Epoch [137/300], Step [350/391],                 Loss: 0.35670, Train_Acc:87.91%
Epoch [137/300], Step [360/391],                 Loss: 0.35704, Train_Acc:87.89%
Epoch [137/300], Step [370/391],                 Loss: 0.35691, Train_Acc:87.91%
Epoch [137/300], Step [380/391],                 Loss: 0.35650, Train_Acc:87.93%
Epoch [137/300], Step [390/391],                 Loss: 0.35583, Train_Acc:87.94%
Accuary on test images:65.30%
Epoch [138/300], Step [10/391],                 Loss: 0.38079, Train_Acc:87.27%
Epoch [138/300], Step [20/391],                 Loss: 0.35313, Train_Acc:88.67%
Epoch [138/300], Step [30/391],                 Loss: 0.33545, Train_Acc:88.96%
Epoch [138/300], Step [40/391],                 Loss: 0.33686, Train_Acc:88.98%
Epoch [138/300], Step [50/391],                 Loss: 0.33951, Train_Acc:88.75%
Epoch [138/300], Step [60/391],                 Loss: 0.34442, Train_Acc:88.53%
Epoch [138/300], Step [70/391],                 Loss: 0.34494, Train_Acc:88.58%
Epoch [138/300], Step [80/391],                 Loss: 0.34391, Train_Acc:88.63%
Epoch [138/300], Step [90/391],                 Loss: 0.34794, Train_Acc:88.47%
Epoch [138/300], Step [100/391],                 Loss: 0.34817, Train_Acc:88.45%
Epoch [138/300], Step [110/391],                 Loss: 0.35050, Train_Acc:88.34%
Epoch [138/300], Step [120/391],                 Loss: 0.35252, Train_Acc:88.24%
Epoch [138/300], Step [130/391],                 Loss: 0.35626, Train_Acc:88.09%
Epoch [138/300], Step [140/391],                 Loss: 0.35573, Train_Acc:88.12%
Epoch [138/300], Step [150/391],                 Loss: 0.35677, Train_Acc:88.11%
Epoch [138/300], Step [160/391],                 Loss: 0.35572, Train_Acc:88.13%
Epoch [138/300], Step [170/391],                 Loss: 0.35696, Train_Acc:88.07%
Epoch [138/300], Step [180/391],                 Loss: 0.35807, Train_Acc:88.01%
Epoch [138/300], Step [190/391],                 Loss: 0.35841, Train_Acc:87.97%
Epoch [138/300], Step [200/391],                 Loss: 0.35890, Train_Acc:87.88%
Epoch [138/300], Step [210/391],                 Loss: 0.36007, Train_Acc:87.82%
Epoch [138/300], Step [220/391],                 Loss: 0.35949, Train_Acc:87.82%
Epoch [138/300], Step [230/391],                 Loss: 0.35700, Train_Acc:87.89%
Epoch [138/300], Step [240/391],                 Loss: 0.35565, Train_Acc:87.92%
Epoch [138/300], Step [250/391],                 Loss: 0.35713, Train_Acc:87.88%
Epoch [138/300], Step [260/391],                 Loss: 0.36017, Train_Acc:87.84%
Epoch [138/300], Step [270/391],                 Loss: 0.36174, Train_Acc:87.77%
Epoch [138/300], Step [280/391],                 Loss: 0.36290, Train_Acc:87.72%
Epoch [138/300], Step [290/391],                 Loss: 0.36234, Train_Acc:87.73%
Epoch [138/300], Step [300/391],                 Loss: 0.36200, Train_Acc:87.75%
Epoch [138/300], Step [310/391],                 Loss: 0.36203, Train_Acc:87.72%
Epoch [138/300], Step [320/391],                 Loss: 0.36224, Train_Acc:87.73%
Epoch [138/300], Step [330/391],                 Loss: 0.36146, Train_Acc:87.74%
Epoch [138/300], Step [340/391],                 Loss: 0.36077, Train_Acc:87.77%
Epoch [138/300], Step [350/391],                 Loss: 0.36001, Train_Acc:87.83%
Epoch [138/300], Step [360/391],                 Loss: 0.35897, Train_Acc:87.84%
Epoch [138/300], Step [370/391],                 Loss: 0.35918, Train_Acc:87.83%
Epoch [138/300], Step [380/391],                 Loss: 0.35956, Train_Acc:87.84%
Epoch [138/300], Step [390/391],                 Loss: 0.35924, Train_Acc:87.84%
Accuary on test images:74.22%
Epoch [139/300], Step [10/391],                 Loss: 0.34810, Train_Acc:88.36%
Epoch [139/300], Step [20/391],                 Loss: 0.32843, Train_Acc:88.52%
Epoch [139/300], Step [30/391],                 Loss: 0.32226, Train_Acc:88.80%
Epoch [139/300], Step [40/391],                 Loss: 0.33210, Train_Acc:88.87%
Epoch [139/300], Step [50/391],                 Loss: 0.32693, Train_Acc:89.02%
Epoch [139/300], Step [60/391],                 Loss: 0.33039, Train_Acc:88.93%
Epoch [139/300], Step [70/391],                 Loss: 0.32845, Train_Acc:89.02%
Epoch [139/300], Step [80/391],                 Loss: 0.33236, Train_Acc:88.85%
Epoch [139/300], Step [90/391],                 Loss: 0.33658, Train_Acc:88.61%
Epoch [139/300], Step [100/391],                 Loss: 0.34217, Train_Acc:88.46%
Epoch [139/300], Step [110/391],                 Loss: 0.34683, Train_Acc:88.28%
Epoch [139/300], Step [120/391],                 Loss: 0.35018, Train_Acc:88.12%
Epoch [139/300], Step [130/391],                 Loss: 0.35493, Train_Acc:88.03%
Epoch [139/300], Step [140/391],                 Loss: 0.35423, Train_Acc:88.05%
Epoch [139/300], Step [150/391],                 Loss: 0.35253, Train_Acc:88.10%
Epoch [139/300], Step [160/391],                 Loss: 0.35246, Train_Acc:88.08%
Epoch [139/300], Step [170/391],                 Loss: 0.35282, Train_Acc:88.04%
Epoch [139/300], Step [180/391],                 Loss: 0.35228, Train_Acc:88.05%
Epoch [139/300], Step [190/391],                 Loss: 0.35276, Train_Acc:88.04%
Epoch [139/300], Step [200/391],                 Loss: 0.35216, Train_Acc:88.04%
Epoch [139/300], Step [210/391],                 Loss: 0.35272, Train_Acc:88.05%
Epoch [139/300], Step [220/391],                 Loss: 0.35369, Train_Acc:88.00%
Epoch [139/300], Step [230/391],                 Loss: 0.35317, Train_Acc:88.02%
Epoch [139/300], Step [240/391],                 Loss: 0.35070, Train_Acc:88.11%
Epoch [139/300], Step [250/391],                 Loss: 0.34939, Train_Acc:88.15%
Epoch [139/300], Step [260/391],                 Loss: 0.35078, Train_Acc:88.09%
Epoch [139/300], Step [270/391],                 Loss: 0.35189, Train_Acc:88.04%
Epoch [139/300], Step [280/391],                 Loss: 0.35152, Train_Acc:88.05%
Epoch [139/300], Step [290/391],                 Loss: 0.35102, Train_Acc:88.08%
Epoch [139/300], Step [300/391],                 Loss: 0.34949, Train_Acc:88.10%
Epoch [139/300], Step [310/391],                 Loss: 0.34859, Train_Acc:88.10%
Epoch [139/300], Step [320/391],                 Loss: 0.34927, Train_Acc:88.05%
Epoch [139/300], Step [330/391],                 Loss: 0.34909, Train_Acc:88.06%
Epoch [139/300], Step [340/391],                 Loss: 0.34945, Train_Acc:88.03%
Epoch [139/300], Step [350/391],                 Loss: 0.34998, Train_Acc:88.04%
Epoch [139/300], Step [360/391],                 Loss: 0.35036, Train_Acc:88.01%
Epoch [139/300], Step [370/391],                 Loss: 0.35072, Train_Acc:87.98%
Epoch [139/300], Step [380/391],                 Loss: 0.35042, Train_Acc:88.00%
Epoch [139/300], Step [390/391],                 Loss: 0.35037, Train_Acc:88.01%
Accuary on test images:77.66%
Epoch [140/300], Step [10/391],                 Loss: 0.34233, Train_Acc:88.36%
Epoch [140/300], Step [20/391],                 Loss: 0.36475, Train_Acc:87.77%
Epoch [140/300], Step [30/391],                 Loss: 0.34887, Train_Acc:88.23%
Epoch [140/300], Step [40/391],                 Loss: 0.35591, Train_Acc:88.24%
Epoch [140/300], Step [50/391],                 Loss: 0.35278, Train_Acc:88.09%
Epoch [140/300], Step [60/391],                 Loss: 0.35461, Train_Acc:88.06%
Epoch [140/300], Step [70/391],                 Loss: 0.35072, Train_Acc:88.17%
Epoch [140/300], Step [80/391],                 Loss: 0.35383, Train_Acc:88.11%
Epoch [140/300], Step [90/391],                 Loss: 0.35380, Train_Acc:88.07%
Epoch [140/300], Step [100/391],                 Loss: 0.35332, Train_Acc:88.10%
Epoch [140/300], Step [110/391],                 Loss: 0.35588, Train_Acc:88.01%
Epoch [140/300], Step [120/391],                 Loss: 0.35404, Train_Acc:88.04%
Epoch [140/300], Step [130/391],                 Loss: 0.35487, Train_Acc:87.94%
Epoch [140/300], Step [140/391],                 Loss: 0.35501, Train_Acc:88.02%
Epoch [140/300], Step [150/391],                 Loss: 0.35738, Train_Acc:87.93%
Epoch [140/300], Step [160/391],                 Loss: 0.35791, Train_Acc:87.88%
Epoch [140/300], Step [170/391],                 Loss: 0.35708, Train_Acc:87.87%
Epoch [140/300], Step [180/391],                 Loss: 0.35461, Train_Acc:87.97%
Epoch [140/300], Step [190/391],                 Loss: 0.35403, Train_Acc:88.04%
Epoch [140/300], Step [200/391],                 Loss: 0.35202, Train_Acc:88.13%
Epoch [140/300], Step [210/391],                 Loss: 0.35140, Train_Acc:88.14%
Epoch [140/300], Step [220/391],                 Loss: 0.35004, Train_Acc:88.16%
Epoch [140/300], Step [230/391],                 Loss: 0.34960, Train_Acc:88.15%
Epoch [140/300], Step [240/391],                 Loss: 0.34876, Train_Acc:88.17%
Epoch [140/300], Step [250/391],                 Loss: 0.34951, Train_Acc:88.15%
Epoch [140/300], Step [260/391],                 Loss: 0.35086, Train_Acc:88.10%
Epoch [140/300], Step [270/391],                 Loss: 0.35074, Train_Acc:88.06%
Epoch [140/300], Step [280/391],                 Loss: 0.35035, Train_Acc:88.08%
Epoch [140/300], Step [290/391],                 Loss: 0.35055, Train_Acc:88.08%
Epoch [140/300], Step [300/391],                 Loss: 0.34951, Train_Acc:88.12%
Epoch [140/300], Step [310/391],                 Loss: 0.34965, Train_Acc:88.13%
Epoch [140/300], Step [320/391],                 Loss: 0.35064, Train_Acc:88.11%
Epoch [140/300], Step [330/391],                 Loss: 0.34997, Train_Acc:88.10%
Epoch [140/300], Step [340/391],                 Loss: 0.34848, Train_Acc:88.12%
Epoch [140/300], Step [350/391],                 Loss: 0.34842, Train_Acc:88.13%
Epoch [140/300], Step [360/391],                 Loss: 0.34908, Train_Acc:88.11%
Epoch [140/300], Step [370/391],                 Loss: 0.34990, Train_Acc:88.08%
Epoch [140/300], Step [380/391],                 Loss: 0.34979, Train_Acc:88.10%
Epoch [140/300], Step [390/391],                 Loss: 0.34951, Train_Acc:88.13%
Accuary on test images:71.62%
Epoch [141/300], Step [10/391],                 Loss: 0.30926, Train_Acc:88.44%
Epoch [141/300], Step [20/391],                 Loss: 0.33508, Train_Acc:88.44%
Epoch [141/300], Step [30/391],                 Loss: 0.34392, Train_Acc:88.26%
Epoch [141/300], Step [40/391],                 Loss: 0.34226, Train_Acc:88.65%
Epoch [141/300], Step [50/391],                 Loss: 0.34517, Train_Acc:88.27%
Epoch [141/300], Step [60/391],                 Loss: 0.35152, Train_Acc:87.93%
Epoch [141/300], Step [70/391],                 Loss: 0.35204, Train_Acc:87.90%
Epoch [141/300], Step [80/391],                 Loss: 0.35276, Train_Acc:87.88%
Epoch [141/300], Step [90/391],                 Loss: 0.35535, Train_Acc:87.80%
Epoch [141/300], Step [100/391],                 Loss: 0.34990, Train_Acc:88.00%
Epoch [141/300], Step [110/391],                 Loss: 0.35123, Train_Acc:87.95%
Epoch [141/300], Step [120/391],                 Loss: 0.35118, Train_Acc:87.98%
Epoch [141/300], Step [130/391],                 Loss: 0.35460, Train_Acc:87.90%
Epoch [141/300], Step [140/391],                 Loss: 0.35622, Train_Acc:87.89%
Epoch [141/300], Step [150/391],                 Loss: 0.35846, Train_Acc:87.90%
Epoch [141/300], Step [160/391],                 Loss: 0.35904, Train_Acc:87.90%
Epoch [141/300], Step [170/391],                 Loss: 0.35993, Train_Acc:87.90%
Epoch [141/300], Step [180/391],                 Loss: 0.36138, Train_Acc:87.77%
Epoch [141/300], Step [190/391],                 Loss: 0.36164, Train_Acc:87.74%
Epoch [141/300], Step [200/391],                 Loss: 0.36163, Train_Acc:87.69%
Epoch [141/300], Step [210/391],                 Loss: 0.36317, Train_Acc:87.63%
Epoch [141/300], Step [220/391],                 Loss: 0.36279, Train_Acc:87.63%
Epoch [141/300], Step [230/391],                 Loss: 0.36260, Train_Acc:87.65%
Epoch [141/300], Step [240/391],                 Loss: 0.35978, Train_Acc:87.75%
Epoch [141/300], Step [250/391],                 Loss: 0.35997, Train_Acc:87.75%
Epoch [141/300], Step [260/391],                 Loss: 0.36092, Train_Acc:87.76%
Epoch [141/300], Step [270/391],                 Loss: 0.36147, Train_Acc:87.73%
Epoch [141/300], Step [280/391],                 Loss: 0.36125, Train_Acc:87.73%
Epoch [141/300], Step [290/391],                 Loss: 0.36210, Train_Acc:87.71%
Epoch [141/300], Step [300/391],                 Loss: 0.36147, Train_Acc:87.75%
Epoch [141/300], Step [310/391],                 Loss: 0.36057, Train_Acc:87.81%
Epoch [141/300], Step [320/391],                 Loss: 0.36083, Train_Acc:87.79%
Epoch [141/300], Step [330/391],                 Loss: 0.36040, Train_Acc:87.80%
Epoch [141/300], Step [340/391],                 Loss: 0.36047, Train_Acc:87.80%
Epoch [141/300], Step [350/391],                 Loss: 0.35996, Train_Acc:87.81%
Epoch [141/300], Step [360/391],                 Loss: 0.36006, Train_Acc:87.82%
Epoch [141/300], Step [370/391],                 Loss: 0.36008, Train_Acc:87.79%
Epoch [141/300], Step [380/391],                 Loss: 0.36029, Train_Acc:87.78%
Epoch [141/300], Step [390/391],                 Loss: 0.35932, Train_Acc:87.81%
Accuary on test images:76.52%
Epoch [142/300], Step [10/391],                 Loss: 0.34509, Train_Acc:88.59%
Epoch [142/300], Step [20/391],                 Loss: 0.33411, Train_Acc:88.67%
Epoch [142/300], Step [30/391],                 Loss: 0.32993, Train_Acc:88.83%
Epoch [142/300], Step [40/391],                 Loss: 0.33313, Train_Acc:88.57%
Epoch [142/300], Step [50/391],                 Loss: 0.33901, Train_Acc:88.22%
Epoch [142/300], Step [60/391],                 Loss: 0.35018, Train_Acc:87.77%
Epoch [142/300], Step [70/391],                 Loss: 0.34801, Train_Acc:87.89%
Epoch [142/300], Step [80/391],                 Loss: 0.34812, Train_Acc:88.04%
Epoch [142/300], Step [90/391],                 Loss: 0.34815, Train_Acc:88.02%
Epoch [142/300], Step [100/391],                 Loss: 0.34706, Train_Acc:88.05%
Epoch [142/300], Step [110/391],                 Loss: 0.35154, Train_Acc:87.90%
Epoch [142/300], Step [120/391],                 Loss: 0.35362, Train_Acc:87.83%
Epoch [142/300], Step [130/391],                 Loss: 0.35706, Train_Acc:87.78%
Epoch [142/300], Step [140/391],                 Loss: 0.35529, Train_Acc:87.92%
Epoch [142/300], Step [150/391],                 Loss: 0.35566, Train_Acc:87.94%
Epoch [142/300], Step [160/391],                 Loss: 0.35383, Train_Acc:88.03%
Epoch [142/300], Step [170/391],                 Loss: 0.35321, Train_Acc:88.09%
Epoch [142/300], Step [180/391],                 Loss: 0.35427, Train_Acc:88.09%
Epoch [142/300], Step [190/391],                 Loss: 0.35462, Train_Acc:88.06%
Epoch [142/300], Step [200/391],                 Loss: 0.35482, Train_Acc:88.06%
Epoch [142/300], Step [210/391],                 Loss: 0.35436, Train_Acc:88.08%
Epoch [142/300], Step [220/391],                 Loss: 0.35382, Train_Acc:88.11%
Epoch [142/300], Step [230/391],                 Loss: 0.35126, Train_Acc:88.22%
Epoch [142/300], Step [240/391],                 Loss: 0.34984, Train_Acc:88.26%
Epoch [142/300], Step [250/391],                 Loss: 0.35055, Train_Acc:88.18%
Epoch [142/300], Step [260/391],                 Loss: 0.35197, Train_Acc:88.16%
Epoch [142/300], Step [270/391],                 Loss: 0.35393, Train_Acc:88.12%
Epoch [142/300], Step [280/391],                 Loss: 0.35418, Train_Acc:88.09%
Epoch [142/300], Step [290/391],                 Loss: 0.35516, Train_Acc:88.03%
Epoch [142/300], Step [300/391],                 Loss: 0.35451, Train_Acc:88.02%
Epoch [142/300], Step [310/391],                 Loss: 0.35373, Train_Acc:88.03%
Epoch [142/300], Step [320/391],                 Loss: 0.35357, Train_Acc:88.02%
Epoch [142/300], Step [330/391],                 Loss: 0.35420, Train_Acc:88.00%
Epoch [142/300], Step [340/391],                 Loss: 0.35484, Train_Acc:87.98%
Epoch [142/300], Step [350/391],                 Loss: 0.35492, Train_Acc:87.98%
Epoch [142/300], Step [360/391],                 Loss: 0.35547, Train_Acc:87.95%
Epoch [142/300], Step [370/391],                 Loss: 0.35540, Train_Acc:87.94%
Epoch [142/300], Step [380/391],                 Loss: 0.35482, Train_Acc:87.96%
Epoch [142/300], Step [390/391],                 Loss: 0.35406, Train_Acc:87.98%
Accuary on test images:71.68%
Epoch [143/300], Step [10/391],                 Loss: 0.36832, Train_Acc:88.05%
Epoch [143/300], Step [20/391],                 Loss: 0.35825, Train_Acc:88.59%
Epoch [143/300], Step [30/391],                 Loss: 0.34799, Train_Acc:88.75%
Epoch [143/300], Step [40/391],                 Loss: 0.35698, Train_Acc:88.38%
Epoch [143/300], Step [50/391],                 Loss: 0.35657, Train_Acc:88.36%
Epoch [143/300], Step [60/391],                 Loss: 0.35785, Train_Acc:88.26%
Epoch [143/300], Step [70/391],                 Loss: 0.35729, Train_Acc:88.14%
Epoch [143/300], Step [80/391],                 Loss: 0.35663, Train_Acc:88.19%
Epoch [143/300], Step [90/391],                 Loss: 0.35806, Train_Acc:88.03%
Epoch [143/300], Step [100/391],                 Loss: 0.35306, Train_Acc:88.20%
Epoch [143/300], Step [110/391],                 Loss: 0.35318, Train_Acc:88.15%
Epoch [143/300], Step [120/391],                 Loss: 0.35141, Train_Acc:88.20%
Epoch [143/300], Step [130/391],                 Loss: 0.35245, Train_Acc:88.14%
Epoch [143/300], Step [140/391],                 Loss: 0.35293, Train_Acc:88.06%
Epoch [143/300], Step [150/391],                 Loss: 0.35470, Train_Acc:88.07%
Epoch [143/300], Step [160/391],                 Loss: 0.35371, Train_Acc:88.09%
Epoch [143/300], Step [170/391],                 Loss: 0.35414, Train_Acc:88.07%
Epoch [143/300], Step [180/391],                 Loss: 0.35380, Train_Acc:88.09%
Epoch [143/300], Step [190/391],                 Loss: 0.35364, Train_Acc:88.04%
Epoch [143/300], Step [200/391],                 Loss: 0.35364, Train_Acc:88.04%
Epoch [143/300], Step [210/391],                 Loss: 0.35273, Train_Acc:88.07%
Epoch [143/300], Step [220/391],                 Loss: 0.35141, Train_Acc:88.14%
Epoch [143/300], Step [230/391],                 Loss: 0.35123, Train_Acc:88.16%
Epoch [143/300], Step [240/391],                 Loss: 0.34841, Train_Acc:88.27%
Epoch [143/300], Step [250/391],                 Loss: 0.34815, Train_Acc:88.27%
Epoch [143/300], Step [260/391],                 Loss: 0.34988, Train_Acc:88.23%
Epoch [143/300], Step [270/391],                 Loss: 0.35195, Train_Acc:88.16%
Epoch [143/300], Step [280/391],                 Loss: 0.35374, Train_Acc:88.08%
Epoch [143/300], Step [290/391],                 Loss: 0.35412, Train_Acc:88.05%
Epoch [143/300], Step [300/391],                 Loss: 0.35503, Train_Acc:88.01%
Epoch [143/300], Step [310/391],                 Loss: 0.35493, Train_Acc:88.00%
Epoch [143/300], Step [320/391],                 Loss: 0.35570, Train_Acc:87.94%
Epoch [143/300], Step [330/391],                 Loss: 0.35524, Train_Acc:87.95%
Epoch [143/300], Step [340/391],                 Loss: 0.35441, Train_Acc:87.98%
Epoch [143/300], Step [350/391],                 Loss: 0.35371, Train_Acc:88.00%
Epoch [143/300], Step [360/391],                 Loss: 0.35335, Train_Acc:88.00%
Epoch [143/300], Step [370/391],                 Loss: 0.35446, Train_Acc:87.97%
Epoch [143/300], Step [380/391],                 Loss: 0.35420, Train_Acc:87.96%
Epoch [143/300], Step [390/391],                 Loss: 0.35347, Train_Acc:88.00%
Accuary on test images:73.74%
Epoch [144/300], Step [10/391],                 Loss: 0.38537, Train_Acc:87.34%
Epoch [144/300], Step [20/391],                 Loss: 0.38844, Train_Acc:87.15%
Epoch [144/300], Step [30/391],                 Loss: 0.36468, Train_Acc:87.79%
Epoch [144/300], Step [40/391],                 Loss: 0.35427, Train_Acc:88.14%
Epoch [144/300], Step [50/391],                 Loss: 0.34898, Train_Acc:88.20%
Epoch [144/300], Step [60/391],                 Loss: 0.35413, Train_Acc:88.01%
Epoch [144/300], Step [70/391],                 Loss: 0.35587, Train_Acc:87.91%
Epoch [144/300], Step [80/391],                 Loss: 0.35679, Train_Acc:87.84%
Epoch [144/300], Step [90/391],                 Loss: 0.36190, Train_Acc:87.60%
Epoch [144/300], Step [100/391],                 Loss: 0.36042, Train_Acc:87.53%
Epoch [144/300], Step [110/391],                 Loss: 0.36263, Train_Acc:87.49%
Epoch [144/300], Step [120/391],                 Loss: 0.36283, Train_Acc:87.47%
Epoch [144/300], Step [130/391],                 Loss: 0.36322, Train_Acc:87.49%
Epoch [144/300], Step [140/391],                 Loss: 0.35996, Train_Acc:87.60%
Epoch [144/300], Step [150/391],                 Loss: 0.36068, Train_Acc:87.58%
Epoch [144/300], Step [160/391],                 Loss: 0.36126, Train_Acc:87.57%
Epoch [144/300], Step [170/391],                 Loss: 0.36117, Train_Acc:87.61%
Epoch [144/300], Step [180/391],                 Loss: 0.36293, Train_Acc:87.58%
Epoch [144/300], Step [190/391],                 Loss: 0.36440, Train_Acc:87.51%
Epoch [144/300], Step [200/391],                 Loss: 0.36523, Train_Acc:87.45%
Epoch [144/300], Step [210/391],                 Loss: 0.36370, Train_Acc:87.57%
Epoch [144/300], Step [220/391],                 Loss: 0.36147, Train_Acc:87.64%
Epoch [144/300], Step [230/391],                 Loss: 0.35796, Train_Acc:87.72%
Epoch [144/300], Step [240/391],                 Loss: 0.35556, Train_Acc:87.83%
Epoch [144/300], Step [250/391],                 Loss: 0.35448, Train_Acc:87.89%
Epoch [144/300], Step [260/391],                 Loss: 0.35539, Train_Acc:87.81%
Epoch [144/300], Step [270/391],                 Loss: 0.35687, Train_Acc:87.76%
Epoch [144/300], Step [280/391],                 Loss: 0.35808, Train_Acc:87.73%
Epoch [144/300], Step [290/391],                 Loss: 0.35769, Train_Acc:87.78%
Epoch [144/300], Step [300/391],                 Loss: 0.35744, Train_Acc:87.78%
Epoch [144/300], Step [310/391],                 Loss: 0.35757, Train_Acc:87.78%
Epoch [144/300], Step [320/391],                 Loss: 0.35759, Train_Acc:87.79%
Epoch [144/300], Step [330/391],                 Loss: 0.35691, Train_Acc:87.80%
Epoch [144/300], Step [340/391],                 Loss: 0.35667, Train_Acc:87.80%
Epoch [144/300], Step [350/391],                 Loss: 0.35663, Train_Acc:87.82%
Epoch [144/300], Step [360/391],                 Loss: 0.35647, Train_Acc:87.80%
Epoch [144/300], Step [370/391],                 Loss: 0.35743, Train_Acc:87.76%
Epoch [144/300], Step [380/391],                 Loss: 0.35783, Train_Acc:87.76%
Epoch [144/300], Step [390/391],                 Loss: 0.35724, Train_Acc:87.80%
Accuary on test images:72.88%
Epoch [145/300], Step [10/391],                 Loss: 0.34801, Train_Acc:87.89%
Epoch [145/300], Step [20/391],                 Loss: 0.36165, Train_Acc:87.62%
Epoch [145/300], Step [30/391],                 Loss: 0.36777, Train_Acc:87.40%
Epoch [145/300], Step [40/391],                 Loss: 0.36810, Train_Acc:87.60%
Epoch [145/300], Step [50/391],                 Loss: 0.36586, Train_Acc:87.55%
Epoch [145/300], Step [60/391],                 Loss: 0.36771, Train_Acc:87.53%
Epoch [145/300], Step [70/391],                 Loss: 0.36033, Train_Acc:87.83%
Epoch [145/300], Step [80/391],                 Loss: 0.35579, Train_Acc:88.07%
Epoch [145/300], Step [90/391],                 Loss: 0.35510, Train_Acc:88.02%
Epoch [145/300], Step [100/391],                 Loss: 0.35249, Train_Acc:88.16%
Epoch [145/300], Step [110/391],                 Loss: 0.35259, Train_Acc:88.13%
Epoch [145/300], Step [120/391],                 Loss: 0.35045, Train_Acc:88.18%
Epoch [145/300], Step [130/391],                 Loss: 0.35437, Train_Acc:88.03%
Epoch [145/300], Step [140/391],                 Loss: 0.35222, Train_Acc:88.15%
Epoch [145/300], Step [150/391],                 Loss: 0.35297, Train_Acc:88.09%
Epoch [145/300], Step [160/391],                 Loss: 0.35303, Train_Acc:88.06%
Epoch [145/300], Step [170/391],                 Loss: 0.35535, Train_Acc:87.92%
Epoch [145/300], Step [180/391],                 Loss: 0.35590, Train_Acc:87.86%
Epoch [145/300], Step [190/391],                 Loss: 0.35630, Train_Acc:87.84%
Epoch [145/300], Step [200/391],                 Loss: 0.35731, Train_Acc:87.81%
Epoch [145/300], Step [210/391],                 Loss: 0.35822, Train_Acc:87.82%
Epoch [145/300], Step [220/391],                 Loss: 0.35806, Train_Acc:87.82%
Epoch [145/300], Step [230/391],                 Loss: 0.35638, Train_Acc:87.88%
Epoch [145/300], Step [240/391],                 Loss: 0.35486, Train_Acc:87.95%
Epoch [145/300], Step [250/391],                 Loss: 0.35330, Train_Acc:88.00%
Epoch [145/300], Step [260/391],                 Loss: 0.35600, Train_Acc:87.90%
Epoch [145/300], Step [270/391],                 Loss: 0.35716, Train_Acc:87.85%
Epoch [145/300], Step [280/391],                 Loss: 0.35595, Train_Acc:87.90%
Epoch [145/300], Step [290/391],                 Loss: 0.35580, Train_Acc:87.91%
Epoch [145/300], Step [300/391],                 Loss: 0.35564, Train_Acc:87.90%
Epoch [145/300], Step [310/391],                 Loss: 0.35400, Train_Acc:87.97%
Epoch [145/300], Step [320/391],                 Loss: 0.35322, Train_Acc:87.99%
Epoch [145/300], Step [330/391],                 Loss: 0.35283, Train_Acc:88.00%
Epoch [145/300], Step [340/391],                 Loss: 0.35258, Train_Acc:88.00%
Epoch [145/300], Step [350/391],                 Loss: 0.35228, Train_Acc:88.01%
Epoch [145/300], Step [360/391],                 Loss: 0.35236, Train_Acc:88.00%
Epoch [145/300], Step [370/391],                 Loss: 0.35284, Train_Acc:87.99%
Epoch [145/300], Step [380/391],                 Loss: 0.35312, Train_Acc:87.99%
Epoch [145/300], Step [390/391],                 Loss: 0.35279, Train_Acc:87.99%
Accuary on test images:77.52%
Epoch [146/300], Step [10/391],                 Loss: 0.38548, Train_Acc:87.42%
Epoch [146/300], Step [20/391],                 Loss: 0.38525, Train_Acc:87.30%
Epoch [146/300], Step [30/391],                 Loss: 0.37962, Train_Acc:87.58%
Epoch [146/300], Step [40/391],                 Loss: 0.36680, Train_Acc:88.09%
Epoch [146/300], Step [50/391],                 Loss: 0.36191, Train_Acc:88.02%
Epoch [146/300], Step [60/391],                 Loss: 0.36436, Train_Acc:87.89%
Epoch [146/300], Step [70/391],                 Loss: 0.36244, Train_Acc:87.99%
Epoch [146/300], Step [80/391],                 Loss: 0.36053, Train_Acc:88.03%
Epoch [146/300], Step [90/391],                 Loss: 0.35912, Train_Acc:87.99%
Epoch [146/300], Step [100/391],                 Loss: 0.35390, Train_Acc:88.09%
Epoch [146/300], Step [110/391],                 Loss: 0.35598, Train_Acc:87.95%
Epoch [146/300], Step [120/391],                 Loss: 0.35443, Train_Acc:87.92%
Epoch [146/300], Step [130/391],                 Loss: 0.35640, Train_Acc:87.89%
Epoch [146/300], Step [140/391],                 Loss: 0.35539, Train_Acc:87.88%
Epoch [146/300], Step [150/391],                 Loss: 0.35533, Train_Acc:87.91%
Epoch [146/300], Step [160/391],                 Loss: 0.35340, Train_Acc:87.94%
Epoch [146/300], Step [170/391],                 Loss: 0.35407, Train_Acc:87.89%
Epoch [146/300], Step [180/391],                 Loss: 0.35339, Train_Acc:87.88%
Epoch [146/300], Step [190/391],                 Loss: 0.35580, Train_Acc:87.82%
Epoch [146/300], Step [200/391],                 Loss: 0.35636, Train_Acc:87.79%
Epoch [146/300], Step [210/391],                 Loss: 0.35637, Train_Acc:87.81%
Epoch [146/300], Step [220/391],                 Loss: 0.35651, Train_Acc:87.80%
Epoch [146/300], Step [230/391],                 Loss: 0.35371, Train_Acc:87.87%
Epoch [146/300], Step [240/391],                 Loss: 0.35235, Train_Acc:87.89%
Epoch [146/300], Step [250/391],                 Loss: 0.35178, Train_Acc:87.91%
Epoch [146/300], Step [260/391],                 Loss: 0.35366, Train_Acc:87.84%
Epoch [146/300], Step [270/391],                 Loss: 0.35547, Train_Acc:87.77%
Epoch [146/300], Step [280/391],                 Loss: 0.35610, Train_Acc:87.77%
Epoch [146/300], Step [290/391],                 Loss: 0.35596, Train_Acc:87.79%
Epoch [146/300], Step [300/391],                 Loss: 0.35637, Train_Acc:87.75%
Epoch [146/300], Step [310/391],                 Loss: 0.35571, Train_Acc:87.79%
Epoch [146/300], Step [320/391],                 Loss: 0.35606, Train_Acc:87.77%
Epoch [146/300], Step [330/391],                 Loss: 0.35450, Train_Acc:87.82%
Epoch [146/300], Step [340/391],                 Loss: 0.35358, Train_Acc:87.85%
Epoch [146/300], Step [350/391],                 Loss: 0.35385, Train_Acc:87.86%
Epoch [146/300], Step [360/391],                 Loss: 0.35511, Train_Acc:87.83%
Epoch [146/300], Step [370/391],                 Loss: 0.35509, Train_Acc:87.83%
Epoch [146/300], Step [380/391],                 Loss: 0.35483, Train_Acc:87.85%
Epoch [146/300], Step [390/391],                 Loss: 0.35415, Train_Acc:87.86%
Accuary on test images:71.74%
Epoch [147/300], Step [10/391],                 Loss: 0.34960, Train_Acc:88.05%
Epoch [147/300], Step [20/391],                 Loss: 0.34030, Train_Acc:88.40%
Epoch [147/300], Step [30/391],                 Loss: 0.33638, Train_Acc:88.54%
Epoch [147/300], Step [40/391],                 Loss: 0.33980, Train_Acc:88.55%
Epoch [147/300], Step [50/391],                 Loss: 0.34376, Train_Acc:88.33%
Epoch [147/300], Step [60/391],                 Loss: 0.34506, Train_Acc:88.22%
Epoch [147/300], Step [70/391],                 Loss: 0.34509, Train_Acc:88.08%
Epoch [147/300], Step [80/391],                 Loss: 0.34847, Train_Acc:87.97%
Epoch [147/300], Step [90/391],                 Loss: 0.35239, Train_Acc:87.83%
Epoch [147/300], Step [100/391],                 Loss: 0.34940, Train_Acc:88.02%
Epoch [147/300], Step [110/391],                 Loss: 0.34955, Train_Acc:88.09%
Epoch [147/300], Step [120/391],                 Loss: 0.34868, Train_Acc:88.16%
Epoch [147/300], Step [130/391],                 Loss: 0.35229, Train_Acc:88.02%
Epoch [147/300], Step [140/391],                 Loss: 0.35149, Train_Acc:88.04%
Epoch [147/300], Step [150/391],                 Loss: 0.35333, Train_Acc:88.00%
Epoch [147/300], Step [160/391],                 Loss: 0.35469, Train_Acc:87.93%
Epoch [147/300], Step [170/391],                 Loss: 0.35655, Train_Acc:87.81%
Epoch [147/300], Step [180/391],                 Loss: 0.35693, Train_Acc:87.76%
Epoch [147/300], Step [190/391],                 Loss: 0.35604, Train_Acc:87.81%
Epoch [147/300], Step [200/391],                 Loss: 0.35698, Train_Acc:87.78%
Epoch [147/300], Step [210/391],                 Loss: 0.35810, Train_Acc:87.76%
Epoch [147/300], Step [220/391],                 Loss: 0.35795, Train_Acc:87.77%
Epoch [147/300], Step [230/391],                 Loss: 0.35656, Train_Acc:87.84%
Epoch [147/300], Step [240/391],                 Loss: 0.35393, Train_Acc:87.93%
Epoch [147/300], Step [250/391],                 Loss: 0.35251, Train_Acc:87.95%
Epoch [147/300], Step [260/391],                 Loss: 0.35478, Train_Acc:87.87%
Epoch [147/300], Step [270/391],                 Loss: 0.35590, Train_Acc:87.84%
Epoch [147/300], Step [280/391],                 Loss: 0.35640, Train_Acc:87.80%
Epoch [147/300], Step [290/391],                 Loss: 0.35707, Train_Acc:87.81%
Epoch [147/300], Step [300/391],                 Loss: 0.35760, Train_Acc:87.76%
Epoch [147/300], Step [310/391],                 Loss: 0.35735, Train_Acc:87.76%
Epoch [147/300], Step [320/391],                 Loss: 0.35719, Train_Acc:87.78%
Epoch [147/300], Step [330/391],                 Loss: 0.35677, Train_Acc:87.82%
Epoch [147/300], Step [340/391],                 Loss: 0.35563, Train_Acc:87.87%
Epoch [147/300], Step [350/391],                 Loss: 0.35548, Train_Acc:87.88%
Epoch [147/300], Step [360/391],                 Loss: 0.35461, Train_Acc:87.91%
Epoch [147/300], Step [370/391],                 Loss: 0.35510, Train_Acc:87.88%
Epoch [147/300], Step [380/391],                 Loss: 0.35524, Train_Acc:87.89%
Epoch [147/300], Step [390/391],                 Loss: 0.35524, Train_Acc:87.89%
Accuary on test images:78.74%
Epoch [148/300], Step [10/391],                 Loss: 0.33224, Train_Acc:88.20%
Epoch [148/300], Step [20/391],                 Loss: 0.33429, Train_Acc:88.52%
Epoch [148/300], Step [30/391],                 Loss: 0.32577, Train_Acc:88.98%
Epoch [148/300], Step [40/391],                 Loss: 0.32703, Train_Acc:88.96%
Epoch [148/300], Step [50/391],                 Loss: 0.32891, Train_Acc:88.80%
Epoch [148/300], Step [60/391],                 Loss: 0.33112, Train_Acc:88.57%
Epoch [148/300], Step [70/391],                 Loss: 0.33279, Train_Acc:88.62%
Epoch [148/300], Step [80/391],                 Loss: 0.33741, Train_Acc:88.49%
Epoch [148/300], Step [90/391],                 Loss: 0.34428, Train_Acc:88.18%
Epoch [148/300], Step [100/391],                 Loss: 0.34657, Train_Acc:88.10%
Epoch [148/300], Step [110/391],                 Loss: 0.35475, Train_Acc:87.83%
Epoch [148/300], Step [120/391],                 Loss: 0.35802, Train_Acc:87.68%
Epoch [148/300], Step [130/391],                 Loss: 0.36066, Train_Acc:87.67%
Epoch [148/300], Step [140/391],                 Loss: 0.35846, Train_Acc:87.78%
Epoch [148/300], Step [150/391],                 Loss: 0.35865, Train_Acc:87.81%
Epoch [148/300], Step [160/391],                 Loss: 0.35995, Train_Acc:87.78%
Epoch [148/300], Step [170/391],                 Loss: 0.36129, Train_Acc:87.74%
Epoch [148/300], Step [180/391],                 Loss: 0.36063, Train_Acc:87.71%
Epoch [148/300], Step [190/391],                 Loss: 0.36093, Train_Acc:87.68%
Epoch [148/300], Step [200/391],                 Loss: 0.36087, Train_Acc:87.71%
Epoch [148/300], Step [210/391],                 Loss: 0.36034, Train_Acc:87.70%
Epoch [148/300], Step [220/391],                 Loss: 0.36102, Train_Acc:87.68%
Epoch [148/300], Step [230/391],                 Loss: 0.35973, Train_Acc:87.72%
Epoch [148/300], Step [240/391],                 Loss: 0.35672, Train_Acc:87.79%
Epoch [148/300], Step [250/391],                 Loss: 0.35512, Train_Acc:87.86%
Epoch [148/300], Step [260/391],                 Loss: 0.35709, Train_Acc:87.79%
Epoch [148/300], Step [270/391],                 Loss: 0.35775, Train_Acc:87.77%
Epoch [148/300], Step [280/391],                 Loss: 0.35822, Train_Acc:87.75%
Epoch [148/300], Step [290/391],                 Loss: 0.35889, Train_Acc:87.73%
Epoch [148/300], Step [300/391],                 Loss: 0.35921, Train_Acc:87.73%
Epoch [148/300], Step [310/391],                 Loss: 0.35865, Train_Acc:87.77%
Epoch [148/300], Step [320/391],                 Loss: 0.35821, Train_Acc:87.81%
Epoch [148/300], Step [330/391],                 Loss: 0.35759, Train_Acc:87.85%
Epoch [148/300], Step [340/391],                 Loss: 0.35678, Train_Acc:87.88%
Epoch [148/300], Step [350/391],                 Loss: 0.35642, Train_Acc:87.90%
Epoch [148/300], Step [360/391],                 Loss: 0.35571, Train_Acc:87.92%
Epoch [148/300], Step [370/391],                 Loss: 0.35642, Train_Acc:87.89%
Epoch [148/300], Step [380/391],                 Loss: 0.35629, Train_Acc:87.91%
Epoch [148/300], Step [390/391],                 Loss: 0.35590, Train_Acc:87.94%
Accuary on test images:77.04%
Epoch [149/300], Step [10/391],                 Loss: 0.36804, Train_Acc:87.27%
Epoch [149/300], Step [20/391],                 Loss: 0.35698, Train_Acc:88.20%
Epoch [149/300], Step [30/391],                 Loss: 0.34418, Train_Acc:88.65%
Epoch [149/300], Step [40/391],                 Loss: 0.34370, Train_Acc:88.50%
Epoch [149/300], Step [50/391],                 Loss: 0.34943, Train_Acc:88.27%
Epoch [149/300], Step [60/391],                 Loss: 0.35394, Train_Acc:88.01%
Epoch [149/300], Step [70/391],                 Loss: 0.35666, Train_Acc:87.96%
Epoch [149/300], Step [80/391],                 Loss: 0.35556, Train_Acc:87.95%
Epoch [149/300], Step [90/391],                 Loss: 0.35922, Train_Acc:87.66%
Epoch [149/300], Step [100/391],                 Loss: 0.35955, Train_Acc:87.60%
Epoch [149/300], Step [110/391],                 Loss: 0.35841, Train_Acc:87.68%
Epoch [149/300], Step [120/391],                 Loss: 0.35883, Train_Acc:87.71%
Epoch [149/300], Step [130/391],                 Loss: 0.35860, Train_Acc:87.73%
Epoch [149/300], Step [140/391],                 Loss: 0.35786, Train_Acc:87.78%
Epoch [149/300], Step [150/391],                 Loss: 0.35716, Train_Acc:87.80%
Epoch [149/300], Step [160/391],                 Loss: 0.35622, Train_Acc:87.81%
Epoch [149/300], Step [170/391],                 Loss: 0.35759, Train_Acc:87.74%
Epoch [149/300], Step [180/391],                 Loss: 0.35701, Train_Acc:87.79%
Epoch [149/300], Step [190/391],                 Loss: 0.35633, Train_Acc:87.80%
Epoch [149/300], Step [200/391],                 Loss: 0.35648, Train_Acc:87.79%
Epoch [149/300], Step [210/391],                 Loss: 0.35681, Train_Acc:87.81%
Epoch [149/300], Step [220/391],                 Loss: 0.35449, Train_Acc:87.88%
Epoch [149/300], Step [230/391],                 Loss: 0.35283, Train_Acc:87.91%
Epoch [149/300], Step [240/391],                 Loss: 0.34984, Train_Acc:88.03%
Epoch [149/300], Step [250/391],                 Loss: 0.34886, Train_Acc:88.06%
Epoch [149/300], Step [260/391],                 Loss: 0.34936, Train_Acc:88.10%
Epoch [149/300], Step [270/391],                 Loss: 0.35040, Train_Acc:88.05%
Epoch [149/300], Step [280/391],                 Loss: 0.34937, Train_Acc:88.06%
Epoch [149/300], Step [290/391],                 Loss: 0.35173, Train_Acc:87.97%
Epoch [149/300], Step [300/391],                 Loss: 0.35295, Train_Acc:87.90%
Epoch [149/300], Step [310/391],                 Loss: 0.35290, Train_Acc:87.90%
Epoch [149/300], Step [320/391],                 Loss: 0.35376, Train_Acc:87.89%
Epoch [149/300], Step [330/391],                 Loss: 0.35443, Train_Acc:87.89%
Epoch [149/300], Step [340/391],                 Loss: 0.35511, Train_Acc:87.85%
Epoch [149/300], Step [350/391],                 Loss: 0.35555, Train_Acc:87.87%
Epoch [149/300], Step [360/391],                 Loss: 0.35536, Train_Acc:87.86%
Epoch [149/300], Step [370/391],                 Loss: 0.35556, Train_Acc:87.85%
Epoch [149/300], Step [380/391],                 Loss: 0.35525, Train_Acc:87.86%
Epoch [149/300], Step [390/391],                 Loss: 0.35489, Train_Acc:87.88%
Accuary on test images:77.98%
Epoch [150/300], Step [10/391],                 Loss: 0.38026, Train_Acc:86.56%
Epoch [150/300], Step [20/391],                 Loss: 0.37557, Train_Acc:87.27%
Epoch [150/300], Step [30/391],                 Loss: 0.37169, Train_Acc:87.08%
Epoch [150/300], Step [40/391],                 Loss: 0.36565, Train_Acc:87.42%
Epoch [150/300], Step [50/391],                 Loss: 0.36012, Train_Acc:87.70%
Epoch [150/300], Step [60/391],                 Loss: 0.35914, Train_Acc:87.68%
Epoch [150/300], Step [70/391],                 Loss: 0.35206, Train_Acc:87.91%
Epoch [150/300], Step [80/391],                 Loss: 0.35347, Train_Acc:87.86%
Epoch [150/300], Step [90/391],                 Loss: 0.35279, Train_Acc:87.89%
Epoch [150/300], Step [100/391],                 Loss: 0.34816, Train_Acc:87.98%
Epoch [150/300], Step [110/391],                 Loss: 0.34903, Train_Acc:87.89%
Epoch [150/300], Step [120/391],                 Loss: 0.35064, Train_Acc:87.80%
Epoch [150/300], Step [130/391],                 Loss: 0.35249, Train_Acc:87.80%
Epoch [150/300], Step [140/391],                 Loss: 0.35191, Train_Acc:87.81%
Epoch [150/300], Step [150/391],                 Loss: 0.35380, Train_Acc:87.74%
Epoch [150/300], Step [160/391],                 Loss: 0.35345, Train_Acc:87.82%
Epoch [150/300], Step [170/391],                 Loss: 0.35456, Train_Acc:87.84%
Epoch [150/300], Step [180/391],                 Loss: 0.35421, Train_Acc:87.81%
Epoch [150/300], Step [190/391],                 Loss: 0.35383, Train_Acc:87.84%
Epoch [150/300], Step [200/391],                 Loss: 0.35341, Train_Acc:87.88%
Epoch [150/300], Step [210/391],                 Loss: 0.35338, Train_Acc:87.88%
Epoch [150/300], Step [220/391],                 Loss: 0.35140, Train_Acc:87.93%
Epoch [150/300], Step [230/391],                 Loss: 0.34978, Train_Acc:87.99%
Epoch [150/300], Step [240/391],                 Loss: 0.34855, Train_Acc:88.03%
Epoch [150/300], Step [250/391],                 Loss: 0.34819, Train_Acc:88.08%
Epoch [150/300], Step [260/391],                 Loss: 0.35060, Train_Acc:88.02%
Epoch [150/300], Step [270/391],                 Loss: 0.35369, Train_Acc:87.93%
Epoch [150/300], Step [280/391],                 Loss: 0.35452, Train_Acc:87.87%
Epoch [150/300], Step [290/391],                 Loss: 0.35404, Train_Acc:87.90%
Epoch [150/300], Step [300/391],                 Loss: 0.35315, Train_Acc:87.90%
Epoch [150/300], Step [310/391],                 Loss: 0.35332, Train_Acc:87.89%
Epoch [150/300], Step [320/391],                 Loss: 0.35447, Train_Acc:87.85%
Epoch [150/300], Step [330/391],                 Loss: 0.35456, Train_Acc:87.83%
Epoch [150/300], Step [340/391],                 Loss: 0.35535, Train_Acc:87.82%
Epoch [150/300], Step [350/391],                 Loss: 0.35570, Train_Acc:87.83%
Epoch [150/300], Step [360/391],                 Loss: 0.35581, Train_Acc:87.84%
Epoch [150/300], Step [370/391],                 Loss: 0.35600, Train_Acc:87.83%
Epoch [150/300], Step [380/391],                 Loss: 0.35633, Train_Acc:87.84%
Epoch [150/300], Step [390/391],                 Loss: 0.35721, Train_Acc:87.84%
Accuary on test images:70.18%
Epoch [151/300], Step [10/391],                 Loss: 0.34081, Train_Acc:88.83%
Epoch [151/300], Step [20/391],                 Loss: 0.32725, Train_Acc:89.22%
Epoch [151/300], Step [30/391],                 Loss: 0.30851, Train_Acc:89.58%
Epoch [151/300], Step [40/391],                 Loss: 0.29700, Train_Acc:90.08%
Epoch [151/300], Step [50/391],                 Loss: 0.28748, Train_Acc:90.44%
Epoch [151/300], Step [60/391],                 Loss: 0.28202, Train_Acc:90.48%
Epoch [151/300], Step [70/391],                 Loss: 0.27588, Train_Acc:90.75%
Epoch [151/300], Step [80/391],                 Loss: 0.26894, Train_Acc:91.05%
Epoch [151/300], Step [90/391],                 Loss: 0.26540, Train_Acc:91.19%
Epoch [151/300], Step [100/391],                 Loss: 0.25915, Train_Acc:91.38%
Epoch [151/300], Step [110/391],                 Loss: 0.25355, Train_Acc:91.58%
Epoch [151/300], Step [120/391],                 Loss: 0.25188, Train_Acc:91.65%
Epoch [151/300], Step [130/391],                 Loss: 0.25066, Train_Acc:91.71%
Epoch [151/300], Step [140/391],                 Loss: 0.24694, Train_Acc:91.82%
Epoch [151/300], Step [150/391],                 Loss: 0.24457, Train_Acc:91.91%
Epoch [151/300], Step [160/391],                 Loss: 0.24021, Train_Acc:92.08%
Epoch [151/300], Step [170/391],                 Loss: 0.23828, Train_Acc:92.17%
Epoch [151/300], Step [180/391],                 Loss: 0.23558, Train_Acc:92.23%
Epoch [151/300], Step [190/391],                 Loss: 0.23284, Train_Acc:92.30%
Epoch [151/300], Step [200/391],                 Loss: 0.23041, Train_Acc:92.39%
Epoch [151/300], Step [210/391],                 Loss: 0.22773, Train_Acc:92.51%
Epoch [151/300], Step [220/391],                 Loss: 0.22450, Train_Acc:92.65%
Epoch [151/300], Step [230/391],                 Loss: 0.22108, Train_Acc:92.75%
Epoch [151/300], Step [240/391],                 Loss: 0.21792, Train_Acc:92.87%
Epoch [151/300], Step [250/391],                 Loss: 0.21528, Train_Acc:92.96%
Epoch [151/300], Step [260/391],                 Loss: 0.21319, Train_Acc:93.03%
Epoch [151/300], Step [270/391],                 Loss: 0.21142, Train_Acc:93.10%
Epoch [151/300], Step [280/391],                 Loss: 0.20845, Train_Acc:93.19%
Epoch [151/300], Step [290/391],                 Loss: 0.20630, Train_Acc:93.27%
Epoch [151/300], Step [300/391],                 Loss: 0.20314, Train_Acc:93.38%
Epoch [151/300], Step [310/391],                 Loss: 0.20049, Train_Acc:93.47%
Epoch [151/300], Step [320/391],                 Loss: 0.19801, Train_Acc:93.57%
Epoch [151/300], Step [330/391],                 Loss: 0.19544, Train_Acc:93.67%
Epoch [151/300], Step [340/391],                 Loss: 0.19286, Train_Acc:93.76%
Epoch [151/300], Step [350/391],                 Loss: 0.18982, Train_Acc:93.87%
Epoch [151/300], Step [360/391],                 Loss: 0.18681, Train_Acc:93.98%
Epoch [151/300], Step [370/391],                 Loss: 0.18419, Train_Acc:94.07%
Epoch [151/300], Step [380/391],                 Loss: 0.18133, Train_Acc:94.16%
Epoch [151/300], Step [390/391],                 Loss: 0.17873, Train_Acc:94.25%
Accuary on test images:89.94%
Epoch [152/300], Step [10/391],                 Loss: 0.12909, Train_Acc:95.47%
Epoch [152/300], Step [20/391],                 Loss: 0.11514, Train_Acc:96.21%
Epoch [152/300], Step [30/391],                 Loss: 0.11681, Train_Acc:95.99%
Epoch [152/300], Step [40/391],                 Loss: 0.12159, Train_Acc:95.94%
Epoch [152/300], Step [50/391],                 Loss: 0.12194, Train_Acc:96.02%
Epoch [152/300], Step [60/391],                 Loss: 0.12205, Train_Acc:96.00%
Epoch [152/300], Step [70/391],                 Loss: 0.12270, Train_Acc:96.04%
Epoch [152/300], Step [80/391],                 Loss: 0.12422, Train_Acc:95.99%
Epoch [152/300], Step [90/391],                 Loss: 0.12439, Train_Acc:95.98%
Epoch [152/300], Step [100/391],                 Loss: 0.12173, Train_Acc:96.12%
Epoch [152/300], Step [110/391],                 Loss: 0.12122, Train_Acc:96.16%
Epoch [152/300], Step [120/391],                 Loss: 0.12138, Train_Acc:96.14%
Epoch [152/300], Step [130/391],                 Loss: 0.12168, Train_Acc:96.17%
Epoch [152/300], Step [140/391],                 Loss: 0.11975, Train_Acc:96.21%
Epoch [152/300], Step [150/391],                 Loss: 0.11989, Train_Acc:96.19%
Epoch [152/300], Step [160/391],                 Loss: 0.11770, Train_Acc:96.28%
Epoch [152/300], Step [170/391],                 Loss: 0.11854, Train_Acc:96.25%
Epoch [152/300], Step [180/391],                 Loss: 0.11776, Train_Acc:96.27%
Epoch [152/300], Step [190/391],                 Loss: 0.11675, Train_Acc:96.31%
Epoch [152/300], Step [200/391],                 Loss: 0.11644, Train_Acc:96.31%
Epoch [152/300], Step [210/391],                 Loss: 0.11576, Train_Acc:96.34%
Epoch [152/300], Step [220/391],                 Loss: 0.11480, Train_Acc:96.40%
Epoch [152/300], Step [230/391],                 Loss: 0.11332, Train_Acc:96.47%
Epoch [152/300], Step [240/391],                 Loss: 0.11205, Train_Acc:96.51%
Epoch [152/300], Step [250/391],                 Loss: 0.11074, Train_Acc:96.55%
Epoch [152/300], Step [260/391],                 Loss: 0.10982, Train_Acc:96.60%
Epoch [152/300], Step [270/391],                 Loss: 0.10960, Train_Acc:96.62%
Epoch [152/300], Step [280/391],                 Loss: 0.10838, Train_Acc:96.66%
Epoch [152/300], Step [290/391],                 Loss: 0.10775, Train_Acc:96.69%
Epoch [152/300], Step [300/391],                 Loss: 0.10659, Train_Acc:96.73%
Epoch [152/300], Step [310/391],                 Loss: 0.10565, Train_Acc:96.75%
Epoch [152/300], Step [320/391],                 Loss: 0.10450, Train_Acc:96.78%
Epoch [152/300], Step [330/391],                 Loss: 0.10363, Train_Acc:96.82%
Epoch [152/300], Step [340/391],                 Loss: 0.10229, Train_Acc:96.88%
Epoch [152/300], Step [350/391],                 Loss: 0.10097, Train_Acc:96.94%
Epoch [152/300], Step [360/391],                 Loss: 0.09976, Train_Acc:96.98%
Epoch [152/300], Step [370/391],                 Loss: 0.09885, Train_Acc:97.01%
Epoch [152/300], Step [380/391],                 Loss: 0.09758, Train_Acc:97.05%
Epoch [152/300], Step [390/391],                 Loss: 0.09650, Train_Acc:97.09%
Accuary on test images:90.34%
Epoch [153/300], Step [10/391],                 Loss: 0.07602, Train_Acc:97.66%
Epoch [153/300], Step [20/391],                 Loss: 0.07161, Train_Acc:97.93%
Epoch [153/300], Step [30/391],                 Loss: 0.07278, Train_Acc:97.99%
Epoch [153/300], Step [40/391],                 Loss: 0.07995, Train_Acc:97.81%
Epoch [153/300], Step [50/391],                 Loss: 0.07938, Train_Acc:97.78%
Epoch [153/300], Step [60/391],                 Loss: 0.08098, Train_Acc:97.72%
Epoch [153/300], Step [70/391],                 Loss: 0.08189, Train_Acc:97.69%
Epoch [153/300], Step [80/391],                 Loss: 0.08167, Train_Acc:97.66%
Epoch [153/300], Step [90/391],                 Loss: 0.08118, Train_Acc:97.66%
Epoch [153/300], Step [100/391],                 Loss: 0.07848, Train_Acc:97.77%
Epoch [153/300], Step [110/391],                 Loss: 0.07815, Train_Acc:97.77%
Epoch [153/300], Step [120/391],                 Loss: 0.07712, Train_Acc:97.82%
Epoch [153/300], Step [130/391],                 Loss: 0.07846, Train_Acc:97.80%
Epoch [153/300], Step [140/391],                 Loss: 0.07780, Train_Acc:97.81%
Epoch [153/300], Step [150/391],                 Loss: 0.07804, Train_Acc:97.79%
Epoch [153/300], Step [160/391],                 Loss: 0.07630, Train_Acc:97.86%
Epoch [153/300], Step [170/391],                 Loss: 0.07608, Train_Acc:97.85%
Epoch [153/300], Step [180/391],                 Loss: 0.07506, Train_Acc:97.89%
Epoch [153/300], Step [190/391],                 Loss: 0.07436, Train_Acc:97.91%
Epoch [153/300], Step [200/391],                 Loss: 0.07377, Train_Acc:97.94%
Epoch [153/300], Step [210/391],                 Loss: 0.07354, Train_Acc:97.95%
Epoch [153/300], Step [220/391],                 Loss: 0.07239, Train_Acc:98.00%
Epoch [153/300], Step [230/391],                 Loss: 0.07167, Train_Acc:98.00%
Epoch [153/300], Step [240/391],                 Loss: 0.07070, Train_Acc:98.04%
Epoch [153/300], Step [250/391],                 Loss: 0.07066, Train_Acc:98.04%
Epoch [153/300], Step [260/391],                 Loss: 0.07026, Train_Acc:98.04%
Epoch [153/300], Step [270/391],                 Loss: 0.06983, Train_Acc:98.06%
Epoch [153/300], Step [280/391],                 Loss: 0.06894, Train_Acc:98.09%
Epoch [153/300], Step [290/391],                 Loss: 0.06871, Train_Acc:98.11%
Epoch [153/300], Step [300/391],                 Loss: 0.06782, Train_Acc:98.14%
Epoch [153/300], Step [310/391],                 Loss: 0.06701, Train_Acc:98.18%
Epoch [153/300], Step [320/391],                 Loss: 0.06627, Train_Acc:98.20%
Epoch [153/300], Step [330/391],                 Loss: 0.06594, Train_Acc:98.21%
Epoch [153/300], Step [340/391],                 Loss: 0.06518, Train_Acc:98.24%
Epoch [153/300], Step [350/391],                 Loss: 0.06468, Train_Acc:98.26%
Epoch [153/300], Step [360/391],                 Loss: 0.06394, Train_Acc:98.28%
Epoch [153/300], Step [370/391],                 Loss: 0.06339, Train_Acc:98.30%
Epoch [153/300], Step [380/391],                 Loss: 0.06269, Train_Acc:98.33%
Epoch [153/300], Step [390/391],                 Loss: 0.06193, Train_Acc:98.35%
Accuary on test images:90.50%
Epoch [154/300], Step [10/391],                 Loss: 0.04241, Train_Acc:99.30%
Epoch [154/300], Step [20/391],                 Loss: 0.04722, Train_Acc:99.10%
Epoch [154/300], Step [30/391],                 Loss: 0.04892, Train_Acc:99.01%
Epoch [154/300], Step [40/391],                 Loss: 0.04994, Train_Acc:98.95%
Epoch [154/300], Step [50/391],                 Loss: 0.05092, Train_Acc:98.83%
Epoch [154/300], Step [60/391],                 Loss: 0.05104, Train_Acc:98.84%
Epoch [154/300], Step [70/391],                 Loss: 0.05199, Train_Acc:98.82%
Epoch [154/300], Step [80/391],                 Loss: 0.05174, Train_Acc:98.76%
Epoch [154/300], Step [90/391],                 Loss: 0.05114, Train_Acc:98.76%
Epoch [154/300], Step [100/391],                 Loss: 0.04953, Train_Acc:98.81%
Epoch [154/300], Step [110/391],                 Loss: 0.04975, Train_Acc:98.81%
Epoch [154/300], Step [120/391],                 Loss: 0.04977, Train_Acc:98.80%
Epoch [154/300], Step [130/391],                 Loss: 0.05032, Train_Acc:98.78%
Epoch [154/300], Step [140/391],                 Loss: 0.04928, Train_Acc:98.83%
Epoch [154/300], Step [150/391],                 Loss: 0.04940, Train_Acc:98.83%
Epoch [154/300], Step [160/391],                 Loss: 0.04906, Train_Acc:98.83%
Epoch [154/300], Step [170/391],                 Loss: 0.04909, Train_Acc:98.82%
Epoch [154/300], Step [180/391],                 Loss: 0.04849, Train_Acc:98.82%
Epoch [154/300], Step [190/391],                 Loss: 0.04785, Train_Acc:98.84%
Epoch [154/300], Step [200/391],                 Loss: 0.04763, Train_Acc:98.85%
Epoch [154/300], Step [210/391],                 Loss: 0.04764, Train_Acc:98.85%
Epoch [154/300], Step [220/391],                 Loss: 0.04716, Train_Acc:98.86%
Epoch [154/300], Step [230/391],                 Loss: 0.04633, Train_Acc:98.90%
Epoch [154/300], Step [240/391],                 Loss: 0.04568, Train_Acc:98.91%
Epoch [154/300], Step [250/391],                 Loss: 0.04534, Train_Acc:98.92%
Epoch [154/300], Step [260/391],                 Loss: 0.04515, Train_Acc:98.93%
Epoch [154/300], Step [270/391],                 Loss: 0.04473, Train_Acc:98.95%
Epoch [154/300], Step [280/391],                 Loss: 0.04420, Train_Acc:98.96%
Epoch [154/300], Step [290/391],                 Loss: 0.04386, Train_Acc:98.98%
Epoch [154/300], Step [300/391],                 Loss: 0.04351, Train_Acc:98.98%
Epoch [154/300], Step [310/391],                 Loss: 0.04317, Train_Acc:98.99%
Epoch [154/300], Step [320/391],                 Loss: 0.04264, Train_Acc:99.00%
Epoch [154/300], Step [330/391],                 Loss: 0.04239, Train_Acc:99.01%
Epoch [154/300], Step [340/391],                 Loss: 0.04189, Train_Acc:99.03%
Epoch [154/300], Step [350/391],                 Loss: 0.04142, Train_Acc:99.05%
Epoch [154/300], Step [360/391],                 Loss: 0.04098, Train_Acc:99.06%
Epoch [154/300], Step [370/391],                 Loss: 0.04072, Train_Acc:99.07%
Epoch [154/300], Step [380/391],                 Loss: 0.04027, Train_Acc:99.09%
Epoch [154/300], Step [390/391],                 Loss: 0.03984, Train_Acc:99.09%
Accuary on test images:90.62%
Epoch [155/300], Step [10/391],                 Loss: 0.03610, Train_Acc:99.30%
Epoch [155/300], Step [20/391],                 Loss: 0.03278, Train_Acc:99.22%
Epoch [155/300], Step [30/391],                 Loss: 0.03144, Train_Acc:99.27%
Epoch [155/300], Step [40/391],                 Loss: 0.03234, Train_Acc:99.32%
Epoch [155/300], Step [50/391],                 Loss: 0.03107, Train_Acc:99.38%
Epoch [155/300], Step [60/391],                 Loss: 0.03099, Train_Acc:99.40%
Epoch [155/300], Step [70/391],                 Loss: 0.03222, Train_Acc:99.36%
Epoch [155/300], Step [80/391],                 Loss: 0.03362, Train_Acc:99.34%
Epoch [155/300], Step [90/391],                 Loss: 0.03284, Train_Acc:99.34%
Epoch [155/300], Step [100/391],                 Loss: 0.03226, Train_Acc:99.36%
Epoch [155/300], Step [110/391],                 Loss: 0.03261, Train_Acc:99.35%
Epoch [155/300], Step [120/391],                 Loss: 0.03197, Train_Acc:99.38%
Epoch [155/300], Step [130/391],                 Loss: 0.03249, Train_Acc:99.34%
Epoch [155/300], Step [140/391],                 Loss: 0.03230, Train_Acc:99.35%
Epoch [155/300], Step [150/391],                 Loss: 0.03230, Train_Acc:99.36%
Epoch [155/300], Step [160/391],                 Loss: 0.03189, Train_Acc:99.36%
Epoch [155/300], Step [170/391],                 Loss: 0.03192, Train_Acc:99.36%
Epoch [155/300], Step [180/391],                 Loss: 0.03215, Train_Acc:99.36%
Epoch [155/300], Step [190/391],                 Loss: 0.03161, Train_Acc:99.38%
Epoch [155/300], Step [200/391],                 Loss: 0.03156, Train_Acc:99.37%
Epoch [155/300], Step [210/391],                 Loss: 0.03124, Train_Acc:99.36%
Epoch [155/300], Step [220/391],                 Loss: 0.03063, Train_Acc:99.38%
Epoch [155/300], Step [230/391],                 Loss: 0.03035, Train_Acc:99.38%
Epoch [155/300], Step [240/391],                 Loss: 0.02987, Train_Acc:99.39%
Epoch [155/300], Step [250/391],                 Loss: 0.02953, Train_Acc:99.40%
Epoch [155/300], Step [260/391],                 Loss: 0.02916, Train_Acc:99.41%
Epoch [155/300], Step [270/391],                 Loss: 0.02905, Train_Acc:99.42%
Epoch [155/300], Step [280/391],                 Loss: 0.02874, Train_Acc:99.42%
Epoch [155/300], Step [290/391],                 Loss: 0.02875, Train_Acc:99.42%
Epoch [155/300], Step [300/391],                 Loss: 0.02836, Train_Acc:99.43%
Epoch [155/300], Step [310/391],                 Loss: 0.02808, Train_Acc:99.44%
Epoch [155/300], Step [320/391],                 Loss: 0.02783, Train_Acc:99.45%
Epoch [155/300], Step [330/391],                 Loss: 0.02764, Train_Acc:99.45%
Epoch [155/300], Step [340/391],                 Loss: 0.02746, Train_Acc:99.45%
Epoch [155/300], Step [350/391],                 Loss: 0.02714, Train_Acc:99.46%
Epoch [155/300], Step [360/391],                 Loss: 0.02681, Train_Acc:99.47%
Epoch [155/300], Step [370/391],                 Loss: 0.02680, Train_Acc:99.47%
Epoch [155/300], Step [380/391],                 Loss: 0.02656, Train_Acc:99.47%
Epoch [155/300], Step [390/391],                 Loss: 0.02632, Train_Acc:99.48%
Accuary on test images:90.46%
Epoch [156/300], Step [10/391],                 Loss: 0.01925, Train_Acc:99.53%
Epoch [156/300], Step [20/391],                 Loss: 0.02025, Train_Acc:99.61%
Epoch [156/300], Step [30/391],                 Loss: 0.01958, Train_Acc:99.61%
Epoch [156/300], Step [40/391],                 Loss: 0.02113, Train_Acc:99.65%
Epoch [156/300], Step [50/391],                 Loss: 0.02112, Train_Acc:99.64%
Epoch [156/300], Step [60/391],                 Loss: 0.02066, Train_Acc:99.64%
Epoch [156/300], Step [70/391],                 Loss: 0.02066, Train_Acc:99.62%
Epoch [156/300], Step [80/391],                 Loss: 0.02030, Train_Acc:99.63%
Epoch [156/300], Step [90/391],                 Loss: 0.01992, Train_Acc:99.65%
Epoch [156/300], Step [100/391],                 Loss: 0.01935, Train_Acc:99.67%
Epoch [156/300], Step [110/391],                 Loss: 0.01951, Train_Acc:99.68%
Epoch [156/300], Step [120/391],                 Loss: 0.01983, Train_Acc:99.66%
Epoch [156/300], Step [130/391],                 Loss: 0.02030, Train_Acc:99.66%
Epoch [156/300], Step [140/391],                 Loss: 0.02025, Train_Acc:99.65%
Epoch [156/300], Step [150/391],                 Loss: 0.02021, Train_Acc:99.66%
Epoch [156/300], Step [160/391],                 Loss: 0.02052, Train_Acc:99.65%
Epoch [156/300], Step [170/391],                 Loss: 0.02057, Train_Acc:99.65%
Epoch [156/300], Step [180/391],                 Loss: 0.02016, Train_Acc:99.66%
Epoch [156/300], Step [190/391],                 Loss: 0.02000, Train_Acc:99.67%
Epoch [156/300], Step [200/391],                 Loss: 0.02024, Train_Acc:99.66%
Epoch [156/300], Step [210/391],                 Loss: 0.02002, Train_Acc:99.67%
Epoch [156/300], Step [220/391],                 Loss: 0.01975, Train_Acc:99.68%
Epoch [156/300], Step [230/391],                 Loss: 0.01941, Train_Acc:99.69%
Epoch [156/300], Step [240/391],                 Loss: 0.01917, Train_Acc:99.70%
Epoch [156/300], Step [250/391],                 Loss: 0.01889, Train_Acc:99.71%
Epoch [156/300], Step [260/391],                 Loss: 0.01912, Train_Acc:99.71%
Epoch [156/300], Step [270/391],                 Loss: 0.01893, Train_Acc:99.72%
Epoch [156/300], Step [280/391],                 Loss: 0.01869, Train_Acc:99.72%
Epoch [156/300], Step [290/391],                 Loss: 0.01860, Train_Acc:99.72%
Epoch [156/300], Step [300/391],                 Loss: 0.01847, Train_Acc:99.72%
Epoch [156/300], Step [310/391],                 Loss: 0.01846, Train_Acc:99.73%
Epoch [156/300], Step [320/391],                 Loss: 0.01832, Train_Acc:99.73%
Epoch [156/300], Step [330/391],                 Loss: 0.01833, Train_Acc:99.73%
Epoch [156/300], Step [340/391],                 Loss: 0.01827, Train_Acc:99.72%
Epoch [156/300], Step [350/391],                 Loss: 0.01806, Train_Acc:99.73%
Epoch [156/300], Step [360/391],                 Loss: 0.01781, Train_Acc:99.73%
Epoch [156/300], Step [370/391],                 Loss: 0.01771, Train_Acc:99.73%
Epoch [156/300], Step [380/391],                 Loss: 0.01747, Train_Acc:99.74%
Epoch [156/300], Step [390/391],                 Loss: 0.01737, Train_Acc:99.74%
Accuary on test images:90.88%
Epoch [157/300], Step [10/391],                 Loss: 0.01454, Train_Acc:99.77%
Epoch [157/300], Step [20/391],                 Loss: 0.01715, Train_Acc:99.69%
Epoch [157/300], Step [30/391],                 Loss: 0.01795, Train_Acc:99.66%
Epoch [157/300], Step [40/391],                 Loss: 0.01805, Train_Acc:99.63%
Epoch [157/300], Step [50/391],                 Loss: 0.01708, Train_Acc:99.67%
Epoch [157/300], Step [60/391],                 Loss: 0.01747, Train_Acc:99.69%
Epoch [157/300], Step [70/391],                 Loss: 0.01723, Train_Acc:99.69%
Epoch [157/300], Step [80/391],                 Loss: 0.01680, Train_Acc:99.71%
Epoch [157/300], Step [90/391],                 Loss: 0.01641, Train_Acc:99.72%
Epoch [157/300], Step [100/391],                 Loss: 0.01601, Train_Acc:99.74%
Epoch [157/300], Step [110/391],                 Loss: 0.01559, Train_Acc:99.76%
Epoch [157/300], Step [120/391],                 Loss: 0.01514, Train_Acc:99.77%
Epoch [157/300], Step [130/391],                 Loss: 0.01515, Train_Acc:99.76%
Epoch [157/300], Step [140/391],                 Loss: 0.01486, Train_Acc:99.77%
Epoch [157/300], Step [150/391],                 Loss: 0.01485, Train_Acc:99.77%
Epoch [157/300], Step [160/391],                 Loss: 0.01453, Train_Acc:99.77%
Epoch [157/300], Step [170/391],                 Loss: 0.01465, Train_Acc:99.77%
Epoch [157/300], Step [180/391],                 Loss: 0.01449, Train_Acc:99.78%
Epoch [157/300], Step [190/391],                 Loss: 0.01452, Train_Acc:99.77%
Epoch [157/300], Step [200/391],                 Loss: 0.01445, Train_Acc:99.77%
Epoch [157/300], Step [210/391],                 Loss: 0.01425, Train_Acc:99.78%
Epoch [157/300], Step [220/391],                 Loss: 0.01404, Train_Acc:99.78%
Epoch [157/300], Step [230/391],                 Loss: 0.01380, Train_Acc:99.79%
Epoch [157/300], Step [240/391],                 Loss: 0.01358, Train_Acc:99.80%
Epoch [157/300], Step [250/391],                 Loss: 0.01354, Train_Acc:99.80%
Epoch [157/300], Step [260/391],                 Loss: 0.01343, Train_Acc:99.80%
Epoch [157/300], Step [270/391],                 Loss: 0.01331, Train_Acc:99.81%
Epoch [157/300], Step [280/391],                 Loss: 0.01325, Train_Acc:99.81%
Epoch [157/300], Step [290/391],                 Loss: 0.01308, Train_Acc:99.81%
Epoch [157/300], Step [300/391],                 Loss: 0.01295, Train_Acc:99.82%
Epoch [157/300], Step [310/391],                 Loss: 0.01281, Train_Acc:99.82%
Epoch [157/300], Step [320/391],                 Loss: 0.01264, Train_Acc:99.83%
Epoch [157/300], Step [330/391],                 Loss: 0.01254, Train_Acc:99.83%
Epoch [157/300], Step [340/391],                 Loss: 0.01245, Train_Acc:99.83%
Epoch [157/300], Step [350/391],                 Loss: 0.01230, Train_Acc:99.84%
Epoch [157/300], Step [360/391],                 Loss: 0.01224, Train_Acc:99.83%
Epoch [157/300], Step [370/391],                 Loss: 0.01230, Train_Acc:99.82%
Epoch [157/300], Step [380/391],                 Loss: 0.01215, Train_Acc:99.83%
Epoch [157/300], Step [390/391],                 Loss: 0.01203, Train_Acc:99.83%
Accuary on test images:90.62%
Epoch [158/300], Step [10/391],                 Loss: 0.01129, Train_Acc:99.84%
Epoch [158/300], Step [20/391],                 Loss: 0.01033, Train_Acc:99.84%
Epoch [158/300], Step [30/391],                 Loss: 0.00977, Train_Acc:99.90%
Epoch [158/300], Step [40/391],                 Loss: 0.00995, Train_Acc:99.90%
Epoch [158/300], Step [50/391],                 Loss: 0.00966, Train_Acc:99.91%
Epoch [158/300], Step [60/391],                 Loss: 0.00952, Train_Acc:99.92%
Epoch [158/300], Step [70/391],                 Loss: 0.00964, Train_Acc:99.91%
Epoch [158/300], Step [80/391],                 Loss: 0.00946, Train_Acc:99.91%
Epoch [158/300], Step [90/391],                 Loss: 0.00975, Train_Acc:99.90%
Epoch [158/300], Step [100/391],                 Loss: 0.00946, Train_Acc:99.91%
Epoch [158/300], Step [110/391],                 Loss: 0.00955, Train_Acc:99.91%
Epoch [158/300], Step [120/391],                 Loss: 0.00966, Train_Acc:99.92%
Epoch [158/300], Step [130/391],                 Loss: 0.00987, Train_Acc:99.91%
Epoch [158/300], Step [140/391],                 Loss: 0.00976, Train_Acc:99.91%
Epoch [158/300], Step [150/391],                 Loss: 0.01021, Train_Acc:99.91%
Epoch [158/300], Step [160/391],                 Loss: 0.01005, Train_Acc:99.91%
Epoch [158/300], Step [170/391],                 Loss: 0.01023, Train_Acc:99.90%
Epoch [158/300], Step [180/391],                 Loss: 0.01008, Train_Acc:99.91%
Epoch [158/300], Step [190/391],                 Loss: 0.00999, Train_Acc:99.91%
Epoch [158/300], Step [200/391],                 Loss: 0.01004, Train_Acc:99.90%
Epoch [158/300], Step [210/391],                 Loss: 0.01006, Train_Acc:99.90%
Epoch [158/300], Step [220/391],                 Loss: 0.00995, Train_Acc:99.90%
Epoch [158/300], Step [230/391],                 Loss: 0.00980, Train_Acc:99.91%
Epoch [158/300], Step [240/391],                 Loss: 0.00968, Train_Acc:99.91%
Epoch [158/300], Step [250/391],                 Loss: 0.00955, Train_Acc:99.91%
Epoch [158/300], Step [260/391],                 Loss: 0.00961, Train_Acc:99.91%
Epoch [158/300], Step [270/391],                 Loss: 0.00970, Train_Acc:99.91%
Epoch [158/300], Step [280/391],                 Loss: 0.00963, Train_Acc:99.91%
Epoch [158/300], Step [290/391],                 Loss: 0.00958, Train_Acc:99.91%
Epoch [158/300], Step [300/391],                 Loss: 0.00950, Train_Acc:99.91%
Epoch [158/300], Step [310/391],                 Loss: 0.00942, Train_Acc:99.91%
Epoch [158/300], Step [320/391],                 Loss: 0.00932, Train_Acc:99.92%
Epoch [158/300], Step [330/391],                 Loss: 0.00927, Train_Acc:99.92%
Epoch [158/300], Step [340/391],                 Loss: 0.00918, Train_Acc:99.92%
Epoch [158/300], Step [350/391],                 Loss: 0.00910, Train_Acc:99.92%
Epoch [158/300], Step [360/391],                 Loss: 0.00902, Train_Acc:99.92%
Epoch [158/300], Step [370/391],                 Loss: 0.00897, Train_Acc:99.92%
Epoch [158/300], Step [380/391],                 Loss: 0.00888, Train_Acc:99.92%
Epoch [158/300], Step [390/391],                 Loss: 0.00879, Train_Acc:99.93%
Accuary on test images:90.82%
Epoch [159/300], Step [10/391],                 Loss: 0.00928, Train_Acc:99.84%
Epoch [159/300], Step [20/391],                 Loss: 0.00870, Train_Acc:99.88%
Epoch [159/300], Step [30/391],                 Loss: 0.00900, Train_Acc:99.90%
Epoch [159/300], Step [40/391],                 Loss: 0.00979, Train_Acc:99.88%
Epoch [159/300], Step [50/391],                 Loss: 0.00913, Train_Acc:99.91%
Epoch [159/300], Step [60/391],                 Loss: 0.00854, Train_Acc:99.92%
Epoch [159/300], Step [70/391],                 Loss: 0.00859, Train_Acc:99.91%
Epoch [159/300], Step [80/391],                 Loss: 0.00829, Train_Acc:99.92%
Epoch [159/300], Step [90/391],                 Loss: 0.00794, Train_Acc:99.93%
Epoch [159/300], Step [100/391],                 Loss: 0.00768, Train_Acc:99.94%
Epoch [159/300], Step [110/391],                 Loss: 0.00768, Train_Acc:99.94%
Epoch [159/300], Step [120/391],                 Loss: 0.00813, Train_Acc:99.92%
Epoch [159/300], Step [130/391],                 Loss: 0.00812, Train_Acc:99.93%
Epoch [159/300], Step [140/391],                 Loss: 0.00807, Train_Acc:99.93%
Epoch [159/300], Step [150/391],                 Loss: 0.00808, Train_Acc:99.93%
Epoch [159/300], Step [160/391],                 Loss: 0.00801, Train_Acc:99.93%
Epoch [159/300], Step [170/391],                 Loss: 0.00804, Train_Acc:99.92%
Epoch [159/300], Step [180/391],                 Loss: 0.00796, Train_Acc:99.93%
Epoch [159/300], Step [190/391],                 Loss: 0.00783, Train_Acc:99.93%
Epoch [159/300], Step [200/391],                 Loss: 0.00776, Train_Acc:99.93%
Epoch [159/300], Step [210/391],                 Loss: 0.00775, Train_Acc:99.93%
Epoch [159/300], Step [220/391],                 Loss: 0.00762, Train_Acc:99.94%
Epoch [159/300], Step [230/391],                 Loss: 0.00755, Train_Acc:99.94%
Epoch [159/300], Step [240/391],                 Loss: 0.00747, Train_Acc:99.94%
Epoch [159/300], Step [250/391],                 Loss: 0.00737, Train_Acc:99.94%
Epoch [159/300], Step [260/391],                 Loss: 0.00732, Train_Acc:99.95%
Epoch [159/300], Step [270/391],                 Loss: 0.00723, Train_Acc:99.95%
Epoch [159/300], Step [280/391],                 Loss: 0.00717, Train_Acc:99.95%
Epoch [159/300], Step [290/391],                 Loss: 0.00718, Train_Acc:99.95%
Epoch [159/300], Step [300/391],                 Loss: 0.00712, Train_Acc:99.95%
Epoch [159/300], Step [310/391],                 Loss: 0.00707, Train_Acc:99.95%
Epoch [159/300], Step [320/391],                 Loss: 0.00700, Train_Acc:99.95%
Epoch [159/300], Step [330/391],                 Loss: 0.00694, Train_Acc:99.96%
Epoch [159/300], Step [340/391],                 Loss: 0.00689, Train_Acc:99.96%
Epoch [159/300], Step [350/391],                 Loss: 0.00684, Train_Acc:99.96%
Epoch [159/300], Step [360/391],                 Loss: 0.00678, Train_Acc:99.96%
Epoch [159/300], Step [370/391],                 Loss: 0.00673, Train_Acc:99.96%
Epoch [159/300], Step [380/391],                 Loss: 0.00665, Train_Acc:99.96%
Epoch [159/300], Step [390/391],                 Loss: 0.00660, Train_Acc:99.96%
Accuary on test images:90.98%
Epoch [160/300], Step [10/391],                 Loss: 0.00521, Train_Acc:100.00%
Epoch [160/300], Step [20/391],                 Loss: 0.00520, Train_Acc:100.00%
Epoch [160/300], Step [30/391],                 Loss: 0.00628, Train_Acc:99.95%
Epoch [160/300], Step [40/391],                 Loss: 0.00617, Train_Acc:99.94%
Epoch [160/300], Step [50/391],                 Loss: 0.00592, Train_Acc:99.95%
Epoch [160/300], Step [60/391],                 Loss: 0.00588, Train_Acc:99.96%
Epoch [160/300], Step [70/391],                 Loss: 0.00578, Train_Acc:99.97%
Epoch [160/300], Step [80/391],                 Loss: 0.00569, Train_Acc:99.97%
Epoch [160/300], Step [90/391],                 Loss: 0.00564, Train_Acc:99.97%
Epoch [160/300], Step [100/391],                 Loss: 0.00561, Train_Acc:99.97%
Epoch [160/300], Step [110/391],                 Loss: 0.00560, Train_Acc:99.96%
Epoch [160/300], Step [120/391],                 Loss: 0.00560, Train_Acc:99.97%
Epoch [160/300], Step [130/391],                 Loss: 0.00559, Train_Acc:99.97%
Epoch [160/300], Step [140/391],                 Loss: 0.00553, Train_Acc:99.97%
Epoch [160/300], Step [150/391],                 Loss: 0.00569, Train_Acc:99.97%
Epoch [160/300], Step [160/391],                 Loss: 0.00568, Train_Acc:99.97%
Epoch [160/300], Step [170/391],                 Loss: 0.00581, Train_Acc:99.97%
Epoch [160/300], Step [180/391],                 Loss: 0.00575, Train_Acc:99.97%
Epoch [160/300], Step [190/391],                 Loss: 0.00579, Train_Acc:99.97%
Epoch [160/300], Step [200/391],                 Loss: 0.00578, Train_Acc:99.96%
Epoch [160/300], Step [210/391],                 Loss: 0.00574, Train_Acc:99.97%
Epoch [160/300], Step [220/391],                 Loss: 0.00569, Train_Acc:99.97%
Epoch [160/300], Step [230/391],                 Loss: 0.00565, Train_Acc:99.97%
Epoch [160/300], Step [240/391],                 Loss: 0.00562, Train_Acc:99.97%
Epoch [160/300], Step [250/391],                 Loss: 0.00558, Train_Acc:99.97%
Epoch [160/300], Step [260/391],                 Loss: 0.00559, Train_Acc:99.97%
Epoch [160/300], Step [270/391],                 Loss: 0.00556, Train_Acc:99.97%
Epoch [160/300], Step [280/391],                 Loss: 0.00557, Train_Acc:99.97%
Epoch [160/300], Step [290/391],                 Loss: 0.00559, Train_Acc:99.97%
Epoch [160/300], Step [300/391],                 Loss: 0.00558, Train_Acc:99.97%
Epoch [160/300], Step [310/391],                 Loss: 0.00558, Train_Acc:99.97%
Epoch [160/300], Step [320/391],                 Loss: 0.00557, Train_Acc:99.97%
Epoch [160/300], Step [330/391],                 Loss: 0.00555, Train_Acc:99.97%
Epoch [160/300], Step [340/391],                 Loss: 0.00553, Train_Acc:99.97%
Epoch [160/300], Step [350/391],                 Loss: 0.00562, Train_Acc:99.97%
Epoch [160/300], Step [360/391],                 Loss: 0.00558, Train_Acc:99.97%
Epoch [160/300], Step [370/391],                 Loss: 0.00554, Train_Acc:99.97%
Epoch [160/300], Step [380/391],                 Loss: 0.00549, Train_Acc:99.97%
Epoch [160/300], Step [390/391],                 Loss: 0.00548, Train_Acc:99.97%
Accuary on test images:91.10%
Epoch [161/300], Step [10/391],                 Loss: 0.00495, Train_Acc:100.00%
Epoch [161/300], Step [20/391],                 Loss: 0.00477, Train_Acc:100.00%
Epoch [161/300], Step [30/391],                 Loss: 0.00459, Train_Acc:100.00%
Epoch [161/300], Step [40/391],                 Loss: 0.00457, Train_Acc:100.00%
Epoch [161/300], Step [50/391],                 Loss: 0.00502, Train_Acc:99.98%
Epoch [161/300], Step [60/391],                 Loss: 0.00493, Train_Acc:99.99%
Epoch [161/300], Step [70/391],                 Loss: 0.00493, Train_Acc:99.99%
Epoch [161/300], Step [80/391],                 Loss: 0.00492, Train_Acc:99.99%
Epoch [161/300], Step [90/391],                 Loss: 0.00488, Train_Acc:99.99%
Epoch [161/300], Step [100/391],                 Loss: 0.00485, Train_Acc:99.99%
Epoch [161/300], Step [110/391],                 Loss: 0.00483, Train_Acc:99.99%
Epoch [161/300], Step [120/391],                 Loss: 0.00482, Train_Acc:99.99%
Epoch [161/300], Step [130/391],                 Loss: 0.00492, Train_Acc:99.99%
Epoch [161/300], Step [140/391],                 Loss: 0.00489, Train_Acc:99.99%
Epoch [161/300], Step [150/391],                 Loss: 0.00488, Train_Acc:99.99%
Epoch [161/300], Step [160/391],                 Loss: 0.00487, Train_Acc:99.99%
Epoch [161/300], Step [170/391],                 Loss: 0.00484, Train_Acc:99.99%
Epoch [161/300], Step [180/391],                 Loss: 0.00482, Train_Acc:99.99%
Epoch [161/300], Step [190/391],                 Loss: 0.00478, Train_Acc:99.99%
Epoch [161/300], Step [200/391],                 Loss: 0.00477, Train_Acc:99.99%
Epoch [161/300], Step [210/391],                 Loss: 0.00476, Train_Acc:99.99%
Epoch [161/300], Step [220/391],                 Loss: 0.00474, Train_Acc:99.99%
Epoch [161/300], Step [230/391],                 Loss: 0.00472, Train_Acc:99.99%
Epoch [161/300], Step [240/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [161/300], Step [250/391],                 Loss: 0.00468, Train_Acc:99.99%
Epoch [161/300], Step [260/391],                 Loss: 0.00470, Train_Acc:99.99%
Epoch [161/300], Step [270/391],                 Loss: 0.00471, Train_Acc:99.99%
Epoch [161/300], Step [280/391],                 Loss: 0.00468, Train_Acc:99.99%
Epoch [161/300], Step [290/391],                 Loss: 0.00466, Train_Acc:99.99%
Epoch [161/300], Step [300/391],                 Loss: 0.00464, Train_Acc:99.99%
Epoch [161/300], Step [310/391],                 Loss: 0.00462, Train_Acc:99.99%
Epoch [161/300], Step [320/391],                 Loss: 0.00462, Train_Acc:100.00%
Epoch [161/300], Step [330/391],                 Loss: 0.00463, Train_Acc:99.99%
Epoch [161/300], Step [340/391],                 Loss: 0.00461, Train_Acc:99.99%
Epoch [161/300], Step [350/391],                 Loss: 0.00461, Train_Acc:99.99%
Epoch [161/300], Step [360/391],                 Loss: 0.00459, Train_Acc:99.99%
Epoch [161/300], Step [370/391],                 Loss: 0.00458, Train_Acc:99.99%
Epoch [161/300], Step [380/391],                 Loss: 0.00455, Train_Acc:99.99%
Epoch [161/300], Step [390/391],                 Loss: 0.00455, Train_Acc:99.99%
Accuary on test images:91.22%
Epoch [162/300], Step [10/391],                 Loss: 0.00466, Train_Acc:100.00%
Epoch [162/300], Step [20/391],                 Loss: 0.00442, Train_Acc:100.00%
Epoch [162/300], Step [30/391],                 Loss: 0.00456, Train_Acc:100.00%
Epoch [162/300], Step [40/391],                 Loss: 0.00455, Train_Acc:100.00%
Epoch [162/300], Step [50/391],                 Loss: 0.00453, Train_Acc:100.00%
Epoch [162/300], Step [60/391],                 Loss: 0.00467, Train_Acc:100.00%
Epoch [162/300], Step [70/391],                 Loss: 0.00468, Train_Acc:100.00%
Epoch [162/300], Step [80/391],                 Loss: 0.00469, Train_Acc:100.00%
Epoch [162/300], Step [90/391],                 Loss: 0.00466, Train_Acc:100.00%
Epoch [162/300], Step [100/391],                 Loss: 0.00457, Train_Acc:100.00%
Epoch [162/300], Step [110/391],                 Loss: 0.00454, Train_Acc:100.00%
Epoch [162/300], Step [120/391],                 Loss: 0.00451, Train_Acc:100.00%
Epoch [162/300], Step [130/391],                 Loss: 0.00450, Train_Acc:100.00%
Epoch [162/300], Step [140/391],                 Loss: 0.00447, Train_Acc:100.00%
Epoch [162/300], Step [150/391],                 Loss: 0.00446, Train_Acc:100.00%
Epoch [162/300], Step [160/391],                 Loss: 0.00442, Train_Acc:100.00%
Epoch [162/300], Step [170/391],                 Loss: 0.00439, Train_Acc:100.00%
Epoch [162/300], Step [180/391],                 Loss: 0.00437, Train_Acc:100.00%
Epoch [162/300], Step [190/391],                 Loss: 0.00434, Train_Acc:100.00%
Epoch [162/300], Step [200/391],                 Loss: 0.00434, Train_Acc:100.00%
Epoch [162/300], Step [210/391],                 Loss: 0.00432, Train_Acc:100.00%
Epoch [162/300], Step [220/391],                 Loss: 0.00429, Train_Acc:100.00%
Epoch [162/300], Step [230/391],                 Loss: 0.00427, Train_Acc:100.00%
Epoch [162/300], Step [240/391],                 Loss: 0.00425, Train_Acc:100.00%
Epoch [162/300], Step [250/391],                 Loss: 0.00424, Train_Acc:100.00%
Epoch [162/300], Step [260/391],                 Loss: 0.00424, Train_Acc:100.00%
Epoch [162/300], Step [270/391],                 Loss: 0.00425, Train_Acc:100.00%
Epoch [162/300], Step [280/391],                 Loss: 0.00424, Train_Acc:100.00%
Epoch [162/300], Step [290/391],                 Loss: 0.00423, Train_Acc:100.00%
Epoch [162/300], Step [300/391],                 Loss: 0.00422, Train_Acc:100.00%
Epoch [162/300], Step [310/391],                 Loss: 0.00420, Train_Acc:100.00%
Epoch [162/300], Step [320/391],                 Loss: 0.00419, Train_Acc:100.00%
Epoch [162/300], Step [330/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [162/300], Step [340/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [162/300], Step [350/391],                 Loss: 0.00416, Train_Acc:100.00%
Epoch [162/300], Step [360/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [162/300], Step [370/391],                 Loss: 0.00412, Train_Acc:100.00%
Epoch [162/300], Step [380/391],                 Loss: 0.00410, Train_Acc:100.00%
Epoch [162/300], Step [390/391],                 Loss: 0.00409, Train_Acc:100.00%
Accuary on test images:91.08%
Epoch [163/300], Step [10/391],                 Loss: 0.00432, Train_Acc:100.00%
Epoch [163/300], Step [20/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [163/300], Step [30/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [163/300], Step [40/391],                 Loss: 0.00406, Train_Acc:100.00%
Epoch [163/300], Step [50/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [163/300], Step [60/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [163/300], Step [70/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [163/300], Step [80/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [163/300], Step [90/391],                 Loss: 0.00401, Train_Acc:100.00%
Epoch [163/300], Step [100/391],                 Loss: 0.00398, Train_Acc:100.00%
Epoch [163/300], Step [110/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [163/300], Step [120/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [163/300], Step [130/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [163/300], Step [140/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [163/300], Step [150/391],                 Loss: 0.00409, Train_Acc:100.00%
Epoch [163/300], Step [160/391],                 Loss: 0.00409, Train_Acc:100.00%
Epoch [163/300], Step [170/391],                 Loss: 0.00425, Train_Acc:100.00%
Epoch [163/300], Step [180/391],                 Loss: 0.00423, Train_Acc:100.00%
Epoch [163/300], Step [190/391],                 Loss: 0.00420, Train_Acc:100.00%
Epoch [163/300], Step [200/391],                 Loss: 0.00421, Train_Acc:100.00%
Epoch [163/300], Step [210/391],                 Loss: 0.00420, Train_Acc:100.00%
Epoch [163/300], Step [220/391],                 Loss: 0.00418, Train_Acc:100.00%
Epoch [163/300], Step [230/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [163/300], Step [240/391],                 Loss: 0.00415, Train_Acc:100.00%
Epoch [163/300], Step [250/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [163/300], Step [260/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [163/300], Step [270/391],                 Loss: 0.00413, Train_Acc:100.00%
Epoch [163/300], Step [280/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [163/300], Step [290/391],                 Loss: 0.00409, Train_Acc:100.00%
Epoch [163/300], Step [300/391],                 Loss: 0.00408, Train_Acc:100.00%
Epoch [163/300], Step [310/391],                 Loss: 0.00406, Train_Acc:100.00%
Epoch [163/300], Step [320/391],                 Loss: 0.00406, Train_Acc:100.00%
Epoch [163/300], Step [330/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [163/300], Step [340/391],                 Loss: 0.00403, Train_Acc:100.00%
Epoch [163/300], Step [350/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [163/300], Step [360/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [163/300], Step [370/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [163/300], Step [380/391],                 Loss: 0.00397, Train_Acc:100.00%
Epoch [163/300], Step [390/391],                 Loss: 0.00396, Train_Acc:100.00%
Accuary on test images:91.14%
Epoch [164/300], Step [10/391],                 Loss: 0.00411, Train_Acc:100.00%
Epoch [164/300], Step [20/391],                 Loss: 0.00504, Train_Acc:99.96%
Epoch [164/300], Step [30/391],                 Loss: 0.00473, Train_Acc:99.97%
Epoch [164/300], Step [40/391],                 Loss: 0.00478, Train_Acc:99.98%
Epoch [164/300], Step [50/391],                 Loss: 0.00465, Train_Acc:99.98%
Epoch [164/300], Step [60/391],                 Loss: 0.00461, Train_Acc:99.99%
Epoch [164/300], Step [70/391],                 Loss: 0.00455, Train_Acc:99.99%
Epoch [164/300], Step [80/391],                 Loss: 0.00451, Train_Acc:99.99%
Epoch [164/300], Step [90/391],                 Loss: 0.00454, Train_Acc:99.99%
Epoch [164/300], Step [100/391],                 Loss: 0.00450, Train_Acc:99.99%
Epoch [164/300], Step [110/391],                 Loss: 0.00446, Train_Acc:99.99%
Epoch [164/300], Step [120/391],                 Loss: 0.00446, Train_Acc:99.99%
Epoch [164/300], Step [130/391],                 Loss: 0.00447, Train_Acc:99.99%
Epoch [164/300], Step [140/391],                 Loss: 0.00450, Train_Acc:99.99%
Epoch [164/300], Step [150/391],                 Loss: 0.00448, Train_Acc:99.99%
Epoch [164/300], Step [160/391],                 Loss: 0.00445, Train_Acc:99.99%
Epoch [164/300], Step [170/391],                 Loss: 0.00443, Train_Acc:99.99%
Epoch [164/300], Step [180/391],                 Loss: 0.00442, Train_Acc:99.99%
Epoch [164/300], Step [190/391],                 Loss: 0.00440, Train_Acc:99.99%
Epoch [164/300], Step [200/391],                 Loss: 0.00438, Train_Acc:99.99%
Epoch [164/300], Step [210/391],                 Loss: 0.00436, Train_Acc:99.99%
Epoch [164/300], Step [220/391],                 Loss: 0.00433, Train_Acc:99.99%
Epoch [164/300], Step [230/391],                 Loss: 0.00430, Train_Acc:99.99%
Epoch [164/300], Step [240/391],                 Loss: 0.00430, Train_Acc:99.99%
Epoch [164/300], Step [250/391],                 Loss: 0.00429, Train_Acc:99.99%
Epoch [164/300], Step [260/391],                 Loss: 0.00428, Train_Acc:99.99%
Epoch [164/300], Step [270/391],                 Loss: 0.00427, Train_Acc:99.99%
Epoch [164/300], Step [280/391],                 Loss: 0.00425, Train_Acc:99.99%
Epoch [164/300], Step [290/391],                 Loss: 0.00423, Train_Acc:99.99%
Epoch [164/300], Step [300/391],                 Loss: 0.00423, Train_Acc:99.99%
Epoch [164/300], Step [310/391],                 Loss: 0.00421, Train_Acc:99.99%
Epoch [164/300], Step [320/391],                 Loss: 0.00419, Train_Acc:100.00%
Epoch [164/300], Step [330/391],                 Loss: 0.00418, Train_Acc:100.00%
Epoch [164/300], Step [340/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [350/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [164/300], Step [360/391],                 Loss: 0.00415, Train_Acc:100.00%
Epoch [164/300], Step [370/391],                 Loss: 0.00414, Train_Acc:100.00%
Epoch [164/300], Step [380/391],                 Loss: 0.00412, Train_Acc:100.00%
Epoch [164/300], Step [390/391],                 Loss: 0.00410, Train_Acc:100.00%
Accuary on test images:90.98%
Epoch [165/300], Step [10/391],                 Loss: 0.00417, Train_Acc:100.00%
Epoch [165/300], Step [20/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [165/300], Step [30/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [165/300], Step [40/391],                 Loss: 0.00404, Train_Acc:100.00%
Epoch [165/300], Step [50/391],                 Loss: 0.00399, Train_Acc:100.00%
Epoch [165/300], Step [60/391],                 Loss: 0.00400, Train_Acc:100.00%
Epoch [165/300], Step [70/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [165/300], Step [80/391],                 Loss: 0.00402, Train_Acc:100.00%
Epoch [165/300], Step [90/391],                 Loss: 0.00400, Train_Acc:100.00%